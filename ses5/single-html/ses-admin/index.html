<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Administration Guide | SUSE Enterprise Storage 5.5 (SES 5 &amp; SES 5.5)</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Administration Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tbazant@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Enterprise Storage 5"/>
<meta property="og:title" content="Administration Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta property="og:description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="twitter:description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-admin">Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-admin" data-id-title="Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">5.5 (SES 5 &amp; SES 5.5)</span></div><div><h1 class="title">Administration Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Jana</span> <span class="surname">Haláčková</span>, and <span class="firstname">Sven</span> <span class="surname">Seeberg</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>05/11/2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#id-1.3.2"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.2.7"><span class="title-name">Available Documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.8"><span class="title-name">Feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.9"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.10"><span class="title-name">About the Making of This Manual</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.11"><span class="title-name">Ceph Contributors</span></a></span></li></ul></li><li><span class="part"><a href="#part-cluster-managment"><span class="title-number">I </span><span class="title-name">Cluster Management</span></a></span><ul><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">1 </span><span class="title-name">Salt Cluster Administration</span></a></span><ul><li><span class="sect1"><a href="#salt-adding-nodes"><span class="title-number">1.1 </span><span class="title-name">Adding New Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-adding-services"><span class="title-number">1.2 </span><span class="title-name">Adding New Roles to Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-node-removing"><span class="title-number">1.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#ds-mon"><span class="title-number">1.4 </span><span class="title-name">Redeploying Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-node-add-disk"><span class="title-number">1.5 </span><span class="title-name">Adding an OSD Disk to a Node</span></a></span></li><li><span class="sect1"><a href="#salt-removing-osd"><span class="title-number">1.6 </span><span class="title-name">Removing an OSD</span></a></span></li><li><span class="sect1"><a href="#ds-osd-replace"><span class="title-number">1.7 </span><span class="title-name">Replacing an OSD Disk</span></a></span></li><li><span class="sect1"><a href="#ds-osd-recover"><span class="title-number">1.8 </span><span class="title-name">Recovering a Reinstalled OSD Node</span></a></span></li><li><span class="sect1"><a href="#salt-automated-installation"><span class="title-number">1.9 </span><span class="title-name">Automated Installation via Salt</span></a></span></li><li><span class="sect1"><a href="#deepsea-rolling-updates"><span class="title-number">1.10 </span><span class="title-name">Updating the Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-salt-cluster-reboot"><span class="title-number">1.11 </span><span class="title-name">Halting or Rebooting Cluster</span></a></span></li><li><span class="sect1"><a href="#ds-custom-cephconf"><span class="title-number">1.12 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span></a></span></li><li><span class="sect1"><a href="#admin-apparmor"><span class="title-number">1.13 </span><span class="title-name">Enabling AppArmor Profiles</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-operate"><span class="title-number">II </span><span class="title-name">Operating a Cluster</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">2 </span><span class="title-name">Introduction</span></a></span></li><li><span class="chapter"><a href="#ceph-operating-services"><span class="title-number">3 </span><span class="title-name">Operating Ceph Services</span></a></span><ul><li><span class="sect1"><a href="#operate-services-systemd"><span class="title-number">3.1 </span><span class="title-name">Operating Ceph Cluster Related Services using <code class="systemitem">systemd</code></span></a></span></li><li><span class="sect1"><a href="#Deepsea-restart"><span class="title-number">3.2 </span><span class="title-name">Restarting Ceph Services using DeepSea</span></a></span></li><li><span class="sect1"><a href="#ceph-cluster-shutdown"><span class="title-number">3.3 </span><span class="title-name">Shutdown and Restart of the Whole Ceph Cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">4 </span><span class="title-name">Determining Cluster State</span></a></span><ul><li><span class="sect1"><a href="#monitor-status"><span class="title-number">4.1 </span><span class="title-name">Checking a Cluster's Status</span></a></span></li><li><span class="sect1"><a href="#monitor-health"><span class="title-number">4.2 </span><span class="title-name">Checking Cluster Health</span></a></span></li><li><span class="sect1"><a href="#monitor-watch"><span class="title-number">4.3 </span><span class="title-name">Watching a Cluster</span></a></span></li><li><span class="sect1"><a href="#monitor-stats"><span class="title-number">4.4 </span><span class="title-name">Checking a Cluster's Usage Stats</span></a></span></li><li><span class="sect1"><a href="#monitor-osdstatus"><span class="title-number">4.5 </span><span class="title-name">Checking OSD Status</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-fullosd"><span class="title-number">4.6 </span><span class="title-name">Checking for Full OSDs</span></a></span></li><li><span class="sect1"><a href="#monitor-monstatus"><span class="title-number">4.7 </span><span class="title-name">Checking Monitor Status</span></a></span></li><li><span class="sect1"><a href="#monitor-pgroupstatus"><span class="title-number">4.8 </span><span class="title-name">Checking Placement Group States</span></a></span></li><li><span class="sect1"><a href="#monitor-adminsocket"><span class="title-number">4.9 </span><span class="title-name">Using the Admin Socket</span></a></span></li></ul></li><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">5 </span><span class="title-name">Monitoring and Alerting</span></a></span><ul><li><span class="sect1"><a href="#config-file"><span class="title-number">5.1 </span><span class="title-name">Configuration File</span></a></span></li><li><span class="sect1"><a href="#custom-alerts"><span class="title-number">5.2 </span><span class="title-name">Custom Alerts</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">6 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span><ul><li><span class="sect1"><a href="#storage-cephx-arch"><span class="title-number">6.1 </span><span class="title-name">Authentication Architecture</span></a></span></li><li><span class="sect1"><a href="#storage-cephx-keymgmt"><span class="title-number">6.2 </span><span class="title-name">Key Management</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">7 </span><span class="title-name">Stored Data Management</span></a></span><ul><li><span class="sect1"><a href="#datamgm-devices"><span class="title-number">7.1 </span><span class="title-name">Devices</span></a></span></li><li><span class="sect1"><a href="#datamgm-buckets"><span class="title-number">7.2 </span><span class="title-name">Buckets</span></a></span></li><li><span class="sect1"><a href="#datamgm-rules"><span class="title-number">7.3 </span><span class="title-name">Rule Sets</span></a></span></li><li><span class="sect1"><a href="#op-crush"><span class="title-number">7.4 </span><span class="title-name">CRUSH Map Manipulation</span></a></span></li><li><span class="sect1"><a href="#scrubbing"><span class="title-number">7.5 </span><span class="title-name">Scrubbing</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">8 </span><span class="title-name">Managing Storage Pools</span></a></span><ul><li><span class="sect1"><a href="#ceph-pools-associate"><span class="title-number">8.1 </span><span class="title-name">Associate Pools with an Application</span></a></span></li><li><span class="sect1"><a href="#ceph-pools-operate"><span class="title-number">8.2 </span><span class="title-name">Operating Pools</span></a></span></li><li><span class="sect1"><a href="#pools-migration"><span class="title-number">8.3 </span><span class="title-name">Pool Migration</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-pool"><span class="title-number">8.4 </span><span class="title-name">Pool Snapshots</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-pool-compression"><span class="title-number">8.5 </span><span class="title-name">Data Compression</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">9 </span><span class="title-name">RADOS Block Device</span></a></span><ul><li><span class="sect1"><a href="#ceph-rbd-commands"><span class="title-number">9.1 </span><span class="title-name">Block Device Commands</span></a></span></li><li><span class="sect1"><a href="#storage-bp-integration-mount-rbd"><span class="title-number">9.2 </span><span class="title-name">Mounting and Unmounting</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-rbd"><span class="title-number">9.3 </span><span class="title-name">Snapshots</span></a></span></li><li><span class="sect1"><a href="#ceph-rbd-mirror"><span class="title-number">9.4 </span><span class="title-name">Mirroring</span></a></span></li><li><span class="sect1"><a href="#rbd-features"><span class="title-number">9.5 </span><span class="title-name">Advanced Features</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span></a></span><ul><li><span class="sect1"><a href="#ec-prerequisite"><span class="title-number">10.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-default-profile"><span class="title-number">10.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-erasure-profiles"><span class="title-number">10.3 </span><span class="title-name">Erasure Code Profiles</span></a></span></li><li><span class="sect1"><a href="#ec-rbd"><span class="title-number">10.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-tiered"><span class="title-number">11 </span><span class="title-name">Cache Tiering</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.4.11.5"><span class="title-number">11.1 </span><span class="title-name">Tiered Storage Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-tiered-caution"><span class="title-number">11.2 </span><span class="title-name">Points to Consider</span></a></span></li><li><span class="sect1"><a href="#id-1.3.4.11.7"><span class="title-number">11.3 </span><span class="title-name">When to Use Cache Tiering</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-tiered-cachemodes"><span class="title-number">11.4 </span><span class="title-name">Cache Modes</span></a></span></li><li><span class="sect1"><a href="#ceph-tier-erasure"><span class="title-number">11.5 </span><span class="title-name">Erasure Coded Pool and Cache Tiering</span></a></span></li><li><span class="sect1"><a href="#ses-tiered-storage"><span class="title-number">11.6 </span><span class="title-name">Setting Up an Example Tiered Storage</span></a></span></li><li><span class="sect1"><a href="#cache-tier-configure"><span class="title-number">11.7 </span><span class="title-name">Configuring a Cache Tier</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">12 </span><span class="title-name">Ceph Cluster Configuration</span></a></span><ul><li><span class="sect1"><a href="#ceph-config-runtime"><span class="title-number">12.1 </span><span class="title-name">Runtime Configuration</span></a></span></li><li><span class="sect1"><a href="#config-osd-and-bluestore"><span class="title-number">12.2 </span><span class="title-name">Ceph OSD and BlueStore</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-dataccess"><span class="title-number">III </span><span class="title-name">Accessing Cluster Data</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">13 </span><span class="title-name">Ceph Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#sec-ceph-rgw-limits"><span class="title-number">13.1 </span><span class="title-name">Object Gateway Restrictions and Naming Limitations</span></a></span></li><li><span class="sect1"><a href="#ogw-deploy"><span class="title-number">13.2 </span><span class="title-name">Deploying the Object Gateway</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-operating"><span class="title-number">13.3 </span><span class="title-name">Operating the Object Gateway Service</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-rgw-configuration"><span class="title-number">13.4 </span><span class="title-name">Configuration Parameters</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-access"><span class="title-number">13.5 </span><span class="title-name">Managing Object Gateway Access</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-https"><span class="title-number">13.6 </span><span class="title-name">Enabling HTTPS/SSL for Object Gateways</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-sync"><span class="title-number">13.7 </span><span class="title-name">Sync Modules</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-ldap"><span class="title-number">13.8 </span><span class="title-name">LDAP Authentication</span></a></span></li><li><span class="sect1"><a href="#ogw-bucket-sharding"><span class="title-number">13.9 </span><span class="title-name">Bucket Index Sharding</span></a></span></li><li><span class="sect1"><a href="#ogw-keystone"><span class="title-number">13.10 </span><span class="title-name">Integrating OpenStack Keystone</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-fed"><span class="title-number">13.11 </span><span class="title-name">Multisite Object Gateways</span></a></span></li><li><span class="sect1"><a href="#ogw-haproxy"><span class="title-number">13.12 </span><span class="title-name">Load Balancing the Object Gateway Servers with HAProxy</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">14 </span><span class="title-name">Ceph iSCSI Gateway</span></a></span><ul><li><span class="sect1"><a href="#ceph-iscsi-connect"><span class="title-number">14.1 </span><span class="title-name">Connecting to lrbd-managed Targets</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-conclude"><span class="title-number">14.2 </span><span class="title-name">Conclusion</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">15 </span><span class="title-name">Clustered File System</span></a></span><ul><li><span class="sect1"><a href="#ceph-cephfs-cephfs-mount"><span class="title-number">15.1 </span><span class="title-name">Mounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-unmount"><span class="title-number">15.2 </span><span class="title-name">Unmounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-fstab"><span class="title-number">15.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-activeactive"><span class="title-number">15.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-failover"><span class="title-number">15.5 </span><span class="title-name">Managing Failover</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></span><ul><li><span class="sect1"><a href="#ceph-nfsganesha-install"><span class="title-number">16.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-config"><span class="title-number">16.2 </span><span class="title-name">Configuration</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-customrole"><span class="title-number">16.3 </span><span class="title-name">Custom NFS Ganesha Roles</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-services"><span class="title-number">16.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-loglevel"><span class="title-number">16.5 </span><span class="title-name">Setting the Log Level</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-verify"><span class="title-number">16.6 </span><span class="title-name">Verifying the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-mount"><span class="title-number">16.7 </span><span class="title-name">Mounting the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-more"><span class="title-number">16.8 </span><span class="title-name">Additional Resources</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-gui"><span class="title-number">IV </span><span class="title-name">Managing Cluster with GUI Tools</span></a></span><ul><li><span class="chapter"><a href="#ceph-oa"><span class="title-number">17 </span><span class="title-name">openATTIC</span></a></span><ul><li><span class="sect1"><a href="#ceph-oa-installation"><span class="title-number">17.1 </span><span class="title-name">openATTIC Deployment and Configuration</span></a></span></li><li><span class="sect1"><a href="#ceph-oa-install-webui"><span class="title-number">17.2 </span><span class="title-name">openATTIC Web User Interface</span></a></span></li><li><span class="sect1"><a href="#ceph-oa-webui-dash"><span class="title-number">17.3 </span><span class="title-name">Dashboard</span></a></span></li><li><span class="sect1"><a href="#ceph-oa-webui-ceph"><span class="title-number">17.4 </span><span class="title-name">Ceph Related Tasks</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-virt"><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">18 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></span><ul><li><span class="sect1"><a href="#ceph-libvirt-cfg-ceph"><span class="title-number">18.1 </span><span class="title-name">Configuring Ceph</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-virt-manager"><span class="title-number">18.2 </span><span class="title-name">Preparing the VM Manager</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-create-vm"><span class="title-number">18.3 </span><span class="title-name">Creating a VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-cfg-vm"><span class="title-number">18.4 </span><span class="title-name">Configuring the VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-summary"><span class="title-number">18.5 </span><span class="title-name">Summary</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">19 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></span><ul><li><span class="sect1"><a href="#ceph-kvm-install"><span class="title-number">19.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#ceph-kvm-usage"><span class="title-number">19.2 </span><span class="title-name">Usage</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.7"><span class="title-number">19.3 </span><span class="title-name">Creating Images with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.8"><span class="title-number">19.4 </span><span class="title-name">Resizing Images with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.9"><span class="title-number">19.5 </span><span class="title-name">Retrieving Image Info with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.10"><span class="title-number">19.6 </span><span class="title-name">Running QEMU with RBD</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.11"><span class="title-number">19.7 </span><span class="title-name">Enabling Discard/TRIM</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.12"><span class="title-number">19.8 </span><span class="title-name">QEMU Cache Options</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-troubleshooting"><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a></span><ul><li><span class="chapter"><a href="#storage-tips"><span class="title-number">20 </span><span class="title-name">Hints and Tips</span></a></span><ul><li><span class="sect1"><a href="#tips-orphaned-partitions"><span class="title-number">20.1 </span><span class="title-name">Identifying Orphaned Partitions</span></a></span></li><li><span class="sect1"><a href="#tips-scrubbing"><span class="title-number">20.2 </span><span class="title-name">Adjusting Scrubbing</span></a></span></li><li><span class="sect1"><a href="#tips-stopping-osd-without-rebalancing"><span class="title-number">20.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span></a></span></li><li><span class="sect1"><a href="#Cluster-Time-Setting"><span class="title-number">20.4 </span><span class="title-name">Time Synchronization of Nodes</span></a></span></li><li><span class="sect1"><a href="#storage-bp-cluster-mntc-unbalanced"><span class="title-number">20.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span></a></span></li><li><span class="sect1"><a href="#storage-tips-ceph-btrfs-subvol"><span class="title-number">20.6 </span><span class="title-name">Btrfs Sub-volume for /var/lib/ceph</span></a></span></li><li><span class="sect1"><a href="#storage-bp-srv-maint-fds-inc"><span class="title-number">20.7 </span><span class="title-name">Increasing File Descriptors</span></a></span></li><li><span class="sect1"><a href="#bp-osd-on-exisitng-partitions"><span class="title-number">20.8 </span><span class="title-name">How to Use Existing Partitions for OSDs Including OSD Journals</span></a></span></li><li><span class="sect1"><a href="#storage-admin-integration"><span class="title-number">20.9 </span><span class="title-name">Integration with Virtualization Software</span></a></span></li><li><span class="sect1"><a href="#storage-bp-net-firewall"><span class="title-number">20.10 </span><span class="title-name">Firewall Settings for Ceph</span></a></span></li><li><span class="sect1"><a href="#storage-bp-network-test"><span class="title-number">20.11 </span><span class="title-name">Testing Network Performance</span></a></span></li><li><span class="sect1"><a href="#storage-bd-replacing-disk"><span class="title-number">20.12 </span><span class="title-name">Replacing Storage Disk</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-faqs"><span class="title-number">21 </span><span class="title-name">Frequently Asked Questions</span></a></span><ul><li><span class="sect1"><a href="#storage-bp-tuneups-pg-num"><span class="title-number">21.1 </span><span class="title-name">How Does the Number of Placement Groups Affect the Cluster Performance?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-tuneups-mix-ssd"><span class="title-number">21.2 </span><span class="title-name">Can I Use SSDs and Hard Disks on the Same Cluster?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-tuneups-ssd-tradeoffs"><span class="title-number">21.3 </span><span class="title-name">What are the Trade-offs of Using a Journal on SSD?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-diskfails"><span class="title-number">21.4 </span><span class="title-name">What Happens When a Disk Fails?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-journalfails"><span class="title-number">21.5 </span><span class="title-name">What Happens When a Journal Disk Fails?</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-troubleshooting"><span class="title-number">22 </span><span class="title-name">Troubleshooting</span></a></span><ul><li><span class="sect1"><a href="#storage-bp-report-bug"><span class="title-number">22.1 </span><span class="title-name">Reporting Software Problems</span></a></span></li><li><span class="sect1"><a href="#storage-bp-cluster-mntc-rados-striping"><span class="title-number">22.2 </span><span class="title-name">Sending Large Objects with <code class="command">rados</code> Fails with Full OSD</span></a></span></li><li><span class="sect1"><a href="#ceph-xfs-corruption"><span class="title-number">22.3 </span><span class="title-name">Corrupted XFS File system</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-toomanypgs"><span class="title-number">22.4 </span><span class="title-name">'Too Many PGs per OSD' Status Message</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-stuckinactive"><span class="title-number">22.5 </span><span class="title-name">'<span class="emphasis"><em>nn</em></span> pg stuck inactive' Status Message</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-osdweight"><span class="title-number">22.6 </span><span class="title-name">OSD Weight is 0</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-osddown"><span class="title-number">22.7 </span><span class="title-name">OSD is Down</span></a></span></li><li><span class="sect1"><a href="#storage-bp-performance-slowosd"><span class="title-number">22.8 </span><span class="title-name">Finding Slow OSDs</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-clockskew"><span class="title-number">22.9 </span><span class="title-name">Fixing Clock Skew Warnings</span></a></span></li><li><span class="sect1"><a href="#storage-bp-performance-net-issues"><span class="title-number">22.10 </span><span class="title-name">Poor Cluster Performance Caused by Network Problems</span></a></span></li><li><span class="sect1"><a href="#trouble-jobcache"><span class="title-number">22.11 </span><span class="title-name"><code class="filename">/var</code> Running Out of Space</span></a></span></li><li><span class="sect1"><a href="#ceph-fix-too-many-pgs"><span class="title-number">22.12 </span><span class="title-name">Too Many PGs Per OSD</span></a></span></li></ul></li></ul></li><li><span class="glossary"><a href="#gloss-storage-glossary"><span class="title-name">Glossary</span></a></span></li><li><span class="appendix"><a href="#app-stage1-custom"><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></span></li><li><span class="appendix"><a href="#app-alerting-default"><span class="title-number">B </span><span class="title-name">Default Alerts for SUSE Enterprise Storage</span></a></span></li><li><span class="appendix"><a href="#app-storage-manual-inst"><span class="title-number">C </span><span class="title-name">Example Procedure of Manual Ceph Installation</span></a></span></li><li><span class="appendix"><a href="#ap-adm-docupdate"><span class="title-number">D </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-adm-docupdates-5main3"><span class="title-number">D.1 </span><span class="title-name">The Latest Documentation Update</span></a></span></li><li><span class="sect1"><a href="#sec-adm-docupdates-5-5"><span class="title-number">D.2 </span><span class="title-name">October, 2018 (Documentation Maintenance Update)</span></a></span></li><li><span class="sect1"><a href="#sec-adm-docupdates-5maint1"><span class="title-number">D.3 </span><span class="title-name">November 2017 (Documentation Maintenance Update)</span></a></span></li><li><span class="sect1"><a href="#sec-adm-docupdates-5"><span class="title-number">D.4 </span><span class="title-name">October, 2017 (Release of SUSE Enterprise Storage 5.5)</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.3.4.6.5.6"><span class="number">6.1 </span><span class="name">Basic <code class="systemitem">cephx</code> Authentication</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.6.5.8"><span class="number">6.2 </span><span class="name"><code class="systemitem">cephx</code> Authentication</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.6.5.10"><span class="number">6.3 </span><span class="name"><code class="systemitem">cephx</code> Authentication - MDS and OSD</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.12.7.3.3"><span class="number">7.1 </span><span class="name">OSDs with Mixed Device Classes</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-iterate-figure"><span class="number">7.2 </span><span class="name">Example Tree</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-mode-indep-figure"><span class="number">7.3 </span><span class="name">Node Replacement Methods</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8.5.3.2.5"><span class="number">8.1 </span><span class="name">Pools before Migration</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8.5.3.3.3"><span class="number">8.2 </span><span class="name">Cache Tier Setup</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8.5.3.4.3"><span class="number">8.3 </span><span class="name">Data Flushing</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8.5.3.5.4"><span class="number">8.4 </span><span class="name">Setting Overlay</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8.5.3.6.3"><span class="number">8.5 </span><span class="name">Migration Complete</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.9.5"><span class="number">9.1 </span><span class="name">RADOS Protocol</span></a></span></li><li><span class="figure"><a href="#ses-tiered-hitset-overview-bloom"><span class="number">11.1 </span><span class="name">Bloom Filter with 3 Stored Objects</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.1.2"><span class="number">14.1 </span><span class="name">iSCSI Initiator Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.2.2"><span class="number">14.2 </span><span class="name">Discover Target Portal</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.3.2"><span class="number">14.3 </span><span class="name">Target Portals</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.4.2"><span class="number">14.4 </span><span class="name">Targets</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.6.2"><span class="number">14.5 </span><span class="name">iSCSI Target Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.3.7.2"><span class="number">14.6 </span><span class="name">Device Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.6.1.2"><span class="number">14.7 </span><span class="name">New Volume Wizard</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.6.2.2"><span class="number">14.8 </span><span class="name">Offline Disk Prompt</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.4.6.3.2"><span class="number">14.9 </span><span class="name">Confirm Volume Selections</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.2.2"><span class="number">14.10 </span><span class="name">iSCSI Initiator Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.4.2"><span class="number">14.11 </span><span class="name">Add Target Server</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.5.2"><span class="number">14.12 </span><span class="name">Manage Multipath Devices</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.5.4"><span class="number">14.13 </span><span class="name">Paths Listing for Multipath</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.6.2"><span class="number">14.14 </span><span class="name">Add Storage Dialog</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.7.2"><span class="number">14.15 </span><span class="name">Custom Space Setting</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.3.4.5.3.7.5"><span class="number">14.16 </span><span class="name">iSCSI Datastore Overview</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.6.3"><span class="number">17.1 </span><span class="name">openATTIC Login Screen</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.6.7"><span class="number">17.2 </span><span class="name">openATTIC Dashboard</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.7.5"><span class="number">17.3 </span><span class="name">Basic Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.7.7"><span class="number">17.4 </span><span class="name">Capacity Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.7.9"><span class="number">17.5 </span><span class="name">Latency Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.7.11"><span class="number">17.6 </span><span class="name">Throughput</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.4.4"><span class="number">17.7 </span><span class="name">List of OSD nodes</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.5.4"><span class="number">17.8 </span><span class="name">List of RBDs</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.5.5.3"><span class="number">17.9 </span><span class="name">RBD Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.5.7.4"><span class="number">17.10 </span><span class="name">RBD Snapshots</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.5.8.3"><span class="number">17.11 </span><span class="name">Deleting RBD</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.5.9.3"><span class="number">17.12 </span><span class="name">Adding a New RBD</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.6.5"><span class="number">17.13 </span><span class="name">List of Pools</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.6.7"><span class="number">17.14 </span><span class="name">Pool Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.6.8.3"><span class="number">17.15 </span><span class="name">Deleting Pools</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.6.9.3"><span class="number">17.16 </span><span class="name">Adding a New Pool</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.7.3"><span class="number">17.17 </span><span class="name">List of Nodes</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.8.5"><span class="number">17.18 </span><span class="name">List of NFS Exports</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.8.7"><span class="number">17.19 </span><span class="name">NFS Export Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.8.9.3"><span class="number">17.20 </span><span class="name">Adding a New NFS Export</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.8.11.4"><span class="number">17.21 </span><span class="name">Editing an NFS Export</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.9.5"><span class="number">17.22 </span><span class="name">List of iSCSI Gateways</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.9.7"><span class="number">17.23 </span><span class="name">Gateway Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.9.8.3"><span class="number">17.24 </span><span class="name">Adding a New iSCSI Gateway</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.10.3"><span class="number">17.25 </span><span class="name">CRUSH Map</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.10.6"><span class="number">17.26 </span><span class="name">Replication rules</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.5"><span class="number">17.27 </span><span class="name">List of Object Gateway Users</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.6.4"><span class="number">17.28 </span><span class="name">Adding a New Object Gateway User</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.6.5.4.4"><span class="number">17.29 </span><span class="name">User quota</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.6.5.5.2"><span class="number">17.30 </span><span class="name">Bucket Quota</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.8.3.1.2.2"><span class="number">17.31 </span><span class="name">Adding a Subuser</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.8.3.2.2.3"><span class="number">17.32 </span><span class="name">View S3 keys</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.8.3.3.2.2"><span class="number">17.33 </span><span class="name">Capabilities</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.9.4"><span class="number">17.34 </span><span class="name">Object Gateway Buckets</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.10.3"><span class="number">17.35 </span><span class="name">Adding a New Bucket</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.11.3"><span class="number">17.36 </span><span class="name">Bucket Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.12.3"><span class="number">17.37 </span><span class="name">Editing an Object Gateway Bucket</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.2.8.11.13.3"><span class="number">17.38 </span><span class="name">Deleting Buckets</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-ds-rmnode"><span class="number">1.1 </span><span class="name">Removing a Salt minion from the Cluster</span></a></span></li><li><span class="example"><a href="#ex-ds-mignode"><span class="number">1.2 </span><span class="name">Migrating Nodes</span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.7"><span class="number">5.1 </span><span class="name">Global Configuration</span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.8"><span class="number">5.2 </span><span class="name"><em class="replaceable">ROUTE</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.9"><span class="number">5.3 </span><span class="name"><em class="replaceable">INHIBIT_RULE</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.10"><span class="number">5.4 </span><span class="name"><em class="replaceable">HTTP_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.11"><span class="number">5.5 </span><span class="name"><em class="replaceable">RECEIVER</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.12"><span class="number">5.6 </span><span class="name"><em class="replaceable">EMAIL_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.13"><span class="number">5.7 </span><span class="name"><em class="replaceable">HIPCHAT_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.14"><span class="number">5.8 </span><span class="name"><em class="replaceable">PAGERDUTY_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.15"><span class="number">5.9 </span><span class="name"><em class="replaceable">PUSHOVER_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.16"><span class="number">5.10 </span><span class="name"><em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.17"><span class="number">5.11 </span><span class="name"><em class="replaceable">ACTION_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.18"><span class="number">5.12 </span><span class="name"><em class="replaceable">FIELD_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.19"><span class="number">5.13 </span><span class="name"><em class="replaceable">OPSGENIE_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.20"><span class="number">5.14 </span><span class="name"><em class="replaceable">VICTOROPS_CONFIG</em></span></a></span></li><li><span class="example"><a href="#alert-webhook"><span class="number">5.15 </span><span class="name"><em class="replaceable">WEBHOOK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.6.22"><span class="number">5.16 </span><span class="name"><em class="replaceable">WECHAT_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.7.10.1.2"><span class="number">5.17 </span><span class="name">Adding Custom Alerts to SUSE Enterprise Storage</span></a></span></li><li><span class="example"><a href="#id-1.3.4.7.12.7.7.4.1.2.6"><span class="number">7.1 </span><span class="name"><code class="command">crushtool --reclassify-root</code></span></a></span></li><li><span class="example"><a href="#id-1.3.4.7.12.7.7.4.3.2.2"><span class="number">7.2 </span><span class="name"><code class="command">crushtool --reclassify-bucket</code></span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.3.1.6"><p>
  Copyright ©
2022

  SUSE LLC
 </p><p>
  Copyright © 2016, RedHat, Inc, and contributors.

 </p><p>
  The text of and illustrations in this document are licensed
  under a Creative Commons Attribution-Share Alike 4.0 International
  ("CC-BY-SA"). An explanation of CC-BY-SA is available at
  <a class="link" href="http://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">http://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
  In accordance with CC-BY-SA, if you distribute this document or an adaptation
  of it, you must provide the URL for the original version.
 </p><p>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International Corp.
  or its subsidiaries in the United States and/or other countries. All other
  trademarks are the property of their respective owners.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="id-1.3.2" data-id-title="About This Guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About This Guide</span> <a title="Permalink" class="permalink" href="#id-1.3.2">#</a></h1></div></div></div><p>
  SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the
  capabilities of the Ceph (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>) storage
  project with the enterprise engineering and support of SUSE. SUSE Enterprise Storage
  5.5 provides IT organizations with the ability to deploy a
  distributed storage architecture that can support a number of use cases using
  commodity hardware platforms.
 </p><p>
  This guide helps you understand the concept of the SUSE Enterprise Storage
  5.5 with the main focus on managing and administrating the Ceph
  infrastructure. It also demonstrates how to use Ceph with other related
  solutions, such as OpenStack or KVM.
 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system as well as documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><section class="sect1" id="id-1.3.2.7" data-id-title="Available Documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available Documentation</span> <a title="Permalink" class="permalink" href="#id-1.3.2.7">#</a></h2></div></div></div><p>

  The following manuals are available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.7.4.1"><span class="term"><span class="intraxref">Book “Administration Guide”</span>
   </span></dt><dd><p>
     The guide describes various administration tasks that are typically
     performed after the installation. The guide also introduces steps to
     integrate Ceph with virtualization solutions such as <code class="systemitem">libvirt</code>, Xen,
     or KVM, and ways to access objects stored in the cluster via iSCSI and
     RADOS gateways.
    </p></dd><dt id="id-1.3.2.7.4.2"><span class="term"><span class="intraxref">Book “Deployment Guide”</span>
   </span></dt><dd><p>
     Guides you through the installation steps of the Ceph cluster and all
     services related to Ceph. The guide also illustrates a basic Ceph
     cluster structure and provides you with related terminology.
    </p></dd></dl></div><p>
  HTML versions of the product manuals can be found in the installed system
  under <code class="filename">/usr/share/doc/manual</code>. Find the latest
  documentation updates at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a> where you can download the
  manuals for your product in multiple formats.
 </p></section><section class="sect1" id="id-1.3.2.8" data-id-title="Feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Feedback</span> <a title="Permalink" class="permalink" href="#id-1.3.2.8">#</a></h2></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.8.4.1"><span class="term">Bugs and Enhancement Requests</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To report bugs for a product component, log in to the Novell Customer Center from
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a> and select <span class="guimenu">My Support</span> / <span class="guimenu">Service Request</span>.
    </p></dd><dt id="id-1.3.2.8.4.2"><span class="term">User Comments</span></dt><dd><p>
     We want to hear your comments and suggestions for this manual and
     the other documentation included with this product. If you have questions,
     suggestions, or corrections, contact doc-team@suse.com, or you can also
     click the <code class="literal">Report Documentation Bug</code> link beside each
     chapter or section heading.
    </p></dd><dt id="id-1.3.2.8.4.3"><span class="term">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.de</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a concise
     description of the problem and refer to the respective section number and
     page (or URL).
    </p></dd></dl></div></section><section class="sect1" id="id-1.3.2.9" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#id-1.3.2.9">#</a></h2></div></div></div><p>
  The following typographical conventions are used in this manual:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">placeholder</em>: replace
    <em class="replaceable">placeholder</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination; keys
    are shown in uppercase as on a keyboard
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> / <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a reference
    to a chapter in another manual.
   </p></li></ul></div></section><section class="sect1" id="id-1.3.2.10" data-id-title="About the Making of This Manual"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">About the Making of This Manual</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10">#</a></h2></div></div></div><p>
  This book is written in GeekoDoc, a subset of DocBook (see
  <a class="link" href="http://www.docbook.org" target="_blank">http://www.docbook.org</a>). The XML source files were
  validated by <code class="command">xmllint</code>, processed by
  <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
  version of Norman Walsh's stylesheets. The final PDF can be formatted through
  FOP from Apache or through XEP from RenderX. The authoring and publishing
  tools used to produce this manual are available in the package
  <code class="systemitem">daps</code>. The DocBook Authoring and
  Publishing Suite (DAPS) is developed as open source software. For more
  information, see <a class="link" href="http://daps.sf.net/" target="_blank">http://daps.sf.net/</a>.
 </p></section><section class="sect1" id="id-1.3.2.11" data-id-title="Ceph Contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph Contributors</span> <a title="Permalink" class="permalink" href="#id-1.3.2.11">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of hundreds of
  contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section></section><div class="part" id="part-cluster-managment" data-id-title="Cluster Management"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Cluster Management </span><a title="Permalink" class="permalink" href="#part-cluster-managment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">1 </span><span class="title-name">Salt Cluster Administration</span></a></span></li><dd class="toc-abstract"><p>
  After you deploy a Ceph cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </p></dd></ul></div><section class="chapter" id="storage-salt-cluster" data-id-title="Salt Cluster Administration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Salt Cluster Administration</span> <a title="Permalink" class="permalink" href="#storage-salt-cluster">#</a></h2></div></div></div><p>
  After you deploy a Ceph cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </p><section class="sect1" id="salt-adding-nodes" data-id-title="Adding New Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Adding New Cluster Nodes</span> <a title="Permalink" class="permalink" href="#salt-adding-nodes">#</a></h2></div></div></div><p>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”</span>:
  </p><div id="id-1.3.3.2.4.3" data-id-title="Prevent Rebalancing" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Prevent Rebalancing</h6><p>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, add all OSDs you intend to add at the same time.
   </p><p>
    Additional way is to set the <code class="option">osd crush initial weight = 0</code>
    option in the <code class="filename">ceph.conf</code> file before adding the OSDs:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Add <code class="option">osd crush initial weight = 0</code> to
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>.
     </p></li><li class="step"><p>
      Create the new configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">MASTER</em> state.apply ceph.configuration.create</pre></div><p>
        Or:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call state.apply ceph.configuration.create</pre></div></li><li class="step"><p>
      Apply the new configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">TARGET</em> state.apply ceph.configuration</pre></div><div id="id-1.3.3.2.4.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
          If this is <span class="emphasis"><em>not</em></span> a new node, but you want to proceed as if
          it were, ensure you remove the <code class="filename">/etc/ceph/destroyedOSDs.yml</code> file
          from the node. Otherwise, any devices from the first attempt will be restored
          with their previous OSD ID and reweight.
        </p><p>
          Run the following commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt 'node*' state.apply ceph.osd</pre></div></div></li><li class="step"><p>
      After the new OSDs are added, adjust their weights as required with the
      <code class="command">ceph osd reweight</code> command in small increments. This
      allows the cluster to rebalance and become healthy between increasing
      increments so it does not overwhelm the cluster and clients accessing
      the cluster.
     </p></li></ol></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install SUSE Linux Enterprise Server 12 SP3 on the new node and configure its network setting so that
     it resolves the Salt master host name correctly. Verify that it has a proper
     connection to both public and cluster networks, and that time
     synchronization is correctly configured. Then install the
     <code class="systemitem">salt-minion</code> package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div><p>
     If the Salt master's host name is different from <code class="literal">salt</code>,
     edit <code class="filename">/etc/salt/minion</code> and add the following:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">DNS_name_of_your_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the <code class="systemitem">salt.minion</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     On the Salt master, accept the salt key of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept <em class="replaceable">NEW_NODE_KEY</em></pre></div></li><li class="step"><p>
     Verify that <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code>
     targets the new Salt minion and/or set the proper DeepSea grain. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.2.2.1 “Matching the Minion Name”</span> of
     <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”, Running Deployment Stages</span> for more details.
    </p></li><li class="step"><p>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information DeepSea expects.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><div id="id-1.3.3.2.4.4.4.3" data-id-title="Possible Restart of DeepSea Stage 0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Possible Restart of DeepSea Stage 0</h6><p>
      If the Salt master rebooted after its kernel update, you need to restart
      DeepSea Stage 0.
     </p></div></li><li class="step"><p>
     Run the discovery stage. It will write new file entries in the
     <code class="filename">/srv/pillar/ceph/proposals</code> directory, where you can
     edit relevant .yml files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     Optionally, change
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="step"><p>
     Run the configuration stage. It reads everything under
     <code class="filename">/srv/pillar/ceph</code> and updates the pillar accordingly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     Pillar stores data which you can access with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div></li><li class="step"><p>
     The configuration and deployment stages include newly added nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div></section><section class="sect1" id="salt-adding-services" data-id-title="Adding New Roles to Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Adding New Roles to Nodes</span> <a title="Permalink" class="permalink" href="#salt-adding-services">#</a></h2></div></div></div><p>
   You can deploy all types of supported roles with DeepSea. See
   <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.5.1.2 “Role Assignment”</span> for more information on supported
   role types and examples of matching them.
  </p><p>
   To add a new service to an existing node, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Adapt <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> to match
     the existing host with a new role. For more details, refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.5.1 “The <code class="filename">policy.cfg</code> File”</span>. For example, if you need to run an
     Object Gateway on a MON node, the line is similar to:
    </p><div class="verbatim-wrap"><pre class="screen">role-rgw/xx/x/example.mon-1.sls</pre></div></li><li class="step"><p>
     Run Stage 2 to update the pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div></li><li class="step"><p>
     Run Stage 3 to deploy core services, or Stage 4 to deploy optional
     services. Running both stages does not hurt.
    </p></li></ol></div></div></section><section class="sect1" id="salt-node-removing" data-id-title="Removing and Reinstalling Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span> <a title="Permalink" class="permalink" href="#salt-node-removing">#</a></h2></div></div></div><div id="id-1.3.3.2.6.2" data-id-title="Removing a Cluster Node Temporarily" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a Cluster Node Temporarily</h6><p>
    The Salt master expects all minions to be present in the cluster and
    responsive. If a minion breaks and is not responsive any more, it causes
    problems to the Salt infrastructure, mainly to DeepSea and openATTIC.
   </p><p>
    Before you fix the minion, delete its key from the Salt master temporarily:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">MINION_HOST_NAME</em></pre></div><p>
    After the minions is fixed, add its key to the Salt master again:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -a <em class="replaceable">MINION_HOST_NAME</em></pre></div></div><p>
   To remove a role from a cluster, edit
   <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and remove the
   corresponding line(s). Then run Stages 2 and 5 as described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”</span>.
  </p><div id="id-1.3.3.2.6.4" data-id-title="Removing OSDs from Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Removing OSDs from Cluster</h6><p>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </p><p>
    Before running stage.5 to do the actual removal, always check which OSD's
    are going to be removed by DeepSea:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run rescinded.ids</pre></div></div><p>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </p><p>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </p><div id="id-1.3.3.2.6.7" data-id-title="Preserving Partitions Created by Other Methods" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Preserving Partitions Created by Other Methods</h6><p>
    Disk drives previously configured by other methods, such as
    <code class="command">ceph-deploy</code>, may still contain partitions. DeepSea
    will not automatically destroy these. The administrator must reclaim these
    drives manually.
   </p></div><div class="complex-example"><div class="example" id="ex-ds-rmnode" data-id-title="Removing a Salt minion from the Cluster"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 1.1: </span><span class="title-name">Removing a Salt minion from the Cluster </span><a title="Permalink" class="permalink" href="#ex-ds-rmnode">#</a></h6></div><div class="example-contents"><p>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <code class="filename">policy.cfg</code> are similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</pre></div><p>
    Then to remove the Salt minion 'data2.ceph', change the lines to the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</pre></div><p>
    Then run stage.2, check which OSD's are going to be removed, and finish by
    running stage.5:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-ds-mignode" data-id-title="Migrating Nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 1.2: </span><span class="title-name">Migrating Nodes </span><a title="Permalink" class="permalink" href="#ex-ds-mignode">#</a></h6></div><div class="example-contents"><p>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    Object Gateway while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </p><p>
    After running Stages 0 and 1 (see <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”, Running Deployment Stages</span>) for the
    new hardware, you named the new gateway <code class="literal">rgw1</code>. If the
    node <code class="literal">data8</code> needs the Object Gateway role removed and the storage
    role added, and the current <code class="filename">policy.cfg</code> looks like
    this:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</pre></div><p>
    Then change it to:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</pre></div><p>
    Run stages 2 to 4, check which OSD's are going to be possibly removed, and
    finish by running stage.5. Stage 3 will add <code class="literal">data8</code> as a
    storage node. For a moment, <code class="literal">data8</code> will have both roles.
    Stage 4 will add the Object Gateway role to <code class="literal">rgw1</code> and stage 5 will
    remove the Object Gateway role from <code class="literal">data8</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div></section><section class="sect1" id="ds-mon" data-id-title="Redeploying Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Redeploying Monitor Nodes</span> <a title="Permalink" class="permalink" href="#ds-mon">#</a></h2></div></div></div><p>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </p><div id="id-1.3.3.2.7.3" data-id-title="The Minimum is Three Monitor Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: The Minimum is Three Monitor Nodes</h6><p>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only two monitor nodes only, you
    need to temporarily assign the monitor role to other cluster nodes before
    you redeploy the failed monitor nodes. After you redeploy the failed
    monitor nodes, you can uninstall the temporary monitor roles.
   </p><p>
    For more information on adding new nodes/roles to the Ceph cluster, see
    <a class="xref" href="#salt-adding-nodes" title="1.1. Adding New Cluster Nodes">Section 1.1, “Adding New Cluster Nodes”</a> and
    <a class="xref" href="#salt-adding-services" title="1.2. Adding New Roles to Nodes">Section 1.2, “Adding New Roles to Nodes”</a>.
   </p><p>
    For more information on removing cluster nodes, refer to
    <a class="xref" href="#salt-node-removing" title="1.3. Removing and Reinstalling Cluster Nodes">Section 1.3, “Removing and Reinstalling Cluster Nodes”</a>.
   </p></div><p>
   There are two basic degrees of a Ceph node failure:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The Salt minion host is broken either physically or on the OS level, and
     does not respond to the <code class="command">salt
     '<em class="replaceable">minion_name</em>' test.ping</code> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”</span>.
    </p></li><li class="listitem"><p>
     The monitor related services failed and refuse to recover, but the host
     responds to the <code class="command">salt '<em class="replaceable">minion_name</em>'
     test.ping</code> call. In such case, follow these steps:
    </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> on the
     Salt master, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]</pre></div></li><li class="step"><p>
     Run DeepSea Stages 2 to 5 to apply the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.3
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.4
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.5</pre></div></li></ol></div></div></section><section class="sect1" id="salt-node-add-disk" data-id-title="Adding an OSD Disk to a Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">Adding an OSD Disk to a Node</span> <a title="Permalink" class="permalink" href="#salt-node-add-disk">#</a></h2></div></div></div><p>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <span class="intraxref">Step 12</span> in
   <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”</span> for more details. After the disk is
   empty, add the disk to the YAML file of the node. The path to the file is
   <code class="filename">/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<em class="replaceable">node_name</em>.yml</code>.
   After saving the file, run DeepSea stages 2 and 3:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><div id="id-1.3.3.2.8.4" data-id-title="Updated Profiles Automatically" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Updated Profiles Automatically</h6><p>
    Instead of manually editing the YAML file, DeepSea can create new
    profiles. To let DeepSea create new profiles, the existing profiles need
    to be moved:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">old</code> /srv/pillar/ceph/proposals/profile-default/
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
    We recommend verifying the suggested proposals before deploying the
    changes. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.5.1.4 “Profile Assignment”</span> for more
    details on viewing proposals.
   </p></div></section><section class="sect1" id="salt-removing-osd" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.6 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="#salt-removing-osd">#</a></h2></div></div></div><p>
   You can remove an Ceph OSD from the cluster by running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> disengage.safety
<code class="prompt user">root@master # </code><code class="command">salt-run</code> remove.osd <em class="replaceable">OSD_ID</em></pre></div><p>
   <em class="replaceable">OSD_ID</em> needs to be a number of the OSD without
   the <code class="literal">osd.</code> prefix. For example, from
   <code class="literal">osd.3</code> only use the digit <code class="literal">3</code>.
  </p><section class="sect2" id="osd-removal-multiple" data-id-title="Removing Multiple OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.6.1 </span><span class="title-name">Removing Multiple OSDs</span> <a title="Permalink" class="permalink" href="#osd-removal-multiple">#</a></h3></div></div></div><p>
    Use the same procedure as mentioned in <a class="xref" href="#salt-removing-osd" title="1.6. Removing an OSD">Section 1.6, “Removing an OSD”</a>
    but simply supply multiple OSD IDs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety
safety is now disabled for cluster ceph

<code class="prompt user">root@master # </code>salt-run remove.osd 1 13 20
Removing osds 1, 13, 20 from minions
Press Ctrl-C to abort
Removing osd 1 from minion data4.ceph
Removing osd 13 from minion data4.ceph
Removing osd 20 from minion data4.ceph
Removing osd 1 from Ceph
Removing osd 13 from Ceph
Removing osd 20 from Ceph</pre></div><div id="imp-removed-osd-in-grains" data-id-title="Removed OSD ID Still Present in grains" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Removed OSD ID Still Present in grains</h6><p>
     After the <code class="command">remove.osd</code> command finishes, the ID of the
     removed OSD is still part of Salt grains and you can see it after
     running <code class="command">salt <em class="replaceable">target</em>
     osd.list</code>. The reason is that if the
     <code class="command">remove.osd</code> command partially fails on removing the data
     disk, the only reference to related partitions on the shared devices is in
     the grains. If we updated the grains immediately, then those partitions
     would be orphaned.
    </p><p>
     To update the grains manually, run <code class="command">salt
     <em class="replaceable">target</em> osd.retain</code>. It is part of
     DeepSea Stage 3, therefore if you are going to run Stage 3 after the OSD
     removal, the grains get updated automatically.
    </p></div><div id="id-1.3.3.2.9.5.5" data-id-title="Automatic Retries" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Automatic Retries</h6><p>
     You can append the <code class="option">timeout</code> parameter (in seconds) after
     which Salt retries the OSD removal:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run remove.osd 20 timeout=6
Removing osd 20 from minion data4.ceph
  Timeout expired - OSD 20 has 22 PGs remaining
Retrying...
Removing osd 20 from Ceph</pre></div></div></section><section class="sect2" id="osd-forced-removal" data-id-title="Removing Broken OSDs Forcefully"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.6.2 </span><span class="title-name">Removing Broken OSDs Forcefully</span> <a title="Permalink" class="permalink" href="#osd-forced-removal">#</a></h3></div></div></div><p>
    There are cases when removing an OSD gracefully (see
    <a class="xref" href="#salt-removing-osd" title="1.6. Removing an OSD">Section 1.6, “Removing an OSD”</a>) fails. This may happen for example if
    the OSD or its journal, Wall or DB are broken, when it suffers from hanging
    I/O operations, or when the OSD disk fails to unmount. In such case, you
    need to force the OSD removal. The following command removes both the data
    partition, and the journal or WAL/DB partitions:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> osd.remove <em class="replaceable">OSD_ID</em> force=True</pre></div><div id="id-1.3.3.2.9.6.4" data-id-title="Hanging Mounts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Hanging Mounts</h6><p>
     If a partition is still mounted on the disk being removed, the command
     will exit with the 'Unmount failed - check for processes on
     <em class="replaceable">DEVICE</em>' message. You can then list all
     processes that access the file system with the <code class="command">fuser -m
     <em class="replaceable">DEVICE</em></code>. If <code class="command">fuser</code>
     returns nothing, try manual <code class="command">unmount
     <em class="replaceable">DEVICE</em></code> and watch the output of
     <code class="command">dmesg</code> or <code class="command">journalctl</code> commands.
    </p></div></section></section><section class="sect1" id="ds-osd-replace" data-id-title="Replacing an OSD Disk"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.7 </span><span class="title-name">Replacing an OSD Disk</span> <a title="Permalink" class="permalink" href="#ds-osd-replace">#</a></h2></div></div></div><p>
   There are several reasons why you may need to replace an OSD disk, for
   example:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The OSD disk failed or is soon going to fail based on SMART information,
     and can no longer be used to store data safely.
    </p></li><li class="listitem"><p>
     You need to upgrade the OSD disk, for example to increase its size.
    </p></li></ul></div><p>
   The replacement procedure is the same for both cases. It is also valid for
   both default and customized CRUSH Maps.
  </p><div id="id-1.3.3.2.10.5" data-id-title="The Number of Free Disks" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: The Number of Free Disks</h6><p>
    When doing an automated OSDs replacement, the number of free disks needs to
    be the same as the number of disks you need to replace. If there are more
    free disks available in the system, it is impossible to guess which free
    disks to replace. Therefore the automated replacement will not be
    performed.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Turn off safety limitations temporarily:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety</pre></div></li><li class="step"><p>
     Suppose that for example '5' is the ID of the OSD whose disk needs to be
     replaced. The following command marks it as
     <span class="bold"><strong>destroyed</strong></span> in the CRUSH Map but leaves
     its original ID:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run replace.osd 5</pre></div><div id="id-1.3.3.2.10.6.2.3" data-id-title="replace.osd and remove.osd" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">replace.osd</code> and <code class="command">remove.osd</code></h6><p>
      The Salt's <code class="command">replace.osd</code> and
      <code class="command">remove.osd</code> (see <a class="xref" href="#salt-removing-osd" title="1.6. Removing an OSD">Section 1.6, “Removing an OSD”</a>)
      commands are identical except that <code class="command">replace.osd</code> leaves
      the OSD as 'destroyed' in the CRUSH Map while
      <code class="command">remove.osd</code> removes all traces from the CRUSH Map.
     </p></div></li><li class="step"><p>
     Manually replace the failed/upgraded OSD drive.
    </p></li><li class="step"><p>
     After replacing the physical drive, you need to modify the configuration
     of the related Salt minion. You can do so either manually or in an automated
     way.
    </p><p>
     To manually change a Salt minion's configuration, see
     <a class="xref" href="#osd-replace-manual" title="1.7.1. Manual Configuration">Section 1.7.1, “Manual Configuration”</a>.
    </p><p>
     To change a Salt minion's configuration in an automated way, see
     <a class="xref" href="#osd-replace-auto" title="1.7.2. Automated Configuration">Section 1.7.2, “Automated Configuration”</a>.
    </p></li><li class="step"><p>
     After you finish either manual or automated configuration of the
     Salt minion, run DeepSea Stage 2 to update the Salt configuration. It
     prints out a summary about the differences between the storage
     configuration and the current setup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
deepsea_minions          : valid
yaml_syntax              : valid
profiles_populated       : valid
public network           : 172.16.21.0/24
cluster network          : 172.16.22.0/24

These devices will be deployed
data1.ceph: /dev/sdb, /dev/sdc, /dev/sdd, /dev/sde, /dev/sdf, /dev/sdg</pre></div><div id="id-1.3.3.2.10.6.5.3" data-id-title="Run salt-run advise.osds" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Run <code class="command">salt-run advise.osds</code></h6><p>
      To summarize the steps that will be taken when the actual replacement is
      deployed, you can run the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run advise.osds
These devices will be deployed

data1.ceph:
  /dev/disk/by-id/cciss-3600508b1001c7c24c537bdec8f3a698f:

Run 'salt-run state.orch ceph.stage.3'</pre></div></div></li><li class="step"><p>
     Run the deployment Stage 3 to deploy the replaced OSD disk:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><section class="sect2" id="osd-replace-manual" data-id-title="Manual Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.7.1 </span><span class="title-name">Manual Configuration</span> <a title="Permalink" class="permalink" href="#osd-replace-manual">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Find the renamed YAML file for the Salt minion. For example, the file for
      the minion named 'data1.ceph' is
     </p><div class="verbatim-wrap"><pre class="screen">/srv/pillar/ceph/proposals/profile-<em class="replaceable">PROFILE_NAME</em>/stack/default/ceph/minions/data1.ceph.yml-replace</pre></div></li><li class="step"><p>
      Rename the file to its original name (without the
      <code class="literal">-replace</code> suffix), edit it, and replace the old device
      with the new device name.
     </p><div id="id-1.3.3.2.10.7.2.2.2" data-id-title="salt osd.report" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: salt osd.report</h6><p>
       Consider using <code class="command">salt '<em class="replaceable">MINION_NAME</em>'
       osd.report</code> to identify the device that has been removed.
      </p></div><p>
      For example, if the <code class="filename">data1.ceph.yml</code> file contains
     </p><div class="verbatim-wrap"><pre class="screen">ceph:
  storage:
    osds:
      [...]
      /dev/disk/by-id/cciss-3600508b1001c93595b70bd0fb700ad38:
        format: bluestore
      [...]</pre></div><p>
      replace the corresponding device path with
     </p><div class="verbatim-wrap"><pre class="screen">ceph:
  storage:
    osds:
      [...]
      /dev/disk/by-id/cciss-3600508b1001c7c24c537bdec8f3a698f:
        format: bluestore
        replace: True
      [...]</pre></div></li></ol></div></div></section><section class="sect2" id="osd-replace-auto" data-id-title="Automated Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.7.2 </span><span class="title-name">Automated Configuration</span> <a title="Permalink" class="permalink" href="#osd-replace-auto">#</a></h3></div></div></div><p>
    While the default profile for Stage 1 may work for the simplest setups,
    this stage can be optionally customized:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Set the <code class="option">stage_discovery:
      <em class="replaceable">CUSTOM_STAGE_NAME</em></code> option in
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>.
     </p></li><li class="step"><p>
      Create the corresponding file
      <code class="filename">/srv/salt/ceph/stage/1/<em class="replaceable">CUSTOM_STAGE_NAME</em>.sls</code>
      and customize it to reflect your specific requirements for Stage 1. See
      <a class="xref" href="#app-stage1-custom" title="Appendix A. DeepSea Stage 1 Custom Example">Appendix A, <em>DeepSea Stage 1 Custom Example</em></a> for an example.
     </p><div id="id-1.3.3.2.10.8.3.2.2" data-id-title="Inspect init.sls" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Inspect <code class="filename">init.sls</code></h6><p>
       Inspect the <code class="filename">/srv/salt/ceph/stage/1/init.sls</code> file to
       see what variables you can use in your custom Stage 1 .sls file.
      </p></div></li><li class="step"><p>
      Refresh the Pillar:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="step"><p>
      Run Stage 1 to generate the new configuration file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li></ol></div></div><div id="id-1.3.3.2.10.8.4" data-id-title="Custom Options" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom Options</h6><p>
     To list all available options, inspect the output of the <code class="command">salt-run
     proposal.help</code> command.
    </p><p>
     If you customized the cluster deployment with a specific command
    </p><div class="verbatim-wrap"><pre class="screen">salt-run proposal.populate <em class="replaceable">OPTION</em>=<em class="replaceable">VALUE</em></pre></div><p>
     use the same configuration when doing the automated configuration.
    </p></div></section></section><section class="sect1" id="ds-osd-recover" data-id-title="Recovering a Reinstalled OSD Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.8 </span><span class="title-name">Recovering a Reinstalled OSD Node</span> <a title="Permalink" class="permalink" href="#ds-osd-recover">#</a></h2></div></div></div><p>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Reinstall the base SUSE Linux Enterprise operating system on the node where the OS broke.
     Install the <span class="package">salt-minion</span> packages on the OSD node,
     delete the old Salt minion key on the Salt master, and register the new
     Salt minion's key it with the Salt master. For more information on the initial
     deployment, see <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”</span>.
    </p></li><li class="step"><p>
     Instead of running the whole of Stage 0, run the following parts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.sync
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.packages.common
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.mines
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.updates</pre></div></li><li class="step"><p>
     Run DeepSea Stages 1 to 5:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></li><li class="step"><p>
     Run DeepSea Stage 0:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </p></li></ol></div></div></section><section class="sect1" id="salt-automated-installation" data-id-title="Automated Installation via Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.9 </span><span class="title-name">Automated Installation via Salt</span> <a title="Permalink" class="permalink" href="#salt-automated-installation">#</a></h2></div></div></div><p>
   The installation can be automated by using the Salt reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a Ceph cluster with the specified behavior.
  </p><div id="id-1.3.3.2.12.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Salt cannot perform dependency checks based on reactor events. There is a
    real risk of putting your Salt master into a death spiral.
   </p></div><p>
   The automated installation requires the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A properly created
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>.
    </p></li><li class="listitem"><p>
     Prepared custom configuration placed to the
     <code class="filename">/srv/pillar/ceph/stack</code> directory.
    </p></li></ul></div><p>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </p><p>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </p><p>
   If the operation is performed properly, edit the file
  </p><div class="verbatim-wrap"><pre class="screen">/etc/salt/master.d/reactor.conf</pre></div><p>
   and replace the following line
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/discovery.sls</pre></div><p>
   with
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/all_stages.sls</pre></div><p>
   Verify that the line is not commented out.
  </p></section><section class="sect1" id="deepsea-rolling-updates" data-id-title="Updating the Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.10 </span><span class="title-name">Updating the Cluster Nodes</span> <a title="Permalink" class="permalink" href="#deepsea-rolling-updates">#</a></h2></div></div></div><p>
   Keep the Ceph cluster nodes up-to-date by applying rolling updates
   regularly.
  </p><div id="id-1.3.3.2.13.3" data-id-title="Access to Software Repositories" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Access to Software Repositories</h6><p>
    Before patching the cluster with latest software packages, verify that all
    its nodes have access to SUSE Linux Enterprise Server repositories that match your version of
    SUSE Enterprise Storage. For SUSE Enterprise Storage 5.5, the following
    repositories are required:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper lr -E
#  | Alias   | Name                              | Enabled | GPG Check | Refresh
---+---------+-----------------------------------+---------+-----------+--------
 4 | [...]   | SUSE-Enterprise-Storage-5-Pool    | Yes     | (r ) Yes  | No
 6 | [...]   | SUSE-Enterprise-Storage-5-Updates | Yes     | (r ) Yes  | Yes
 9 | [...]   | SLES12-SP3-Pool                   | Yes     | (r ) Yes  | No
11 | [...]   | SLES12-SP3-Updates                | Yes     | (r ) Yes  | Yes</pre></div></div><div id="id-1.3.3.2.13.4" data-id-title="Repository Staging" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Repository Staging</h6><p>
    If you use a staging tool—for example, SUSE Manager, Subscription Management Tool, or
    Repository Mirroring Tool—that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for SUSE Linux Enterprise Server and SUSE Enterprise Storage are
    created at the same point in time.
   </p><p>
    We strongly recommend to use a staging tool to apply patches which have
    <code class="literal">frozen</code> or <code class="literal">staged</code> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </p></div><p>
    To update the software packages on all cluster nodes to the latest version,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the <span class="package">deepsea</span>, <span class="package">salt-master</span>,
      and <span class="package">salt-minion</span> packages and restart relevant services
      on the Salt master:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'roles:master' state.apply ceph.updates.master</pre></div></li><li class="step"><p>
      Update and restart the <span class="package">salt-minion</span> package on all
      cluster nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'cluster:ceph' state.apply ceph.updates.salt</pre></div></li><li class="step"><p>
      Update all other software packages on the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
      Restart Ceph related services:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart</pre></div></li></ol></div></div><div id="id-1.3.3.2.13.7" data-id-title="Possible Downtime of Ceph Services" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Possible Downtime of Ceph Services</h6><p>
    When applying updates to Ceph cluster nodes, Ceph services may be
    restarted. If there is a single point of failure for services such as
    Object Gateway, NFS Ganesha, or iSCSI, the client machines may be temporarily
    disconnected from related services.
   </p></div><p>
   If DeepSea detects a running Ceph cluster, it applies available updates,
   restarts running Ceph services, and optionally restarts nodes sequentially
   if a kernel update was installed. DeepSea follows Ceph's official
   recommendation of first updating the monitors, then the OSDs, and lastly
   additional services, such as Metadata Server, Object Gateway, iSCSI Gateway, or NFS Ganesha. DeepSea
   stops the update process if it detects an issue in the cluster. A trigger
   for that can be:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ceph reports 'HEALTH_ERR' for longer then 300 seconds.
    </p></li><li class="listitem"><p>
     Salt minions are queried for their assigned services to be still up and
     running after an update. The update fails if the services are down for
     more than 900 seconds.
    </p></li></ul></div><p>
   Making these arrangements ensures that even with corrupted or failing
   updates, the Ceph cluster is still operational.
  </p><p>
   DeepSea Stage 0 updates the system via <code class="command">zypper update</code>
   and optionally reboots the system if the kernel is updated. If you want to
   eliminate the possibility of a forced reboot of potentially all nodes,
   either make sure that the latest kernel is installed and running before
   initiating DeepSea Stage 0, or disable automatic node reboots as described
   in <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.1.5 “Updates and Reboots during Stage 0”</span>.
  </p><div id="id-1.3.3.2.13.12" data-id-title="zypper patch" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">zypper patch</code></h6><p>
    If you prefer to update the system using the <code class="command">zypper
    patch</code> command, edit
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
    following line:
   </p><div class="verbatim-wrap"><pre class="screen">update_method_init: zypper-patch</pre></div></div><p>
   You can change the default update/reboot behavior of DeepSea Stage 0 by
   adding/changing the <code class="option">stage_prep_master</code> and
   <code class="option">stage_prep_minion</code> options. For more information, see
   <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.1.5 “Updates and Reboots during Stage 0”</span>.
  </p></section><section class="sect1" id="sec-salt-cluster-reboot" data-id-title="Halting or Rebooting Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.11 </span><span class="title-name">Halting or Rebooting Cluster</span> <a title="Permalink" class="permalink" href="#sec-salt-cluster-reboot">#</a></h2></div></div></div><p>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Tell the Ceph cluster not to mark OSDs as out:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd set noout</pre></div></li><li class="step"><p>
     Stop daemons and nodes in the following order:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Storage clients
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph Monitor
      </p></li></ol></div></li><li class="step"><p>
     If required, perform maintenance tasks.
    </p></li><li class="step"><p>
     Start the nodes and servers in the reverse order of the shutdown process:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Ceph Monitor
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Storage clients
      </p></li></ol></div></li><li class="step"><p>
     Remove the noout flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd unset noout</pre></div></li></ol></div></div></section><section class="sect1" id="ds-custom-cephconf" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.12 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="#ds-custom-cephconf">#</a></h2></div></div></div><p>
   If you need to put custom settings into the <code class="filename">ceph.conf</code>
   file, you can do so by modifying the configuration files in the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d</code>
   directory:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     global.conf
    </p></li><li class="listitem"><p>
     mon.conf
    </p></li><li class="listitem"><p>
     mgr.conf
    </p></li><li class="listitem"><p>
     mds.conf
    </p></li><li class="listitem"><p>
     osd.conf
    </p></li><li class="listitem"><p>
     client.conf
    </p></li><li class="listitem"><p>
     rgw.conf
    </p></li></ul></div><div id="id-1.3.3.2.15.4" data-id-title="Unique rgw.conf" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unique <code class="filename">rgw.conf</code></h6><p>
    The Object Gateway offers a lot flexibility and is unique compared to the other
    <code class="filename">ceph.conf</code> sections. All other Ceph components have
    static headers such as <code class="literal">[mon]</code> or
    <code class="literal">[osd]</code>. The Object Gateway has unique headers such as
    <code class="literal">[client.rgw.rgw1]</code>. This means that the
    <code class="filename">rgw.conf</code> file needs a header entry. For examples, see
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw.conf</code></pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw-ssl.conf</code></pre></div></div><div id="id-1.3.3.2.15.5" data-id-title="Run Stage 3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Run Stage 3</h6><p>
    After you make custom changes to the above mentioned configuration files,
    run Stages 3 and 4 to apply these changes to the cluster nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></div><p>
   These files are included from the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
   template file, and correspond to the different sections that the Ceph
   configuration file accepts. Putting a configuration snippet in the correct
   file enables DeepSea to place it into the correct section. You do not need
   to add any of the section headers.
  </p><div id="id-1.3.3.2.15.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <code class="literal">[osd.1]</code>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </p></div><section class="sect2" id="id-1.3.3.2.15.8" data-id-title="Overriding the Defaults"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.12.1 </span><span class="title-name">Overriding the Defaults</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.15.8">#</a></h3></div></div></div><p>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>
    file:
   </p><div class="verbatim-wrap"><pre class="screen">auth cluster required = none
auth service required = none
auth client required = none</pre></div><p>
    When redefining the default values, Ceph related tools such as
    <code class="command">rados</code> may issue warnings that specific values from the
    <code class="filename">ceph.conf.j2</code> were redefined in
    <code class="filename">global.conf</code>. These warnings are caused by one
    parameter assigned twice in the resulting <code class="filename">ceph.conf</code>.
   </p><p>
    As a workaround for this specific case, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Change the current directory to
      <code class="filename">/srv/salt/ceph/configuration/create</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/create</pre></div></li><li class="step"><p>
      Copy <code class="filename">default.sls</code> to <code class="filename">custom.sls</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp default.sls custom.sls</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom.sls</code> and change
      <code class="option">ceph.conf.j2</code> to <code class="option">custom-ceph.conf.j2</code>.
     </p></li><li class="step"><p>
      Change current directory to
      <code class="filename">/srv/salt/ceph/configuration/files</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/files</pre></div></li><li class="step"><p>
      Copy <code class="filename">ceph.conf.j2</code> to
      <code class="filename">custom-ceph.conf.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp ceph.conf.j2 custom-ceph.conf.j2</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom-ceph.conf.j2</code> and delete the following
      line:
     </p><div class="verbatim-wrap"><pre class="screen">{% include "ceph/configuration/files/rbd.conf" %}</pre></div><p>
      Edit <code class="filename">global.yml</code> and add the following line:
     </p><div class="verbatim-wrap"><pre class="screen">configuration_create: custom</pre></div></li><li class="step"><p>
      Refresh the pillar:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh</pre></div></li><li class="step"><p>
      Run Stage 3:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><p>
    Now you should have only one entry for each value definition. To re-create
    the configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.configuration.create</pre></div><p>
    and then verify the contents of
    <code class="filename">/srv/salt/ceph/configuration/cache/ceph.conf</code>.
   </p></section><section class="sect2" id="id-1.3.3.2.15.9" data-id-title="Including Configuration Files"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.12.2 </span><span class="title-name">Including Configuration Files</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.15.9">#</a></h3></div></div></div><p>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <code class="filename">osd.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</pre></div><p>
    In the previous example, the <code class="filename">osd1.conf</code>,
    <code class="filename">osd2.conf</code>, <code class="filename">osd3.conf</code>, and
    <code class="filename">osd4.conf</code> files contain the configuration options
    specific to the related OSD.
   </p><div id="id-1.3.3.2.15.9.5" data-id-title="Runtime Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Runtime Configuration</h6><p>
     Changes made to Ceph configuration files take effect after the related
     Ceph daemons restart. See <a class="xref" href="#ceph-config-runtime" title="12.1. Runtime Configuration">Section 12.1, “Runtime Configuration”</a> for more
     information on changing the Ceph runtime configuration.
    </p></div></section></section><section class="sect1" id="admin-apparmor" data-id-title="Enabling AppArmor Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.13 </span><span class="title-name">Enabling AppArmor Profiles</span> <a title="Permalink" class="permalink" href="#admin-apparmor">#</a></h2></div></div></div><p>
   AppArmor is a security solution that confines programs by a specific profile.
   For more details, refer to
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#part-apparmor" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-security/#part-apparmor</a>.
  </p><p>
   DeepSea provides three states for AppArmor profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular AppArmor state, run:
  </p><div class="verbatim-wrap"><pre class="screen">salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<em class="replaceable">STATE</em></pre></div><p>
   To put the AppArmor profiles in an 'enforce' state:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce</pre></div><p>
   To put the AppArmor profiles in a 'complain' status:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain</pre></div><p>
   To disable the AppArmor profiles:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable</pre></div><div id="id-1.3.3.2.16.11" data-id-title="Enabling the AppArmor Service" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Enabling the AppArmor Service</h6><p>
    Each of these three calls verifies if AppArmor is installed and installs it if
    not, and starts and enables the related <code class="systemitem">systemd</code> service. DeepSea will
    warn you if AppArmor was installed and started/enabled in another way and
    therefore runs without DeepSea profiles.
   </p></div></section></section></div><div class="part" id="part-operate" data-id-title="Operating a Cluster"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Operating a Cluster </span><a title="Permalink" class="permalink" href="#part-operate">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">2 </span><span class="title-name">Introduction</span></a></span></li><dd class="toc-abstract"><p>
  In this part of the manual you will learn how to start or stop Ceph
  services, monitor a cluster's state, use and modify CRUSH Maps, or manage
  storage pools.
 </p></dd><li><span class="chapter"><a href="#ceph-operating-services"><span class="title-number">3 </span><span class="title-name">Operating Ceph Services</span></a></span></li><dd class="toc-abstract"><p>
  You can operate Ceph services either using <code class="systemitem">systemd</code>, or using DeepSea.
 </p></dd><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">4 </span><span class="title-name">Determining Cluster State</span></a></span></li><dd class="toc-abstract"><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups and Metadata Servers.
 </p></dd><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">5 </span><span class="title-name">Monitoring and Alerting</span></a></span></li><dd class="toc-abstract"><p>
  By default, DeepSea deploys a monitoring and alerting stack on the
  Salt master. It consists of the following components:
 </p></dd><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">6 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span></li><dd class="toc-abstract"><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p></dd><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">7 </span><span class="title-name">Stored Data Management</span></a></span></li><dd class="toc-abstract"><p>The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OSDs directly rather than through a centralized server or broker. With an algorithmically determined method of storing and retrieving data, Ceph avoids a…</p></dd><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">8 </span><span class="title-name">Managing Storage Pools</span></a></span></li><dd class="toc-abstract"><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p></dd><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">9 </span><span class="title-name">RADOS Block Device</span></a></span></li><dd class="toc-abstract"><p>A block is a sequence of bytes, for example a 4MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, such as hard disks, CDs, floppy disks. The ubiquity of block device interfaces makes a virtual block device an ideal candidate to interact with a…</p></dd><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span></a></span></li><dd class="toc-abstract"><p>Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality of replicated pools (for example it cannot store metadata for RBD pools), but require less raw storage. A default erasure pool capable of stor…</p></dd><li><span class="chapter"><a href="#cha-ceph-tiered"><span class="title-number">11 </span><span class="title-name">Cache Tiering</span></a></span></li><dd class="toc-abstract"><p>
  A <span class="emphasis"><em>cache tier</em></span> is an additional storage layer implemented
  between the client and the standard storage. It is designed to speed up
  access to pools stored on slow hard disks and erasure coded pools.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">12 </span><span class="title-name">Ceph Cluster Configuration</span></a></span></li><dd class="toc-abstract"><p>
  This chapter provides a list of important Ceph cluster settings and their
  description. The settings are sorted by topic.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-operating" data-id-title="Introduction"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Introduction</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating">#</a></h2></div></div></div><p>
  In this part of the manual you will learn how to start or stop Ceph
  services, monitor a cluster's state, use and modify CRUSH Maps, or manage
  storage pools.
 </p><p>
  It also includes advanced topics, for example how to manage users and
  authentication in general, how to manage pool and RADOS device snapshots, how
  to set up erasure coded pools, or how to increase the cluster performance
  with cache tiering.
 </p></section><section class="chapter" id="ceph-operating-services" data-id-title="Operating Ceph Services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Operating Ceph Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services">#</a></h2></div></div></div><p>
  You can operate Ceph services either using <code class="systemitem">systemd</code>, or using DeepSea.
 </p><section class="sect1" id="operate-services-systemd" data-id-title="Operating Ceph Cluster Related Services using systemd"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Operating Ceph Cluster Related Services using <code class="systemitem">systemd</code></span> <a title="Permalink" class="permalink" href="#operate-services-systemd">#</a></h2></div></div></div><p>
   Use the <code class="command">systemctl</code> command to operate all Ceph related
   services. The operation takes place on the node you are currently logged in
   to. You need to have <code class="systemitem">root</code> privileges to be able to operate on Ceph
   services.
  </p><section class="sect2" id="ceph-operating-services-targets" data-id-title="Starting, Stopping, and Restarting Services using Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.1.1 </span><span class="title-name">Starting, Stopping, and Restarting Services using Targets</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-targets">#</a></h3></div></div></div><p>
    To simplify starting, stopping, and restarting all the services of a
    particular type (for example all Ceph services, or all MONs, or all OSDs)
    on a node, Ceph provides the following <code class="systemitem">systemd</code> unit files:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</pre></div><p>
    To start/stop/restart all Ceph services on the node, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph.target
<code class="prompt user">root # </code>systemctl stop ceph.target
<code class="prompt user">root # </code>systemctl restart ceph.target</pre></div><p>
    To start/stop/restart all OSDs on the node, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd.target
<code class="prompt user">root # </code>systemctl stop ceph-osd.target
<code class="prompt user">root # </code>systemctl restart ceph-osd.target</pre></div><p>
    Commands for the other targets are analogous.
   </p></section><section class="sect2" id="ceph-operating-services-individual" data-id-title="Starting, Stopping, and Restarting Individual Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.1.2 </span><span class="title-name">Starting, Stopping, and Restarting Individual Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-individual">#</a></h3></div></div></div><p>
    You can operate individual services using the following parameterized
    <code class="systemitem">systemd</code> unit files:
   </p><div class="verbatim-wrap"><pre class="screen">ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-mgr@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service</pre></div><p>
    To use these commands, you first need to identify the name of the service
    you want to operate. See
    <a class="xref" href="#ceph-operating-services-finding-names" title="3.1.3. Identifying Individual Services">Section 3.1.3, “Identifying Individual Services”</a> to learn more about
    services identification.
   </p><p>
    To start/stop/restart the <code class="literal">osd.1</code> service, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd@1.service
<code class="prompt user">root # </code>systemctl stop ceph-osd@1.service
<code class="prompt user">root # </code>systemctl restart ceph-osd@1.service</pre></div><p>
    Commands for the other service types are analogous.
   </p></section><section class="sect2" id="ceph-operating-services-finding-names" data-id-title="Identifying Individual Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.1.3 </span><span class="title-name">Identifying Individual Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-finding-names">#</a></h3></div></div></div><p>
    You can find out the names/numbers of a particular type of service in
    several ways. The following commands provide results for services
    <code class="literal">ceph*</code> and <code class="literal">lrbd*</code>. You can run them on
    any node of the Ceph cluster.
   </p><p>
    To list all (even inactive) services of type <code class="literal">ceph*</code> and
    <code class="literal">lrbd*</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl list-units --all --type=service ceph* lrbd*</pre></div><p>
    To list only the inactive services, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl list-units --all --state=inactive --type=service ceph* lrbd*</pre></div><p>
    You can also use <code class="command">salt</code> to query services across multiple
    nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">TARGET</em> cmd.shell \
 "systemctl list-units --all --type=service ceph* lrbd* | sed -e '/^$/,$ d'"</pre></div><p>
    Query storage nodes only:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'roles:storage' cmd.shell \
 'systemctl list-units --all --type=service ceph* lrbd*'</pre></div></section><section class="sect2" id="ceph-operating-services-status" data-id-title="Service Status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.1.4 </span><span class="title-name">Service Status</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-status">#</a></h3></div></div></div><p>
    You can query <code class="systemitem">systemd</code> for the status of services. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl status ceph-osd@1.service
<code class="prompt user">root # </code>systemctl status ceph-mon@<em class="replaceable">HOSTNAME</em>.service</pre></div><p>
    Replace <em class="replaceable">HOSTNAME</em> with the host name the daemon
    is running on.
   </p><p>
    If you do not know the exact name/number of the service, see
    <a class="xref" href="#ceph-operating-services-finding-names" title="3.1.3. Identifying Individual Services">Section 3.1.3, “Identifying Individual Services”</a>.
   </p></section></section><section class="sect1" id="Deepsea-restart" data-id-title="Restarting Ceph Services using DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Restarting Ceph Services using DeepSea</span> <a title="Permalink" class="permalink" href="#Deepsea-restart">#</a></h2></div></div></div><p>
   After applying updates to the cluster nodes, the affected Ceph related
   services need to be restarted. Normally, restarts are performed
   automatically by DeepSea. This section describes how to restart the
   services manually.
  </p><div id="id-1.3.4.3.5.3" data-id-title="Watching the Restart" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Watching the Restart</h6><p>
    The process of restarting the cluster may take some time. You can watch the
    events by using the Salt event bus by running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.event pretty=True</pre></div><p>
    Another command to monitor active jobs is
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run jobs.active</pre></div></div><section class="sect2" id="deepsea-restart-all" data-id-title="Restarting All Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.2.1 </span><span class="title-name">Restarting All Services</span> <a title="Permalink" class="permalink" href="#deepsea-restart-all">#</a></h3></div></div></div><div id="id-1.3.4.3.5.4.2" data-id-title="Interruption of Services" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Interruption of Services</h6><p>
     If Ceph related services—specifically iSCSI or
     NFS Ganesha—are configured as single points of access with no High Availability
     setup, restarting then will result in their temporary outage as viewed
     from the client side.
    </p></div><div id="id-1.3.4.3.5.4.3" data-id-title="Samba not Managed by DeepSea" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Samba not Managed by DeepSea</h6><p>
     Because DeepSea and openATTIC do not currently support Samba deployments, you
     need to manage Samba related services manually. For more details, see
     <span class="intraxref">Book “Deployment Guide”, Chapter 13 “Exporting Ceph Data via Samba”</span>.
    </p></div><p>
    To restart <span class="emphasis"><em>all</em></span> services on the cluster, run the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart</pre></div><p>
    All roles you have configured restart in the following order: Ceph Monitor, Ceph Manager,
    Ceph OSD, Metadata Server, Object Gateway, iSCSI Gateway, NFS Ganesha. To keep the downtime low and to find
    potential issues as early as possible, nodes are restarted sequentially.
    For example, only one monitoring node is restarted at a time.
   </p><p>
    The command waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </p></section><section class="sect2" id="deepsea-restart-specific" data-id-title="Restarting Specific Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.2.2 </span><span class="title-name">Restarting Specific Services</span> <a title="Permalink" class="permalink" href="#deepsea-restart-specific">#</a></h3></div></div></div><p>
    To restart a specific service on the cluster, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.<em class="replaceable">service_name</em></pre></div><p>
    For example, to restart all Object Gateways, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.rgw</pre></div><p>
    You can use the following targets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mon</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mgr</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.osd</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mds</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.rgw</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.igw</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.ganesha</pre></div></section></section><section class="sect1" id="ceph-cluster-shutdown" data-id-title="Shutdown and Restart of the Whole Ceph Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.3 </span><span class="title-name">Shutdown and Restart of the Whole Ceph Cluster</span> <a title="Permalink" class="permalink" href="#ceph-cluster-shutdown">#</a></h2></div></div></div><p>
   Shutting down and restarting the cluster may be necessary in the case of a
   planned power outage. To stop all Ceph related services and restart
   without issue, follow the steps below.
  </p><div class="procedure" id="id-1.3.4.3.6.3" data-id-title="Shutting Down the Whole Ceph Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 3.1: </span><span class="title-name">Shutting Down the Whole Ceph Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.6.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Shut down or disconnect any clients accessing the cluster.
    </p></li><li class="step"><p>
     To prevent CRUSH from automatically rebalancing the cluster, set the
     cluster to <code class="literal">noout</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd set noout</pre></div></li><li class="step"><p>
     Disable safety measures:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety</pre></div></li><li class="step"><p>
     Stop all Ceph services in the following order:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Stop NFS Ganesha:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:ganesha and I@cluster:ceph' ceph.terminate.ganesha</pre></div></li><li class="step"><p>
       Stop Object Gateways:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:rgw and I@cluster:ceph' ceph.terminate.rgw</pre></div></li><li class="step"><p>
       Stop Metadata Servers:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:mds and I@cluster:ceph' ceph.terminate.mds</pre></div></li><li class="step"><p>
       Stop iSCSI Gateways:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:igw and I@cluster:ceph' ceph.terminate.igw</pre></div></li><li class="step"><p>
       Stop Ceph OSDs:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:storage and I@cluster:ceph' ceph.terminate.storage</pre></div></li><li class="step"><p>
       Stop Ceph Managers:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:mgr and I@cluster:ceph' ceph.terminate.mgr</pre></div></li><li class="step"><p>
       Stop Ceph Monitors:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'I@roles:mon  and I@cluster:ceph' ceph.terminate.mon</pre></div></li></ol></li><li class="step"><p>
     Power off all cluster nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'G@deepsea:*' cmd.run "shutdown -h"</pre></div></li></ol></div></div><div class="procedure" id="id-1.3.4.3.6.4" data-id-title="Starting the Whole Ceph Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 3.2: </span><span class="title-name">Starting the Whole Ceph Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.6.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Power on the Admin Node.
    </p></li><li class="step"><p>
     Power on the Ceph Monitor nodes.
    </p></li><li class="step"><p>
     Power on the Ceph OSD nodes.
    </p></li><li class="step"><p>
     Unset the previously set <code class="literal">noout</code> flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd unset noout</pre></div></li><li class="step"><p>
     Power on all configured gateways.
    </p></li><li class="step"><p>
     Power on or connect cluster clients.
    </p></li></ol></div></div></section></section><section class="chapter" id="ceph-monitor" data-id-title="Determining Cluster State"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Determining Cluster State</span> <a title="Permalink" class="permalink" href="#ceph-monitor">#</a></h2></div></div></div><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups and Metadata Servers.
 </p><div id="id-1.3.4.4.4" data-id-title="Interactive Mode" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Interactive Mode</h6><p>
   To run the <code class="command">ceph</code> tool in an interactive mode, type
   <code class="command">ceph</code> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <code class="command">ceph</code> commands in a row. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</pre></div></div><section class="sect1" id="monitor-status" data-id-title="Checking a Clusters Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Checking a Cluster's Status</span> <a title="Permalink" class="permalink" href="#monitor-status">#</a></h2></div></div></div><p>
   To check a cluster's status, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph status</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph -s</pre></div><p>
   In interactive mode, type <code class="command">status</code> and press
   <span class="keycap">Enter</span>.
  </p><div class="verbatim-wrap"><pre class="screen">ceph&gt; status</pre></div><p>
   Ceph will print the cluster status. For example, a tiny Ceph cluster
   consisting of one monitor and two OSDs may print the following:
  </p><div class="verbatim-wrap"><pre class="screen">cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</pre></div></section><section class="sect1" id="monitor-health" data-id-title="Checking Cluster Health"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Checking Cluster Health</span> <a title="Permalink" class="permalink" href="#monitor-health">#</a></h2></div></div></div><p>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</pre></div><div id="id-1.3.4.4.6.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph -c <em class="replaceable">/path/to/conf</em> -k <em class="replaceable">/path/to/keyring</em> health</pre></div></div><p>
   The Ceph cluster returns one of the following health codes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.6.6.1"><span class="term">OSD_DOWN</span></dt><dd><p>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </p><p>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<code class="filename">/var/log/ceph/ceph-osd.*</code>) may contain debugging
      information.
     </p></dd><dt id="id-1.3.4.4.6.6.2"><span class="term">OSD_<em class="replaceable">crush type</em>_DOWN, for example OSD_HOST_DOWN</span></dt><dd><p>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </p></dd><dt id="id-1.3.4.4.6.6.3"><span class="term">OSD_ORPHAN</span></dt><dd><p>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rm osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.4.4.6.6.4"><span class="term">OSD_OUT_OF_ORDER_FULL</span></dt><dd><p>
      The usage thresholds for <span class="emphasis"><em>backfillfull</em></span> (defaults to
      0.90), <span class="emphasis"><em>nearfull</em></span> (defaults to 0.85),
      <span class="emphasis"><em>full</em></span> (defaults to 0.95), and/or
      <span class="emphasis"><em>failsafe_full</em></span> are not ascending. In particular, we
      expect <span class="emphasis"><em>backfillfull</em></span> &lt;
      <span class="emphasis"><em>nearfull</em></span>, <span class="emphasis"><em>nearfull</em></span> &lt;
      <span class="emphasis"><em>full</em></span>, and <span class="emphasis"><em>full</em></span> &lt;
      <span class="emphasis"><em>failsafe_full</em></span>.
     </p><p>
      To read the current values, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%</pre></div><p>
      The thresholds can be adjusted with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephadm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephadm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div></dd><dt id="id-1.3.4.4.6.6.5"><span class="term">OSD_FULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>full</em></span> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph df</pre></div><p>
      The currently defined <span class="emphasis"><em>full</em></span> ratio can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd dump | grep full_ratio</pre></div><p>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div><p>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </p></dd><dt id="id-1.3.4.4.6.6.6"><span class="term">OSD_BACKFILLFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>backfillfull</em></span>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.4.4.6.6.7"><span class="term">OSD_NEARFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>nearfull</em></span>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.4.4.6.6.8"><span class="term">OSDMAP_FLAGS</span></dt><dd><p>
      One or more cluster flags of interest has been set. With the exception of
      <span class="emphasis"><em>full</em></span>, these flags can be set or cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set <em class="replaceable">flag</em>
<code class="prompt user">cephadm &gt; </code>ceph osd unset <em class="replaceable">flag</em></pre></div><p>
      These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.6.6.8.2.4.1"><span class="term">full</span></dt><dd><p>
         The cluster is flagged as full and cannot service writes.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.2"><span class="term">pauserd, pausewr</span></dt><dd><p>
         Paused reads or writes.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.3"><span class="term">noup</span></dt><dd><p>
         OSDs are not allowed to start.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.4"><span class="term">nodown</span></dt><dd><p>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <span class="emphasis"><em>down</em></span>.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.5"><span class="term">noin</span></dt><dd><p>
         OSDs that were previously marked <span class="emphasis"><em>out</em></span> will not be
         marked back <span class="emphasis"><em>in</em></span> when they start.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.6"><span class="term">noout</span></dt><dd><p>
         <span class="emphasis"><em>Down</em></span> OSDs will not automatically be marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.7"><span class="term">nobackfill, norecover, norebalance</span></dt><dd><p>
         Recovery or data rebalancing is suspended.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.8"><span class="term">noscrub, nodeep_scrub</span></dt><dd><p>
         Scrubbing (see <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>) is disabled.
        </p></dd><dt id="id-1.3.4.4.6.6.8.2.4.9"><span class="term">notieragent</span></dt><dd><p>
         Cache tiering activity is suspended.
        </p></dd></dl></div></dd><dt id="id-1.3.4.4.6.6.9"><span class="term">OSD_FLAGS</span></dt><dd><p>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.4.6.6.9.2.2.1"><span class="term">noup</span></dt><dd><p>
         OSD is not allowed to start.
        </p></dd><dt id="id-1.3.4.4.6.6.9.2.2.2"><span class="term">nodown</span></dt><dd><p>
         Failure reports for this OSD will be ignored.
        </p></dd><dt id="id-1.3.4.4.6.6.9.2.2.3"><span class="term">noin</span></dt><dd><p>
         If this OSD was previously marked <span class="emphasis"><em>out</em></span>
         automatically after a failure, it will not be marked
         <span class="emphasis"><em>in</em></span> when it starts.
        </p></dd><dt id="id-1.3.4.4.6.6.9.2.2.4"><span class="term">noout</span></dt><dd><p>
         If this OSD is down, it will not be automatically marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd></dl></div><p>
      Per-OSD flags can be set and cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd add-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em>
<code class="prompt user">cephadm &gt; </code>ceph osd rm-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em></pre></div></dd><dt id="id-1.3.4.4.6.6.10"><span class="term">OLD_CRUSH_TUNABLES</span></dt><dd><p>
      The CRUSH Map is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <code class="option">mon_crush_min_required_version</code>
      configuration option.
     </p></dd><dt id="id-1.3.4.4.6.6.11"><span class="term">OLD_CRUSH_STRAW_CALC_VERSION</span></dt><dd><p>
      The CRUSH Map is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The CRUSH Map should be
      updated to use the newer method (<code class="option">straw_calc_version</code>=1).
     </p></dd><dt id="id-1.3.4.4.6.6.12"><span class="term">CACHE_POOL_NO_HIT_SET</span></dt><dd><p>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_type <em class="replaceable">type</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_period <em class="replaceable">period-in-seconds</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_count <em class="replaceable">number-of-hitsets</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_fpp <em class="replaceable">target-false-positive-rate</em></pre></div><p>
      For more information on cache tiering, see
      <a class="xref" href="#cha-ceph-tiered" title="Chapter 11. Cache Tiering">Chapter 11, <em>Cache Tiering</em></a>.
     </p></dd><dt id="id-1.3.4.4.6.6.13"><span class="term">OSD_NO_SORTBITWISE</span></dt><dd><p>
      No pre-luminous v12 OSDs are running but the <code class="option">sortbitwise</code>
      flag has not been set. You need to set the <code class="option">sortbitwise</code>
      flag before luminous v12 or newer OSDs can start:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set sortbitwise</pre></div></dd><dt id="id-1.3.4.4.6.6.14"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph df detail</pre></div><p>
      You can either raise the pool quota with
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_objects <em class="replaceable">num-objects</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_bytes <em class="replaceable">num-bytes</em></pre></div><p>
      or delete some existing data to reduce usage.
     </p></dd><dt id="id-1.3.4.4.6.6.15"><span class="term">PG_AVAILABILITY</span></dt><dd><p>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow IO
      requests to be serviced. Problematic PG states include
      <span class="emphasis"><em>peering</em></span>, <span class="emphasis"><em>stale</em></span>,
      <span class="emphasis"><em>incomplete</em></span>, and the lack of
      <span class="emphasis"><em>active</em></span> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.4.4.6.6.16"><span class="term">PG_DEGRADED</span></dt><dd><p>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <span class="emphasis"><em>degraded</em></span> or
      <span class="emphasis"><em>undersized</em></span> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <span class="emphasis"><em>clean</em></span> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.4.4.6.6.17"><span class="term">PG_DEGRADED_FULL</span></dt><dd><p>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <span class="emphasis"><em>backfill_toofull</em></span> or
      <span class="emphasis"><em>recovery_toofull</em></span> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <span class="emphasis"><em>backfillfull</em></span> threshold.
     </p></dd><dt id="id-1.3.4.4.6.6.18"><span class="term">PG_DAMAGED</span></dt><dd><p>
      Data scrubbing (see <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <span class="emphasis"><em>inconsistent</em></span> or
      <span class="emphasis"><em>snaptrim_error</em></span> flag is set, indicating an earlier
      scrub operation found a problem, or that the <span class="emphasis"><em>repair</em></span>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </p></dd><dt id="id-1.3.4.4.6.6.19"><span class="term">OSD_SCRUB_ERRORS</span></dt><dd><p>
      Recent OSD scrubs have uncovered inconsistencies.
     </p></dd><dt id="id-1.3.4.4.6.6.20"><span class="term">CACHE_POOL_NEAR_FULL</span></dt><dd><p>
      A cache tier pool is nearly full. Full in this context is determined by
      the <span class="emphasis"><em>target_max_bytes</em></span> and
      <span class="emphasis"><em>target_max_objects</em></span> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_objects <em class="replaceable">objects</em></pre></div><p>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </p><p>
      Find more information about cache tiering in
      <a class="xref" href="#cha-ceph-tiered" title="Chapter 11. Cache Tiering">Chapter 11, <em>Cache Tiering</em></a>.
     </p></dd><dt id="id-1.3.4.4.6.6.21"><span class="term">TOO_FEW_PGS</span></dt><dd><p>
      The number of PGs in use is below the configurable threshold of
      <code class="option">mon_pg_warn_min_per_osd</code> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </p><p>
      See
      <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" target="_blank">Placement
      Groups</a> for details on calculating an appropriate number of
      placement groups for your pool.
     </p></dd><dt id="id-1.3.4.4.6.6.22"><span class="term">TOO_MANY_PGS</span></dt><dd><p>
      The number of PGs in use is above the configurable threshold of
      <code class="option">mon_pg_warn_max_per_osd</code> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the Ceph Managers and Ceph Monitors.
     </p><p>
      While the <code class="option">pg_num</code> value for existing pools cannot be
      reduced. The <code class="option">pgp_num</code> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <code class="option">pgp_num</code> value can
      be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">value</em></pre></div></dd><dt id="id-1.3.4.4.6.6.23"><span class="term">SMALLER_PGP_NUM</span></dt><dd><p>
      One or more pools has a <code class="option">pgp_num</code> value less than
      <code class="option">pg_num</code>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <code class="option">pgp_num</code> to match
      <code class="option">pg_num</code>, triggering the data migration, with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">pg_num_value</em></pre></div></dd><dt id="id-1.3.4.4.6.6.24"><span class="term">MANY_OBJECTS_PER_PG</span></dt><dd><p>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration option on the
      monitors.
     </p></dd><dt id="id-1.3.4.4.6.6.25"><span class="term">POOL_APP_NOT_ENABLED¶</span></dt><dd><p>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd pool init <em class="replaceable">pool_name</em></pre></div><p>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool application enable foo</pre></div></dd><dt id="id-1.3.4.4.6.6.26"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <code class="option">mon_pool_quota_crit_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.3.4.4.6.6.27"><span class="term">POOL_NEAR_FULL</span></dt><dd><p>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <code class="option">mon_pool_quota_warn_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.3.4.4.6.6.28"><span class="term">OBJECT_MISPLACED</span></dt><dd><p>
      One or more objects in the cluster are not stored on the node where the
      cluster wants it. This is an indication that data migration caused by a
      recent cluster change has not yet completed. Misplaced data is not a
      dangerous condition in itself. Data consistency is never at risk, and old
      copies of objects are never removed until the desired number of new
      copies (in the desired locations) are present.
     </p></dd><dt id="id-1.3.4.4.6.6.29"><span class="term">OBJECT_UNFOUND</span></dt><dd><p>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, a down OSD can be brought back online that has the more
      recent copy of the unfound object. Candidate OSDs can be identified from
      the peering state for the PG(s) responsible for the unfound object:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.4.4.6.6.30"><span class="term">REQUEST_SLOW</span></dt><dd><p>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph daemon osd.<em class="replaceable">id</em> ops</pre></div><p>
      You can see a summary of the slowest recent requests:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph daemon osd.<em class="replaceable">id</em> dump_historic_ops</pre></div><p>
      You can find the location of an OSD with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd find osd.<em class="replaceable">id</em></pre></div></dd><dt id="id-1.3.4.4.6.6.31"><span class="term">REQUEST_STUCK</span></dt><dd><p>
      One or more OSD requests have been blocked for a longer time, for example
      4096 seconds. This is an indication that either the cluster has been
      unhealthy for an extended period of time (for example not enough running
      OSDs or inactive PGs) or there is some internal problem with the OSD.
     </p></dd><dt id="id-1.3.4.4.6.6.32"><span class="term">PG_NOT_SCRUBBED</span></dt><dd><p>
      One or more PGs have not been scrubbed (see <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>)
      recently. PGs are normally scrubbed every
      <code class="option">mon_scrub_interval</code> seconds, and this warning triggers
      when <code class="option">mon_warn_not_scrubbed</code> such intervals have elapsed
      without a scrub. PGs will not scrub if they are not flagged as clean,
      which may happen if they are misplaced or degraded (see PG_AVAILABILITY
      and PG_DEGRADED above). You can manually initiate a scrub of a clean PG
      with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph pg scrub <em class="replaceable">pgid</em></pre></div></dd><dt id="id-1.3.4.4.6.6.33"><span class="term">PG_NOT_DEEP_SCRUBBED</span></dt><dd><p>
      One or more PGs has not been deep scrubbed (see
      <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>) recently. PGs are normally scrubbed every
      <code class="option">osd_deep_mon_scrub_interval</code> seconds, and this warning
      triggers when <code class="option">mon_warn_not_deep_scrubbed</code> seconds have
      elapsed without a scrub. PGs will not (deep)scrub if they are not flagged
      as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph pg deep-scrub <em class="replaceable">pgid</em></pre></div></dd></dl></div></section><section class="sect1" id="monitor-watch" data-id-title="Watching a Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">Watching a Cluster</span> <a title="Permalink" class="permalink" href="#monitor-watch">#</a></h2></div></div></div><p>
   You can find the immediate state of the cluster using <code class="command">ceph
   -s</code>. For example, a tiny Ceph cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 &gt; max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean</pre></div><p>
   The output provides the following information:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Cluster ID
    </p></li><li class="listitem"><p>
     Cluster health status
    </p></li><li class="listitem"><p>
     The monitor map epoch and the status of the monitor quorum
    </p></li><li class="listitem"><p>
     The OSD map epoch and the status of OSDs
    </p></li><li class="listitem"><p>
     The status of Ceph Managers.
    </p></li><li class="listitem"><p>
     The status of Object Gateways.
    </p></li><li class="listitem"><p>
     The placement group map version
    </p></li><li class="listitem"><p>
     The number of placement groups and pools
    </p></li><li class="listitem"><p>
     The <span class="emphasis"><em>notional</em></span> amount of data stored and the number of
     objects stored; and,
    </p></li><li class="listitem"><p>
     The total amount of data stored.
    </p></li></ul></div><div id="id-1.3.4.4.7.6" data-id-title="How Ceph Calculates Data Usage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: How Ceph Calculates Data Usage</h6><p>
    The <code class="literal">used</code> value reflects the actual amount of raw storage
    used. The <code class="literal">xxx GB / xxx GB</code> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because Ceph creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </p></div><p>
   Other commands that display immediate status information are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">ceph pg stat</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph osd pool stats</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df detail</code>
    </p></li></ul></div><p>
   To get the information updated in real time, put any of these commands
   (including <code class="command">ceph -s</code>) as an argument of the
   <code class="command">watch</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>watch -n 10 'ceph -s'</pre></div><p>
   Press <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>
   when you are tired of watching.
  </p></section><section class="sect1" id="monitor-stats" data-id-title="Checking a Clusters Usage Stats"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.4 </span><span class="title-name">Checking a Cluster's Usage Stats</span> <a title="Permalink" class="permalink" href="#monitor-stats">#</a></h2></div></div></div><p>
   To check a cluster’s data usage and distribution among pools, use the
   <code class="command">ceph df</code> command. To get more details, use <code class="command">ceph
   df detail</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    65886G     45826G        7731M            16
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    data         1      1726M        10        17676G        1629
    rbd          4      5897M        27        22365G        3547
    ecpool       6        69M       0.2        35352G          31
[...]</pre></div><p>
   The <code class="literal">GLOBAL</code> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">SIZE</code>: The overall storage capacity of the cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">AVAIL</code>: The amount of free space available in the
     cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">RAW USED</code>: The amount of raw storage used.
    </p></li><li class="listitem"><p>
     <code class="literal">% RAW USED</code>: The percentage of raw storage used. Use
     this number in conjunction with the <code class="literal">full ratio</code> and
     <code class="literal">near full ratio</code> to ensure that you are not reaching
     your cluster’s capacity. See
     <a class="link" href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit" target="_blank">Storage
     Capacity</a> for additional details.
    </p><div id="id-1.3.4.4.8.5.4.2" data-id-title="Cluster Fill Level" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Cluster Fill Level</h6><p>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </p><p>
      Use the command <code class="command">ceph osd df tree</code> to list the fill
      level of all OSDs.
     </p></div></li></ul></div><p>
   The <code class="literal">POOLS</code> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <span class="emphasis"><em>does not</em></span> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">NAME</code>: The name of the pool.
    </p></li><li class="listitem"><p>
     <code class="literal">ID</code>: The pool ID.
    </p></li><li class="listitem"><p>
     <code class="literal">USED</code>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </p></li><li class="listitem"><p>
     <code class="literal">%USED</code>: The notional percentage of storage used per
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">MAX AVAIL</code>: The maximum available space in the given
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">OBJECTS</code>: The notional number of objects stored per
     pool.
    </p></li></ul></div><div id="id-1.3.4.4.8.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </p></div></section><section class="sect1" id="monitor-osdstatus" data-id-title="Checking OSD Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.5 </span><span class="title-name">Checking OSD Status</span> <a title="Permalink" class="permalink" href="#monitor-osdstatus">#</a></h2></div></div></div><p>
   You can check OSDs to ensure they are up and on by executing:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd dump</pre></div><p>
   You can also view OSDs according to their position in the CRUSH map.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tree</pre></div><p>
   Ceph will print a CRUSH tree with a host, its OSDs, whether they are up
   and their weight.
  </p><div class="verbatim-wrap"><pre class="screen"># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</pre></div></section><section class="sect1" id="storage-bp-monitoring-fullosd" data-id-title="Checking for Full OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.6 </span><span class="title-name">Checking for Full OSDs</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-fullosd">#</a></h2></div></div></div><p>
   Ceph prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <code class="command">mon osd full ratio</code>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <code class="command">mon osd nearfull ratio</code> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </p><p>
   Full OSD nodes will be reported by <code class="command">ceph health</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</pre></div><p>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </p><div id="id-1.3.4.4.10.8" data-id-title="Preventing Full OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Preventing Full OSDs</h6><p>
    After an OSD becomes full—it uses 100% of its disk space—it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Each OSD's disk space (usually mounted under
      <code class="filename">/var/lib/ceph/osd/osd-{1,2..}</code>) needs to be placed on
      a dedicated underlying disk or partition.
     </p></li><li class="listitem"><p>
      Check the Ceph configuration files and make sure that Ceph does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </p></li><li class="listitem"><p>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </p></li></ul></div></div></section><section class="sect1" id="monitor-monstatus" data-id-title="Checking Monitor Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.7 </span><span class="title-name">Checking Monitor Status</span> <a title="Permalink" class="permalink" href="#monitor-monstatus">#</a></h2></div></div></div><p>
   After you start the cluster and before first reading and/or writing data,
   check the Ceph Monitors quorum status. When the cluster is already serving
   requests, check the Ceph Monitors status periodically to ensure that they are
   running.
  </p><p>
   To display the monitor map, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mon stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mon dump</pre></div><p>
   To check the quorum status for the monitor cluster, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph quorum_status</pre></div><p>
   Ceph will return the quorum status. For example, a Ceph cluster
   consisting of three monitors may return the following:
  </p><div class="verbatim-wrap"><pre class="screen">{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</pre></div></section><section class="sect1" id="monitor-pgroupstatus" data-id-title="Checking Placement Group States"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.8 </span><span class="title-name">Checking Placement Group States</span> <a title="Permalink" class="permalink" href="#monitor-pgroupstatus">#</a></h2></div></div></div><p>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <code class="literal">active</code> and
   <code class="literal">clean</code>. For a detailed discussion, refer to
   <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg" target="_blank">Monitoring
   OSDs and Placement Groups.</a>
  </p></section><section class="sect1" id="monitor-adminsocket" data-id-title="Using the Admin Socket"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.9 </span><span class="title-name">Using the Admin Socket</span> <a title="Permalink" class="permalink" href="#monitor-adminsocket">#</a></h2></div></div></div><p>
   The Ceph admin socket allows you to query a daemon via a socket interface.
   By default, Ceph sockets reside under <code class="filename">/var/run/ceph</code>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --admin-daemon /var/run/ceph/<em class="replaceable">socket-name</em></pre></div><p>
   To view the available admin socket commands, execute the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --admin-daemon /var/run/ceph/<em class="replaceable">socket-name</em> help</pre></div><p>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to
   <a class="link" href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config" target="_blank">Viewing
   a Configuration at Runtime</a>for details.
  </p><p>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <code class="command">ceph tell</code>
   <em class="replaceable">daemon-type</em>.<em class="replaceable">id</em>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </p></section></section><section class="chapter" id="monitoring-alerting" data-id-title="Monitoring and Alerting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Monitoring and Alerting</span> <a title="Permalink" class="permalink" href="#monitoring-alerting">#</a></h2></div></div></div><p>
  By default, DeepSea deploys a monitoring and alerting stack on the
  Salt master. It consists of the following components:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Prometheus</strong></span> monitoring and alerting
    toolkit.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Grafana</strong></span> visualization and alerting
    software.
   </p></li><li class="listitem"><p>
    The <code class="systemitem">prometheus-ceph_exporter</code>
    service running on the Salt master.
   </p></li><li class="listitem"><p>
    The <code class="systemitem">prometheus-node_exporter</code>
    service running on all Salt minions.
   </p></li></ul></div><p>
  The Prometheus configuration and <span class="emphasis"><em>scrape</em></span> targets
  (exporting daemons) are setup automatically by DeepSea. DeepSea also
  deploys a list of default alerts, for example <code class="literal">health
  error</code>, <code class="literal">10% OSDs down</code>, or <code class="literal">pgs
  inactive</code>.
 </p><p>
   The Alertmanager handles alerts sent by the Prometheus server. It takes
   care of de-duplicating, grouping, and routing them to the correct receiver.
   It also takes care of silencing of alerts. Alertmanager is configured via
   the command line flags and a configuration file that defines inhibition
   rules, notification routing and notification receivers.
  </p><section class="sect1" id="config-file" data-id-title="Configuration File"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Configuration File</span> <a title="Permalink" class="permalink" href="#config-file">#</a></h2></div></div></div><p>
    Alertmanager's configuration is different for each deployment. Therefore,
    DeepSea does not ship any related defaults. You need to provide your own
    <code class="filename">alertmanager.yml</code> configuration file. The
    <span class="package">alertmanager</span> package by default installs a
    configuration file <code class="filename">/etc/prometheus/alertmanager.yml</code>
    which can serve as an example configuration. If you prefer to have your
    Alertmanager configuration managed by DeepSea, add the following key to
    your pillar, for example to the
    <code class="filename">/srv/pillar/ceph/stack/ceph/minions/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
    file:
   </p><p>
    For a complete example of Alertmanager's configuration file, see
    <a class="xref" href="#app-alerting-default" title="Appendix B. Default Alerts for SUSE Enterprise Storage">Appendix B, <em>Default Alerts for SUSE Enterprise Storage</em></a>.
   </p><div class="verbatim-wrap"><pre class="screen">monitoring:
 alertmanager_config:
   /path/to/your/alertmanager/config.yml</pre></div><p>
    Alertmanager's configuration file is written in the YAML format. It
    follows the scheme described below. Parameters in brackets are optional.
    For non-list parameters the default value is used. The following generic
    placeholders are used in the scheme:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.6.6.1"><span class="term"><em class="replaceable">DURATION</em></span></dt><dd><p>
       A duration matching the regular expression
       <code class="literal">[0-9]+(ms|[smhdwy])</code>
      </p></dd><dt id="id-1.3.4.5.6.6.2"><span class="term"><em class="replaceable">LABELNAME</em></span></dt><dd><p>
       A string matching the regular expression
       <code class="literal">[a-zA-Z_][a-zA-Z0-9_]*</code>
      </p></dd><dt id="id-1.3.4.5.6.6.3"><span class="term"><em class="replaceable">LABELVALUE</em></span></dt><dd><p>
       A string of unicode characters.
      </p></dd><dt id="id-1.3.4.5.6.6.4"><span class="term"><em class="replaceable">FILEPATH</em></span></dt><dd><p>
       A valid path in the current working directory.
      </p></dd><dt id="id-1.3.4.5.6.6.5"><span class="term"><em class="replaceable">BOOLEAN</em></span></dt><dd><p>
       A boolean that can take the values <code class="literal">true</code>
       or <code class="literal">false</code>.
      </p></dd><dt id="id-1.3.4.5.6.6.6"><span class="term"><em class="replaceable">STRING</em></span></dt><dd><p>
       A regular string.
      </p></dd><dt id="id-1.3.4.5.6.6.7"><span class="term"><em class="replaceable">SECRET</em></span></dt><dd><p>
       A regular string that is a secret. For example, a password.
      </p></dd><dt id="id-1.3.4.5.6.6.8"><span class="term"><em class="replaceable">TMPL_STRING</em></span></dt><dd><p>
       A string which is template-expanded before usage.
      </p></dd><dt id="id-1.3.4.5.6.6.9"><span class="term"><em class="replaceable">TMPL_SECRET</em></span></dt><dd><p>
       A secret string which is template-expanded before usage.
      </p></dd></dl></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.7" data-id-title="Global Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.1: </span><span class="title-name">Global Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.7">#</a></h6></div><div class="example-contents"><p>
     Parameters in the <code class="literal">global:</code> configuration are valid in
     all other configuration contexts. They also serve as defaults for other
     configuration sections.
    </p><div class="verbatim-wrap"><pre class="screen">global:
# the time after which an alert is declared resolved if it has not been updated
[ resolve_timeout: <em class="replaceable">DURATION</em> | default = 5m ]

# The default SMTP From header field.
[ smtp_from: <em class="replaceable">TMPL_STRING</em> ]
# The default SMTP smarthost used for sending emails, including port number.
# Port number usually is 25, or 587 for SMTP over TLS
# (sometimes referred to as STARTTLS).
# Example: smtp.example.org:587
[ smtp_smarthost: <em class="replaceable">STRING</em> ]
# The default host name to identify to the SMTP server.
[ smtp_hello: <em class="replaceable">STRING</em> | default = "localhost" ]
[ smtp_auth_username: <em class="replaceable">STRING</em> ]
# SMTP Auth using LOGIN and PLAIN.
[ smtp_auth_password: <em class="replaceable">SECRET</em> ]
# SMTP Auth using PLAIN.
[ smtp_auth_identity: <em class="replaceable">STRING</em> ]
# SMTP Auth using CRAM-MD5.
[ smtp_auth_secret: <em class="replaceable">SECRET</em> ]
# The default SMTP TLS requirement.
[ smtp_require_tls: <em class="replaceable">BOOL</em> | default = true ]

# The API URL to use for Slack notifications.
[ slack_api_url: <em class="replaceable">STRING</em> ]
[ victorops_api_key: <em class="replaceable">STRING</em> ]
[ victorops_api_url: <em class="replaceable">STRING</em> | default = "https://victorops.example.com/integrations/alert/" ]
[ pagerduty_url: <em class="replaceable">STRING</em> | default = "https://pagerduty.example.com/v2/enqueue" ]
[ opsgenie_api_key: <em class="replaceable">STRING</em> ]
[ opsgenie_api_url: <em class="replaceable">STRING</em> | default = "https://opsgenie.example.com/" ]
[ hipchat_api_url: <em class="replaceable">STRING</em> | default = "https://hipchat.example.com/" ]
[ hipchat_auth_token: <em class="replaceable">SECRET</em> ]
[ wechat_api_url: <em class="replaceable">STRING</em> | default = "https://wechat.example.com/cgi-bin/" ]
[ wechat_api_secret: <em class="replaceable">SECRET</em> ]
[ wechat_api_corp_id: <em class="replaceable">STRING</em> ]

# The default HTTP client configuration
[ http_config: <em class="replaceable">HTTP_CONFIG</em> ]

# Files from which custom notification template definitions are read.
# The last component may use a wildcard matcher, e.g. 'templates/*.tmpl'.
templates:
[ - <em class="replaceable">FILEPATH</em> ... ]

# The root node of the routing tree.
route: <em class="replaceable">ROUTE</em>

# A list of notification receivers.
receivers:
- <em class="replaceable">RECEIVER</em> ...

# A list of inhibition rules.
inhibit_rules:
[ - <em class="replaceable">INHIBIT_RULE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.8" data-id-title="ROUTE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.2: </span><span class="title-name"><em class="replaceable">ROUTE</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.8">#</a></h6></div><div class="example-contents"><p>
     A <em class="replaceable">ROUTE</em> block defines a node in a routing
     tree. Unspecified parameters are inherited from its parent node. Every
     alert enters the routing tree at the configured top-level route, which
     needs to match all alerts, then traversing the child nodes. If the
     <code class="option">continue</code> option is set to <code class="literal">false</code>,
     the traversing stops after the first matched child. Setting the option to
     <code class="literal">true</code> on a matched node, the alert continues to match
     against subsequent siblings. If an alert does not match any children of a
     node, the alert is handled based on the configuration parameters of the
     current node.
    </p><div class="verbatim-wrap"><pre class="screen">[ receiver: <em class="replaceable">STRING</em> ]
[ group_by: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]

# If an alert should continue matching subsequent sibling nodes.
[ continue: <em class="replaceable">BOOLEAN</em> | default = false ]

# A set of equality matchers an alert has to fulfill to match a node.
match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]

# A set of regex-matchers an alert has to fulfill to match a node.
match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Time to wait before sending a notification for a group of alerts.
[ group_wait: <em class="replaceable">DURATION</em> | default = 30s ]

# Time to wait before sending a notification about new alerts
# added to a group of alerts for which an initial notification has
# already been sent.
[ group_interval: <em class="replaceable">DURATION</em> | default = 5m ]

# Time to wait before re-sending a notification
[ repeat_interval: <em class="replaceable">DURATION</em> | default = 4h ]

# Possible child routes.
routes:
 [ - <em class="replaceable">ROUTE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.9" data-id-title="INHIBIT_RULE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.3: </span><span class="title-name"><em class="replaceable">INHIBIT_RULE</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.9">#</a></h6></div><div class="example-contents"><p>
     An inhibition rule mutes a target alert that matches a set of matchers
     when a source alert exists that matches another set of matchers. Both
     alerts need to share the same label values for the label names in the
     <code class="option">equal</code> list.
    </p><p>
     Alerts can match and therefore inhibit themselves. Do not write
     inhibition rules where an alert matches both source and target.
    </p><div class="verbatim-wrap"><pre class="screen"># Matchers that need to be fulfilled for the alerts to be muted.
target_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
target_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Matchers for which at least one alert needs to exist so that the
# inhibition occurs.
source_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
source_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Labels with an equal value in the source and target
# alert for the inhibition to take effect.
[ equal: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.10" data-id-title="HTTP_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.4: </span><span class="title-name"><em class="replaceable">HTTP_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.10">#</a></h6></div><div class="example-contents"><p>
     <em class="replaceable">HTTP_CONFIG</em> configures the HTTP client used by
     the receiver to communicate with API services.
    </p><p>
     Note that <code class="option">basic_auth</code>, <code class="option">bearer_token</code> and
     <code class="option">bearer_token_file</code> options are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Sets the 'Authorization' header with the user name and password.
basic_auth:
[ username: <em class="replaceable">STRING</em> ]
[ password: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token.
[ bearer_token: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token read from a file.
[ bearer_token_file: <em class="replaceable">FILEPATH</em> ]

# TLS settings.
tls_config:
# CA certificate to validate the server certificate with.
[ ca_file: <em class="replaceable">FILEPATH</em> ]
# Certificate and key files for client cert authentication to the server.
[ cert_file: <em class="replaceable">FILEPATH</em> ]
[ key_file: <em class="replaceable">FILEPATH</em> ]
# ServerName extension to indicate the name of the server.
# http://tools.ietf.org/html/rfc4366#section-3.1
[ server_name: <em class="replaceable">STRING</em> ]
# Disable validation of the server certificate.
[ insecure_skip_verify: <em class="replaceable">BOOLEAN</em> | default = false]

# Optional proxy URL.
[ proxy_url: <em class="replaceable">STRING</em> ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.11" data-id-title="RECEIVER"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.5: </span><span class="title-name"><em class="replaceable">RECEIVER</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.11">#</a></h6></div><div class="example-contents"><p>
     Receiver is a named configuration for one or more notification
     integrations.
    </p><p>
     Instead of adding new receivers, we recommend implementing custom
     notification integrations using the webhook receiver (see
     <a class="xref" href="#alert-webhook" title="WEBHOOK_CONFIG">Example 5.15, “<em class="replaceable">WEBHOOK_CONFIG</em>”</a>).
    </p><div class="verbatim-wrap"><pre class="screen"># The unique name of the receiver.
name: <em class="replaceable">STRING</em>

# Configurations for several notification integrations.
email_configs:
[ - <em class="replaceable">EMAIL_CONFIG</em>, ... ]
hipchat_configs:
[ - <em class="replaceable">HIPCHAT_CONFIG</em>, ... ]
pagerduty_configs:
[ - <em class="replaceable">PAGERDUTY_CONFIG</em>, ... ]
pushover_configs:
[ - <em class="replaceable">PUSHOVER_CONFIG</em>, ... ]
slack_configs:
[ - <em class="replaceable">SLACK_CONFIG</em>, ... ]
opsgenie_configs:
[ - <em class="replaceable">OPSGENIE_CONFIG</em>, ... ]
webhook_configs:
[ - <em class="replaceable">WEBHOOK_CONFIG</em>, ... ]
victorops_configs:
[ - <em class="replaceable">VICTOROPS_CONFIG</em>, ... ]
wechat_configs:
[ - <em class="replaceable">WECHAT_CONFIG</em>, ... ]</pre></div></div></div></div><div class="example" id="id-1.3.4.5.6.12" data-id-title="EMAIL_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.6: </span><span class="title-name"><em class="replaceable">EMAIL_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.12">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The email address to send notifications to.
to: <em class="replaceable">TMPL_STRING</em>

# The sender address.
[ from: <em class="replaceable">TMPL_STRING</em> | default = global.smtp_from ]

# The SMTP host through which emails are sent.
[ smarthost: <em class="replaceable">STRING</em> | default = global.smtp_smarthost ]

# The host name to identify to the SMTP server.
[ hello: <em class="replaceable">STRING</em> | default = global.smtp_hello ]

# SMTP authentication details.
[ auth_username: <em class="replaceable">STRING</em> | default = global.smtp_auth_username ]
[ auth_password: <em class="replaceable">SECRET</em> | default = global.smtp_auth_password ]
[ auth_secret: <em class="replaceable">SECRET</em> | default = global.smtp_auth_secret ]
[ auth_identity: <em class="replaceable">STRING</em> | default = global.smtp_auth_identity ]

# The SMTP TLS requirement.
[ require_tls: <em class="replaceable">BOOL</em> | default = global.smtp_require_tls ]

# The HTML body of the email notification.
[ html: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "email.default.html" . }}' ]
# The text body of the email notification.
[ text: <em class="replaceable">TMPL_STRING</em> ]

# Further headers email header key/value pairs. Overrides any headers
# previously set by the notification implementation.
[ headers: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.13" data-id-title="HIPCHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.7: </span><span class="title-name"><em class="replaceable">HIPCHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.13">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The HipChat Room ID.
room_id: <em class="replaceable">TMPL_STRING</em>
# The authentication token.
[ auth_token: <em class="replaceable">SECRET</em> | default = global.hipchat_auth_token ]
# The URL to send API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.hipchat_api_url ]

# A label to be shown in addition to the sender's name.
[ from:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.from" . }}' ]
# The message body.
[ message:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.message" . }}' ]
# Whether this message will trigger a user notification.
[ notify:  <em class="replaceable">BOOLEAN</em> | default = false ]
# Determines how the message is treated by the alertmanager and rendered inside HipChat. Valid values are 'text' and 'html'.
[ message_format:  <em class="replaceable">STRING</em> | default = 'text' ]
# Background color for message.
[ color:  <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}red{{ else }}green{{ end }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.6.14" data-id-title="PAGERDUTY_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.8: </span><span class="title-name"><em class="replaceable">PAGERDUTY_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.14">#</a></h6></div><div class="example-contents"><p>
     The <code class="option">routing_key</code> and <code class="option">service_key</code> options
     are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The PagerDuty integration key (when using 'Events API v2').
routing_key: <em class="replaceable">TMPL_SECRET</em>
# The PagerDuty integration key (when using 'Prometheus').
service_key: <em class="replaceable">TMPL_SECRET</em>

# The URL to send API requests to.
[ url: <em class="replaceable">STRING</em> | default = global.pagerduty_url ]

# The client identification of the Alertmanager.
[ client:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.client" . }}' ]
# A backlink to the notification sender.
[ client_url:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.clientURL" . }}' ]

# The incident description.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.description" .}}' ]

# Severity of the incident.
[ severity: <em class="replaceable">TMPL_STRING</em> | default = 'error' ]

# A set of arbitrary key/value pairs that provide further details.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } | default = {
 firing:       '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
 resolved:     '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
 num_firing:   '{{ .Alerts.Firing | len }}'
 num_resolved: '{{ .Alerts.Resolved | len }}'
} ]

# The HTTP client's configuration.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div></div><div class="example" id="id-1.3.4.5.6.15" data-id-title="PUSHOVER_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.9: </span><span class="title-name"><em class="replaceable">PUSHOVER_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.15">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The recipient user key.
user_key: <em class="replaceable">SECRET</em>

# Registered application’s API token.
token: <em class="replaceable">SECRET</em>

# Notification title.
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.title" . }}' ]

# Notification message.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.message" . }}' ]

# A supplementary URL displayed together with the message.
[ url: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.url" . }}' ]

# Priority.
[ priority: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}2{{ else }}0{{ end }}' ]

# How often the Pushover servers will send the same notification (at least 30 seconds).
[ retry: <em class="replaceable">DURATION</em> | default = 1m ]

# How long your notification will continue to be retried (unless the user
# acknowledges the notification).
[ expire: <em class="replaceable">DURATION</em> | default = 1h ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.16" data-id-title="SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.10: </span><span class="title-name"><em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.16">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The Slack webhook URL.
[ api_url: <em class="replaceable">SECRET</em> | default = global.slack_api_url ]

# The channel or user to send notifications to.
channel: <em class="replaceable">TMPL_STRING</em>

# API request data as defined by the Slack webhook API.
[ icon_emoji: <em class="replaceable">TMPL_STRING</em> ]
[ icon_url: <em class="replaceable">TMPL_STRING</em> ]
[ link_names: <em class="replaceable">BOOLEAN</em> | default = false ]
[ username: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.username" . }}' ]
# The following parameters define the attachment.
actions:
[ <em class="replaceable">ACTION_CONFIG</em> ... ]
[ color: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}' ]
[ fallback: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.fallback" . }}' ]
fields:
[ <em class="replaceable">FIELD_CONFIG</em> ... ]
[ footer: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.footer" . }}' ]
[ pretext: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.pretext" . }}' ]
[ short_fields: <em class="replaceable">BOOLEAN</em> | default = false ]
[ text: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.text" . }}' ]
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.title" . }}' ]
[ title_link: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.titlelink" . }}' ]
[ image_url: <em class="replaceable">TMPL_STRING</em> ]
[ thumb_url: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.17" data-id-title="ACTION_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.11: </span><span class="title-name"><em class="replaceable">ACTION_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.17">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Provide a button to tell Slack you want to render a button.
type: <em class="replaceable">TMPL_STRING</em>
# Label for the button.
text: <em class="replaceable">TMPL_STRING</em>
# http or https URL to deliver users to. If you specify invalid URLs, the message will be posted with no button.
url: <em class="replaceable">TMPL_STRING</em>
#  If set to 'primary', the button will be green, indicating the best forward action to take
#  'danger' turns the button red, indicating a destructive action.
[ style: <em class="replaceable">TMPL_STRING</em> [ default = '' ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.18" data-id-title="FIELD_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.12: </span><span class="title-name"><em class="replaceable">FIELD_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.18">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># A bold heading without markup above the <code class="option">value</code> text.
title: <em class="replaceable">TMPL_STRING</em>
# The text of the field. It can span across several lines.
value: <em class="replaceable">TMPL_STRING</em>
# A flag indicating if <code class="option">value</code> is short enough to be displayed together with other values.
[ short: <em class="replaceable">BOOLEAN</em> | default = slack_config.short_fields ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.19" data-id-title="OPSGENIE_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.13: </span><span class="title-name"><em class="replaceable">OPSGENIE_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.19">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key to use with the OpsGenie API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.opsgenie_api_key ]

# The host to send OpsGenie API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.opsgenie_api_url ]

# Alert text (maximum is 130 characters).
[ message: <em class="replaceable">TMPL_STRING</em> ]

# A description of the incident.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.description" . }}' ]

# A backlink to the sender.
[ source: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.source" . }}' ]

# A set of arbitrary key/value pairs that provide further detail.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]

# Comma separated list of team responsible for notifications.
[ teams: <em class="replaceable">TMPL_STRING</em> ]

# Comma separated list of tags attached to the notifications.
[ tags: <em class="replaceable">TMPL_STRING</em> ]

# Additional alert note.
[ note: <em class="replaceable">TMPL_STRING</em> ]

# Priority level of alert, one of P1, P2, P3, P4, and P5.
[ priority: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.4.5.6.20" data-id-title="VICTOROPS_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.14: </span><span class="title-name"><em class="replaceable">VICTOROPS_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.20">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key for talking to the VictorOps API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.victorops_api_key ]

# The VictorOps API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.victorops_api_url ]

# A key used to map the alert to a team.
routing_key: <em class="replaceable">TMPL_STRING</em>

# Describes the behavior of the alert (one of 'CRITICAL', 'WARNING', 'INFO').
[ message_type: <em class="replaceable">TMPL_STRING</em> | default = 'CRITICAL' ]

# Summary of the alerted problem.
[ entity_display_name: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.entity_display_name" . }}' ]

# Long explanation of the alerted problem.
[ state_message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.state_message" . }}' ]

# The monitoring tool the state message is from.
[ monitoring_tool: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.monitoring_tool" . }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="alert-webhook" data-id-title="WEBHOOK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.15: </span><span class="title-name"><em class="replaceable">WEBHOOK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#alert-webhook">#</a></h6></div><div class="example-contents"><p>
     You can utilize the webhook receiver to configure a generic receiver.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The endpoint for sending HTTP POST requests.
url: <em class="replaceable">STRING</em>

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div><p>
     Alertmanager sends HTTP POST requests in the following JSON format:
    </p><div class="verbatim-wrap"><pre class="screen">{
 "version": "4",
 "groupKey": <em class="replaceable">STRING</em>, // identifycation of the group of alerts (to deduplicate)
 "status": "&lt;resolved|firing&gt;",
 "receiver": <em class="replaceable">STRING</em>,
 "groupLabels": <em class="replaceable">OBJECT</em>,
 "commonLabels": <em class="replaceable">OBJECT</em>,
 "commonAnnotations": <em class="replaceable">OBJECT</em>,
 "externalURL": <em class="replaceable">STRING</em>, // backlink to Alertmanager.
 "alerts": [
   {
     "status": "&lt;resolved|firing&gt;",
     "labels": <em class="replaceable">OBJECT</em>,
     "annotations": <em class="replaceable">OBJECT</em>,
     "startsAt": "&lt;rfc3339&gt;",
     "endsAt": "&lt;rfc3339&gt;",
     "generatorURL": <em class="replaceable">STRING</em> // identifies the entity that caused the alert
   },
   ...
 ]
}</pre></div><p>
     The webhook receiver allows for integration with the following
     notification mechanisms:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       DingTalk (https://github.com/timonwong/prometheus-webhook-dingtalk)
      </p></li><li class="listitem"><p>
       IRC Bot (https://github.com/multimfi/bot)
      </p></li><li class="listitem"><p>
       JIRAlert (https://github.com/free/jiralert)
      </p></li><li class="listitem"><p>
       Phabricator / Maniphest (https://github.com/knyar/phalerts)
      </p></li><li class="listitem"><p>
       prom2teams: forwards notifications to Microsoft Teams
       (https://github.com/idealista/prom2teams)
      </p></li><li class="listitem"><p>
       SMS: supports multiple providers
       (https://github.com/messagebird/sachet)
      </p></li><li class="listitem"><p>
       Telegram bot (https://github.com/inCaller/prometheus_bot)
      </p></li></ul></div></div></div></div><div class="example" id="id-1.3.4.5.6.22" data-id-title="WECHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.16: </span><span class="title-name"><em class="replaceable">WECHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.22">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The API key to use for the WeChat API.
[ api_secret: <em class="replaceable">SECRET</em> | default = global.wechat_api_secret ]

# The WeChat API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.wechat_api_url ]

# The corp id used to authenticate.
[ corp_id: <em class="replaceable">STRING</em> | default = global.wechat_api_corp_id ]

# API request data as defined by the WeChat API.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "wechat.default.message" . }}' ]
[ agent_id: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.agent_id" . }}' ]
[ to_user: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_user" . }}' ]
[ to_party: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_party" . }}' ]
[ to_tag: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_tag" . }}' ]</pre></div></div></div></section><section class="sect1" id="custom-alerts" data-id-title="Custom Alerts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Custom Alerts</span> <a title="Permalink" class="permalink" href="#custom-alerts">#</a></h2></div></div></div><p>
    You can define your custom alert conditions to send notifications to an
    external service. Prometheus uses its own expression language for defining
    custom alerts. Following is an example of a rule with an alert:
   </p><div class="verbatim-wrap"><pre class="screen">groups:
- name: example
 rules:
  # alert on high deviation from average PG count
  - alert: high pg count deviation
   expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
   for: 5m
   labels:
    severity: warning
    type: ses_default
   annotations:
   description: &gt;
    OSD {{ $labels.osd }} deviates by more then 30% from average PG count</pre></div><p>
    The optional <code class="literal">for</code> clause specifies the time Prometheus
    waits between first encountering a new expression output vector
    element and counting an alert as firing. In this case, Prometheus
    checks that the alert continues to be active for 5 minutes before firing
    the alert. Elements in a pending state are active, but not firing yet.
   </p><p>
    The <code class="literal">labels</code> clause specifies a set of additional labels
    attached to the alert. Conflicting labels will be overwritten. Labels can
    be templated (see <a class="xref" href="#alertmanager-templates" title="5.2.1. Templates">Section 5.2.1, “Templates”</a> for more
    details on templating).
   </p><p>
    The <code class="literal">annotations</code> clause specifies informational labels.
    You can use them to store additional information, for example alert
    descriptions or runbook links. Annotations can be templated (see
    <a class="xref" href="#alertmanager-templates" title="5.2.1. Templates">Section 5.2.1, “Templates”</a> for more details on templating).
   </p><p>
    To add your custom alerts to SUSE Enterprise Storage, either...
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      place your YAML files with custom alerts in the
      <code class="filename">/etc/prometheus/alerts</code> directory
     </p></li></ul></div><p>
    or
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      provide a list of paths to your custom alert files in the pillar under
      the <code class="option">monitoring:custom_alerts</code> key. DeepSea Stage 2 or
      the <code class="command">salt <em class="replaceable">SALT_MASTER</em> state.apply
      ceph.monitoring.prometheus</code> command adds your alert files
      in the right place.
     </p><div class="complex-example"><div class="example" id="id-1.3.4.5.7.10.1.2" data-id-title="Adding Custom Alerts to SUSE Enterprise Storage"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.17: </span><span class="title-name">Adding Custom Alerts to SUSE Enterprise Storage </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.7.10.1.2">#</a></h6></div><div class="example-contents"><p>
       A file with custom alerts is in
       <code class="filename">/root/my_alerts/my_alerts.yml</code> on the Salt master.
       If you add
      </p><div class="verbatim-wrap"><pre class="screen">monitoring:
 custom_alerts:
   - /root/my_alerts/my_alerts.yml</pre></div><p>
       to the
       <code class="filename">/srv/pillar/ceph/cluster/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
       file, DeepSea creates the
       <code class="filename">/etc/prometheus/alerts/my_alerts.yml</code> file and
       restarts Prometheus.
      </p></div></div></div></li></ul></div><section class="sect2" id="alertmanager-templates" data-id-title="Templates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.1 </span><span class="title-name">Templates</span> <a title="Permalink" class="permalink" href="#alertmanager-templates">#</a></h3></div></div></div><p>
     You can use templates for label and annotation values. The
     <code class="varname">$labels</code> variable includes the label key and value pairs of
     an alert instance, while <code class="varname">$value</code> holds the evaluated
     value of an alert instance.
    </p><p>
     The following example inserts a firing element label and value:
    </p><div class="verbatim-wrap"><pre class="screen">{{ $labels.<em class="replaceable">LABELNAME</em> }}
{{ $value }}</pre></div></section><section class="sect2" id="id-1.3.4.5.7.12" data-id-title="Inspecting Alerts at Runtime"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.2 </span><span class="title-name">Inspecting Alerts at Runtime</span> <a title="Permalink" class="permalink" href="#id-1.3.4.5.7.12">#</a></h3></div></div></div><p>
     If you need to verify which alerts are active, you have several options:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Navigate to the <span class="guimenu">Alerts</span> tab of Prometheus. It
       shows you the exact label sets for which defined alerts are active.
       Prometheus also stores synthetic time series for pending and firing
       alerts. They have the following form:
      </p><div class="verbatim-wrap"><pre class="screen">ALERTS{alertname="<em class="replaceable">ALERT_NAME</em>", alertstate="pending|firing", <em class="replaceable">ADDITIONAL_ALERT_LABELS</em>}</pre></div><p>
       The sample value is 1 if the alert is active (pending or firing). The
       series is marked <code class="literal">stale</code> when the alert is inactive.
      </p></li><li class="listitem"><p>
       In the Prometheus web interface at the URL address
       http://<em class="replaceable">PROMETHEUS_HOST_IP</em>:9090/alerts,
       inspect alerts and their state (<code class="literal">INACTIVE</code>,
       <code class="literal">PENDING</code> or <code class="literal">FIRING</code>).
      </p></li><li class="listitem"><p>
       In the Alertmanager web interface at the URL address
       http://:<em class="replaceable">PROMETHEUS_HOST_IP</em>9093/#/alerts,
       inspect alerts and silence them if desired.
      </p></li></ul></div></section></section></section><section class="chapter" id="cha-storage-cephx" data-id-title="Authentication with cephx"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span> <a title="Permalink" class="permalink" href="#cha-storage-cephx">#</a></h2></div></div></div><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p><div id="id-1.3.4.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   The <code class="systemitem">cephx</code> protocol does not address data encryption in transport, such as
   TLS/SSL.
  </p></div><section class="sect1" id="storage-cephx-arch" data-id-title="Authentication Architecture"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Authentication Architecture</span> <a title="Permalink" class="permalink" href="#storage-cephx-arch">#</a></h2></div></div></div><p>
   <code class="systemitem">cephx</code> uses shared secret keys for authentication, meaning both the client
   and Ceph Monitors have a copy of the client’s secret key. The authentication
   protocol enables both parties to prove to each other that they have a copy
   of the key without actually revealing it. This provides mutual
   authentication, which means the cluster is sure the user possesses the
   secret key, and the user is sure that the cluster has a copy of the secret
   key as well.
  </p><p>
   A key scalability feature of Ceph is to avoid a centralized interface to
   the Ceph object store. This means that Ceph clients can interact with
   OSDs directly. To protect data, Ceph provides its <code class="systemitem">cephx</code> authentication
   system, which authenticates Ceph clients.
  </p><p>
   Each monitor can authenticate clients and distribute keys, so there is no
   single point of failure or bottleneck when using <code class="systemitem">cephx</code>. The monitor
   returns an authentication data structure that contains a session key for use
   in obtaining Ceph services. This session key is itself encrypted with the
   client’s permanent secret key, so that only the client can request
   services from the Ceph monitors. The client then uses the session key to
   request its desired services from the monitor, and the monitor provides the
   client with a ticket that will authenticate the client to the OSDs that
   actually handle data. Ceph monitors and OSDs share a secret, so the client
   can use the ticket provided by the monitor with any OSD or metadata server
   in the cluster. <code class="systemitem">cephx</code> tickets expire, so an attacker cannot use an expired
   ticket or session key obtained wrongfully.
  </p><p>
   To use <code class="systemitem">cephx</code>, an administrator must setup clients/users first. In the
   following diagram, the
   <code class="systemitem">client.admin</code> user invokes
   <code class="command">ceph auth get-or-create-key</code> from the command line to
   generate a user name and secret key. Ceph’s <code class="command">auth</code>
   subsystem generates the user name and key, stores a copy with the monitor(s)
   and transmits the user’s secret back to the
   <code class="systemitem">client.admin</code> user. This means that
   the client and the monitor share a secret key.
  </p><div class="figure" id="id-1.3.4.6.5.6"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring.png" target="_blank"><img src="images/cephx_keyring.png" width="" alt="Basic cephx Authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.1: </span><span class="title-name">Basic <code class="systemitem">cephx</code> Authentication </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.6">#</a></h6></div></div><p>
   To authenticate with the monitor, the client passes the user name to the
   monitor. The monitor generates a session key and encrypts it with the secret
   key associated with the user name and transmits the encrypted ticket back to
   the client. The client then decrypts the data with the shared secret key to
   retrieve the session key. The session key identifies the user for the
   current session. The client then requests a ticket related to the user,
   which is signed by the session key. The monitor generates a ticket, encrypts
   it with the user’s secret key and transmits it back to the client. The
   client decrypts the ticket and uses it to sign requests to OSDs and metadata
   servers throughout the cluster.
  </p><div class="figure" id="id-1.3.4.6.5.8"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring2.png" target="_blank"><img src="images/cephx_keyring2.png" width="" alt="cephx Authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.2: </span><span class="title-name"><code class="systemitem">cephx</code> Authentication </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.8">#</a></h6></div></div><p>
   The <code class="systemitem">cephx</code> protocol authenticates ongoing communications between the client
   machine and the Ceph servers. Each message sent between a client and a
   server after the initial authentication is signed using a ticket that the
   monitors, OSDs, and metadata servers can verify with their shared secret.
  </p><div class="figure" id="id-1.3.4.6.5.10"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring3.png" target="_blank"><img src="images/cephx_keyring3.png" width="" alt="cephx Authentication - MDS and OSD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.3: </span><span class="title-name"><code class="systemitem">cephx</code> Authentication - MDS and OSD </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.5.10">#</a></h6></div></div><div id="id-1.3.4.6.5.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The protection offered by this authentication is between the Ceph client
    and the Ceph cluster hosts. The authentication is not extended beyond the
    Ceph client. If the user accesses the Ceph client from a remote host,
    Ceph authentication is not applied to the connection between the user’s
    host and the client host.
   </p></div></section><section class="sect1" id="storage-cephx-keymgmt" data-id-title="Key Management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Key Management</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt">#</a></h2></div></div></div><p>
   This section describes Ceph client users and their authentication and
   authorization with the Ceph storage cluster. <span class="emphasis"><em>Users</em></span>
   are either individuals or system actors such as applications, which use
   Ceph clients to interact with the Ceph storage cluster daemons.
  </p><p>
   When Ceph runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user (usually via the command line). If you do not
   specify a user name, Ceph will use
   <code class="systemitem">client.admin</code> as the default user
   name. If you do not specify a keyring, Ceph will look for a keyring via
   the keyring setting in the Ceph configuration file. For example, if you
   execute the <code class="command">ceph health</code> command without specifying a user
   name or keyring, Ceph interprets the command like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</pre></div><p>
   Alternatively, you may use the <code class="literal">CEPH_ARGS</code> environment
   variable to avoid re-entering the user name and secret.
  </p><section class="sect2" id="storage-cephx-keymgmt-backgrnd" data-id-title="Background Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Background Information</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-backgrnd">#</a></h3></div></div></div><p>
    Regardless of the type of Ceph client (for example, block device, object
    storage, file system, native API), Ceph stores all data as objects within
    <span class="emphasis"><em>pools</em></span>. Ceph users need to have access to pools in
    order to read and write data. Additionally, Ceph users must have execute
    permissions to use Ceph's administrative commands. The following concepts
    will help you understand Ceph user management.
   </p><section class="sect3" id="id-1.3.4.6.6.6.3" data-id-title="User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1.1 </span><span class="title-name">User</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.6.3">#</a></h4></div></div></div><p>
     A user is either an individual or a system actor such as an application.
     Creating users allows you to control who (or what) can access your Ceph
     storage cluster, its pools, and the data within pools.
    </p><p>
     Ceph uses <span class="emphasis"><em>types</em></span> of users. For the purposes of user
     management, the type will always be <code class="literal">client</code>. Ceph
     identifies users in period (.) delimited form, consisting of the user type
     and the user ID. For example, <code class="literal">TYPE.ID</code>,
     <code class="literal">client.admin</code>, or <code class="literal">client.user1</code>. The
     reason for user typing is that Ceph monitors, OSDs, and metadata servers
     also use the cephx protocol, but they are not clients. Distinguishing the
     user type helps to distinguish between client users and other users,
     streamlining access control, user monitoring, and traceability.
    </p><div id="id-1.3.4.6.6.6.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      A Ceph storage cluster user is not the same as a Ceph object storage
      user or a Ceph file system user. The Ceph Object Gateway uses a Ceph storage
      cluster user to communicate between the gateway daemon and the storage
      cluster, but the gateway has its own user management functionality for
      end users. The Ceph file system uses POSIX semantics. The user space
      associated with it is not the same as a Ceph storage cluster user.
     </p></div></section><section class="sect3" id="id-1.3.4.6.6.6.4" data-id-title="Authorization and Capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1.2 </span><span class="title-name">Authorization and Capabilities</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.6.4">#</a></h4></div></div></div><p>
     Ceph uses the term 'capabilities' (caps) to describe authorizing an
     authenticated user to exercise the functionality of the monitors, OSDs,
     and metadata servers. Capabilities can also restrict access to data within
     a pool or pool namespace. A Ceph administrative user sets a user's
     capabilities when creating or updating a user.
    </p><p>
     Capability syntax follows the form:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">daemon-type</em> 'allow <em class="replaceable">capability</em>' [...]</pre></div><p>
     Following is a list of capabilities for each service type:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.6.6.4.6.1"><span class="term">Monitor capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code> and <code class="literal">allow profile
        <em class="replaceable">cap</em></code>.
       </p><div class="verbatim-wrap"><pre class="screen">mon 'allow rwx'
mon 'allow profile osd'</pre></div></dd><dt id="id-1.3.4.6.6.6.4.6.2"><span class="term">OSD capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code>, <code class="literal">class-read</code>,
        <code class="literal">class-write</code> and <code class="literal">profile osd</code>.
        Additionally, OSD capabilities also allow for pool and namespace
        settings.
       </p><div class="verbatim-wrap"><pre class="screen">osd 'allow <em class="replaceable">capability</em>' [pool=<em class="replaceable">poolname</em>] [namespace=<em class="replaceable">namespace-name</em>]</pre></div></dd><dt id="id-1.3.4.6.6.6.4.6.3"><span class="term">MDS capability</span></dt><dd><p>
        simply requires <code class="literal">allow</code>, or blank.
       </p><div class="verbatim-wrap"><pre class="screen">mds 'allow'</pre></div></dd></dl></div><p>
     The following entries describe each capability:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.6.6.4.8.1"><span class="term">allow</span></dt><dd><p>
        Precedes access settings for a daemon. Implies <code class="literal">rw</code>
        for MDS only.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.2"><span class="term">r</span></dt><dd><p>
        Gives the user read access. Required with monitors to retrieve the
        CRUSH map.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.3"><span class="term">w</span></dt><dd><p>
        Gives the user write access to objects.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.4"><span class="term">x</span></dt><dd><p>
        Gives the user the capability to call class methods (both read and
        write) and to conduct <code class="literal">auth</code> operations on monitors.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.5"><span class="term">class-read</span></dt><dd><p>
        Gives the user the capability to call class read methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.6"><span class="term">class-write</span></dt><dd><p>
        Gives the user the capability to call class write methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.7"><span class="term">*</span></dt><dd><p>
        Gives the user read, write, and execute permissions for a particular
        daemon/pool, and the ability to execute admin commands.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.8"><span class="term">profile osd</span></dt><dd><p>
        Gives a user permissions to connect as an OSD to other OSDs or
        monitors. Conferred on OSDs to enable OSDs to handle replication
        heartbeat traffic and status reporting.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.9"><span class="term">profile mds</span></dt><dd><p>
        Gives a user permissions to connect as an MDS to other MDSs or
        monitors.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.10"><span class="term">profile bootstrap-osd</span></dt><dd><p>
        Gives a user permissions to bootstrap an OSD. Delegated to deployment
        tools so that they have permissions to add keys when bootstrapping an
        OSD.
       </p></dd><dt id="id-1.3.4.6.6.6.4.8.11"><span class="term">profile bootstrap-mds</span></dt><dd><p>
        Gives a user permissions to bootstrap a metadata server. Delegated to
        deployment tools so they have permissions to add keys when
        bootstrapping a metadata server.
       </p></dd></dl></div></section></section><section class="sect2" id="storage-cephx-keymgmt-usermgmt" data-id-title="Managing Users"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.2 </span><span class="title-name">Managing Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt">#</a></h3></div></div></div><p>
    User management functionality provides Ceph cluster administrators with
    the ability to create, update, and delete users directly in the Ceph
    cluster.
   </p><p>
    When you create or delete users in the Ceph cluster, you may need to
    distribute keys to clients so that they can be added to keyrings. See
    <a class="xref" href="#storage-cephx-keymgmt-keyringmgmt" title="6.2.3. Keyring Management">Section 6.2.3, “Keyring Management”</a> for details.
   </p><section class="sect3" id="id-1.3.4.6.6.7.4" data-id-title="Listing Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.1 </span><span class="title-name">Listing Users</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.4">#</a></h4></div></div></div><p>
     To list the users in your cluster, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth list</pre></div><p>
     Ceph will list all users in your cluster. For example, in a cluster with
     two nodes, <code class="command">ceph auth list</code> output looks similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd</pre></div><div id="id-1.3.4.6.6.7.4.6" data-id-title="TYPE.ID Notation" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: TYPE.ID Notation</h6><p>
      Note that the <code class="literal">TYPE.ID</code> notation for users applies such
      that <code class="literal">osd.0</code> specifies a user of type
      <code class="literal">osd</code> and its ID is <code class="literal">0</code>.
      <code class="literal">client.admin</code> is a user of type
      <code class="literal">client</code> and its ID is <code class="literal">admin</code>. Note
      also that each entry has a <code class="literal">key:
      <em class="replaceable">value</em></code> entry, and one or more
      <code class="literal">caps:</code> entries.
     </p><p>
      You may use the <code class="option">-o <em class="replaceable">filename</em></code>
      option with <code class="command">ceph auth list</code> to save the output to a
      file.
     </p></div></section><section class="sect3" id="id-1.3.4.6.6.7.5" data-id-title="Getting Information about Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.2 </span><span class="title-name">Getting Information about Users</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.5">#</a></h4></div></div></div><p>
     To retrieve a specific user, key, and capabilities, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get <em class="replaceable">TYPE.ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get client.admin
exported keyring for client.admin
[client.admin]
	key = AQA19uZUqIwkHxAAFuUwvq0eJD4S173oFRxe0g==
	caps mds = "allow"
	caps mon = "allow *"
 caps osd = "allow *"</pre></div><p>
     Developers may also execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth export <em class="replaceable">TYPE.ID</em></pre></div><p>
     The <code class="command">auth export</code> command is identical to <code class="command">auth
     get</code>, but also prints the internal authentication ID.
    </p></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-useradd" data-id-title="Adding Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.3 </span><span class="title-name">Adding Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-useradd">#</a></h4></div></div></div><p>
     Adding a user creates a user name (<code class="literal">TYPE.ID</code>), a secret
     key, and any capabilities included in the command you use to create the
     user.
    </p><p>
     A user's key enables the user to authenticate with the Ceph storage
     cluster. The user's capabilities authorize the user to read, write, or
     execute on Ceph monitors (mon), Ceph OSDs (osd), or Ceph metadata
     servers (mds).
    </p><p>
     There are a few commands available to add a user:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.6.7.6.5.1"><span class="term"><code class="command">ceph auth add</code></span></dt><dd><p>
        This command is the canonical way to add a user. It will create the
        user, generate a key, and add any specified capabilities.
       </p></dd><dt id="id-1.3.4.6.6.7.6.5.2"><span class="term"><code class="command">ceph auth get-or-create</code></span></dt><dd><p>
        This command is often the most convenient way to create a user, because
        it returns a keyfile format with the user name (in brackets) and the
        key. If the user already exists, this command simply returns the user
        name and key in the keyfile format. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd><dt id="id-1.3.4.6.6.7.6.5.3"><span class="term"><code class="command">ceph auth get-or-create-key</code></span></dt><dd><p>
        This command is a convenient way to create a user and return the user's
        key (only). This is useful for clients that need the key only (for
        example <code class="systemitem">libvirt</code>). If the user already exists, this command simply
        returns the key. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd></dl></div><p>
     When creating client users, you may create a user with no capabilities. A
     user with no capabilities can authenticate but nothing more. Such client
     cannot retrieve the cluster map from the monitor. However, you can create
     a user with no capabilities if you want to defer adding capabilities later
     using the <code class="command">ceph auth caps</code> command.
    </p><p>
     A typical user has at least read capabilities on the Ceph monitor and
     read and write capabilities on Ceph OSDs. Additionally, a user's OSD
     permissions are often restricted to accessing a particular pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth add client.john mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephadm &gt; </code>ceph auth get-or-create client.paul mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephadm &gt; </code>ceph auth get-or-create client.george mon 'allow r' osd \
 'allow rw pool=liverpool' -o george.keyring
<code class="prompt user">cephadm &gt; </code>ceph auth get-or-create-key client.ringo mon 'allow r' osd \
 'allow rw pool=liverpool' -o ringo.key</pre></div><div id="id-1.3.4.6.6.7.6.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If you provide a user with capabilities to OSDs, but you <span class="emphasis"><em>do
      not</em></span> restrict access to particular pools, the user will have
      access to <span class="emphasis"><em>all</em></span> pools in the cluster.
     </p></div></section><section class="sect3" id="id-1.3.4.6.6.7.7" data-id-title="Modifying User Capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.4 </span><span class="title-name">Modifying User Capabilities</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.7">#</a></h4></div></div></div><p>
     The <code class="command">ceph auth caps</code> command allows you to specify a user
     and change the user's capabilities. Setting new capabilities will
     overwrite current ones. To view current capabilities run <code class="command">ceph
     auth get
     <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em></code>.
     To add capabilities, you also need to specify the existing capabilities
     when using the following form:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth caps <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em> <em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]' [<em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]']</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get client.john
<code class="prompt user">cephadm &gt; </code>ceph auth caps client.john mon 'allow r' osd 'allow rw pool=prague'
<code class="prompt user">cephadm &gt; </code>ceph auth caps client.paul mon 'allow rw' osd 'allow r pool=prague'
<code class="prompt user">cephadm &gt; </code>ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'</pre></div><p>
     To remove a capability, you may reset the capability. If you want the user
     to have no access to a particular daemon that was previously set, specify
     an empty string:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth caps client.ringo mon ' ' osd ' '</pre></div></section><section class="sect3" id="id-1.3.4.6.6.7.8" data-id-title="Deleting Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.5 </span><span class="title-name">Deleting Users</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.8">#</a></h4></div></div></div><p>
     To delete a user, use <code class="command">ceph auth del</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth del <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     If you created users with permissions strictly for a pool that no longer
     exists, you should consider deleting those users too.
    </p></section><section class="sect3" id="id-1.3.4.6.6.7.9" data-id-title="Printing a Users Key"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.6 </span><span class="title-name">Printing a User's Key</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.9">#</a></h4></div></div></div><p>
     To print a user’s authentication key to standard output, execute the
     following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth print-key <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     Printing a user's key is useful when you need to populate client software
     with a user's key (such as <code class="systemitem">libvirt</code>), as in the following example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph host:/ mount_point \
-o name=client.user,secret=`ceph auth print-key client.user`</pre></div></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-userimp" data-id-title="Importing Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.7 </span><span class="title-name">Importing Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-userimp">#</a></h4></div></div></div><p>
     To import one or more users, use <code class="command">ceph auth import</code> and
     specify a keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><div id="id-1.3.4.6.6.7.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The Ceph storage cluster will add new users, their keys and their
      capabilities and will update existing users, their keys and their
      capabilities.
     </p></div></section></section><section class="sect2" id="storage-cephx-keymgmt-keyringmgmt" data-id-title="Keyring Management"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.3 </span><span class="title-name">Keyring Management</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-keyringmgmt">#</a></h3></div></div></div><p>
    When you access Ceph via a Ceph client, the client will look for a
    local keyring. Ceph presets the keyring setting with the following four
    keyring names by default so you do not need to set them in your Ceph
    configuration file unless you want to override the defaults:
   </p><div class="verbatim-wrap"><pre class="screen">/etc/ceph/<em class="replaceable">cluster</em>.<em class="replaceable">name</em>.keyring
/etc/ceph/<em class="replaceable">cluster</em>.keyring
/etc/ceph/keyring
/etc/ceph/keyring.bin</pre></div><p>
    The <em class="replaceable">cluster</em> metavariable is your Ceph cluster
    name as defined by the name of the Ceph configuration file.
    <code class="filename">ceph.conf</code> means that the cluster name is
    <code class="literal">ceph</code>, thus <code class="literal">ceph.keyring</code>. The
    <em class="replaceable">name</em> metavariable is the user type and user ID,
    for example <code class="literal">client.admin</code>, thus
    <code class="literal">ceph.client.admin.keyring</code>.
   </p><p>
    After you create a user (for example
    <code class="systemitem">client.ringo</code>), you must get the
    key and add it to a keyring on a Ceph client so that the user can access
    the Ceph storage cluster.
   </p><p>
    <a class="xref" href="#storage-cephx-keymgmt" title="6.2. Key Management">Section 6.2, “Key Management”</a> details how to list, get, add,
    modify and delete users directly in the Ceph storage cluster. However,
    Ceph also provides the <code class="command">ceph-authtool</code> utility to allow
    you to manage keyrings from a Ceph client.
   </p><section class="sect3" id="creating-keyring" data-id-title="Creating a Keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.3.1 </span><span class="title-name">Creating a Keyring</span> <a title="Permalink" class="permalink" href="#creating-keyring">#</a></h4></div></div></div><p>
     When you use the procedures in <a class="xref" href="#storage-cephx-keymgmt" title="6.2. Key Management">Section 6.2, “Key Management”</a> to
     create users, you need to provide user keys to the Ceph client(s) so
     that the client can retrieve the key for the specified user and
     authenticate with the Ceph storage cluster. Ceph clients access
     keyrings to look up a user name and retrieve the user's key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool --create-keyring /path/to/keyring</pre></div><p>
     When creating a keyring with multiple users, we recommend using the
     cluster name (for example <em class="replaceable">cluster</em>.keyring) for
     the keyring file name and saving it in the <code class="filename">/etc/ceph</code>
     directory so that the keyring configuration default setting will pick up
     the file name without requiring you to specify it in the local copy of
     your Ceph configuration file. For example, create
     <code class="filename">ceph.keyring</code> by executing the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring</pre></div><p>
     When creating a keyring with a single user, we recommend using the cluster
     name, the user type and the user name and saving it in the
     <code class="filename">/etc/ceph</code> directory. For example,
     <code class="filename">ceph.client.admin.keyring</code> for the
     <code class="systemitem">client.admin</code> user.
    </p></section><section class="sect3" id="id-1.3.4.6.6.8.8" data-id-title="Adding a User to a Keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.3.2 </span><span class="title-name">Adding a User to a Keyring</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.8.8">#</a></h4></div></div></div><p>
     When you add a user to the Ceph storage cluster (see
     <a class="xref" href="#storage-cephx-keymgmt-usermgmt-useradd" title="6.2.2.3. Adding Users">Section 6.2.2.3, “Adding Users”</a>), you can
     retrieve the user, key and capabilities, and save the user to a keyring.
    </p><p>
     If you only want to use one user per keyring, the <code class="command">ceph auth
     get</code> command with the <code class="option">-o</code> option will save the
     output in the keyring file format. For example, to create a keyring for
     the <code class="systemitem">client.admin</code> user, execute
     the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring</pre></div><p>
     When you want to import users to a keyring, you can use
     <code class="command">ceph-authtool</code> to specify the destination keyring and
     the source keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring \
  --import-keyring /etc/ceph/ceph.client.admin.keyring</pre></div><div id="id-1.3.4.6.6.8.8.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If your keyring is compromised, delete your key from the
      <code class="filename">/etc/ceph</code> directory and recreate a new key using the same
      instructions from <a class="xref" href="#creating-keyring" title="6.2.3.1. Creating a Keyring">Section 6.2.3.1, “Creating a Keyring”</a>.
    </p></div></section><section class="sect3" id="id-1.3.4.6.6.8.9" data-id-title="Creating a User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.3.3 </span><span class="title-name">Creating a User</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.8.9">#</a></h4></div></div></div><p>
     Ceph provides the <code class="command">ceph auth add</code> command to create a
     user directly in the Ceph storage cluster. However, you can also create
     a user, keys and capabilities directly on a Ceph client keyring. Then,
     you can import the user to the Ceph storage cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool -n client.ringo --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.keyring</pre></div><p>
     You can also create a keyring and add a new user to the keyring
     simultaneously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx' --gen-key</pre></div><p>
     In the previous scenarios, the new user
     <code class="systemitem">client.ringo</code> is only in the
     keyring. To add the new user to the Ceph storage cluster, you must still
     add the new user to the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth add client.ringo -i /etc/ceph/ceph.keyring</pre></div></section><section class="sect3" id="id-1.3.4.6.6.8.10" data-id-title="Modifying Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.3.4 </span><span class="title-name">Modifying Users</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.8.10">#</a></h4></div></div></div><p>
     To modify the capabilities of a user record in a keyring, specify the
     keyring and the user followed by the capabilities:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx'</pre></div><p>
     To update the modified user within the Ceph cluster environment, you
     must import the changes from the keyring to the user entry in the Ceph
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><p>
     See <a class="xref" href="#storage-cephx-keymgmt-usermgmt-userimp" title="6.2.2.7. Importing Users">Section 6.2.2.7, “Importing Users”</a> for details
     on updating a Ceph storage cluster user from a keyring.
    </p></section></section><section class="sect2" id="storage-cephx-keymgmt-cmdline" data-id-title="Command Line Usage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.4 </span><span class="title-name">Command Line Usage</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-cmdline">#</a></h3></div></div></div><p>
    The <code class="command">ceph</code> command supports the following options related
    to the user name and secret manipulation:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.6.9.3.1"><span class="term"><code class="option">--id</code> or <code class="option">--user</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">id</code>, <code class="option">name</code> and <code class="option">-n</code>
       options enable you to specify the ID portion of the user name (for
       example <code class="systemitem">admin</code> or
       <code class="systemitem">user1</code>). You can specify the
       user with the --id and omit the type. For example, to specify user
       client.foo enter the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --id foo --keyring /path/to/keyring health
<code class="prompt user">cephadm &gt; </code>ceph --user foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.3.4.6.6.9.3.2"><span class="term"><code class="option">--name</code> or <code class="option">-n</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">--name</code> and <code class="option">-n</code> options enable you to
       specify the fully qualified user name. You must specify the user type
       (typically <code class="literal">client</code>) with the user ID:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --name client.foo --keyring /path/to/keyring health
<code class="prompt user">cephadm &gt; </code>ceph -n client.foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.3.4.6.6.9.3.3"><span class="term"><code class="option">--keyring</code></span></dt><dd><p>
       The path to the keyring containing one or more user name and secret. The
       <code class="option">--secret</code> option provides the same functionality, but it
       does not work with Object Gateway, which uses <code class="option">--secret</code> for
       another purpose. You may retrieve a keyring with <code class="command">ceph auth
       get-or-create</code> and store it locally. This is a preferred
       approach, because you can switch user names without switching the
       keyring path:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --id foo --keyring /path/to/keyring mypool/myimage</pre></div></dd></dl></div></section></section></section><section class="chapter" id="cha-storage-datamgm" data-id-title="Stored Data Management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Stored Data Management</span> <a title="Permalink" class="permalink" href="#cha-storage-datamgm">#</a></h2></div></div></div><p>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers Ceph clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, Ceph
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </p><p>
  CRUSH requires a map of your cluster, and uses the CRUSH Map to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </p><p>
  CRUSH maps contain a list of OSDs, a list of 'buckets' for aggregating the
  devices into physical locations, and a list of rules that tell CRUSH how it
  should replicate data in a Ceph cluster’s pools. By reflecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </p><p>
  After you deploy a Ceph cluster, a default CRUSH Map is generated. It is
  fine for your Ceph sandbox environment. However, when you deploy a
  large-scale data cluster, you should give significant consideration to
  developing a custom CRUSH Map, because it will help you manage your Ceph
  cluster, improve performance and ensure data safety.
 </p><p>
  For example, if an OSD goes down, a CRUSH Map can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </p><p>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </p><p>
  A custom CRUSH Map can also help you identify the physical locations where
  Ceph stores redundant copies of data when the placement group(s) associated
  with a failed host are in a degraded state.
 </p><p>
  There are three main sections to a CRUSH Map.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#datamgm-devices" title="7.1. Devices">Devices</a> consist of any
    object storage device corresponding to a <code class="systemitem">ceph-osd</code>
    daemon.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-buckets" title="7.2. Buckets">Buckets</a> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-rules" title="7.3. Rule Sets">Rule Sets</a> consist of the
    manner of selecting buckets.
   </p></li></ul></div><section class="sect1" id="datamgm-devices" data-id-title="Devices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Devices</span> <a title="Permalink" class="permalink" href="#datamgm-devices">#</a></h2></div></div></div><p>
   To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   CRUSH Map.
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device <em class="replaceable">NUM</em> osd.<em class="replaceable">OSD_NAME</em> class <em class="replaceable">CLASS_NAME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3class ssd</pre></div><p>
   As a general rule, an OSD daemon maps to a single disk.
  </p><section class="sect2" id="crush-devclasses" data-id-title="Device Classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.1 </span><span class="title-name">Device Classes</span> <a title="Permalink" class="permalink" href="#crush-devclasses">#</a></h3></div></div></div><p>
    The flexibility of the CRUSH Map in controlling data placement is one of
    the Ceph's strengths. It is also one of the most difficult parts of the
    cluster to manage. <span class="emphasis"><em>Device classes</em></span> automate one of the
    most common reasons why CRUSH Maps are directly manually edited.
   </p><section class="sect3" id="id-1.3.4.7.12.7.3" data-id-title="The CRUSH Management Problem"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.1 </span><span class="title-name">The CRUSH Management Problem</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.3">#</a></h4></div></div></div><p>
     Ceph clusters are frequently built with multiple types of storage
     devices: HDDs, SSDs, NVMe’s, or even mixed classes of the above. We call
     these different types of storage devices <span class="emphasis"><em>device
     classes</em></span> to avoid confusion between the
     <span class="emphasis"><em>type</em></span> property of CRUSH buckets (e.g., host, rack,
     row, see <a class="xref" href="#datamgm-buckets" title="7.2. Buckets">Section 7.2, “Buckets”</a> for more details). Ceph OSDs
     backed by SSDs are much faster than those backed by spinning disks, making
     them better suited for certain workloads. Ceph makes it easy to create
     RADOS pools for different data sets or workloads and to assign different
     CRUSH rules to control data placement for those pools.
    </p><div class="figure" id="id-1.3.4.7.12.7.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/device_classes.svg" target="_blank"><img src="images/device_classes.svg" width="" alt="OSDs with Mixed Device Classes"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.1: </span><span class="title-name">OSDs with Mixed Device Classes </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.3.3">#</a></h6></div></div><p>
     However, setting up the CRUSH rules to place data only on a certain class
     of device is tedious. Rules work in terms of the CRUSH hierarchy, but if
     the devices are mixed into the same hosts or racks (as in the sample
     hierarchy above), they will (by default) be mixed together and appear in
     the same subtrees of the hierarchy. Manually separating them out into
     separate trees involved creating multiple versions of each intermediate
     node for each device class in previous versions of SUSE Enterprise Storage.
    </p></section><section class="sect3" id="id-1.3.4.7.12.7.4" data-id-title="Device Classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.2 </span><span class="title-name">Device Classes</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.4">#</a></h4></div></div></div><p>
     An elegant solution that Ceph offers is to add a property called
     <span class="emphasis"><em>device class</em></span> to each OSD. By default, OSDs will
     automatically set their device classes to either 'hdd', 'ssd', or 'nvme'
     based on the hardware properties exposed by the Linux kernel. These device
     classes are reported in a new column of the <code class="command">ceph osd
     tree</code> command output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000</pre></div><p>
     If the automatic device class detection fails for example because the
     device driver is not properly exposing information about the device via
     <code class="filename">/sys/block</code>, you can adjust device classes from the
     command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
<code class="prompt user">cephadm &gt; </code>ceph osd crush set-device-class ssd osd.2 osd.3
set osd(s) 2,3 to class 'ssd'</pre></div></section><section class="sect3" id="crush-placement-rules" data-id-title="CRUSH Placement Rules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.3 </span><span class="title-name">CRUSH Placement Rules</span> <a title="Permalink" class="permalink" href="#crush-placement-rules">#</a></h4></div></div></div><p>
     CRUSH rules can restrict placement to a specific device class. For
     example, you can create a 'fast'
     <span class="bold"><strong>replicated</strong></span> pool that distributes data
     only over SSD disks by running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rule create-replicated <em class="replaceable">RULE_NAME</em> <em class="replaceable">ROOT</em> <em class="replaceable">FAILURE_DOMAIN_TYPE</em> <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rule create-replicated fast default host ssd</pre></div><p>
     Create a pool named 'fast_pool' and assign it to the 'fast' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create fast_pool 128 128 replicated fast</pre></div><p>
     The process for creating <span class="bold"><strong>erasure code</strong></span>
     rules is slightly different. First, you create an erasure code profile
     that includes a property for your desired device class. Then use that
     profile when creating the erasure coded pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd erasure-code-profile set myprofile \
 k=4 m=2 crush-device-class=ssd crush-failure-domain=host
<code class="prompt user">cephadm &gt; </code>ceph osd pool create mypool 64 erasure myprofile</pre></div><p>
     If you need to manually edit the CRUSH Map to customize your rule, the
     syntax has been extended to allow the device class to be specified. For
     example, the CRUSH rule generated by the above commands looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">rule ecpool {
  id 2
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default <span class="bold"><strong>class ssd</strong></span>
  step chooseleaf indep 0 type host
  step emit
}</pre></div><p>
     The important difference there is that the 'take' command includes the
     additional 'class <em class="replaceable">CLASS_NAME</em>' suffix.
    </p></section><section class="sect3" id="crush-additional-commands" data-id-title="Additional Commands"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.4 </span><span class="title-name">Additional Commands</span> <a title="Permalink" class="permalink" href="#crush-additional-commands">#</a></h4></div></div></div><p>
     To list device classes used in a CRUSH Map, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush class ls
[
  "hdd",
  "ssd"
]</pre></div><p>
     To list existing CRUSH rules, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rule ls
replicated_rule
fast</pre></div><p>
     To view details of the CRUSH rule named 'fast', run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rule dump fast
{
		"rule_id": 1,
		"rule_name": "fast",
		"ruleset": 1,
		"type": 1,
		"min_size": 1,
		"max_size": 10,
		"steps": [
						{
										"op": "take",
										"item": -21,
										"item_name": "default~ssd"
						},
						{
										"op": "chooseleaf_firstn",
										"num": 0,
										"type": "host"
						},
						{
										"op": "emit"
						}
		]
}</pre></div><p>
     To list OSDs that belong to a 'ssd' class, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush class ls-osd ssd
0
1</pre></div></section><section class="sect3" id="device-classes-reclassify" data-id-title="Migrating from a Legacy SSD Rule to Device Classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.5 </span><span class="title-name">Migrating from a Legacy SSD Rule to Device Classes</span> <a title="Permalink" class="permalink" href="#device-classes-reclassify">#</a></h4></div></div></div><p>
     In SUSE Enterprise Storage prior to version 5, you needed to manually edit the
     CRUSH Map and maintain a parallel hierarchy for each specialized device
     type (such as SSD) in order to write rules that apply to these devices.
     Since SUSE Enterprise Storage 5, the device class feature has enabled this
     transparently.
    </p><p>
     You can transform a legacy rule and hierarchy to the new class-based rules
     by using the <code class="command">crushtool</code> command. There are several types
     of transformation possible:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.12.7.7.4.1"><span class="term"><code class="command">crushtool --reclassify-root <em class="replaceable">ROOT_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This command takes everything in the hierarchy beneath
        <em class="replaceable">ROOT_NAME</em> and adjusts any rules that
        reference that root via
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em></pre></div><p>
        to instead
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em> class <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
        It renumbers the buckets so that the old IDs are used for the specified
        class’s 'shadow tree'. As a consequence, no data movement occurs.
       </p><div class="complex-example"><div class="example" id="id-1.3.4.7.12.7.7.4.1.2.6" data-id-title="crushtool --reclassify-root"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.1: </span><span class="title-name"><code class="command">crushtool --reclassify-root</code> </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.7.4.1.2.6">#</a></h6></div><div class="example-contents"><p>
         Consider the following existing rule:
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div><p>
         If you reclassify the root 'default' as class 'hdd', the rule will
         become
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default class hdd
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div></div></div></div></dd><dt id="id-1.3.4.7.12.7.7.4.2"><span class="term"><code class="command">crushtool --set-subtree-class <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This method marks every device in the subtree rooted at
        <em class="replaceable">BUCKET_NAME</em> with the specified device class.
       </p><p>
        <code class="option">--set-subtree-class</code> is normally used in conjunction
        with the <code class="option">--reclassify-root</code> option to ensure that all
        devices in that root are labeled with the correct class. However some
        of those devices may intentionally have a different class, and
        therefore you do not want to relabel them. In such cases, exclude the
        <code class="option">--set-subtree-class</code> option. Keep in mind that such
        remapping will not be perfect, because the previous rule is distributed
        across devices of multiple classes but the adjusted rules will only map
        to devices of the specified device class.
       </p></dd><dt id="id-1.3.4.7.12.7.7.4.3"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">MATCH_PATTERN</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">DEFAULT_PATTERN</em></code></span></dt><dd><p>
        This method allows merging a parallel type-specific hierarchy with the
        normal hierarchy. For example, many users have CRUSH Maps similar to
        the following one:
       </p><div class="example" id="id-1.3.4.7.12.7.7.4.3.2.2" data-id-title="crushtool --reclassify-bucket"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.2: </span><span class="title-name"><code class="command">crushtool --reclassify-bucket</code> </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.7.4.3.2.2">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">host node1 {
   id -2           # do not change unnecessarily
   # weight 109.152
   alg straw
   hash 0  # rjenkins1
   item osd.0 weight 9.096
   item osd.1 weight 9.096
   item osd.2 weight 9.096
   item osd.3 weight 9.096
   item osd.4 weight 9.096
   item osd.5 weight 9.096
   [...]
}

host node1-ssd {
   id -10          # do not change unnecessarily
   # weight 2.000
   alg straw
   hash 0  # rjenkins1
   item osd.80 weight 2.000
   [...]
}

root default {
   id -1           # do not change unnecessarily
   alg straw
   hash 0  # rjenkins1
   item node1 weight 110.967
   [...]
}

root ssd {
   id -18          # do not change unnecessarily
   # weight 16.000
   alg straw
   hash 0  # rjenkins1
   item node1-ssd weight 2.000
   [...]
}</pre></div></div></div><p>
        This function reclassifies each bucket that matches a given pattern.
        The pattern can look like <code class="literal">%suffix</code> or
        <code class="literal">prefix%</code>. In the above example, you would use the
        pattern <code class="literal">%-ssd</code>. For each matched bucket, the
        remaining portion of the name that matches the '%' wild card specifies
        the base bucket. All devices in the matched bucket are labeled with the
        specified device class and then moved to the base bucket. If the base
        bucket does not exist (for example if 'node12-ssd' exists but 'node12'
        does not), then it is created and linked underneath the specified
        default parent bucket. The old bucket IDs are preserved for the new
        shadow buckets to prevent data movement. Rules with the
        <code class="literal">take</code> steps that reference the old buckets are
        adjusted.
       </p></dd><dt id="id-1.3.4.7.12.7.7.4.4"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">BASE_BUCKET</em></code></span></dt><dd><p>
        You can use the <code class="option">--reclassify-bucket</code> option without a
        wild card to map a single bucket. For example, in the previous example,
        we want the 'ssd' bucket to be mapped to the default bucket.
       </p><p>
        The final command to convert the map comprised of the above fragments
        would be as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd getcrushmap -o original
<code class="prompt user">cephadm &gt; </code>crushtool -i original --reclassify \
  --set-subtree-class default hdd \
  --reclassify-root default hdd \
  --reclassify-bucket %-ssd ssd default \
  --reclassify-bucket ssd ssd default \
  -o adjusted</pre></div><p>
        In order to verify that the conversion is correct, there is a
        <code class="option">--compare</code> option that tests a large sample of inputs
        to the CRUSH Map and compares if the same result comes back out. These
        inputs are controlled by the same options that apply to the
        <code class="option">--test</code>. For the above example the command would be as
        follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent</pre></div><div id="id-1.3.4.7.12.7.7.4.4.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
         If there were differences, you would see what ratio of inputs are
         remapped in the parentheses.
        </p></div><p>
        If you are satisfied with the adjusted CRUSH Map, you can apply it to
        the cluster:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd setcrushmap -i adjusted</pre></div></dd></dl></div></section><section class="sect3" id="id-1.3.4.7.12.7.8" data-id-title="For More Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.6 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.12.7.8">#</a></h4></div></div></div><p>
     Find more details on CRUSH Maps in <a class="xref" href="#op-crush" title="7.4. CRUSH Map Manipulation">Section 7.4, “CRUSH Map Manipulation”</a>.
    </p><p>
     Find more details on Ceph pools in general in
     <a class="xref" href="#ceph-pools" title="Chapter 8. Managing Storage Pools">Chapter 8, <em>Managing Storage Pools</em></a>.
    </p><p>
     Find more details about erasure coded pools in
     <a class="xref" href="#cha-ceph-erasure" title="Chapter 10. Erasure Coded Pools">Chapter 10, <em>Erasure Coded Pools</em></a>.
    </p></section></section></section><section class="sect1" id="datamgm-buckets" data-id-title="Buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Buckets</span> <a title="Permalink" class="permalink" href="#datamgm-buckets">#</a></h2></div></div></div><p>
   CRUSH maps contain a list of OSDs, which can be organized into 'buckets' for
   aggregating the devices into physical locations.
  </p><div class="informaltable"><table style="border: none;"><colgroup><col/><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        0
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        osd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        An OSD daemon (osd.1, osd.2, etc.).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        host
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A host name containing one or more OSDs.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        2
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        chassis
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Chassis of which the rack is composed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        rack
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A computer rack. The default is <code class="literal">unknownrack</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        row
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A row in a series of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        5
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pdu
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Power distribution unit.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        6
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pod
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        7
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        room
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A room containing racks and rows of hosts.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        datacenter
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A physical data center containing rooms.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        9
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        region
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        10
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        root
       </p>
      </td><td>
       
      </td></tr></tbody></table></div><div id="id-1.3.4.7.13.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    You can modify the existing types and create your own bucket types.
   </p></div><p>
   Ceph’s deployment tools generate a CRUSH Map that contains a bucket for
   each host, and a root named 'default', which is useful for the default
   <code class="literal">rbd</code> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </p><p>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <code class="literal">straw2</code> by default), and
   the hash (<code class="literal">0</code> by default, reflecting CRUSH Hash
   <code class="literal">rjenkins1</code>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </p><div class="verbatim-wrap"><pre class="screen">[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</pre></div><p>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </p><div class="verbatim-wrap"><pre class="screen">host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</pre></div></section><section class="sect1" id="datamgm-rules" data-id-title="Rule Sets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.3 </span><span class="title-name">Rule Sets</span> <a title="Permalink" class="permalink" href="#datamgm-rules">#</a></h2></div></div></div><p>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default CRUSH Map has a rule for the default root. If you want more
   roots and more rules, you need to create them later or they will be created
   automatically when new pools are created.
  </p><div id="id-1.3.4.7.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </p></div><p>
   A rule takes the following form:
  </p><div class="verbatim-wrap"><pre class="screen">rule <em class="replaceable">rulename</em> {

        ruleset <em class="replaceable">ruleset</em>
        type <em class="replaceable">type</em>
        min_size <em class="replaceable">min-size</em>
        max_size <em class="replaceable">max-size</em>
        step <em class="replaceable">step</em>

}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.14.6.1"><span class="term">ruleset</span></dt><dd><p>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <code class="literal">0</code>.
     </p></dd><dt id="id-1.3.4.7.14.6.2"><span class="term">type</span></dt><dd><p>
      A string. Describes a rule for either for 'replicated' or 'erasure' coded
      pool. This option is required. Default is <code class="literal">replicated</code>.
     </p></dd><dt id="id-1.3.4.7.14.6.3"><span class="term">min_size</span></dt><dd><p>
      An integer. If a pool group makes fewer replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">2</code>.
     </p></dd><dt id="id-1.3.4.7.14.6.4"><span class="term">max_size</span></dt><dd><p>
      An integer. If a pool group makes more replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">10</code>.
     </p></dd><dt id="id-1.3.4.7.14.6.5"><span class="term">step take <em class="replaceable">bucket</em></span></dt><dd><p>
      Takes a bucket specified by a name, and begins iterating down the tree.
      This option is required. For an explanation about iterating through the
      tree, see <a class="xref" href="#datamgm-rules-step-iterate" title="7.3.1. Iterating Through the Node Tree">Section 7.3.1, “Iterating Through the Node Tree”</a>.
     </p></dd><dt id="id-1.3.4.7.14.6.6"><span class="term">step <em class="replaceable">target</em><em class="replaceable">mode</em><em class="replaceable">num</em> type <em class="replaceable">bucket-type</em></span></dt><dd><p>
      <em class="replaceable">target</em> can either be <code class="literal">choose</code>
      or <code class="literal">chooseleaf</code>. When set to <code class="literal">choose</code>,
      a number of buckets is selected. <code class="literal">chooseleaf</code> directly
      selects the OSDs (leaf nodes) from the sub-tree of each bucket in the set
      of buckets.
     </p><p>
      <em class="replaceable">mode</em> can either be <code class="literal">firstn</code>
      or <code class="literal">indep</code>. See
      <a class="xref" href="#datamgm-rules-step-mode" title="7.3.2. firstn and indep">Section 7.3.2, “firstn and indep”</a>.
     </p><p>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <em class="replaceable">num</em> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <em class="replaceable">num</em> &lt;
      0, it means N - <em class="replaceable">num</em>; and, if
      <em class="replaceable">num</em> == 0, choose N buckets (all available).
      Follows <code class="literal">step take</code> or <code class="literal">step choose</code>.
     </p></dd><dt id="id-1.3.4.7.14.6.7"><span class="term">step emit</span></dt><dd><p>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <code class="literal">step choose</code>.
     </p></dd></dl></div><section class="sect2" id="datamgm-rules-step-iterate" data-id-title="Iterating Through the Node Tree"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.3.1 </span><span class="title-name">Iterating Through the Node Tree</span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate">#</a></h3></div></div></div><p>
    The structure defined with the buckets can be viewed as a node tree.
    Buckets are nodes and OSDs are leafs in this tree.
   </p><p>
    Rules in the CRUSH Map define how OSDs are selected from this tree. A rule
    starts with a node and then iterates down the tree to return a set of OSDs.
    It is not possible to define which branch needs to be selected. Instead the
    CRUSH algorithm assures that the set of OSDs fulfills the replication
    requirements and evenly distributes the data.
   </p><p>
    With <code class="literal">step take</code> <em class="replaceable">bucket</em> the
    iteration through the node tree begins at the given bucket (not bucket
    type). If OSDs from all branches in the tree are to be returned, the bucket
    must be the root bucket. Otherwise the following steps are only iterating
    through a sub-tree.
   </p><p>
    After <code class="literal">step take</code> one or more <code class="literal">step
    choose</code> entries follow in the rule definition. Each <code class="literal">step
    choose</code> chooses a defined number of nodes (or branches) from the
    previously selected upper node.
   </p><p>
    In the end the selected OSDs are returned with <code class="literal">step
    emit</code>.
   </p><p>
    <code class="literal">step chooseleaf</code> is a convenience function that directly
    selects OSDs from branches of the given bucket.
   </p><p>
    <a class="xref" href="#datamgm-rules-step-iterate-figure" title="Example Tree">Figure 7.2, “Example Tree”</a> provides an example of
    how <code class="literal">step</code> is used to iterate through a tree. The orange
    arrows and numbers correspond to <code class="literal">example1a</code> and
    <code class="literal">example1b</code>, while blue corresponds to
    <code class="literal">example2</code> in the following rule definitions.
   </p><div class="figure" id="datamgm-rules-step-iterate-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-step.png" target="_blank"><img src="images/crush-step.png" width="" alt="Example Tree"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.2: </span><span class="title-name">Example Tree </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate-figure">#</a></h6></div></div><div class="verbatim-wrap"><pre class="screen"># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</pre></div></section><section class="sect2" id="datamgm-rules-step-mode" data-id-title="firstn and indep"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.3.2 </span><span class="title-name">firstn and indep</span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-mode">#</a></h3></div></div></div><p>
    A CRUSH rule defines replacements for failed nodes or OSDs (see
    <a class="xref" href="#datamgm-rules" title="7.3. Rule Sets">Section 7.3, “Rule Sets”</a>). The keyword <code class="literal">step</code>
    requires either <code class="literal">firstn</code> or <code class="literal">indep</code> as
    parameter. Figure <a class="xref" href="#datamgm-rules-step-mode-indep-figure" title="Node Replacement Methods">Figure 7.3, “Node Replacement Methods”</a>
    provides an example.
   </p><p>
    <code class="literal">firstn</code> adds replacement nodes to the end of the list of
    active nodes. In case of a failed node, the following healthy nodes are
    shifted to the left to fill the gap of the failed node. This is the default
    and desired method for <span class="emphasis"><em>replicated pools</em></span>, because a
    secondary node already has all data and therefore can take over the duties
    of the primary node immediately.
   </p><p>
    <code class="literal">indep</code> selects fixed replacement nodes for each active
    node. The replacement of a failed node does not change the order of the
    remaining nodes. This is desired for <span class="emphasis"><em>erasure coded
    pools</em></span>. In erasure coded pools the data stored on a node depends
    on its position in the node selection. When the order of nodes changes, all
    data on affected nodes needs to be relocated.
   </p><div class="figure" id="datamgm-rules-step-mode-indep-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-firstn-indep.png" target="_blank"><img src="images/crush-firstn-indep.png" width="" alt="Node Replacement Methods"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.3: </span><span class="title-name">Node Replacement Methods </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-mode-indep-figure">#</a></h6></div></div></section></section><section class="sect1" id="op-crush" data-id-title="CRUSH Map Manipulation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.4 </span><span class="title-name">CRUSH Map Manipulation</span> <a title="Permalink" class="permalink" href="#op-crush">#</a></h2></div></div></div><p>
   This section introduces ways to basic CRUSH Map manipulation, such as
   editing a CRUSH Map, changing CRUSH Map parameters, and
   adding/moving/removing an OSD.
  </p><section class="sect2" id="id-1.3.4.7.15.3" data-id-title="Editing a CRUSH Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.1 </span><span class="title-name">Editing a CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.15.3">#</a></h3></div></div></div><p>
    To edit an existing CRUSH map, do the following:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Get a CRUSH Map. To get the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd getcrushmap -o <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will output (<code class="option">-o</code>) a compiled CRUSH Map to the
      file name you specified. Since the CRUSH Map is in a compiled form, you
      must decompile it first before you can edit it.
     </p></li><li class="step"><p>
      Decompile a CRUSH Map. To decompile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>crushtool -d <em class="replaceable">compiled-crushmap-filename</em> \
 -o <em class="replaceable">decompiled-crushmap-filename</em></pre></div><p>
      Ceph will decompile (<code class="option">-d</code>) the compiled CRUSH Mapand
      output (<code class="option">-o</code>) it to the file name you specified.
     </p></li><li class="step"><p>
      Edit at least one of Devices, Buckets and Rules parameters.
     </p></li><li class="step"><p>
      Compile a CRUSH Map. To compile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>crushtool -c <em class="replaceable">decompiled-crush-map-filename</em> \
 -o <em class="replaceable">compiled-crush-map-filename</em></pre></div><p>
      Ceph will store a compiled CRUSH Mapto the file name you specified.
     </p></li><li class="step"><p>
      Set a CRUSH Map. To set the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd setcrushmap -i <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will input the compiled CRUSH Map of the file name you specified
      as the CRUSH Map for the cluster.
     </p></li></ol></div></div><div id="id-1.3.4.7.15.3.4" data-id-title="Use Versioning System" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use Versioning System</h6><p>
     Use a versioning system—such as git or svn—for the exported
     and modified CRUSH Map files. It makes a possible rollback simple.
    </p></div><div id="id-1.3.4.7.15.3.5" data-id-title="Test the New CRUSH Map" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Test the New CRUSH Map</h6><p>
     Test the new adjusted CRUSH Map using the <code class="command">crushtool
     --test</code> command, and compare to the state before applying the new
     CRUSH Map. You may find the following command switches useful:
     <code class="option">--show-statistics</code>, <code class="option">--show-mappings</code>,
     <code class="option">--show-bad-mappings</code>, <code class="option">--show-utilization</code>,
     <code class="option">--show-utilization-all</code>,
     <code class="option">--show-choose-tries</code>
    </p></div></section><section class="sect2" id="op-crush-addosd" data-id-title="Add/Move an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.2 </span><span class="title-name">Add/Move an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-addosd">#</a></h3></div></div></div><p>
    To add or move an OSD in the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush set <em class="replaceable">id_or_name</em> <em class="replaceable">weight</em> root=<em class="replaceable">pool-name</em>
<em class="replaceable">bucket-type</em>=<em class="replaceable">bucket-name</em> ...</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.15.4.4.1"><span class="term">id</span></dt><dd><p>
       An integer. The numeric ID of the OSD. This option is required.
      </p></dd><dt id="id-1.3.4.7.15.4.4.2"><span class="term">name</span></dt><dd><p>
       A string. The full name of the OSD. This option is required.
      </p></dd><dt id="id-1.3.4.7.15.4.4.3"><span class="term">weight</span></dt><dd><p>
       A double. The CRUSH weight for the OSD. This option is required.
      </p></dd><dt id="id-1.3.4.7.15.4.4.4"><span class="term">root</span></dt><dd><p>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </p></dd><dt id="id-1.3.4.7.15.4.4.5"><span class="term">bucket-type</span></dt><dd><p>
       Key/value pairs. You may specify the OSD’s location in the CRUSH
       hierarchy.
      </p></dd></dl></div><p>
    The following example adds <code class="literal">osd.0</code> to the hierarchy, or
    moves the OSD from a previous location.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-osdweight" data-id-title="Difference between ceph osd reweight and ceph osd crush reweight"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.3 </span><span class="title-name">Difference between <code class="command">ceph osd reweight</code> and <code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#op-crush-osdweight">#</a></h3></div></div></div><p>
    There are two similar commands that change the 'weight' of an Ceph OSD.
    Context of their usage is different and may cause confusion.
   </p><section class="sect3" id="id-1.3.4.7.15.5.3" data-id-title="ceph osd reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.4.3.1 </span><span class="title-name"><code class="command">ceph osd reweight</code></span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.15.5.3">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd reweight</code> sets an override weight on the Ceph OSD.
     This value is in the range 0 to 1, and forces CRUSH to re-place of the
     data that would otherwise live on this drive. It does
     <span class="bold"><strong>not</strong></span> change the weights assigned to the
     buckets above the OSD, and is a corrective measure in case the normal
     CRUSH distribution is not working out quite right. For example, if one of
     your OSDs is at 90% and the others are at 40%, you could reduce this
     weight to try and compensate for it.
    </p><div id="id-1.3.4.7.15.5.3.5" data-id-title="OSD Weight is Temporary" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: OSD Weight is Temporary</h6><p>
      Note that <code class="command">ceph osd reweight</code> is not a persistent
      setting. When an OSD gets marked out, its weight will be set to 0 and
      when it gets marked in again, the weight will be changed to 1.
     </p></div></section><section class="sect3" id="id-1.3.4.7.15.5.4" data-id-title="ceph osd crush reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.4.3.2 </span><span class="title-name"><code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#id-1.3.4.7.15.5.4">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd crush reweight</code> sets the
     <span class="bold"><strong>CRUSH</strong></span> weight of the OSD. This weight is
     an arbitrary value—generally the size of the disk in TB—and
     controls how much data the system tries to allocate to the OSD.
    </p></section></section><section class="sect2" id="op-crush-osdremove" data-id-title="Remove an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.4 </span><span class="title-name">Remove an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-osdremove">#</a></h3></div></div></div><p>
    To remove an OSD from the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush remove <em class="replaceable">OSD_NAME</em></pre></div></section><section class="sect2" id="op-crush-addbaucket" data-id-title="Add a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.5 </span><span class="title-name">Add a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-addbaucket">#</a></h3></div></div></div><p>
    To add a bucket in the CRUSH Map of a running cluster, execute the
    <code class="command">ceph osd crush add-bucket</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush add-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em></pre></div></section><section class="sect2" id="op-crush-movebucket" data-id-title="Move a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.6 </span><span class="title-name">Move a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-movebucket">#</a></h3></div></div></div><p>
    To move a bucket to a different location or position in the CRUSH Map
    hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush move <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em>=<em class="replaceable">BUCKET_NAME</em> [...]</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-rmbucket" data-id-title="Remove a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.4.7 </span><span class="title-name">Remove a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-rmbucket">#</a></h3></div></div></div><p>
    To remove a bucket from the CRUSH Map hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush remove <em class="replaceable">BUCKET_NAME</em></pre></div><div id="id-1.3.4.7.15.9.4" data-id-title="Empty Bucket Only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Empty Bucket Only</h6><p>
     A bucket must be empty before removing it from the CRUSH hierarchy.
    </p></div></section></section><section class="sect1" id="scrubbing" data-id-title="Scrubbing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.5 </span><span class="title-name">Scrubbing</span> <a title="Permalink" class="permalink" href="#scrubbing">#</a></h2></div></div></div><p>
   In addition to making multiple copies of objects, Ceph insures data
   integrity by <span class="emphasis"><em>scrubbing</em></span> placement groups (find more
   information about placement groups in
   <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 5.5 and Ceph”, Section 1.4.2 “Placement Group”</span>). Ceph scrubbing is analogous
   to running <code class="command">fsck</code> on the object storage layer. For each
   placement group, Ceph generates a catalog of all objects and compares each
   primary object and its replicas to ensure that no objects are missing or
   mismatched. Daily light scrubbing checks the object size and attributes,
   while weekly deep scrubbing reads the data and uses checksums to ensure data
   integrity.
  </p><p>
   Scrubbing is important for maintaining data integrity, but it can reduce
   performance. You can adjust the following settings to increase or decrease
   scrubbing operations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.16.4.1"><span class="term"><code class="option">osd max scrubs</code></span></dt><dd><p>
      The maximum number of simultaneous scrub operations for a Ceph OSD. Default
      is 1.
     </p></dd><dt id="id-1.3.4.7.16.4.2"><span class="term"><code class="option">osd scrub begin hour</code>, <code class="option">osd scrub end hour</code></span></dt><dd><p>
      The hours of day (0 to 24) that define a time window when the scrubbing
      can happen. By default begins at 0 and ends at 24.
     </p><div id="id-1.3.4.7.16.4.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       If the placement group’s scrub interval exceeds the <code class="option">osd scrub
       max interval</code> setting, the scrub will happen no matter what time
       window you define for scrubbing.
      </p></div></dd><dt id="id-1.3.4.7.16.4.3"><span class="term"><code class="option">osd scrub during recovery</code></span></dt><dd><p>
      Allows scrubs during recovery. Setting this to 'false' will disable
      scheduling new scrubs while there is an active recovery. Already running
      scrubs will continue. This option is useful for reducing load on busy
      clusters. Default is 'true'.
     </p></dd><dt id="id-1.3.4.7.16.4.4"><span class="term"><code class="option">osd scrub thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub thread times out. Default is
      60.
     </p></dd><dt id="id-1.3.4.7.16.4.5"><span class="term"><code class="option">osd scrub finalize thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub finalize thread time out.
      Default is 60*10.
     </p></dd><dt id="id-1.3.4.7.16.4.6"><span class="term"><code class="option">osd scrub load threshold</code></span></dt><dd><p>
      The normalized maximum load. Ceph will not scrub when the system load
      (as defined by the ratio of <code class="literal">getloadavg()</code> / number of
      <code class="literal">online cpus</code>) is higher than this number. Default is
      0.5.
     </p></dd><dt id="id-1.3.4.7.16.4.7"><span class="term"><code class="option">osd scrub min interval</code></span></dt><dd><p>
      The minimal interval in seconds for scrubbing Ceph OSD when the Ceph
      cluster load is low. Default is 60*60*24 (once a day).
     </p></dd><dt id="id-1.3.4.7.16.4.8"><span class="term"><code class="option">osd scrub max interval</code></span></dt><dd><p>
      The maximum interval in seconds for scrubbing Ceph OSD irrespective of
      cluster load. 7*60*60*24 (once a week).
     </p></dd><dt id="id-1.3.4.7.16.4.9"><span class="term"><code class="option">osd scrub chunk min</code></span></dt><dd><p>
      The minimal number of object store chunks to scrub during single
      operation. Ceph blocks writes to a single chunk during scrub. Default
      is 5.
     </p></dd><dt id="id-1.3.4.7.16.4.10"><span class="term"><code class="option">osd scrub chunk max</code></span></dt><dd><p>
      The maximum number of object store chunks to scrub during single
      operation. Default is 25.
     </p></dd><dt id="id-1.3.4.7.16.4.11"><span class="term"><code class="option">osd scrub sleep</code></span></dt><dd><p>
      Time to sleep before scrubbing next group of chunks. Increasing this
      value slows down the whole scrub operation while client operations are
      less impacted. Default is 0.
     </p></dd><dt id="id-1.3.4.7.16.4.12"><span class="term"><code class="option">osd deep scrub interval</code></span></dt><dd><p>
      The interval for 'deep' scrubbing (fully reading all data). The
      <code class="option">osd scrub load threshold</code> option does not affect this
      setting. Default is 60*60*24*7 (once a week).
     </p></dd><dt id="id-1.3.4.7.16.4.13"><span class="term"><code class="option">osd scrub interval randomize ratio</code></span></dt><dd><p>
      Add a random delay to the <code class="option">osd scrub min interval</code> value
      when scheduling the next scrub job for a placement group. The delay is a
      random value smaller than the result of <code class="option">osd scrub min
      interval</code> * <code class="option">osd scrub interval randomized ratio</code>.
      Therefore, the default setting practically randomly spreads the scrubs
      out in the allowed time window of [1, 1.5] * <code class="option">osd scrub min
      interval</code>. Default is 0.5
     </p></dd><dt id="id-1.3.4.7.16.4.14"><span class="term"><code class="option">osd deep scrub stride</code></span></dt><dd><p>
      Read size when doing a deep scrub. Default is 524288 (512 kB).
     </p></dd></dl></div></section></section><section class="chapter" id="ceph-pools" data-id-title="Managing Storage Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Managing Storage Pools</span> <a title="Permalink" class="permalink" href="#ceph-pools">#</a></h2></div></div></div><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Resilience</em></span>: You can set how many OSDs, buckets, or
    leaves are allowed to fail without losing data. For replicated pools, it is
    the desired number of copies/replicas of an object. New pools are created
    with a default count of replicas set to 3. For erasure coded pools, it is
    the number of coding chunks (that is <span class="emphasis"><em>m=2</em></span> in the
    erasure code profile).
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Placement Groups</em></span>: are internal data structures for
    storing data in a pool across OSDs. The way Ceph stores data into PGs is
    defined in a CRUSH Map. You can set the number of placement groups for a
    pool at its creation. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole. 
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>CRUSH Rules</em></span>: When you store data in a pool, objects
    and its replicas (or chunks in case of erasure coded pools) are placed
    according to the CRUSH ruleset mapped to the pool. You can create a custom
    CRUSH rule for your pool.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Snapshots</em></span>: When you create snapshots with
    <code class="command">ceph osd pool mksnap</code>, you effectively take a snapshot of
    a particular pool.
   </p></li></ul></div><p>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </p><section class="sect1" id="ceph-pools-associate" data-id-title="Associate Pools with an Application"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">Associate Pools with an Application</span> <a title="Permalink" class="permalink" href="#ceph-pools-associate">#</a></h2></div></div></div><p>
   Before using pools, you need to associate them with an application. Pools
   that will be used with CephFS, or pools that are automatically created by
   Object Gateway are automatically associated.
  </p><p>
   For other cases, you can manually associate a free-form application name
   with a pool:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool application enable <em class="replaceable">pool_name</em> <em class="replaceable">application_name</em></pre></div><div id="id-1.3.4.8.6.5" data-id-title="Default Application Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Default Application Names</h6><p>
    CephFS uses the application name <code class="literal">cephfs</code>, RADOS Block Device uses
    <code class="literal">rbd</code>, and Object Gateway uses <code class="literal">rgw</code>.
   </p></div><p>
   A pool can be associated with multiple applications, and each application
   can have its own metadata. You can display the application metadata for a
   given pool using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool application get <em class="replaceable">pool_name</em></pre></div></section><section class="sect1" id="ceph-pools-operate" data-id-title="Operating Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.2 </span><span class="title-name">Operating Pools</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate">#</a></h2></div></div></div><p>
   This section introduces practical information to perform basic tasks with
   pools. You can find out how to list, create, and delete pools, as well as
   show pool statistics or manage snapshots of a pool.
  </p><section class="sect2" id="id-1.3.4.8.7.3" data-id-title="List Pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.1 </span><span class="title-name">List Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.7.3">#</a></h3></div></div></div><p>
    To list your cluster’s pools, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool ls</pre></div></section><section class="sect2" id="ceph-pools-operate-add-pool" data-id-title="Create a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.2 </span><span class="title-name">Create a Pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-add-pool">#</a></h3></div></div></div><p>
    A pool can either be 'replicated' to recover from lost OSDs by keeping
    multiple copies of the objects or 'erasure' to get a kind of generalized
    RAID5/6 capability. The replicated pools require more raw storage, while
    erasure coded pools require less raw storage. Default is 'replicated'.
   </p><p>
    To create a replicated pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create <em class="replaceable">pool_name</em> <em class="replaceable">pg_num</em> <em class="replaceable">pgp_num</em> replicated <em class="replaceable">crush_ruleset_name</em> \
<em class="replaceable">expected_num_objects</em></pre></div><p>
    To create an erasure coded pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create <em class="replaceable">pool_name</em> <em class="replaceable">pg_num</em> <em class="replaceable">pgp_num</em> erasure <em class="replaceable">erasure_code_profile</em> \
 <em class="replaceable">crush_ruleset_name</em> <em class="replaceable">expected_num_objects</em></pre></div><p>
    The <code class="command">ceph osd pool create</code> can fail if you exceed the
    limit of placement groups per OSD. The limit is set with the option
    <code class="option">mon_max_pg_per_osd</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.7.4.8.1"><span class="term">pool_name</span></dt><dd><p>
       The name of the pool. It must be unique. This option is required.
      </p></dd><dt id="id-1.3.4.8.7.4.8.2"><span class="term">pg_num</span></dt><dd><p>
       The total number of placement groups for the pool. This option is
       required. Default value is 8.
      </p></dd><dt id="id-1.3.4.8.7.4.8.3"><span class="term">pgp_num</span></dt><dd><p>
       The total number of placement groups for placement purposes. This should
       be equal to the total number of placement groups, except for placement
       group splitting scenarios. This option is required. Default value is 8.
      </p></dd><dt id="id-1.3.4.8.7.4.8.4"><span class="term">crush_ruleset_name</span></dt><dd><p>
       The name of the crush ruleset for this pool. If the specified ruleset
       does not exist, the creation of replicated pool will fail with -ENOENT.
       For replicated pools it is the ruleset specified by the <code class="varname">osd
       pool default crush replicated ruleset</code> configuration variable.
       This ruleset must exist. For erasure pools it is 'erasure-code' if the
       default erasure code profile is used or
       <em class="replaceable">POOL_NAME</em> otherwise. This ruleset will be
       created implicitly if it does not exist already.
      </p></dd><dt id="id-1.3.4.8.7.4.8.5"><span class="term">erasure_code_profile=profile</span></dt><dd><p>
       For erasure coded pools only. Use the erasure code profile. It must be
       an existing profile as defined by <code class="command">osd erasure-code-profile
       set</code>.
      </p><p>
       When you create a pool, set the number of placement groups to a
       reasonable value. Consider the total number of placement groups per OSD
       too. Placement groups are computationally expensive, so performance will
       degrade when you have many pools with many placement groups (for example
       50 pools with 100 placement groups each).
      </p><p>
       See
       <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/" target="_blank">Placement
       Groups</a> for details on calculating an appropriate number of
       placement groups for your pool.
      </p></dd><dt id="id-1.3.4.8.7.4.8.6"><span class="term">expected_num_objects</span></dt><dd><p>
       The expected number of objects for this pool. By setting this value
       (together with a negative <code class="option">filestore merge threshold</code>),
       the PG folder splitting happens at the pool creation time. This avoids
       the latency impact with a runtime folder splitting.
      </p></dd></dl></div></section><section class="sect2" id="id-1.3.4.8.7.5" data-id-title="Set Pool Quotas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.3 </span><span class="title-name">Set Pool Quotas</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.7.5">#</a></h3></div></div></div><p>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool-name</em> <em class="replaceable">max_objects</em> <em class="replaceable">obj-count</em> <em class="replaceable">max_bytes</em> <em class="replaceable">bytes</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set-quota data max_objects 10000</pre></div><p>
    To remove a quota, set its value to 0.
   </p></section><section class="sect2" id="ceph-pools-operate-del-pool" data-id-title="Delete a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.4 </span><span class="title-name">Delete a Pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-del-pool">#</a></h3></div></div></div><div id="id-1.3.4.8.7.6.2" data-id-title="Pool Deletion is Not Reversible" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Pool Deletion is Not Reversible</h6><p>
     Pools may contain important data. Deleting a pool causes all data in the
     pool to disappear, and there is no way to recover it.
    </p></div><p>
    Because inadvertent pool deletion is a real danger, Ceph implements two
    mechanisms that prevent pools from being deleted. Both mechanisms must be
    disabled before a pool can be deleted.
   </p><p>
    The first mechanism is the <code class="literal">NODELETE</code> flag. Each pool has
    this flag, and its default value is 'false'. To find out the value of this
    flag on a pool, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool get <em class="replaceable">pool_name</em> nodelete</pre></div><p>
    If it outputs <code class="literal">nodelete: true</code>, it is not possible to
    delete the pool until you change the flag using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">pool_name</em> nodelete false</pre></div><p>
    The second mechanism is the cluster-wide configuration parameter
    <code class="option">mon allow pool delete</code>, which defaults to 'false'. This
    means that, by default, it is not possible to delete a pool. The error
    message displayed is:
   </p><div class="verbatim-wrap"><pre class="screen">Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</pre></div><p>
    To delete the pool in spite of this safety setting, you can temporarily set
    <code class="option">mon allow pool delete</code> to 'true', delete the pool, and then
    return the parameter to 'false':
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<code class="prompt user">cephadm &gt; </code>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<code class="prompt user">cephadm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=false</pre></div><p>
    The <code class="command">injectargs</code> command displays the following message:
   </p><div class="verbatim-wrap"><pre class="screen">injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</pre></div><p>
    This is merely confirming that the command was executed successfully. It is
    not an error.
   </p><p>
    If you created your own rulesets and rules for a pool you created, you
    should consider removing them when you no longer need your pool.
   </p></section><section class="sect2" id="id-1.3.4.8.7.7" data-id-title="Rename a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.5 </span><span class="title-name">Rename a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.7.7">#</a></h3></div></div></div><p>
    To rename a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool rename <em class="replaceable">current-pool-name</em> <em class="replaceable">new-pool-name</em></pre></div><p>
    If you rename a pool and you have per-pool capabilities for an
    authenticated user, you must update the user’s capabilities with the new
    pool name.
   </p></section><section class="sect2" id="id-1.3.4.8.7.8" data-id-title="Show Pool Statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.6 </span><span class="title-name">Show Pool Statistics</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.7.8">#</a></h3></div></div></div><p>
    To show a pool’s usage statistics, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</pre></div></section><section class="sect2" id="id-1.3.4.8.7.9" data-id-title="Get Pool Values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.7 </span><span class="title-name">Get Pool Values</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.7.9">#</a></h3></div></div></div><p>
    To get a value from a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool get <em class="replaceable">pool-name</em> <em class="replaceable">key</em></pre></div><p>
    You can get values for keys listed in <a class="xref" href="#ceph-pools-values" title="8.2.8. Set Pool Values">Section 8.2.8, “Set Pool Values”</a>
    plus the following keys:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.7.9.5.1"><span class="term">pg_num</span></dt><dd><p>
       The number of placement groups for the pool.
      </p></dd><dt id="id-1.3.4.8.7.9.5.2"><span class="term">pgp_num</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than
       <code class="literal">pg_num</code>.
      </p></dd></dl></div><div id="id-1.3.4.8.7.9.6" data-id-title="All Pools Values" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: All Pool's Values</h6><p>
     To list all values related to a specific pool, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> all</pre></div></div></section><section class="sect2" id="ceph-pools-values" data-id-title="Set Pool Values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.8 </span><span class="title-name">Set Pool Values</span> <a title="Permalink" class="permalink" href="#ceph-pools-values">#</a></h3></div></div></div><p>
    To set a value to a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">pool-name</em> <em class="replaceable">key</em> <em class="replaceable">value</em></pre></div><p>
    You may set values for the following keys:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.7.10.5.1"><span class="term">size</span></dt><dd><p>
       Sets the number of replicas for objects in the pool. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="8.2.9. Set the Number of Object Replicas">Section 8.2.9, “Set the Number of Object Replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.3.4.8.7.10.5.2"><span class="term">min_size</span></dt><dd><p>
       Sets the minimum number of replicas required for I/O. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="8.2.9. Set the Number of Object Replicas">Section 8.2.9, “Set the Number of Object Replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.3.4.8.7.10.5.3"><span class="term">crash_replay_interval</span></dt><dd><p>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </p></dd><dt id="id-1.3.4.8.7.10.5.4"><span class="term">pg_num</span></dt><dd><p>
       The number of placement groups for the pool. If you add new OSDs to the
       cluster, verify the value for placement groups on all pools targeted for
       the new OSDs, for details refer to
       <a class="xref" href="#storage-bp-cluster-mntc-add-pgnum" title="8.2.10. Increasing the Number of Placement Groups">Section 8.2.10, “Increasing the Number of Placement Groups”</a>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.5"><span class="term">pgp_num</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement.
      </p></dd><dt id="id-1.3.4.8.7.10.5.6"><span class="term">crush_ruleset</span></dt><dd><p>
       The ruleset to use for mapping object placement in the cluster.
      </p></dd><dt id="id-1.3.4.8.7.10.5.7"><span class="term">hashpspool</span></dt><dd><p>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to the
       default 0, the cluster starts backfilling to have a correct placement of
       all PGs again. Be aware that this can create quite substantial I/O load
       on a cluster, therefore do not enable the flag from 0 to 1 on a highly
       loaded production clusters.
      </p></dd><dt id="id-1.3.4.8.7.10.5.8"><span class="term">nodelete</span></dt><dd><p>
       Prevents the pool from being removed.
      </p></dd><dt id="id-1.3.4.8.7.10.5.9"><span class="term">nopgchange</span></dt><dd><p>
       Prevents the pool's <code class="option">pg_num</code> and <code class="option">pgp_num</code>
       from being changed.
      </p></dd><dt id="id-1.3.4.8.7.10.5.10"><span class="term">nosizechange</span></dt><dd><p>
       Prevents the pool's size from being changed.
      </p></dd><dt id="id-1.3.4.8.7.10.5.11"><span class="term">write_fadvise_dontneed</span></dt><dd><p>
       Set/Unset the <code class="literal">WRITE_FADVISE_DONTNEED</code> flag on a given
       pool.
      </p></dd><dt id="id-1.3.4.8.7.10.5.12"><span class="term">noscrub,nodeep-scrub</span></dt><dd><p>
       Disables (deep)-scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </p></dd><dt id="id-1.3.4.8.7.10.5.13"><span class="term">hit_set_type</span></dt><dd><p>
       Enables hit set tracking for cache pools. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. This option can have the
       following values: <code class="literal">bloom</code>,
       <code class="literal">explicit_hash</code>, <code class="literal">explicit_object</code>.
       Default is <code class="literal">bloom</code>, other values are for testing only.
      </p></dd><dt id="id-1.3.4.8.7.10.5.14"><span class="term">hit_set_count</span></dt><dd><p>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <code class="systemitem">ceph-osd</code> daemon.
       Default is <code class="literal">0</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.15"><span class="term">hit_set_period</span></dt><dd><p>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <code class="systemitem">ceph-osd</code> daemon.
      </p></dd><dt id="id-1.3.4.8.7.10.5.16"><span class="term">hit_set_fpp</span></dt><dd><p>
       The false positive probability for the bloom hit set type. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. Valid range is 0.0 - 1.0
       Default is <code class="literal">0.05</code>
      </p></dd><dt id="id-1.3.4.8.7.10.5.17"><span class="term">use_gmt_hitset</span></dt><dd><p>
       Force OSDs to use GMT (Greenwich Mean Time) time stamps when creating a
       hit set for cache tiering. This ensures that nodes in different time
       zones return the same result. Default is <code class="literal">1</code>. This
       value should not be changed.
      </p></dd><dt id="id-1.3.4.8.7.10.5.18"><span class="term">cache_target_dirty_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <code class="literal">0.4</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.19"><span class="term">cache_target_dirty_high_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <code class="literal">0.6</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.20"><span class="term">cache_target_full_ratio</span></dt><dd><p>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <code class="literal">0.8</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.21"><span class="term">target_max_bytes</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_bytes</code> threshold is triggered.
      </p></dd><dt id="id-1.3.4.8.7.10.5.22"><span class="term">target_max_objects</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_objects</code> threshold is triggered.
      </p></dd><dt id="id-1.3.4.8.7.10.5.23"><span class="term">hit_set_grade_decay_rate</span></dt><dd><p>
       Temperature decay rate between two successive
       <code class="literal">hit_set</code>s. Default is <code class="literal">20</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.24"><span class="term">hit_set_search_last_n</span></dt><dd><p>
       Count at most <code class="literal">N</code> appearances in
       <code class="literal">hit_set</code>s for temperature calculation. Default is
       <code class="literal">1</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.25"><span class="term">cache_min_flush_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </p></dd><dt id="id-1.3.4.8.7.10.5.26"><span class="term">cache_min_evict_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </p></dd><dt id="id-1.3.4.8.7.10.5.27"><span class="term">fast_read</span></dt><dd><p>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <span class="emphasis"><em>jerasure</em></span> and <span class="emphasis"><em>isa</em></span> erasure
       plug-ins, when the first <code class="literal">K</code> replies return, then the
       client’s request is served immediately using the data decoded from
       these replies. This approach cause more CPU load and less disk/network
       load. Currently, this flag is only supported for erasure coding pools.
       Default is <code class="literal">0</code>.
      </p></dd><dt id="id-1.3.4.8.7.10.5.28"><span class="term">scrub_min_interval</span></dt><dd><p>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_min_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.3.4.8.7.10.5.29"><span class="term">scrub_max_interval</span></dt><dd><p>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_max_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.3.4.8.7.10.5.30"><span class="term">deep_scrub_interval</span></dt><dd><p>
       The interval in seconds for the pool <span class="emphasis"><em>deep</em></span>
       scrubbing. The default <code class="literal">0</code> means that the
       <code class="option">osd_deep_scrub</code> value from the Ceph configuration file
       is used.
      </p></dd></dl></div></section><section class="sect2" id="ceph-pools-options-num-of-replicas" data-id-title="Set the Number of Object Replicas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.9 </span><span class="title-name">Set the Number of Object Replicas</span> <a title="Permalink" class="permalink" href="#ceph-pools-options-num-of-replicas">#</a></h3></div></div></div><p>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The <em class="replaceable">num-replicas</em> includes the object itself. For
    example if you want the object and two copies of the object for a total of
    three instances of the object, specify 3.
   </p><div id="id-1.3.4.8.7.11.5" data-id-title="Do not Set Less than 3 Replicas" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Do not Set Less than 3 Replicas</h6><p>
     If you set the <em class="replaceable">num-replicas</em> to 2, there will be
     only <span class="emphasis"><em>one</em></span> copy of your data. If you lose one object
     instance, you need to trust that the other copy has not been corrupted for
     example since the last scrubbing during recovery (refer to
     <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a> for details).
    </p><p>
     Setting a pool to one replica means that there is exactly
     <span class="emphasis"><em>one</em></span> instance of the data object in the pool. If the
     OSD fails, you lose the data. A possible usage for a pool with one replica
     is storing temporary data for a short time.
    </p></div><div id="id-1.3.4.8.7.11.6" data-id-title="Setting More than 3 Replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Setting More than 3 Replicas</h6><p>
     Setting 4 replicas for a pool increases the reliability by 25%.
    </p><p>
     In case of two data centers, you need to set at least 4 replicas for a
     pool to have two copies in each data center so that in case one data
     center is lost, still two copies exist and you can still lose one disk
     without loosing data.
    </p></div><div id="id-1.3.4.8.7.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     An object might accept I/Os in degraded mode with fewer than <code class="literal">pool
     size</code> replicas. To set a minimum number of required replicas for
     I/O, you should use the <code class="literal">min_size</code> setting. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set data min_size 2</pre></div><p>
     This ensures that no object in the data pool will receive I/O with fewer
     than <code class="literal">min_size</code> replicas.
    </p></div><div id="id-1.3.4.8.7.11.8" data-id-title="Get the Number of Object Replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Get the Number of Object Replicas</h6><p>
     To get the number of object replicas, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd dump | grep 'replicated size'</pre></div><p>
     Ceph will list the pools, with the <code class="literal">replicated size</code>
     attribute highlighted. By default, Ceph creates two replicas of an
     object (a total of three copies, or a size of 3).
    </p></div></section><section class="sect2" id="storage-bp-cluster-mntc-add-pgnum" data-id-title="Increasing the Number of Placement Groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.10 </span><span class="title-name">Increasing the Number of Placement Groups</span> <a title="Permalink" class="permalink" href="#storage-bp-cluster-mntc-add-pgnum">#</a></h3></div></div></div><p>
    When creating a new pool, you specify the number of placement groups for
    the pool (see <a class="xref" href="#ceph-pools-operate-add-pool" title="8.2.2. Create a Pool">Section 8.2.2, “Create a Pool”</a>
    
    ). After adding more OSDs to the cluster, you usually need to increase the
    number of placement groups as well for performance and data durability
    reasons. For each placement group, OSD and monitor nodes need memory,
    network and CPU at all times and even more during recovery. From which
    follows that minimizing the number of placement groups saves significant
    amounts of resources. On the other hand - too small number of placement
    groups causes unequal data distribution among OSDs.
   </p><div id="id-1.3.4.8.7.12.3" data-id-title="Too High Value of pg_num" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Too High Value of <code class="option">pg_num</code></h6><p>
     When changing the <code class="option">pg_num</code> value for a pool, it may happen
     that the new number of placement groups exceeds the allowed limit. For
     example
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</pre></div><p>
     The limit prevents extreme placement group splitting, and is derived from
     the <code class="option">mon_osd_max_split_count</code> value.
    </p></div><div id="id-1.3.4.8.7.12.4" data-id-title="Reducing the Number of Placement Groups not Possible" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Reducing the Number of Placement Groups not Possible</h6><p>
     While increasing the number of placement groups on a pool is possible at
     any time, reducing is <span class="bold"><strong>not</strong></span> possible.
    </p></div><p>
    To determine the right new number of placement groups for a resized cluster
    is a complex task. One approach is to continuously grow the number of
    placement groups up to the state when the cluster performance is
    satisfactory. To determine the new incremented number of placement groups,
    you need to get the value of the <code class="option">mon_osd_max_split_count</code>
    parameter (default is 32), and add it to the current number of placement
    groups. To give you a basic idea, take a look at the following script:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<code class="prompt user">cephadm &gt; </code>pg_num=`ceph osd pool get <em class="replaceable">POOL_NAME</em> pg_num | cut -f2 -d: | tr -d ' '`
<code class="prompt user">cephadm &gt; </code>echo "current pg_num value: $pg_num, max increment: $max_inc"
<code class="prompt user">cephadm &gt; </code>next_pg_num="$(($pg_num+$max_inc))"
<code class="prompt user">cephadm &gt; </code>echo "allowed increment of pg_num: $next_pg_num"</pre></div><p>
    After finding out the next number of placement groups, increase it with
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">NEXT_PG_NUM</em></pre></div></section></section><section class="sect1" id="pools-migration" data-id-title="Pool Migration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.3 </span><span class="title-name">Pool Migration</span> <a title="Permalink" class="permalink" href="#pools-migration">#</a></h2></div></div></div><p>
   When creating a pool (see <a class="xref" href="#ceph-pools-operate-add-pool" title="8.2.2. Create a Pool">Section 8.2.2, “Create a Pool”</a>) you
   need to specify its initial parameters, such as the pool type or the number
   of placement groups. If you later decide to change any of these
   parameters—for example when converting a replicated pool into an
   erasure coded one, or decreasing the number of placement groups—, you
   need to migrate the pool data to another one whose parameters suit your
   deployment.
  </p><p>
   This section describes two migration methods—a <span class="emphasis"><em>cache
   tier</em></span> method for general pool data migration, and a method using
   <code class="command">rbd migrate</code> sub-commands to migrate RBD images to a new
   pool. Each method has its specifics and limitations.
  </p><section class="sect2" id="pool-migrate-limits" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.1 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#pool-migrate-limits">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can use the <span class="emphasis"><em>cache tier</em></span> method to migrate from a
      replicated pool to either an EC pool or another replicated pool.
      Migrating from an EC pool is not supported.
     </p></li><li class="listitem"><p>
      You cannot migrate RBD images and CephFS exports from a replicated pool
      to an EC pool. The reason is that EC pools do not support
      <code class="literal">omap</code>, while RBD and CephFS use
      <code class="literal">omap</code> to store its metadata. For example, the header
      object of the RBD will fail to be flushed. But you can migrate data to EC
      pool, leaving metadata in replicated pool.
     </p></li><li class="listitem"><p>
      The <code class="command">rbd migration</code> method allows migrating images with
      minimal client downtime. You only need to stop the client before the
      <code class="option">prepare</code> step and start it afterward. Note that only a
      <code class="systemitem">librbd</code> client that supports this feature (Ceph
      Nautilus or newer) will be able to open the image just after the
      <code class="option">prepare</code> step, while older
      <code class="systemitem">librbd</code> clients or the
      <code class="systemitem">krbd</code> clients will not be able to open the image
      until the <code class="option">commit</code> step is executed.
     </p></li></ul></div></section><section class="sect2" id="pool-migrate-cache-tier" data-id-title="Migrate Using Cache Tier"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.2 </span><span class="title-name">Migrate Using Cache Tier</span> <a title="Permalink" class="permalink" href="#pool-migrate-cache-tier">#</a></h3></div></div></div><p>
    The principle is simple—include the pool that you need to migrate
    into a cache tier in reverse order. Find more details on cache tiers in
    <a class="xref" href="#cha-ceph-tiered" title="Chapter 11. Cache Tiering">Chapter 11, <em>Cache Tiering</em></a>. The following example migrates a
    replicated pool named 'testpool' to an erasure coded pool:
   </p><div class="procedure" id="id-1.3.4.8.8.5.3" data-id-title="Migrating Replicated to Erasure Coded Pool"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 8.1: </span><span class="title-name">Migrating Replicated to Erasure Coded Pool </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a new erasure coded pool named 'newpool'. Refer to
      <a class="xref" href="#ceph-pools-operate-add-pool" title="8.2.2. Create a Pool">Section 8.2.2, “Create a Pool”</a> for detailed explanation of
      pool creation parameters.
     </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm &gt; </code>ceph osd pool create newpool <em class="replaceable">PG_NUM</em> <em class="replaceable">PGP_NUM</em> erasure default</pre></div><p>
      Verify that the used client keyring provides at least the same
      capabilities for 'newpool' as it does for 'testpool'.
     </p><p>
      Now you have two pools: the original replicated 'testpool' filled with
      data, and the new empty erasure coded 'newpool':
     </p><div class="figure" id="id-1.3.4.8.8.5.3.2.5"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate1.png" target="_blank"><img src="images/pool_migrate1.png" width="" alt="Pools before Migration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.1: </span><span class="title-name">Pools before Migration </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3.2.5">#</a></h6></div></div></li><li class="step"><p>
      Setup the cache tier and configure the replicated pool 'testpool' as a
      cache pool. The <code class="option">-force-nonempty</code> option allows adding a
      cache tier even if the pool already has data:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<code class="prompt user">cephadm &gt; </code>ceph osd tier add newpool testpool --force-nonempty
<code class="prompt user">cephadm &gt; </code>ceph osd tier cache-mode testpool proxy</pre></div><div class="figure" id="id-1.3.4.8.8.5.3.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate2.png" target="_blank"><img src="images/pool_migrate2.png" width="" alt="Cache Tier Setup"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.2: </span><span class="title-name">Cache Tier Setup </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3.3.3">#</a></h6></div></div></li><li class="step"><p>
      Force the cache pool to move all objects to the new pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados -p testpool cache-flush-evict-all</pre></div><div class="figure" id="id-1.3.4.8.8.5.3.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate3.png" target="_blank"><img src="images/pool_migrate3.png" width="" alt="Data Flushing"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.3: </span><span class="title-name">Data Flushing </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3.4.3">#</a></h6></div></div></li><li class="step"><p>
      Until all the data has been flushed to the new erasure coded pool, you
      need to specify an overlay so that objects are searched on the old pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier set-overlay newpool testpool</pre></div><p>
      With the overlay, all operations are forwarded to the old replicated
      'testpool':
     </p><div class="figure" id="id-1.3.4.8.8.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate4.png" target="_blank"><img src="images/pool_migrate4.png" width="" alt="Setting Overlay"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.4: </span><span class="title-name">Setting Overlay </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3.5.4">#</a></h6></div></div><p>
      Now you can switch all the clients to access objects on the new pool.
     </p></li><li class="step"><p>
      After all data is migrated to the erasure coded 'newpool', remove the
      overlay and the old cache pool 'testpool':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier remove-overlay newpool
<code class="prompt user">cephadm &gt; </code>ceph osd tier remove newpool testpool</pre></div><div class="figure" id="id-1.3.4.8.8.5.3.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate5.png" target="_blank"><img src="images/pool_migrate5.png" width="" alt="Migration Complete"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.5: </span><span class="title-name">Migration Complete </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8.5.3.6.3">#</a></h6></div></div></li><li class="step"><p>
      Run
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'</pre></div></li></ol></div></div></section><section class="sect2" id="migrate-rbd-image" data-id-title="Migrating RBD Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.3 </span><span class="title-name">Migrating RBD Images</span> <a title="Permalink" class="permalink" href="#migrate-rbd-image">#</a></h3></div></div></div><p>
    The following is the recommended way to migrate RBD images from one
    replicated pool to another replicated pool.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop clients (such as a virtual machine) from accessing the RBD image.
     </p></li><li class="step"><p>
      Create a new image in the target pool, with the parent set to the source
      image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd migration prepare <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em> <em class="replaceable">TARGET_POOL</em>/<em class="replaceable">IMAGE</em></pre></div><div id="id-1.3.4.8.8.6.3.2.3" data-id-title="Migrate Only Data to an EC Pool" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Migrate Only Data to an EC Pool</h6><p>
       If you need to migrate only the image data to a new EC pool and leave
       the metadata in the original replicated pool, run the following command
       instead:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd migration prepare <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em> \
 --data-pool <em class="replaceable">TARGET_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></div></li><li class="step"><p>
      Let clients access the image in the target pool.
     </p></li><li class="step"><p>
      Migrate data to the target pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd migration execute <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li><li class="step"><p>
      Remove the old image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd migration commit <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li></ol></div></div></section></section><section class="sect1" id="cha-ceph-snapshots-pool" data-id-title="Pool Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.4 </span><span class="title-name">Pool Snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-pool">#</a></h2></div></div></div><p>
   Pool snapshots are snapshots of the state of the whole Ceph pool. With
   pool snapshots, you can retain the history of the pool's state. Creating
   pool snapshots consumes storage space proportional to the pool size. Always
   check the related storage for enough disk space before creating a snapshot
   of a pool.
  </p><section class="sect2" id="id-1.3.4.8.9.3" data-id-title="Make a Snapshot of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.1 </span><span class="title-name">Make a Snapshot of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.9.3">#</a></h3></div></div></div><p>
    To make a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool mksnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</pre></div></section><section class="sect2" id="id-1.3.4.8.9.4" data-id-title="List Snapshots of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.2 </span><span class="title-name">List Snapshots of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.9.4">#</a></h3></div></div></div><p>
    To list existing snapshots of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados lssnap -p <em class="replaceable">POOL_NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps</pre></div></section><section class="sect2" id="id-1.3.4.8.9.5" data-id-title="Remove a Snapshot of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.3 </span><span class="title-name">Remove a Snapshot of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.4.8.9.5">#</a></h3></div></div></div><p>
    To remove a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool rmsnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div></section></section><section class="sect1" id="sec-ceph-pool-compression" data-id-title="Data Compression"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.5 </span><span class="title-name">Data Compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression">#</a></h2></div></div></div><p>
   BlueStore (find more details in <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 5.5 and Ceph”, Section 1.5 “BlueStore”</span>)
   provides on-the-fly data compression to save disk space. The compression
   ratio depends on the data stored in the system. Note that compression /
   de-compression requires additional CPU power.
  </p><p>
   You can configure data compression globally (see
   <a class="xref" href="#sec-ceph-pool-bluestore-compression-options" title="8.5.3. Global Compression Options">Section 8.5.3, “Global Compression Options”</a>), and then
   override specific compression settings for each individual pool.
  </p><p>
   You can enable or disable pool data compression, or change the compression
   algorithm and mode at any time, regardless of whether the pool contains data
   or not.
  </p><p>
   No compression will be applied to existing data after enabling the pool
   compression.
  </p><p>
   After disabling the compression of a pool, all its data will be
   decompressed.
  </p><section class="sect2" id="sec-ceph-pool-compression-enable" data-id-title="Enable Compression"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.5.1 </span><span class="title-name">Enable Compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-enable">#</a></h3></div></div></div><p>
    To enable data compression for a pool named
    <em class="replaceable">POOL_NAME</em>, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm <em class="replaceable">COMPRESSION_ALGORITHM</em>
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_mode <em class="replaceable">COMPRESSION_MODE</em></pre></div><div id="id-1.3.4.8.10.7.4" data-id-title="Disabling Pool Compression" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling Pool Compression</h6><p>
     To disable data compression for a pool, use 'none' as the compression
     algorithm:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm none</pre></div></div></section><section class="sect2" id="sec-ceph-pool-compression-options" data-id-title="Pool Compression Options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.5.2 </span><span class="title-name">Pool Compression Options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-options">#</a></h3></div></div></div><p>
    A full list of compression settings:
   </p><div class="variablelist"><dl class="variablelist"><dt id="compr-algorithm"><span class="term">compression_algorithm</span></dt><dd><p>
       Possible values are <code class="literal">none</code>, <code class="literal">zstd</code>,
       <code class="literal">snappy</code>. Default is <code class="literal">snappy</code>.
      </p><p>
       Which compression algorithm to use depends on the specific use case.
       Several recommendations follow:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Use the default <code class="literal">snappy</code> as long as you do not have a
         good reason to change it.
        </p></li><li class="listitem"><p>
         <code class="literal">zstd</code> offers a good compression ratio, but causes
         high CPU overhead when compressing small amounts of data.
        </p></li><li class="listitem"><p>
         Run a benchmark of these algorithms on a sample of your actual data
         while keeping an eye on the CPU and memory usage of your cluster.
        </p></li></ul></div></dd><dt id="compr-mode"><span class="term">compression_mode</span></dt><dd><p>
       Possible values are <code class="literal">none</code>,
       <code class="literal">aggressive</code>, <code class="literal">passive</code>,
       <code class="literal">force</code>. Default is <code class="literal">none</code>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">none</code>: compress never
        </p></li><li class="listitem"><p>
         <code class="literal">passive</code>: compress if hinted
         <code class="literal">COMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">aggressive</code>: compress unless hinted
         <code class="literal">INCOMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">force</code>: compress always
        </p></li></ul></div></dd><dt id="compr-ratio"><span class="term">compression_required_ratio</span></dt><dd><p>
       Value: Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Default is
       <code class="literal">0.875</code>, which means that if the compression does not
       reduce the occupied space by at least 12,5%, the object will not be
       compressed.
      </p><p>
       Objects above this ratio will not be stored compressed because of the
       low net gain.
      </p></dd><dt id="id-1.3.4.8.10.8.3.4"><span class="term">compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed.
      </p></dd><dt id="id-1.3.4.8.10.8.3.5"><span class="term">compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed.
      </p></dd></dl></div></section><section class="sect2" id="sec-ceph-pool-bluestore-compression-options" data-id-title="Global Compression Options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.5.3 </span><span class="title-name">Global Compression Options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-bluestore-compression-options">#</a></h3></div></div></div><p>
    The following configuration options can be set in the Ceph configuration
    and apply to all OSDs and not only a single pool. The pool specific
    configuration listed in <a class="xref" href="#sec-ceph-pool-compression-options" title="8.5.2. Pool Compression Options">Section 8.5.2, “Pool Compression Options”</a>
    take precedence.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.8.10.9.3.1"><span class="term">bluestore_compression_algorithm</span></dt><dd><p>
       See <a class="xref" href="#compr-algorithm">compression_algorithm</a>.
      </p></dd><dt id="id-1.3.4.8.10.9.3.2"><span class="term">bluestore_compression_mode</span></dt><dd><p>
       See <a class="xref" href="#compr-mode">compression_mode</a>
      </p></dd><dt id="id-1.3.4.8.10.9.3.3"><span class="term">bluestore_compression_required_ratio</span></dt><dd><p>
       See <a class="xref" href="#compr-ratio">compression_required_ratio</a>
      </p></dd><dt id="id-1.3.4.8.10.9.3.4"><span class="term">bluestore_compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed. The setting is ignored by
       default in favor of
       <code class="option">bluestore_compression_min_blob_size_hdd</code> and
       <code class="option">bluestore_compression_min_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.3.4.8.10.9.3.5"><span class="term">bluestore_compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed before they will be split
       into smaller chunks. The setting is ignored by default in favor of
       <code class="option">bluestore_compression_max_blob_size_hdd</code> and
       <code class="option">bluestore_compression_max_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.3.4.8.10.9.3.6"><span class="term">bluestore_compression_min_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">8K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on solid-state
       drive.
      </p></dd><dt id="id-1.3.4.8.10.9.3.7"><span class="term">bluestore_compression_max_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">64K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on solid-state
       drive before they will be split into smaller chunks.
      </p></dd><dt id="id-1.3.4.8.10.9.3.8"><span class="term">bluestore_compression_min_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">128K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on hard disks.
      </p></dd><dt id="id-1.3.4.8.10.9.3.9"><span class="term">bluestore_compression_max_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">512K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on hard disks
       before they will be split into smaller chunks.
      </p></dd></dl></div></section></section></section><section class="chapter" id="ceph-rbd" data-id-title="RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9 </span><span class="title-name">RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ceph-rbd">#</a></h2></div></div></div><p>
  A block is a sequence of bytes, for example a 4MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like Ceph.
 </p><p>
  Ceph block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a Ceph cluster. Ceph block
  devices leverage RADOS capabilities such as snapshotting, replication, and
  consistency. Ceph's RADOS Block Devices (RBD) interact with OSDs using kernel modules or
  the <code class="systemitem">librbd</code> library.
 </p><div class="figure" id="id-1.3.4.9.5"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_rbd_schema.png" target="_blank"><img src="images/ceph_rbd_schema.png" width="" alt="RADOS Protocol"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.1: </span><span class="title-name">RADOS Protocol </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.5">#</a></h6></div></div><p>
  Ceph's block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as QEMU, or
  cloud-based computing systems such as OpenStack that rely on <code class="systemitem">libvirt</code>. You
  can use the same cluster to operate the Object Gateway, CephFS, and RADOS Block Devices
  simultaneously.
 </p><section class="sect1" id="ceph-rbd-commands" data-id-title="Block Device Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Block Device Commands</span> <a title="Permalink" class="permalink" href="#ceph-rbd-commands">#</a></h2></div></div></div><p>
   The <code class="command">rbd</code> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </p><section class="sect2" id="ceph-rbd-cmds-create" data-id-title="Creating a Block Device Image in a Replicated Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.1 </span><span class="title-name">Creating a Block Device Image in a Replicated Pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create">#</a></h3></div></div></div><p>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <a class="xref" href="#ceph-pools" title="Chapter 8. Managing Storage Pools">Chapter 8, <em>Managing Storage Pools</em></a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd create --size <em class="replaceable">MEGABYTES</em> <em class="replaceable">POOL-NAME</em>/<em class="replaceable">IMAGE-NAME</em></pre></div><p>
    For example, to create a 1GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd create --size 1024 mypool/myimage</pre></div><div id="id-1.3.4.9.7.3.6" data-id-title="Image Size Units" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Image Size Units</h6><p>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </p></div></section><section class="sect2" id="ceph-rbd-cmds-create-ec" data-id-title="Creating a Block Device Image in an Erasure Coded Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.2 </span><span class="title-name">Creating a Block Device Image in an Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create-ec">#</a></h3></div></div></div><p>
    As of SUSE Enterprise Storage 5.5, it is possible to store data of a
    block device image directly in erasure coded (EC) pools. RADOS Block Device image
    consists of <span class="emphasis"><em>data</em></span> and <span class="emphasis"><em>metadata</em></span>
    parts. You can store only the 'data' part of an RADOS Block Device image in an EC pool.
    The pool needs to have the 'overwrite' flag set to
    <span class="emphasis"><em>true</em></span>, and that is only possible if all OSDs where the
    pool is stored use BlueStore.
   </p><p>
    You cannot store the image's 'metadata' part in an EC pool. You need to
    specify the replicated pool for storing image's metadata with the
    <code class="option">--pool=</code> option of the <code class="command">rbd create</code>
    command.
   </p><p>
    Use the following steps to create an RBD image in a newly created EC pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd pool create <em class="replaceable">POOL_NAME</em> 12 12 erasure
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> allow_ec_overwrites true

#Metadata will reside in pool "<em class="replaceable">OTHER_POOL</em>", and data in pool "<em class="replaceable">POOL_NAME</em>"
<code class="prompt user">cephadm &gt; </code><code class="command">rbd</code> create <em class="replaceable">IMAGE_NAME</em> --size=1G --data-pool <em class="replaceable">POOL_NAME</em> --pool=<em class="replaceable">OTHER_POOL</em></pre></div></section><section class="sect2" id="ceph-rbd-cmds-list" data-id-title="Listing Block Device Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.3 </span><span class="title-name">Listing Block Device Images</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-list">#</a></h3></div></div></div><p>
    To list block devices in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd ls mypool</pre></div></section><section class="sect2" id="ceph-rbd-cmds-info" data-id-title="Retrieving Image Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.4 </span><span class="title-name">Retrieving Image Information</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-info">#</a></h3></div></div></div><p>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd info mypool/myimage</pre></div></section><section class="sect2" id="ceph-rbd-cmds-resize" data-id-title="Resizing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.5 </span><span class="title-name">Resizing a Block Device Image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-resize">#</a></h3></div></div></div><p>
    RADOS Block Device images are thin provisioned—they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <code class="option">--size</code> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> # to increase
<code class="prompt user">cephadm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --allow-shrink # to decrease</pre></div></section><section class="sect2" id="ceph-rbd-cmds-rm" data-id-title="Removing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.6 </span><span class="title-name">Removing a Block Device Image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-rm">#</a></h3></div></div></div><p>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd rm mypool/myimage</pre></div></section></section><section class="sect1" id="storage-bp-integration-mount-rbd" data-id-title="Mounting and Unmounting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.2 </span><span class="title-name">Mounting and Unmounting</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-mount-rbd">#</a></h2></div></div></div><p>
   After you create a RADOS Block Device, you can use it as any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure your Ceph cluster includes a pool with the disk image you want
     to map. Assume the pool is called <code class="literal">mypool</code> and the image
     is <code class="literal">myimage</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
     Map the image to a new block device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.4.9.8.3.2.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
      To specify a user name, use <code class="option">--id
      <em class="replaceable">user-name</em></code>. If you use
      <code class="systemitem">cephx</code> authentication, you also need to specify a
      secret. It may come from a keyring or a file containing the secret:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
      or
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
     List all mapped devices:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div><p>
     The device we want to work on is <code class="filename">/dev/rbd0</code>.
    </p><div id="id-1.3.4.9.8.3.3.4" data-id-title="RBD Device Path" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: RBD Device Path</h6><p>
      Instead of
      <code class="filename">/dev/rbd<em class="replaceable">DEVICE_NUMBER</em></code>,
      you can use
      <code class="filename">/dev/rbd/<em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></code>
      as a persistent device path. For example:
     </p><div class="verbatim-wrap"><pre class="screen">/dev/rbd/mypool/myimage</pre></div></div></li><li class="step"><p>
     Make an XFS file system on the <code class="filename">/dev/rbd0</code> device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div></li><li class="step"><p>
     Mount the device and check it is correctly mounted. Replace
     <code class="filename">/mnt</code> with your mount point.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount /dev/rbd0 /mnt
<code class="prompt user">root # </code>mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</pre></div><p>
     Now you can move data to and from the device as if it was a local
     directory.
    </p><div id="id-1.3.4.9.8.3.5.4" data-id-title="Increasing the Size of RBD Device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increasing the Size of RBD Device</h6><p>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
        Increase the size of the RBD image, for example up to 10GB.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
        Grow the file system to fill up the new size of the device.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></div></li><li class="step"><p>
     After you finish accessing the device, you can unmap and unmount it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd unmap /dev/rbd0
<code class="prompt user">root # </code>unmount /mnt</pre></div></li></ol></div></div><div id="id-1.3.4.9.8.4" data-id-title="Manual (Un)mounting" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Manual (Un)mounting</h6><p>
    Since manually mapping and mounting RBD images after boot and unmounting
    and unmapping them before shutdown can be tedious, an
    <code class="command">rbdmap</code> script and <code class="systemitem">systemd</code> unit is provided. Refer to
    <a class="xref" href="#ceph-rbd-rbdmap" title="9.2.1. rbdmap: Map RBD Devices at Boot Time">Section 9.2.1, “rbdmap: Map RBD Devices at Boot Time”</a>.
   </p></div><section class="sect2" id="ceph-rbd-rbdmap" data-id-title="rbdmap: Map RBD Devices at Boot Time"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.2.1 </span><span class="title-name">rbdmap: Map RBD Devices at Boot Time</span> <a title="Permalink" class="permalink" href="#ceph-rbd-rbdmap">#</a></h3></div></div></div><p>
    <code class="command">rbdmap</code> is a shell script that automates <code class="command">rbd
    map</code> and <code class="command">rbd unmap</code> operations on one or more
    RBD images. Although you can run the script manually at any time, the main
    advantage is automatic mapping and mounting of RBD images at boot time (and
    unmounting and unmapping at shutdown), as triggered by the Init system. A
    <code class="systemitem">systemd</code> unit file, <code class="filename">rbdmap.service</code> is included with
    the <code class="systemitem">ceph-common</code> package for this purpose.
   </p><p>
    The script takes a single argument, which can be either
    <code class="option">map</code> or <code class="option">unmap</code>. In either case, the script
    parses a configuration file. It defaults to
    <code class="filename">/etc/ceph/rbdmap</code>, but can be overridden via an
    environment variable <code class="literal">RBDMAPFILE</code>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </p><p>
    The configuration file has the following format:
   </p><div class="verbatim-wrap"><pre class="screen">image_specification rbd_options</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.9.8.5.6.1"><span class="term"><code class="option">image_specification</code></span></dt><dd><p>
       Path to an image within a pool. Specify as
       <em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>.
      </p></dd><dt id="id-1.3.4.9.8.5.6.2"><span class="term"><code class="option">rbd_options</code></span></dt><dd><p>
       An optional list of parameters to be passed to the underlying
       <code class="command">rbd map</code> command. These parameters and their values
       should be specified as a comma-separated string, for example:
      </p><div class="verbatim-wrap"><pre class="screen">PARAM1=VAL1,PARAM2=VAL2,...</pre></div><p>
       The example makes the <code class="command">rbdmap</code> script run the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --PARAM1 VAL1 --PARAM2 VAL2</pre></div><p>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbdmap map mypool/myimage id=rbd_user,keyring=/etc/ceph/ceph.client.rbd.keyring</pre></div></dd></dl></div><p>
    When run as <code class="command">rbdmap map</code>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using <code class="command">the rbd map</code> command) and then mount
    the image.
   </p><p>
    When run as <code class="command">rbdmap unmap</code>, images listed in the
    configuration file will be unmounted and unmapped.
   </p><p>
    <code class="command">rbdmap unmap-all</code> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </p><p>
    If successful, the rbd map operation maps the image to a /dev/rbdX device,
    at which point a udev rule is triggered to create a friendly device name
    symbolic link
    <code class="filename">/dev/rbd/<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em></code>
    pointing to the real mapped device.
   </p><p>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <code class="filename">/etc/fstab</code>.
    When writing <code class="filename">/etc/fstab</code> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early—before the device in
    question even exists, as <code class="filename">rbdmap.service</code> is typically
    triggered quite late in the boot sequence.
   </p><p>
    For a complete list of <code class="command">rbd</code> options, see the
    <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>).
   </p><p>
    For examples of the <code class="command">rbdmap</code> usage, see the
    <code class="command">rbdmap</code> manual page (<code class="command">man 8 rbdmap</code>).
   </p></section><section class="sect2" id="id-1.3.4.9.8.6" data-id-title="Increasing the Size of RBD Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.2.2 </span><span class="title-name">Increasing the Size of RBD Device</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.8.6">#</a></h3></div></div></div><p>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Increase the size of the RBD image, for example up to 10GB.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
      Grow the file system to fill up the new size of the device.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></section></section><section class="sect1" id="cha-ceph-snapshots-rbd" data-id-title="Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.3 </span><span class="title-name">Snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-rbd">#</a></h2></div></div></div><p>
   An RBD snapshot is a snapshot of a RADOS Block Device image. With snapshots, you retain a
   history of the image's state. Ceph also supports snapshot layering, which
   allows you to clone VM images quickly and easily. Ceph supports block
   device snapshots using the <code class="command">rbd</code> command and many
   higher-level interfaces, including QEMU, <code class="systemitem">libvirt</code>,
   OpenStack, and CloudStack.
  </p><div id="id-1.3.4.9.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </p></div><section class="sect2" id="id-1.3.4.9.9.4" data-id-title="Cephx Notes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.1 </span><span class="title-name">Cephx Notes</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.4">#</a></h3></div></div></div><p>
    When <code class="systemitem">cephx</code> is enabled (see
    <a class="link" href="http://ceph.com/docs/master/rados/configuration/auth-config-ref/" target="_blank">http://ceph.com/docs/master/rados/configuration/auth-config-ref/</a>
    for more information), you must specify a user name or ID and a path to the
    keyring containing the corresponding key for the user. See
    <a class="link" href="http://ceph.com/docs/master/rados/operations/user-management/" target="_blank">User
    Management</a> for more details. You may also add the
    <code class="systemitem">CEPH_ARGS</code> environment variable to avoid re-entry
    of the following parameters.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --id <em class="replaceable">user-ID</em> --keyring=/path/to/secret <em class="replaceable">commands</em>
<code class="prompt user">cephadm &gt; </code>rbd --name <em class="replaceable">username</em> --keyring=/path/to/secret <em class="replaceable">commands</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --id admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em>
<code class="prompt user">cephadm &gt; </code>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em></pre></div><div id="id-1.3.4.9.9.4.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Add the user and secret to the <code class="systemitem">CEPH_ARGS</code>
     environment variable so that you do not need to enter them each time.
    </p></div></section><section class="sect2" id="id-1.3.4.9.9.5" data-id-title="Snapshot Basics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.2 </span><span class="title-name">Snapshot Basics</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <code class="command">rbd</code> command on the command line.
   </p><section class="sect3" id="id-1.3.4.9.9.5.3" data-id-title="Create Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.2.1 </span><span class="title-name">Create Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5.3">#</a></h4></div></div></div><p>
     To create a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     create</code> option, the pool name, and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap create --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap create <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool rbd snap create --snap snapshot1 image1
<code class="prompt user">cephadm &gt; </code>rbd snap create rbd/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.4.9.9.5.4" data-id-title="List Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.2.2 </span><span class="title-name">List Snapshots</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5.4">#</a></h4></div></div></div><p>
     To list snapshots of an image, specify the pool name and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap ls <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap ls <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool rbd snap ls image1
<code class="prompt user">cephadm &gt; </code>rbd snap ls rbd/image1</pre></div></section><section class="sect3" id="id-1.3.4.9.9.5.5" data-id-title="Rollback Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.2.3 </span><span class="title-name">Rollback Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5.5">#</a></h4></div></div></div><p>
     To rollback to a snapshot with <code class="command">rbd</code>, specify the
     <code class="option">snap rollback</code> option, the pool name, the image name, and
     the snapshot name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rollback --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap rollback <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 snap rollback --snap snapshot1 image1
<code class="prompt user">cephadm &gt; </code>rbd snap rollback pool1/image1@snapshot1</pre></div><div id="id-1.3.4.9.9.5.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <span class="emphasis"><em>faster to
      clone</em></span> from a snapshot <span class="emphasis"><em>than to rollback</em></span> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </p></div></section><section class="sect3" id="id-1.3.4.9.9.5.6" data-id-title="Delete a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.2.4 </span><span class="title-name">Delete a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5.6">#</a></h4></div></div></div><p>
     To delete a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     rm</code> option, the pool name, the image name, and the user name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rm --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap rm <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 snap rm --snap snapshot1 image1
<code class="prompt user">cephadm &gt; </code>rbd snap rm pool1/image1@snapshot1</pre></div><div id="id-1.3.4.9.9.5.6.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </p></div></section><section class="sect3" id="id-1.3.4.9.9.5.7" data-id-title="Purge Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.2.5 </span><span class="title-name">Purge Snapshots</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.5.7">#</a></h4></div></div></div><p>
     To delete all snapshots for an image with <code class="command">rbd</code>, specify
     the <code class="option">snap purge</code> option and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap purge <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap purge <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 snap purge image1
<code class="prompt user">cephadm &gt; </code>rbd snap purge pool1/image1</pre></div></section></section><section class="sect2" id="ceph-snapshoti-layering" data-id-title="Layering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.3 </span><span class="title-name">Layering</span> <a title="Permalink" class="permalink" href="#ceph-snapshoti-layering">#</a></h3></div></div></div><p>
    Ceph supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables Ceph block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics—making it possible to create clones rapidly.
   </p><div id="id-1.3.4.9.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a Ceph block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </p></div><p>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </p><p>
    A COW clone of a snapshot behaves exactly like any other Ceph block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <span class="emphasis"><em>must</em></span> protect the snapshot before you clone it.
   </p><div id="id-1.3.4.9.9.6.6" data-id-title="--image-format 1 not Supported" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">--image-format 1</code> not Supported</h6><p>
     You cannot create snapshots of images created with the deprecated
     <code class="command">rbd create --image-format 1</code> option. Ceph only
     supports cloning of the default <span class="emphasis"><em>format 2</em></span> images.
    </p></div><section class="sect3" id="id-1.3.4.9.9.6.7" data-id-title="Getting Started with Layering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.1 </span><span class="title-name">Getting Started with Layering</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.6.7">#</a></h4></div></div></div><p>
     Ceph block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </p><p>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Image Template</em></span>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, SUSE Linux Enterprise Server), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <code class="command">zypper ref &amp;&amp; zypper patch</code>
       followed by <code class="command">rbd snap create</code>). As the image matures,
       the user can clone any one of the snapshots.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Extended Template</em></span>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Template Pool</em></span>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Image Migration/Recovery</em></span>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </p></li></ul></div></section><section class="sect3" id="id-1.3.4.9.9.6.8" data-id-title="Protecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.2 </span><span class="title-name">Protecting a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.6.8">#</a></h4></div></div></div><p>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap protect \
 --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap protect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<code class="prompt user">cephadm &gt; </code>rbd snap protect pool1/image1@snapshot1</pre></div><div id="id-1.3.4.9.9.6.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You cannot delete a protected snapshot.
     </p></div></section><section class="sect3" id="id-1.3.4.9.9.6.9" data-id-title="Cloning a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.3 </span><span class="title-name">Cloning a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.6.9">#</a></h4></div></div></div><p>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd clone --pool <em class="replaceable">pool-name</em> --image <em class="replaceable">parent-image</em> \
 --snap <em class="replaceable">snap-name</em> --dest-pool <em class="replaceable">pool-name</em> \
 --dest <em class="replaceable">child-image</em>
<code class="prompt user">cephadm &gt; </code>rbd clone <em class="replaceable">pool-name</em>/<em class="replaceable">parent-image</em>@<em class="replaceable">snap-name</em> \
<em class="replaceable">pool-name</em>/<em class="replaceable">child-image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd clone pool1/image1@snapshot1 pool1/image2</pre></div><div id="id-1.3.4.9.9.6.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </p></div></section><section class="sect3" id="id-1.3.4.9.9.6.10" data-id-title="Unprotecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.4 </span><span class="title-name">Unprotecting a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.6.10">#</a></h4></div></div></div><p>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <span class="emphasis"><em>not</em></span> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap unprotect --image <em class="replaceable">image-name</em> \
 --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm &gt; </code>rbd snap unprotect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<code class="prompt user">cephadm &gt; </code>rbd snap unprotect pool1/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.4.9.9.6.11" data-id-title="Listing Children of a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.5 </span><span class="title-name">Listing Children of a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.9.6.11">#</a></h4></div></div></div><p>
     To list the children of a snapshot, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> children --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snap-name</em>
<code class="prompt user">cephadm &gt; </code>rbd children <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 children --image image1 --snap snapshot1
<code class="prompt user">cephadm &gt; </code>rbd children pool1/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-flatten" data-id-title="Flattening a Cloned Image"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.3.3.6 </span><span class="title-name">Flattening a Cloned Image</span> <a title="Permalink" class="permalink" href="#rbd-flatten">#</a></h4></div></div></div><p>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> flatten --image <em class="replaceable">image-name</em>
<code class="prompt user">cephadm &gt; </code>rbd flatten <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool pool1 flatten --image image1
<code class="prompt user">cephadm &gt; </code>rbd flatten pool1/image1</pre></div><div id="id-1.3.4.9.9.6.12.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </p></div></section></section></section><section class="sect1" id="ceph-rbd-mirror" data-id-title="Mirroring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.4 </span><span class="title-name">Mirroring</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror">#</a></h2></div></div></div><p>
   RBD images can be asynchronously mirrored between two Ceph clusters. This
   capability uses the RBD journaling image feature to ensure crash-consistent
   replication between clusters. Mirroring is configured on a per-pool basis
   within peer clusters and can be configured to automatically mirror all
   images within a pool or only a specific subset of images. Mirroring is
   configured using the <code class="command">rbd</code> command. The
   <code class="systemitem">rbd-mirror</code> daemon is responsible for pulling image
   updates from the remote peer cluster and applying them to the image within
   the local cluster.
  </p><div id="id-1.3.4.9.10.3" data-id-title="rbd-mirror Daemon" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: rbd-mirror Daemon</h6><p>
    To use RBD mirroring, you need to have two Ceph clusters, each running
    the <code class="systemitem">rbd-mirror</code> daemon.
   </p></div><div id="id-1.3.4.9.10.4" data-id-title="RADOS Block Devices Exported via iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: RADOS Block Devices Exported via iSCSI</h6><p>
    You cannot mirror RBD devices that are exported via iSCSI using
    <code class="systemitem">lrbd</code>.
   </p><p>
    Refer to <a class="xref" href="#cha-ceph-iscsi" title="Chapter 14. Ceph iSCSI Gateway">Chapter 14, <em>Ceph iSCSI Gateway</em></a> for more details on iSCSI.
   </p></div><section class="sect2" id="rbd-mirror-daemon" data-id-title="rbd-mirror Daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.4.1 </span><span class="title-name">rbd-mirror Daemon</span> <a title="Permalink" class="permalink" href="#rbd-mirror-daemon">#</a></h3></div></div></div><p>
    The two <code class="systemitem">rbd-mirror</code> daemons are responsible for
    watching image journals on the remote, peer cluster and replaying the
    journal events against the local cluster. The RBD image journaling feature
    records all modifications to the image in the order they occur. This
    ensures that a crash-consistent mirror of the remote image is available
    locally.
   </p><p>
    The <code class="systemitem">rbd-mirror</code> daemon is available in the
    <span class="package">rbd-mirror</span> package. You can install the package on OSD
    nodes, gateway nodes, or even on dedicated nodes. We do not recommend
    installing the <span class="package">rbd-mirror</span> on the Salt master/admin node.
    Install, enable, and start <span class="package">rbd-mirror</span>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install rbd-mirror
<code class="prompt user">root # </code>systemctl enable ceph-rbd-mirror@<em class="replaceable">server_name</em>.service
<code class="prompt user">root # </code>systemctl start ceph-rbd-mirror@<em class="replaceable">server_name</em>.service</pre></div><div id="id-1.3.4.9.10.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Each <code class="systemitem">rbd-mirror</code> daemon requires the ability to
     connect to both clusters simultaneously.
    </p></div></section><section class="sect2" id="ceph-rbd-mirror-poolconfig" data-id-title="Pool Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.4.2 </span><span class="title-name">Pool Configuration</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror-poolconfig">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <code class="command">rbd</code> command. Mirroring is configured on a per-pool basis
    within the Ceph clusters.
   </p><p>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named 'local' and 'remote', are
    accessible from a single host for clarity.
   </p><p>
    See the <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>)
    for additional details on how to connect to different Ceph clusters.
   </p><div id="id-1.3.4.9.10.6.5" data-id-title="Multiple Clusters" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple Clusters</h6><p>
     The cluster name in the following examples corresponds to a Ceph
     configuration file of the same name
     <code class="filename">/etc/ceph/remote.conf</code>. See the
     <a class="link" href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#running-multiple-clusters" target="_blank">ceph-conf</a>
     documentation for how to configure multiple clusters.
    </p></div><section class="sect3" id="id-1.3.4.9.10.6.6" data-id-title="Enable Mirroring on a Pool"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.2.1 </span><span class="title-name">Enable Mirroring on a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.6.6">#</a></h4></div></div></div><p>
     To enable mirroring on a pool, specify the <code class="command">mirror pool
     enable</code> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.9.10.6.6.3.1"><span class="term">pool</span></dt><dd><p>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </p></dd><dt id="id-1.3.4.9.10.6.6.3.2"><span class="term">image</span></dt><dd><p>
        Mirroring needs to be explicitly enabled on each image. See
        <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="9.4.3.2. Enable Image Mirroring">Section 9.4.3.2, “Enable Image Mirroring”</a> for more
        information.
       </p></dd></dl></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool enable <em class="replaceable">POOL_NAME</em> pool
<code class="prompt user">cephadm &gt; </code>rbd --cluster remote mirror pool enable <em class="replaceable">POOL_NAME</em> pool</pre></div></section><section class="sect3" id="id-1.3.4.9.10.6.7" data-id-title="Disable Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.2.2 </span><span class="title-name">Disable Mirroring</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.6.7">#</a></h4></div></div></div><p>
     To disable mirroring on a pool, specify the <code class="command">mirror pool
     disable</code> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool disable <em class="replaceable">POOL_NAME</em>
<code class="prompt user">cephadm &gt; </code>rbd --cluster remote mirror pool disable <em class="replaceable">POOL_NAME</em></pre></div></section><section class="sect3" id="id-1.3.4.9.10.6.8" data-id-title="Add Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.2.3 </span><span class="title-name">Add Cluster Peer</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.6.8">#</a></h4></div></div></div><p>
     In order for the <code class="systemitem">rbd-mirror</code> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <code class="command">mirror pool peer
     add</code> subcommand, the pool name, and a cluster specification:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool peer add <em class="replaceable">POOL_NAME</em> client.remote@remote
<code class="prompt user">cephadm &gt; </code>rbd --cluster remote mirror pool peer add <em class="replaceable">POOL_NAME</em> client.local@local</pre></div></section><section class="sect3" id="id-1.3.4.9.10.6.9" data-id-title="Remove Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.2.4 </span><span class="title-name">Remove Cluster Peer</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.6.9">#</a></h4></div></div></div><p>
     To remove a mirroring peer cluster, specify the <code class="command">mirror pool peer
     remove</code> subcommand, the pool name, and the peer UUID (available
     from the <code class="command">rbd mirror pool info</code> command):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 55672766-c02b-4729-8567-f13a66893445
<code class="prompt user">cephadm &gt; </code>rbd --cluster remote mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 60c0e299-b38f-4234-91f6-eed0a367be08</pre></div></section></section><section class="sect2" id="rbd-mirror-imageconfig" data-id-title="Image Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.4.3 </span><span class="title-name">Image Configuration</span> <a title="Permalink" class="permalink" href="#rbd-mirror-imageconfig">#</a></h3></div></div></div><p>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer Ceph cluster.
   </p><p>
    Mirrored RBD images are designated as either <span class="emphasis"><em>primary</em></span>
    or <span class="emphasis"><em>non-primary</em></span>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </p><p>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="9.4.3.2. Enable Image Mirroring">Section 9.4.3.2, “Enable Image Mirroring”</a>) by the
    <code class="command">rbd</code> command).
   </p><section class="sect3" id="id-1.3.4.9.10.7.5" data-id-title="Image Journaling Support"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.3.1 </span><span class="title-name">Image Journaling Support</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.7.5">#</a></h4></div></div></div><p>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. Before an image can be
     mirrored to a peer cluster, the journaling feature must be enabled. The
     feature can be enabled at the time of image creation by providing the
     <code class="option">--image-feature exclusive-lock,journaling</code> option to the
     <code class="command">rbd</code> command.
    </p><p>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <code class="command">feature enable</code> subcommand, the pool and image name, and
     the feature name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local feature enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> journaling</pre></div><div id="id-1.3.4.9.10.7.5.5" data-id-title="Option Dependency" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Option Dependency</h6><p>
      The <code class="option">journaling</code> feature is dependent on the
      <code class="option">exclusive-lock</code> feature. If the
      <code class="option">exclusive-lock</code> feature is not already enabled, you need
      to enable it prior to enabling the <code class="option">journaling</code> feature.
     </p></div><div id="id-1.3.4.9.10.7.5.6" data-id-title="Journaling on All New Images" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Journaling on All New Images</h6><p>
      You can enable journaling on all new images by default by appending the
      <code class="literal">journaling</code> value to the <code class="option">rbd default
      features</code> option in the Ceph configuration file. For example:
     </p><div class="verbatim-wrap"><pre class="screen">rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</pre></div><p>
      Before applying such change, carefully consider if enabling journaling on
      all new images is good for your deployment because it can have negative
      performance impact.
     </p></div></section><section class="sect3" id="rbd-mirror-enable-image-mirroring" data-id-title="Enable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.3.2 </span><span class="title-name">Enable Image Mirroring</span> <a title="Permalink" class="permalink" href="#rbd-mirror-enable-image-mirroring">#</a></h4></div></div></div><p>
     If mirroring is configured in the 'image' mode, then it is necessary to
     explicitly enable mirroring for each image within the pool. To enable
     mirroring for a specific image, specify the <code class="command">mirror image
     enable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror image enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.4.9.10.7.7" data-id-title="Disable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.3.3 </span><span class="title-name">Disable Image Mirroring</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.7.7">#</a></h4></div></div></div><p>
     To disable mirroring for a specific image, specify the <code class="command">mirror
     image disable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror image disable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.4.9.10.7.8" data-id-title="Image Promotion and Demotion"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.3.4 </span><span class="title-name">Image Promotion and Demotion</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.7.8">#</a></h4></div></div></div><p>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </p><div id="id-1.3.4.9.10.7.8.3" data-id-title="Forced Promotion" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Forced Promotion</h6><p>
      Promotion can be forced using the <code class="option">--force</code> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <code class="command">resync</code>subcommand is issued.
     </p></div><p>
     To demote a specific image to non-primary, specify the <code class="command">mirror
     image demote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror image demote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To demote all primary images within a pool to non-primary, specify the
     <code class="command">mirror pool demote</code> subcommand along with the pool name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool demote <em class="replaceable">POOL_NAME</em></pre></div><p>
     To promote a specific image to primary, specify the <code class="command">mirror image
     promote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster remote mirror image promote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To promote all non-primary images within a pool to primary, specify the
     <code class="command">mirror pool promote</code> subcommand along with the pool
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --cluster local mirror pool promote <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.4.9.10.7.8.12" data-id-title="Split I/O Load" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Split I/O Load</h6><p>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the IO load and stage failover or failback.
     </p></div></section><section class="sect3" id="id-1.3.4.9.10.7.9" data-id-title="Force Image Resync"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.4.3.5 </span><span class="title-name">Force Image Resync</span> <a title="Permalink" class="permalink" href="#id-1.3.4.9.10.7.9">#</a></h4></div></div></div><p>
     If a split-brain event is detected by the
     <code class="systemitem">rbd-mirror</code> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out of date and then request a
     resync to the primary image. To request an image resync, specify the
     <code class="command">mirror image resync</code> subcommand along with the pool and
     image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd mirror image resync <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section></section><section class="sect2" id="rbd-mirror-status" data-id-title="Mirror Status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.4.4 </span><span class="title-name">Mirror Status</span> <a title="Permalink" class="permalink" href="#rbd-mirror-status">#</a></h3></div></div></div><p>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <code class="command">mirror image
    status</code> and <code class="command">mirror pool status</code> subcommands:
   </p><p>
    To request the mirror image status, specify the <code class="command">mirror image
    status</code> subcommand along with the pool and image name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd mirror image status <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
    To request the mirror pool summary status, specify the <code class="command">mirror pool
    status</code> subcommand along with the pool name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd mirror pool status <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.4.9.10.8.7" data-id-title="" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: </h6><p>
     Adding the <code class="option">--verbose</code> option to the <code class="command">mirror pool
     status</code> subcommand will additionally output status details for
     every mirroring image in the pool.
    </p></div></section></section><section class="sect1" id="rbd-features" data-id-title="Advanced Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.5 </span><span class="title-name">Advanced Features</span> <a title="Permalink" class="permalink" href="#rbd-features">#</a></h2></div></div></div><p>
   RADOS Block Device supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the Ceph configuration file by using the
   <code class="option">rbd_default_features</code> option.
  </p><p>
   You can specify the values of the <code class="option">rbd_default_features</code>
   option in two ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     As a sum of features' internal values. Each feature has its own internal
     value—for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = 17</pre></div></li><li class="listitem"><p>
     As a comma-separated list of features. The previous example will look as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = layering,fast-diff</pre></div></li></ul></div><div id="id-1.3.4.9.11.5" data-id-title="Features not Supported by iSCSI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Features not Supported by iSCSI</h6><p>
    RBD images with the following features will not be supported by iSCSI:
    <code class="option">deep-flatten</code>, <code class="option">striping</code>,
    <code class="option">exclusive-lock</code>, <code class="option">object-map</code>,
    <code class="option">journaling</code>, <code class="option">fast-diff</code>
   </p></div><p>
   List of advanced RBD features follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.9.11.7.1"><span class="term"><code class="option">layering</code></span></dt><dd><p>
      Layering enables you to use cloning.
     </p><p>
      Internal value is 1, default is 'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.2"><span class="term"><code class="option">striping</code></span></dt><dd><p>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottleneck
      for large or busy RADOS Block Devices.
     </p><p>
      Internal value is 2, default is 'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.3"><span class="term"><code class="option">exclusive-lock</code></span></dt><dd><p>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.4"><span class="term"><code class="option">object-map</code></span></dt><dd><p>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </p><p>
      Internal value is 8, default is 'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.5"><span class="term"><code class="option">fast-diff</code></span></dt><dd><p>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image, and the actual
      data usage of a snapshot.
     </p><p>
      Internal value is 16, default is 'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.6"><span class="term"><code class="option">deep-flatten</code></span></dt><dd><p>
      Deep-flatten makes the <code class="command">rbd flatten</code> (see
      <a class="xref" href="#rbd-flatten" title="9.3.3.6. Flattening a Cloned Image">Section 9.3.3.6, “Flattening a Cloned Image”</a>) work on all the snapshots of an image, in
      addition to the image itself. Without it, snapshots of an image will
      still rely on the parent, therefore you will not be able to delete the
      parent image until the snapshots are deleted. Deep-flatten makes a parent
      independent of its clones, even if they have snapshots.
     </p><p>
      Internal value is 32, default is 'yes'.
     </p></dd><dt id="id-1.3.4.9.11.7.7"><span class="term"><code class="option">journaling</code></span></dt><dd><p>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <a class="xref" href="#ceph-rbd-mirror" title="9.4. Mirroring">Section 9.4, “Mirroring”</a>) utilizes the journal to replicate a
      crash consistent image to a remote cluster.
     </p><p>
      Internal value is 64, default is 'no'.
     </p></dd></dl></div></section></section><section class="chapter" id="cha-ceph-erasure" data-id-title="Erasure Coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure">#</a></h2></div></div></div><p>
  Ceph provides an alternative to the normal replication of data in pools,
  called <span class="emphasis"><em>erasure</em></span> or <span class="emphasis"><em>erasure coded</em></span>
  pool. Erasure pools do not provide all functionality of
  <span class="emphasis"><em>replicated</em></span> pools (for example it cannot store metadata
  for RBD pools), but require less raw storage. A default erasure pool capable
  of storing 1 TB of data requires 1,5 TB of raw storage, allowing a single
  disk failure. This compares favorably to a replicated pool which needs 2 TB
  of raw storage for the same purpose.
 </p><p>
  For background information on Erasure Code, see
  <a class="link" href="https://en.wikipedia.org/wiki/Erasure_code" target="_blank">https://en.wikipedia.org/wiki/Erasure_code</a>.
 </p><div id="id-1.3.4.10.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   When using FileStore, you cannot access erasure coded pools with the RBD
   interface unless you have a cache tier configured. Refer to
   <a class="xref" href="#ceph-tier-erasure" title="11.5. Erasure Coded Pool and Cache Tiering">Section 11.5, “Erasure Coded Pool and Cache Tiering”</a> for more details, or use BlueStore.
  </p></div><section class="sect1" id="ec-prerequisite" data-id-title="Prerequisite for Erasure Coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="#ec-prerequisite">#</a></h2></div></div></div><p>
   To make use of erasure coding, you need to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Define an erasure rule in the CRUSH Map.
    </p></li><li class="listitem"><p>
     Define an erasure code profile that specifies the coding algorithm to be
     used.
    </p></li><li class="listitem"><p>
     Create a pool using the previously mentioned rule and profile.
    </p></li></ul></div><p>
   Keep in mind that changing the profile and the details in the profile will
   not be possible after the pool was created and has data.
  </p><p>
   Ensure that the CRUSH rules for <span class="emphasis"><em>erasure pools</em></span> use
   <code class="literal">indep</code> for <code class="literal">step</code>. For details see
   <a class="xref" href="#datamgm-rules-step-mode" title="7.3.2. firstn and indep">Section 7.3.2, “firstn and indep”</a>.
  </p></section><section class="sect1" id="cha-ceph-erasure-default-profile" data-id-title="Creating a Sample Erasure Coded Pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-default-profile">#</a></h2></div></div></div><p>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     The command <code class="command">ceph osd pool create</code> is used to create a
     pool with type <span class="emphasis"><em>erasure</em></span>. The <code class="literal">12</code>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</pre></div></li><li class="step"><p>
     The string <code class="literal">ABCDEFGHI</code> is written into an object called
     <code class="literal">NYAN</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -</pre></div></li><li class="step"><p>
     For testing purposes OSDs can now be disabled, for example by
     disconnecting them from the network.
    </p></li><li class="step"><p>
     To test whether the pool can handle the failure of devices, the content of
     the file can be accessed with the <code class="command">rados</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div></li></ol></div></div></section><section class="sect1" id="cha-ceph-erasure-erasure-profiles" data-id-title="Erasure Code Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Erasure Code Profiles</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-erasure-profiles">#</a></h2></div></div></div><p>
   When the <code class="command">ceph osd pool create</code> command is invoked to
   create an <span class="emphasis"><em>erasure pool</em></span>, the default profile is used,
   unless another profile is specified. Profiles define the redundancy of data.
   This is done by setting two parameters, arbitrarily named
   <code class="literal">k</code> and <code class="literal">m</code>. k and m define in how many
   <code class="literal">chunks</code> a piece of data is split and how many coding
   chunks are created. Redundant chunks are then stored on different OSDs.
  </p><p>
   Definitions required for erasure pool profiles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.10.8.4.1"><span class="term">chunk</span></dt><dd><p>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </p></dd><dt id="id-1.3.4.10.8.4.2"><span class="term">k</span></dt><dd><p>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <code class="literal">k = 2</code> a
      10KB object will be divided into <code class="literal">k</code> objects of 5KB
      each.
     </p></dd><dt id="id-1.3.4.10.8.4.3"><span class="term">m</span></dt><dd><p>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </p></dd><dt id="id-1.3.4.10.8.4.4"><span class="term">crush-failure-domain</span></dt><dd><p>
      defines to which devices the chunks are distributed. A bucket type needs
      to be set as value. For all bucket types, see
      <a class="xref" href="#datamgm-buckets" title="7.2. Buckets">Section 7.2, “Buckets”</a>. If the failure domain is
      <code class="literal">rack</code>, the chunks will be stored on different racks to
      increase the resilience in case of rack failures. Keep in mind that this
      requires k+m racks.
     </p></dd></dl></div><p>
   With the default erasure code profile used in
   <a class="xref" href="#cha-ceph-erasure-default-profile" title="10.2. Creating a Sample Erasure Coded Pool">Section 10.2, “Creating a Sample Erasure Coded Pool”</a>, you will not lose
   cluster data if a single OSD or host fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage are
   required for 1 TB of data (due to k=2,m=1). This is equivalent to a common
   RAID 5 configuration. For comparison: a replicated pool needs 2 TB of raw
   storage to store 1 TB of data.
  </p><p>
   The settings of the default profile can be displayed with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</pre></div><p>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new one (see
   <a class="xref" href="#pools-migration" title="8.3. Pool Migration">Section 8.3, “Pool Migration”</a>).
  </p><p>
   The most important parameters of the profile are <code class="literal">k</code>,
   <code class="literal">m</code> and <code class="literal">crush-failure-domain</code> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66% overhead, the following profile can be defined. Note that
   this is only valid with a CRUSH Map that has buckets of type 'rack':
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd erasure-code-profile set <em class="replaceable">myprofile</em> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</pre></div><p>
   The example <a class="xref" href="#cha-ceph-erasure-default-profile" title="10.2. Creating a Sample Erasure Coded Pool">Section 10.2, “Creating a Sample Erasure Coded Pool”</a> can be
   repeated with this new profile:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create ecpool 12 12 erasure <em class="replaceable">myprofile</em>
<code class="prompt user">cephadm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<code class="prompt user">cephadm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div><p>
   The NYAN object will be divided in three (<code class="literal">k=3</code>) and two
   additional chunks will be created (<code class="literal">m=2</code>). The value of
   <code class="literal">m</code> defines how many OSDs can be lost simultaneously
   without losing any data. The <code class="literal">crush-failure-domain=rack</code>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/ceph_erasure_obj.png" target="_blank"><img src="images/ceph_erasure_obj.png" width=""/></a></div></div><p>
   For more information about the erasure code profiles, see
   <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile" target="_blank">http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile</a>.
  </p></section><section class="sect1" id="ec-rbd" data-id-title="Erasure Coded Pools with RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ec-rbd">#</a></h2></div></div></div><p>
   To mark an EC pool as a RBD pool, tag it accordingly:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool application enable rbd <em class="replaceable">ec_pool_name</em></pre></div><p>
   RBD can store image <span class="emphasis"><em>data</em></span> in EC pools. However, the
   image header and metadata still needs to be stored in a replicated pool.
   Assuming you have the pool named 'rbd' for this purpose:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd create rbd/<em class="replaceable">image_name</em> --size 1T --data-pool <em class="replaceable">ec_pool_name</em></pre></div><p>
   You can use the image normally like any other image, except that all of the
   data will be stored in the <em class="replaceable">ec_pool_name</em> pool
   instead of 'rbd' pool.
  </p></section></section><section class="chapter" id="cha-ceph-tiered" data-id-title="Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11 </span><span class="title-name">Cache Tiering</span> <a title="Permalink" class="permalink" href="#cha-ceph-tiered">#</a></h2></div></div></div><p>
  A <span class="emphasis"><em>cache tier</em></span> is an additional storage layer implemented
  between the client and the standard storage. It is designed to speed up
  access to pools stored on slow hard disks and erasure coded pools.
 </p><p>
  Typically cache tiering involves creating a pool of relatively fast storage
  devices (for example SSD drives) configured to act as a cache tier, and a
  backing pool of slower and cheaper devices configured to act as a storage
  tier. The size of the cache pool is usually 10-20% of the storage pool.
 </p><section class="sect1" id="id-1.3.4.11.5" data-id-title="Tiered Storage Terminology"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Tiered Storage Terminology</span> <a title="Permalink" class="permalink" href="#id-1.3.4.11.5">#</a></h2></div></div></div><p>
   Cache tiering recognizes two types of pools: a <span class="emphasis"><em>cache
   pool</em></span> and a <span class="emphasis"><em>storage pool</em></span>.
  </p><div id="id-1.3.4.11.5.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    For general information on pools, see <a class="xref" href="#ceph-pools" title="Chapter 8. Managing Storage Pools">Chapter 8, <em>Managing Storage Pools</em></a>.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.5.4.1"><span class="term">storage pool</span></dt><dd><p>
      Either a standard replicated pool that stores several copies of an object
      in the Ceph storage cluster, or an erasure coded pool (see
      <a class="xref" href="#cha-ceph-erasure" title="Chapter 10. Erasure Coded Pools">Chapter 10, <em>Erasure Coded Pools</em></a>).
     </p><p>
      The storage pool is sometimes referred to as 'backing' or 'cold' storage.
     </p></dd><dt id="id-1.3.4.11.5.4.2"><span class="term">cache pool</span></dt><dd><p>
      A standard replicated pool stored on a relatively small but fast storage
      device with their own ruleset in a CRUSH Map.
     </p><p>
      The cache pool is also referred to as 'hot' storage.
     </p></dd></dl></div></section><section class="sect1" id="sec-ceph-tiered-caution" data-id-title="Points to Consider"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.2 </span><span class="title-name">Points to Consider</span> <a title="Permalink" class="permalink" href="#sec-ceph-tiered-caution">#</a></h2></div></div></div><p>
   Cache tiering may <span class="emphasis"><em>degrade</em></span> the cluster performance for
   specific workloads. The following points show some of its aspects that you
   need to consider:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="emphasis"><em>Workload-dependent</em></span>: Whether a cache will improve
     performance is dependent on the workload. Because there is a cost
     associated with moving objects into or out of the cache, it can be more
     effective when most of the requests touch a small number of objects. The
     cache pool should be large enough to capture the working set for your
     workload to avoid thrashing.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em>Difficult to benchmark</em></span>: Most performance benchmarks
     may show low performance with cache tiering. The reason is that they
     request a big set of objects, and it takes a long time for the cache to
     'warm up'.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em>Possibly low performance</em></span>: For workloads that are not
     suitable for cache tiering, performance is often slower than a normal
     replicated pool without cache tiering enabled.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em><code class="systemitem">librados</code> object enumeration</em></span>:
     If your application is using <code class="systemitem">librados</code> directly
     and relies on object enumeration, cache tiering may not work as expected.
     (This is not a problem for Object Gateway, RBD, or CephFS.)
    </p></li></ul></div></section><section class="sect1" id="id-1.3.4.11.7" data-id-title="When to Use Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.3 </span><span class="title-name">When to Use Cache Tiering</span> <a title="Permalink" class="permalink" href="#id-1.3.4.11.7">#</a></h2></div></div></div><p>
   Consider using cache tiering in the following cases:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Your erasure coded pools are stored on FileStore and you need to access
     them via RADOS Block Device. For more information on RBD, see
     <a class="xref" href="#ceph-rbd" title="Chapter 9. RADOS Block Device">Chapter 9, <em>RADOS Block Device</em></a>.
    </p></li><li class="listitem"><p>
     Your erasure coded pools are stored on FileStore and you need to access
     them via iSCSI. For more information on iSCSI, refer to
     <a class="xref" href="#cha-ceph-iscsi" title="Chapter 14. Ceph iSCSI Gateway">Chapter 14, <em>Ceph iSCSI Gateway</em></a>.
    </p></li><li class="listitem"><p>
     You have a limited number of high-performance storage and a large
     collection of low-performance storage, and need to access the stored data
     faster.
    </p></li></ul></div></section><section class="sect1" id="sec-ceph-tiered-cachemodes" data-id-title="Cache Modes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.4 </span><span class="title-name">Cache Modes</span> <a title="Permalink" class="permalink" href="#sec-ceph-tiered-cachemodes">#</a></h2></div></div></div><p>
   The cache tiering agent handles the migration of data between the cache tier
   and the backing storage tier. Administrators have the ability to configure
   how this migration takes place. There are two main scenarios:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.8.3.1"><span class="term">write-back mode</span></dt><dd><p>
      In write-back mode, Ceph clients write data to the cache tier and
      receive an ACK from the cache tier. In time, the data written to the
      cache tier migrates to the storage tier and gets flushed from the cache
      tier. Conceptually, the cache tier is overlaid 'in front' of the backing
      storage tier. When a Ceph client needs data that resides in the storage
      tier, the cache tiering agent migrates the data to the cache tier on
      read, then it is sent to the Ceph client. Thereafter, the Ceph client
      can perform I/O using the cache tier, until the data becomes inactive.
      This is ideal for mutable, data such as photo or video editing, or
      transactional data.
     </p></dd><dt id="id-1.3.4.11.8.3.2"><span class="term">read-only mode</span></dt><dd><p>
      In read-only mode, Ceph clients write data directly to the backing
      tier. On read, Ceph copies the requested objects from the backing tier
      to the cache tier. Stale objects get removed from the cache tier based on
      the defined policy. This approach is ideal for immutable data such as
      presenting pictures or videos on a social network, DNA data, or X-ray
      imaging, because reading data from a cache pool that might contain
      out-of-date data provides weak consistency. Do not use read-only mode for
      mutable data.
     </p></dd></dl></div></section><section class="sect1" id="ceph-tier-erasure" data-id-title="Erasure Coded Pool and Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.5 </span><span class="title-name">Erasure Coded Pool and Cache Tiering</span> <a title="Permalink" class="permalink" href="#ceph-tier-erasure">#</a></h2></div></div></div><p>
   Erasure coded pools require more resources than replicated pools. To
   overcome these limitations, we recommended to set a cache tier before the
   erasure coded pool. This it a requirement when using FileStore.
  </p><p>
   For example, if the <span class="quote">“<span class="quote">hot-storage</span>”</span> pool is made of fast storage,
   the <span class="quote">“<span class="quote">ecpool</span>”</span> created in
   <a class="xref" href="#cha-ceph-erasure-erasure-profiles" title="10.3. Erasure Code Profiles">Section 10.3, “Erasure Code Profiles”</a> can be speeded up with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier add ecpool hot-storage
<code class="prompt user">cephadm &gt; </code>ceph osd tier cache-mode hot-storage writeback
<code class="prompt user">cephadm &gt; </code>ceph osd tier set-overlay ecpool hot-storage</pre></div><p>
   This will place the <span class="quote">“<span class="quote">hot-storage</span>”</span> pool as a tier of ecpool in
   write-back mode so that every write and read to the ecpool is actually using
   the hot-storage and benefits from its flexibility and speed.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool ecpool create --size 10 myvolume</pre></div><p>
   For more information about cache tiering, see
   <a class="xref" href="#cha-ceph-tiered" title="Chapter 11. Cache Tiering">Chapter 11, <em>Cache Tiering</em></a>.
  </p></section><section class="sect1" id="ses-tiered-storage" data-id-title="Setting Up an Example Tiered Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.6 </span><span class="title-name">Setting Up an Example Tiered Storage</span> <a title="Permalink" class="permalink" href="#ses-tiered-storage">#</a></h2></div></div></div><p>
   This section illustrates how to set up a fast SSD cache tier (hot storage)
   in front of a standard hard disk (cold storage).
  </p><div id="id-1.3.4.11.10.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    The following example is for illustration purposes only and includes a
    setup with one root and one rule for the SSD part residing on a single
    Ceph node.
   </p><p>
    In the production environment, cluster setups typically include more root
    and rule entries for the hot storage, and also mixed nodes, with both SSDs
    and SATA disks.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create two additional CRUSH rules, 'replicated_ssd' for the fast SSD
     caching device class, and 'replicated_hdd' for the slower HDD device
     class:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rule create-replicated replicated_ssd default host ssd
<code class="prompt user">cephadm &gt; </code>ceph osd crush rule create-replicated replicated_hdd default host hdd</pre></div></li><li class="step"><p>
     Switch all existing pools to the 'replicated_hdd' rule. This prevents
     Ceph from storing data to the newly added SSD devices:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> crush_rule replicated_hdd</pre></div></li><li class="step"><p>
     Turn the machine into a Ceph node using DeepSea. Install the software
     and configure the host machine as described in
     <a class="xref" href="#salt-adding-nodes" title="1.1. Adding New Cluster Nodes">Section 1.1, “Adding New Cluster Nodes”</a>. Let us assume that its name is
     <em class="replaceable">node-4</em>. This node needs to have 4 OSD disks.
    </p><p>
     Turn the machines into a Ceph nodes using DeepSea. Install the
     software and configure as described in
     <a class="xref" href="#salt-adding-nodes" title="1.1. Adding New Cluster Nodes">Section 1.1, “Adding New Cluster Nodes”</a>. In this example, the nodes have 4 OSD
     disks.
    </p><div class="verbatim-wrap"><pre class="screen">[...]
host node-4 {
   id -5  # do not change unnecessarily
   # weight 0.012
   alg straw
   hash 0  # rjenkins1
   item osd.6 weight 0.003
   item osd.7 weight 0.003
   item osd.8 weight 0.003
   item osd.9 weight 0.003
}
[...]</pre></div></li><li class="step"><p>
     Edit the CRUSH map for the hot storage pool mapped to the OSDs backed by
     the fast SSD drives. Define a second hierarchy with a root node for the
     SSDs (as 'root ssd'). Additionally, change the weight and a CRUSH rule for
     the SSDs. For more information on CRUSH map, see
     <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/crush-map/" target="_blank">http://docs.ceph.com/docs/master/rados/operations/crush-map/</a>.
    </p><p>
     Edit the CRUSH map directly with command line tools such as
     <code class="command">getcrushmap</code> and <code class="command">crushtool</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd crush rm-device-class osd.6 osd.7 osd.8 osd.9
<code class="prompt user">cephadm &gt; </code>ceph osd crush set-device-class ssd osd.6 osd.7 osd.8 osd.9</pre></div></li><li class="step"><p>
     Create the hot storage pool to be used for cache tiering. Use the new
     'ssd' rule for it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create hot-storage 100 100 replicated ssd</pre></div></li><li class="step"><p>
     Create the cold storage pool using the default 'replicated_ruleset' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</pre></div></li><li class="step"><p>
     Then, setting up a cache tier involves associating a backing storage pool
     with a cache pool, in this case, cold storage (= storage pool) with hot
     storage (= cache pool):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier add cold-storage hot-storage</pre></div></li><li class="step"><p>
     To set the cache mode to 'writeback', execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier cache-mode hot-storage writeback</pre></div><p>
     For more information about cache modes, see
     <a class="xref" href="#sec-ceph-tiered-cachemodes" title="11.4. Cache Modes">Section 11.4, “Cache Modes”</a>.
    </p><p>
     Writeback cache tiers overlay the backing storage tier, so they require
     one additional step: you must direct all client traffic from the storage
     pool to the cache pool. To direct client traffic directly to the cache
     pool, execute the following, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tier set-overlay cold-storage hot-storage</pre></div></li></ol></div></div></section><section class="sect1" id="cache-tier-configure" data-id-title="Configuring a Cache Tier"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.7 </span><span class="title-name">Configuring a Cache Tier</span> <a title="Permalink" class="permalink" href="#cache-tier-configure">#</a></h2></div></div></div><p>
   There are several options you can use to configure cache tiers. Use the
   following syntax:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> <em class="replaceable">key</em> <em class="replaceable">value</em></pre></div><section class="sect2" id="ses-tiered-hitset" data-id-title="Hit Set"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.7.1 </span><span class="title-name">Hit Set</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Hit set</em></span> parameters allow for tuning of <span class="emphasis"><em>cache
    pools</em></span>. Hit sets in Ceph are usually bloom filters and provide
    a memory-efficient way of tracking objects that are already in the cache
    pool.
   </p><p>
    The hit set is a bit array that is used to store the result of a set of
    hashing functions applied on object names. Initially, all bits are set to
    <code class="literal">0</code>. When an object is added to the hit set, its name is
    hashed and the result is mapped on different positions in the hit set,
    where the value of the bit is then set to <code class="literal">1</code>.
   </p><p>
    To find out whether an object exists in the cache, the object name is
    hashed again. If any bit is <code class="literal">0</code>, the object is definitely
    not in the cache and needs to be retrieved from cold storage.
   </p><p>
    It is possible that the results of different objects are stored in the same
    location of the hit set. By chance, all bits can be <code class="literal">1</code>
    without the object being in the cache. Therefore, hit sets working with a
    bloom filter can only tell whether an object is definitely not in the cache
    and needs to be retrieved from cold storage.
   </p><p>
    A cache pool can have more than one hit set tracking file access over time.
    The setting <code class="literal">hit_set_count</code> defines how many hit sets are
    being used, and <code class="literal">hit_set_period</code> defines for how long each
    hit set has been used. After the period has expired, the next hit set is
    used. If the number of hit sets is exhausted, the memory from the oldest
    hit set is freed and a new hit set is created. The values of
    <code class="literal">hit_set_count</code> and <code class="literal">hit_set_period</code>
    multiplied by each other define the overall time frame in which access to
    objects has been tracked.
   </p><div class="figure" id="ses-tiered-hitset-overview-bloom"><div class="figure-contents"><div class="mediaobject"><a href="images/bloom-filter.png" target="_blank"><img src="images/bloom-filter.png" width="" alt="Bloom Filter with 3 Stored Objects"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 11.1: </span><span class="title-name">Bloom Filter with 3 Stored Objects </span><a title="Permalink" class="permalink" href="#ses-tiered-hitset-overview-bloom">#</a></h6></div></div><p>
    Compared to the number of hashed objects, a hit set based on a bloom filter
    is very memory-efficient. Less than 10 bits are required to reduce the
    false positive probability below 1%. The false positive probability can be
    defined with <code class="literal">hit_set_fpp</code>. Based on the number of objects
    in a placement group and the false positive probability Ceph
    automatically calculates the size of the hit set.
   </p><p>
    The required storage on the cache pool can be limited with
    <code class="literal">min_write_recency_for_promote</code> and
    <code class="literal">min_read_recency_for_promote</code>. If the value is set to
    <code class="literal">0</code>, all objects are promoted to the cache pool as soon as
    they are read or written and this persists until they are evicted. Any
    value greater than <code class="literal">0</code> defines the number of hit sets
    ordered by age that are searched for the object. If the object is found in
    a hit set, it will be promoted to the cache pool. Keep in mind that backup
    of objects may also cause them to be promoted to the cache. A full backup
    with the value of '0' can cause all data to be promoted to the cache tier
    while active data gets removed from the cache tier. Therefore, changing
    this setting based on the backup strategy may be useful.
   </p><div id="id-1.3.4.11.11.4.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The longer the period and the higher the
     <code class="option">min_read_recency_for_promote</code> and
     <code class="option">min_write_recency_for_promote</code> values, the more RAM the
     <code class="systemitem">ceph-osd</code> daemon consumes. In
     particular, when the agent is active to flush or evict cache objects, all
     <code class="option">hit_set_count</code> hit sets are loaded into RAM.
    </p></div><section class="sect3" id="ceph-tier-gmt-hitset" data-id-title="Use GMT for Hit Set"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.7.1.1 </span><span class="title-name">Use GMT for Hit Set</span> <a title="Permalink" class="permalink" href="#ceph-tier-gmt-hitset">#</a></h4></div></div></div><p>
     Cache tier setups have a bloom filter called <span class="emphasis"><em>hit set</em></span>.
     The filter tests whether an object belongs to a set of either hot or cold
     objects. The objects are added to the hit set using time stamps appended
     to their names.
    </p><p>
     If cluster machines are placed in different time zones and the time stamps
     are derived from the local time, objects in a hit set can have misleading
     names consisting of future or past time stamps. In the worst case, objects
     may not exist in the hit set at all.
    </p><p>
     To prevent this, the <code class="option">use_gmt_hitset</code> defaults to '1' on a
     newly created cache tier setups. This way, you force OSDs to use GMT
     (Greenwich Mean Time) time stamps when creating the object names for the
     hit set.
    </p><div id="id-1.3.4.11.11.4.11.5" data-id-title="Leave the Default Value" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Leave the Default Value</h6><p>
      Do not touch the default value '1' of <code class="option">use_gmt_hitset</code>. If
      errors related to this option are not caused by your cluster setup, never
      change it manually. Otherwise, the cluster behavior may become
      unpredictable.
     </p></div></section></section><section class="sect2" id="id-1.3.4.11.11.5" data-id-title="Cache Sizing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.7.2 </span><span class="title-name">Cache Sizing</span> <a title="Permalink" class="permalink" href="#id-1.3.4.11.11.5">#</a></h3></div></div></div><p>
    The cache tiering agent performs two main functions:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.11.5.3.1"><span class="term">Flushing</span></dt><dd><p>
       The agent identifies modified (dirty) objects and forwards them to the
       storage pool for long-term storage.
      </p></dd><dt id="id-1.3.4.11.11.5.3.2"><span class="term">Evicting</span></dt><dd><p>
       The agent identifies objects that have not been modified (clean) and
       evicts the least recently used among them from the cache.
      </p></dd></dl></div><section class="sect3" id="cache-tier-config-absizing" data-id-title="Absolute Sizing"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.7.2.1 </span><span class="title-name">Absolute Sizing</span> <a title="Permalink" class="permalink" href="#cache-tier-config-absizing">#</a></h4></div></div></div><p>
     The cache tiering agent can flush or evict objects based on the total
     number of bytes or the total number of objects. To specify a maximum
     number of bytes, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> target_max_bytes <em class="replaceable">num_of_bytes</em></pre></div><p>
     To specify the maximum number of objects, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> target_max_objects <em class="replaceable">num_of_objects</em></pre></div><div id="id-1.3.4.11.11.5.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph is not able to determine the size of a cache pool automatically,
      so the configuration on the absolute size is required here. Otherwise,
      flush and evict will not work. If you specify both limits, the cache
      tiering agent will begin flushing or evicting when either threshold is
      triggered.
     </p></div><div id="id-1.3.4.11.11.5.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      All client requests will be blocked only when
      <code class="option">target_max_bytes</code> or <code class="option">target_max_objects</code>
      reached.
     </p></div></section><section class="sect3" id="cache-tier-config-relsizing" data-id-title="Relative Sizing"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.7.2.2 </span><span class="title-name">Relative Sizing</span> <a title="Permalink" class="permalink" href="#cache-tier-config-relsizing">#</a></h4></div></div></div><p>
     The cache tiering agent can flush or evict objects relative to the size of
     the cache pool (specified by <code class="option">target_max_bytes</code> or
     <code class="option">target_max_objects</code> in
     <a class="xref" href="#cache-tier-config-absizing" title="11.7.2.1. Absolute Sizing">Section 11.7.2.1, “Absolute Sizing”</a>). When the cache pool
     consists of a certain percentage of modified (dirty) objects, the cache
     tiering agent will flush them to the storage pool. To set the
     <code class="option">cache_target_dirty_ratio</code>, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_dirty_ratio <em class="replaceable">0.0...1.0</em></pre></div><p>
     For example, setting the value to 0.4 will begin flushing modified (dirty)
     objects when they reach 40% of the cache pool's capacity:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</pre></div><p>
     When the dirty objects reach a certain percentage of the capacity, flush
     them at a higher speed. Use
     <code class="option">cache_target_dirty_high_ratio</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_dirty_high_ratio <em class="replaceable">0.0..1.0</em></pre></div><p>
     When the cache pool reaches a certain percentage of its capacity, the
     cache tiering agent will evict objects to maintain free capacity. To set
     the <code class="option">cache_target_full_ratio</code>, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_full_ratio <em class="replaceable">0.0..1.0</em></pre></div></section></section><section class="sect2" id="id-1.3.4.11.11.6" data-id-title="Cache Age"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.7.3 </span><span class="title-name">Cache Age</span> <a title="Permalink" class="permalink" href="#id-1.3.4.11.11.6">#</a></h3></div></div></div><p>
    You can specify the minimum age of a recently modified (dirty) object
    before the cache tiering agent flushes it to the backing storage pool. Note
    that this will only apply if the cache actually needs to flush/evict
    objects:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_min_flush_age <em class="replaceable">num_of_seconds</em></pre></div><p>
    You can specify the minimum age of an object before it will be evicted from
    the cache tier:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_min_evict_age <em class="replaceable">num_of_seconds</em></pre></div></section><section class="sect2" id="ses-tiered-hitset-examples" data-id-title="Examples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.7.4 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples">#</a></h3></div></div></div><section class="sect3" id="ses-tiered-hitset-examples-memory" data-id-title="Large Cache Pool and Small Memory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.7.4.1 </span><span class="title-name">Large Cache Pool and Small Memory</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples-memory">#</a></h4></div></div></div><p>
     If lots of storage and only a small amount of RAM is available, all
     objects can be promoted to the cache pool as soon as they are accessed.
     The hit set is kept small. The following is a set of example configuration
     values:
    </p><div class="verbatim-wrap"><pre class="screen">hit_set_count = 1
hit_set_period = 3600
hit_set_fpp = 0.05
min_write_recency_for_promote = 0
min_read_recency_for_promote = 0</pre></div></section><section class="sect3" id="ses-tiered-hitset-examples-storage" data-id-title="Small Cache Pool and Large Memory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.7.4.2 </span><span class="title-name">Small Cache Pool and Large Memory</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples-storage">#</a></h4></div></div></div><p>
     If a small amount of storage but a comparably large amount of memory is
     available, the cache tier can be configured to promote a limited number of
     objects into the cache pool. Twelve hit sets, of which each is used over a
     period of 14,400 seconds, provide tracking for a total of 48 hours. If an
     object has been accessed in the last 8 hours, it is promoted to the cache
     pool. The set of example configuration values then is:
    </p><div class="verbatim-wrap"><pre class="screen">hit_set_count = 12
hit_set_period = 14400
hit_set_fpp = 0.01
min_write_recency_for_promote = 2
min_read_recency_for_promote = 2</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-configuration" data-id-title="Ceph Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12 </span><span class="title-name">Ceph Cluster Configuration</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration">#</a></h2></div></div></div><p>
  This chapter provides a list of important Ceph cluster settings and their
  description. The settings are sorted by topic.
 </p><section class="sect1" id="ceph-config-runtime" data-id-title="Runtime Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Runtime Configuration</span> <a title="Permalink" class="permalink" href="#ceph-config-runtime">#</a></h2></div></div></div><p>
   <a class="xref" href="#ds-custom-cephconf" title="1.12. Adjusting ceph.conf with Custom Settings">Section 1.12, “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</a> describes how to make changes to the
   Ceph configuration file <code class="filename">ceph.conf</code>. However, the
   actual cluster behavior is determined not by the current state of the
   <code class="filename">ceph.conf</code> file but by the configuration of the running
   Ceph daemons, which is stored in memory.
  </p><p>
   You can query an individual Ceph daemon for a particular configuration
   setting using the <span class="emphasis"><em>admin socket</em></span> on the node where the
   daemon is running. For example, the following command gets the value of the
   <code class="option">osd_max_write_size</code> configuration parameter from daemon
   named <code class="literal">osd.0</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</pre></div><p>
   You can also <span class="emphasis"><em>change</em></span> the daemons' settings at runtime.
   Remember that this change is temporary and will be lost after the next
   daemon restart. For example, the following command changes the
   <code class="option">osd_max_write_size</code> parameter to '50' for all OSDs in the
   cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell osd.* injectargs --osd_max_write_size 50</pre></div><div id="id-1.3.4.12.4.7" data-id-title="injectargs is Not Reliable" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: <code class="command">injectargs</code> is Not Reliable</h6><p>
    Unfortunately, changing the cluster settings with the
    <code class="command">injectargs</code> command is not 100% reliable. If you need to
    be sure that the changed parameter is active, change it in the
    configuration files on all cluster nodes and restart all daemons in the
    cluster.
   </p></div></section><section class="sect1" id="config-osd-and-bluestore" data-id-title="Ceph OSD and BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Ceph OSD and BlueStore</span> <a title="Permalink" class="permalink" href="#config-osd-and-bluestore">#</a></h2></div></div></div><section class="sect2" id="config-auto-cache-sizing" data-id-title="Automatic Cache Sizing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.1 </span><span class="title-name">Automatic Cache Sizing</span> <a title="Permalink" class="permalink" href="#config-auto-cache-sizing">#</a></h3></div></div></div><p>
    BlueStore can be configured to automatically resize its caches when
    <code class="option">tc_malloc</code> is configured as the memory allocator and the
    <code class="option">bluestore_cache_autotune</code> setting is enabled. This option
    is currently enabled by default. BlueStore will attempt to keep OSD heap
    memory usage under a designated target size via the
    <code class="option">osd_memory_target</code> configuration option. This is a best
    effort algorithm and caches will not shrink smaller than the amount
    specified by <code class="option">osd_memory_cache_min</code>. Cache ratios will be
    chosen based on a hierarchy of priorities. If priority information is not
    available, the <code class="option">bluestore_cache_meta_ratio</code> and
    <code class="option">bluestore_cache_kv_ratio</code> options are used as fallbacks.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.5.2.3.1"><span class="term"><code class="option">bluestore_cache_autotune</code></span></dt><dd><p>
       Automatically tunes the ratios assigned to different BlueStore caches
       while respecting minimum values. Default is <code class="option">True</code>.
      </p></dd><dt id="id-1.3.4.12.5.2.3.2"><span class="term"><code class="option">osd_memory_target</code></span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, try to keep this
       many bytes mapped in memory.
      </p><div id="id-1.3.4.12.5.2.3.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        This may not exactly match the RSS memory usage of the process. While
        the total amount of heap memory mapped by the process should generally
        stay close to this target, there is no guarantee that the kernel will
        actually reclaim memory that has been unmapped.
       </p></div></dd><dt id="id-1.3.4.12.5.2.3.3"><span class="term"><code class="option">osd_memory_cache_min</code></span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, set the minimum
       amount of memory used for caches.
      </p><div id="id-1.3.4.12.5.2.3.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        Setting this value too low can result in significant cache thrashing.
       </p></div></dd></dl></div></section></section></section></div><div class="part" id="part-dataccess" data-id-title="Accessing Cluster Data"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Accessing Cluster Data </span><a title="Permalink" class="permalink" href="#part-dataccess">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">13 </span><span class="title-name">Ceph Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">14 </span><span class="title-name">Ceph iSCSI Gateway</span></a></span></li><dd class="toc-abstract"><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span>.
  
 </p></dd><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">15 </span><span class="title-name">Clustered File System</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></span></li><dd class="toc-abstract"><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-nfs" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-gw" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-gw">#</a></h2></div></div></div><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p><section class="sect1" id="sec-ceph-rgw-limits" data-id-title="Object Gateway Restrictions and Naming Limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">Object Gateway Restrictions and Naming Limitations</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-limits">#</a></h2></div></div></div><p>
   Following is a list of important Object Gateway limits:
  </p><section class="sect2" id="ogw-limits-bucket" data-id-title="Bucket Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.1 </span><span class="title-name">Bucket Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-bucket">#</a></h3></div></div></div><p>
    When approaching Object Gateway via the S3 API, bucket names are limited to
    DNS-compliant names with a dash character '-' allowed. When approaching
    Object Gateway via the Swift API, you may use any combination of UTF-8 supported
    characters except for a slash character '/'. The maximum length of a bucket
    name is 255 characters. Bucket names must be unique.
   </p><div id="id-1.3.5.2.4.3.3" data-id-title="Use DNS-compliant Bucket Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use DNS-compliant Bucket Names</h6><p>
     Although you may use any UTF-8 based bucket name via the Swift API, it
     is recommended to name buckets with regard to the S3 naming limitations to
     avoid problems accessing the same bucket via the S3 API.
    </p></div></section><section class="sect2" id="ogw-limits-object" data-id-title="Stored Object Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.2 </span><span class="title-name">Stored Object Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-object">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.4.4.2.1"><span class="term">Maximum number of object per user</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.3.5.2.4.4.2.2"><span class="term">Maximum number of object per bucket</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.3.5.2.4.4.2.3"><span class="term">Maximum size of an object to upload / store</span></dt><dd><p>
       Single uploads are restricted to 5GB. Use multipart for larger object
       sizes. The maximum number of multipart chunks is 10000.
      </p></dd></dl></div></section><section class="sect2" id="ogw-limits-http" data-id-title="HTTP Header Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.3 </span><span class="title-name">HTTP Header Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-http">#</a></h3></div></div></div><p>
    HTTP header and request limitation depend on the Web front-end used. The
    default CivetWeb restricts the number of HTTP headers to 64 headers, and
    the size of the HTTP header to 16kB.
   </p></section></section><section class="sect1" id="ogw-deploy" data-id-title="Deploying the Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.2 </span><span class="title-name">Deploying the Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-deploy">#</a></h2></div></div></div><p>
   The recommended way of deploying the Ceph Object Gateway is via the DeepSea
   infrastructure by adding the relevant <code class="literal">role-rgw [...]</code>
   line(s) into the <code class="filename">policy.cfg</code> file on the Salt master, and
   running required DeepSea stages.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     To include the Object Gateway during the Ceph cluster deployment process, refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.3 “Cluster Deployment”</span> and
     <span class="intraxref">Book “Deployment Guide”, Chapter 4 “Deploying with DeepSea/Salt”, Section 4.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="listitem"><p>
     To add the Object Gateway role to an already deployed cluster, refer to
     <a class="xref" href="#salt-adding-services" title="1.2. Adding New Roles to Nodes">Section 1.2, “Adding New Roles to Nodes”</a>.
    </p></li></ul></div></section><section class="sect1" id="ceph-rgw-operating" data-id-title="Operating the Object Gateway Service"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.3 </span><span class="title-name">Operating the Object Gateway Service</span> <a title="Permalink" class="permalink" href="#ceph-rgw-operating">#</a></h2></div></div></div><p>
   Object Gateway service is operated with the <code class="command">systemctl</code> command. You
   need to have <code class="systemitem">root</code> privileges to operate the Object Gateway service. Note that
   <em class="replaceable">gateway_host</em> is the host name of the server whose
   Object Gateway instance you need to operate.
  </p><p>
   The following subcommands are supported for the Object Gateway service:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.6.4.1"><span class="term">systemctl status ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Prints the status information of the service.
     </p></dd><dt id="id-1.3.5.2.6.4.2"><span class="term">systemctl start ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Starts the service if it is not already running.
     </p></dd><dt id="id-1.3.5.2.6.4.3"><span class="term">systemctl restart ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Restarts the service.
     </p></dd><dt id="id-1.3.5.2.6.4.4"><span class="term">systemctl stop ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Stops the running service.
     </p></dd><dt id="id-1.3.5.2.6.4.5"><span class="term">systemctl enable ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Enables the service so that it is automatically started on system
      start-up.
     </p></dd><dt id="id-1.3.5.2.6.4.6"><span class="term">systemctl disable ceph-radosgw@rgw.<em class="replaceable">gateway_host</em></span></dt><dd><p>
      Disables the service so that it is not automatically started on system
      start-up.
     </p></dd></dl></div></section><section class="sect1" id="sec-ceph-rgw-configuration" data-id-title="Configuration Parameters"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.4 </span><span class="title-name">Configuration Parameters</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-configuration">#</a></h2></div></div></div><p>
   You can influence the Object Gateway behavior by a number of options in the
   <code class="filename">ceph.conf</code> file under the section named
  </p><div class="verbatim-wrap"><pre class="screen">[client.radosgw.<em class="replaceable">INSTANCE_NAME</em>]</pre></div><p>
   If an option is not specified, its default value is used. A complete list of
   the Object Gateway options follows:
  </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">General Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.7.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.7.5.2"><span class="term">rgw frontends</span></dt><dd><p>
      Configures the HTTP front end(s). Specify multiple front ends in a
      comma-delimited list. Each front end configuration may include a list of
      options separated by spaces, where each option is in the form
      “key=value” or “key”. Default is
     </p><div class="verbatim-wrap"><pre class="screen">rgw frontends = civetweb port=7480</pre></div><div id="id-1.3.5.2.7.5.2.2.3" data-id-title="tcp_nodelay" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">tcp_nodelay</code></h6><p>
       This option may affect the transfer rate of sending TCP packets,
       depending on the data chunk sizes. If set to '1', the socket option will
       disable Nagle's algorithm on the connection. Therefore packets will be
       sent as soon as possible instead of waiting for a full buffer or timeout
       to occur.
      </p></div></dd><dt id="id-1.3.5.2.7.5.3"><span class="term">rgw data</span></dt><dd><p>
      Sets the location of the data files for the Object Gateway. Default is
      <code class="filename">/var/lib/ceph/radosgw/<em class="replaceable">CLUSTER_ID</em></code>.
     </p></dd><dt id="id-1.3.5.2.7.5.4"><span class="term">rgw enable apis</span></dt><dd><p>
      Enables the specified APIs. Default is 's3, swift, swift_auth, admin All
      APIs'.
     </p></dd><dt id="id-1.3.5.2.7.5.5"><span class="term">rgw cache enabled</span></dt><dd><p>
      Enables or disables the Object Gateway cache. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.5.6"><span class="term">rgw cache lru size</span></dt><dd><p>
      The number of entries in the Object Gateway cache. Default is 10000.
     </p></dd><dt id="id-1.3.5.2.7.5.7"><span class="term">rgw socket path</span></dt><dd><p>
      The socket path for the domain socket.
      <code class="option">FastCgiExternalServer</code> uses this socket. If you do not
      specify a socket path, the Object Gateway will not run as an external server. The
      path you specify here needs to be the same as the path specified in the
      <code class="filename">rgw.conf</code> file.
     </p></dd><dt id="id-1.3.5.2.7.5.8"><span class="term">rgw fcgi socket backlog</span></dt><dd><p>
      The socket backlog for fcgi. Default is 1024.
     </p></dd><dt id="id-1.3.5.2.7.5.9"><span class="term">rgw host</span></dt><dd><p>
      The host for the Object Gateway instance. It can be an IP address or a hostname.
      Default is 0.0.0.0
     </p></dd><dt id="id-1.3.5.2.7.5.10"><span class="term">rgw port</span></dt><dd><p>
      The port number where the instance listens for requests. If not
      specified, the Object Gateway runs external FastCGI.
     </p></dd><dt id="id-1.3.5.2.7.5.11"><span class="term">rgw dns name</span></dt><dd><p>
      The DNS name of the served domain.
     </p></dd><dt id="id-1.3.5.2.7.5.12"><span class="term">rgw script uri</span></dt><dd><p>
      The alternative value for the SCRIPT_URI if not set in the request.
     </p></dd><dt id="id-1.3.5.2.7.5.13"><span class="term">rgw request uri</span></dt><dd><p>
      The alternative value for the REQUEST_URI if not set in the request.
     </p></dd><dt id="id-1.3.5.2.7.5.14"><span class="term">rgw print continue</span></dt><dd><p>
      Enable 100-continue if it is operational. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.5.15"><span class="term">rgw remote addr param</span></dt><dd><p>
      The remote address parameter. For example, the HTTP field containing the
      remote address, or the X-Forwarded-For address if a reverse proxy is
      operational. Default is REMOTE_ADDR.
     </p></dd><dt id="id-1.3.5.2.7.5.16"><span class="term">rgw op thread timeout</span></dt><dd><p>
      The timeout in seconds for open threads. Default is 600.
     </p></dd><dt id="id-1.3.5.2.7.5.17"><span class="term">rgw op thread suicide timeout</span></dt><dd><p>
      The time timeout in seconds before the Object Gateway process dies. Disabled if
      set to 0 (default).
     </p></dd><dt id="id-1.3.5.2.7.5.18"><span class="term">rgw thread pool size</span></dt><dd><p>
      Number of threads for the CivetWeb server. Increase to a higher value if
      you need to serve more requests. Defaults to 100 threads.
     </p></dd><dt id="id-1.3.5.2.7.5.19"><span class="term">rgw num rados handles</span></dt><dd><p>
      The number of RADOS cluster handles

      for Object Gateway. Having a configurable number of RADOS handles results in
      significant performance boost for all types of workloads. Each Object Gateway
      worker thread now gets to pick a RADOS handle for its lifetime. Default
      is 1.
     </p></dd><dt id="id-1.3.5.2.7.5.20"><span class="term">rgw num control oids</span></dt><dd><p>
      The number of notification objects used for cache synchronization between
      different rgw instances. Default is 8.
     </p></dd><dt id="id-1.3.5.2.7.5.21"><span class="term">rgw init timeout</span></dt><dd><p>
      The number of seconds before the Object Gateway gives up on initialization.
      Default is 30.
     </p></dd><dt id="id-1.3.5.2.7.5.22"><span class="term">rgw mime types file</span></dt><dd><p>
      The path and location of the MIME types. Used for Swift auto-detection of
      object types. Default is <code class="filename">/etc/mime.types</code>.
     </p></dd><dt id="id-1.3.5.2.7.5.23"><span class="term">rgw gc max objs</span></dt><dd><p>
      The maximum number of objects that may be handled by garbage collection
      in one garbage collection processing cycle. Default is 32.
     </p></dd><dt id="id-1.3.5.2.7.5.24"><span class="term">rgw gc obj min wait</span></dt><dd><p>
      The minimum wait time before the object may be removed and handled by
      garbage collection processing. Default is 2 * 3600.
     </p></dd><dt id="id-1.3.5.2.7.5.25"><span class="term">rgw gc processor max time</span></dt><dd><p>
      The maximum time between the beginning of two consecutive garbage
      collection processing cycles. Default is 3600.
     </p></dd><dt id="id-1.3.5.2.7.5.26"><span class="term">rgw gc processor period</span></dt><dd><p>
      The cycle time for garbage collection processing. Default is 3600.
     </p></dd><dt id="id-1.3.5.2.7.5.27"><span class="term">rgw s3 success create obj status</span></dt><dd><p>
      The alternate success status response for <code class="literal">create-obj</code>.
      Default is 0.
     </p></dd><dt id="id-1.3.5.2.7.5.28"><span class="term">rgw resolve cname</span></dt><dd><p>
      Whether the Object Gateway should use DNS CNAME record of the request host name
      field (if host name is not equal to the Object Gateway DNS name). Default is
      'false'.
     </p></dd><dt id="id-1.3.5.2.7.5.29"><span class="term">rgw obj stripe size</span></dt><dd><p>
      The size of an object stripe for Object Gateway objects. Default is 4 &lt;&lt; 20.
     </p></dd><dt id="id-1.3.5.2.7.5.30"><span class="term">rgw extended http attrs</span></dt><dd><p>
      Add a new set of attributes that can be set on an entity (for example a
      user, a bucket or an object). These extra attributes can be set through
      HTTP header fields when putting the entity or modifying it using POST
      method. If set, these attributes will return as HTTP fields when
      requesting GET/HEAD on the entity. Default is 'content_foo, content_bar,
      x-foo-bar'.
     </p></dd><dt id="id-1.3.5.2.7.5.31"><span class="term">rgw exit timeout secs</span></dt><dd><p>
      Number of seconds to wait for a process before exiting unconditionally.
      Default is 120.
     </p></dd><dt id="id-1.3.5.2.7.5.32"><span class="term">rgw get obj window size</span></dt><dd><p>
      The window size in bytes for a single object request. Default is '16
      &lt;&lt; 20'.
     </p></dd><dt id="id-1.3.5.2.7.5.33"><span class="term">rgw get obj max req size</span></dt><dd><p>
      The maximum request size of a single GET operation sent to the Ceph
      Storage Cluster. Default is 4 &lt;&lt; 20.
     </p></dd><dt id="id-1.3.5.2.7.5.34"><span class="term">rgw relaxed s3 bucket names</span></dt><dd><p>
      Enables relaxed S3 bucket names rules for US region buckets. Default is
      'false'.
     </p></dd><dt id="id-1.3.5.2.7.5.35"><span class="term">rgw list buckets max chunk</span></dt><dd><p>
      The maximum number of buckets to retrieve in a single operation when
      listing user buckets. Default is 1000.
     </p></dd><dt id="id-1.3.5.2.7.5.36"><span class="term">rgw override bucket index max shards</span></dt><dd><p>
      Represents the number of shards for the bucket index object. Setting 0
      (default) indicates there is no sharding. It is not recommended to set a
      value too large (for example 1000) as it increases the cost for bucket
      listing. This variable should be set in the client or global sections so
      that it is automatically applied to <code class="command">radosgw-admin</code>
      commands.
     </p></dd><dt id="id-1.3.5.2.7.5.37"><span class="term">rgw curl wait timeout ms</span></dt><dd><p>
      The timeout in milliseconds for certain <code class="command">curl</code> calls.
      Default is 1000.
     </p></dd><dt id="id-1.3.5.2.7.5.38"><span class="term">rgw copy obj progress</span></dt><dd><p>
      Enables output of object progress during long copy operations. Default is
      'true'.
     </p></dd><dt id="id-1.3.5.2.7.5.39"><span class="term">rgw copy obj progress every bytes</span></dt><dd><p>
      The minimum bytes between copy progress output. Default is 1024 * 1024.
     </p></dd><dt id="id-1.3.5.2.7.5.40"><span class="term">rgw admin entry</span></dt><dd><p>
      The entry point for an admin request URL. Default is 'admin'.
     </p></dd><dt id="id-1.3.5.2.7.5.41"><span class="term">rgw content length compat</span></dt><dd><p>
      Enable compatibility handling of FCGI requests with both CONTENT_LENGTH
      AND HTTP_CONTENT_LENGTH set. Default is 'false'.
     </p></dd><dt id="id-1.3.5.2.7.5.42"><span class="term">rgw bucket quota ttl</span></dt><dd><p>
      The amount of time in seconds that cached quota information is trusted.
      After this timeout, the quota information will be re-fetched from the
      cluster. Default is 600.
     </p></dd><dt id="id-1.3.5.2.7.5.43"><span class="term">rgw user quota bucket sync interval</span></dt><dd><p>
      The amount of time in seconds for which the bucket quota information is
      accumulated before syncing to the cluster. During this time, other Object Gateway
      instances will not see the changes in the bucket quota stats related to
      operations on this instance. Default is 180.
     </p></dd><dt id="id-1.3.5.2.7.5.44"><span class="term">rgw user quota sync interval</span></dt><dd><p>
      The amount of time in seconds for which user quota information is
      accumulated before syncing to the cluster. During this time, other Object Gateway
      instances will not see the changes in the user quota stats related to
      operations on this instance. Default is 180.
     </p></dd><dt id="id-1.3.5.2.7.5.45"><span class="term">rgw bucket default quota max objects</span></dt><dd><p>
      Default maximum number of objects per bucket. It is set on new users if
      no other quota is specified, and has no effect on existing users. This
      variable should be set in the client or global sections so that it is
      automatically applied to <code class="command">radosgw-admin</code> commands.
      Default is -1.
     </p></dd><dt id="id-1.3.5.2.7.5.46"><span class="term">rgw bucket default quota max size</span></dt><dd><p>
      Default maximum capacity per bucket in bytes. It is set on new users if
      no other quota is specified, and has no effect on existing users. Default
      is -1.
     </p></dd><dt id="id-1.3.5.2.7.5.47"><span class="term">rgw user default quota max objects</span></dt><dd><p>
      Default maximum number of objects for a user. This includes all objects
      in all buckets owned by the user. It is set on new users if no other
      quota is specified, and has no effect on existing users. Default is -1.
     </p></dd><dt id="id-1.3.5.2.7.5.48"><span class="term">rgw user default quota max size</span></dt><dd><p>
      The value for user maximum size quota in bytes set on new users if no
      other quota is specified. It has no effect on existing users. Default is
      -1.
     </p></dd><dt id="id-1.3.5.2.7.5.49"><span class="term">rgw verify ssl</span></dt><dd><p>
      Verify SSL certificates while making requests. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.5.50"><span class="term">rgw max chunk size</span></dt><dd><p>
      Maximum size of a chunk of data that will be read in a single operation.
      Increasing the value to 4MB (4194304) will provide better performance
      when processing large objects. Default is 128kB (131072).
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Multisite Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.7.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.7.6.2"><span class="term">rgw zone</span></dt><dd><p>
      The name of the zone for the gateway instance. If no zone is set, a
      cluster-wide default can be configured with the <code class="command">radosgw-admin
      zone default</code> command.
     </p></dd><dt id="id-1.3.5.2.7.6.3"><span class="term">rgw zonegroup</span></dt><dd><p>
      The name of the zonegroup for the gateway instance. If no zonegroup is
      set, a cluster-wide default can be configured with the
      <code class="command">radosgw-admin zonegroup default</code> command.
     </p></dd><dt id="id-1.3.5.2.7.6.4"><span class="term">rgw realm</span></dt><dd><p>
      The name of the realm for the gateway instance. If no realm is set, a
      cluster-wide default can be configured with the<code class="command">radosgw-admin
      realm default</code> command.
     </p></dd><dt id="id-1.3.5.2.7.6.5"><span class="term">rgw run sync thread</span></dt><dd><p>
      If there are other zones in the realm to synchronize from, spawn threads
      to handle the synchronization of data and metadata. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.6.6"><span class="term">rgw data log window</span></dt><dd><p>
      The data log entries window in seconds. Default is 30/
     </p></dd><dt id="id-1.3.5.2.7.6.7"><span class="term">rgw data log changes size</span></dt><dd><p>
      The number of in-memory entries to hold for the data changes log. Default
      is 1000.
     </p></dd><dt id="id-1.3.5.2.7.6.8"><span class="term">rgw data log obj prefix</span></dt><dd><p>
      The object name prefix for the data log. Default is 'data_log'.
     </p></dd><dt id="id-1.3.5.2.7.6.9"><span class="term">rgw data log num shards</span></dt><dd><p>
      The number of shards (objects) on which to keep the data changes log.
      Default is 128.
     </p></dd><dt id="id-1.3.5.2.7.6.10"><span class="term">rgw md log max shards</span></dt><dd><p>
      The maximum number of shards for the metadata log. Default is 64.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Swift Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.7.7">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.7.7.2"><span class="term">rgw enforce swift acls</span></dt><dd><p>
      Enforces the Swift Access Control List (ACL) settings. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.7.3"><span class="term">rgw swift token expiration</span></dt><dd><p>
      The time in seconds for expiring a Swift token. Default is 24 * 3600.
     </p></dd><dt id="id-1.3.5.2.7.7.4"><span class="term">rgw swift url</span></dt><dd><p>
      The URL for the Ceph Object Gateway Swift API.
     </p></dd><dt id="id-1.3.5.2.7.7.5"><span class="term">rgw swift url prefix</span></dt><dd><p>
      The URL prefix for the Swift StorageURL that goes in front of the
      “/v1” part. This allows to run several Gateway instances on the same
      host. For compatibility, setting this configuration variable to empty
      causes the default “/swift” to be used. Use explicit prefix “/”
      to start StorageURL at the root.
     </p><div id="id-1.3.5.2.7.7.5.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       Setting this option to “/” will not work if S3 API is enabled. Keep
       in mind that disabling S3 will make impossible to deploy the Object Gateway in
       the multisite configuration!
      </p></div></dd><dt id="id-1.3.5.2.7.7.6"><span class="term">rgw swift auth url</span></dt><dd><p>
      Default URL for verifying v1 authentication tokens when the internal
      Swift authentication is not used.
     </p></dd><dt id="id-1.3.5.2.7.7.7"><span class="term">rgw swift auth entry</span></dt><dd><p>
      The entry point for a Swift authentication URL. Default is 'auth'.
     </p></dd><dt id="id-1.3.5.2.7.7.8"><span class="term">rgw swift versioning enabled</span></dt><dd><p>
      Enables the Object Versioning of OpenStack Object Storage API. This
      allows clients to put the <code class="literal">X-Versions-Location</code>
      attribute on containers that should be versioned. The attribute specifies
      the name of container storing archived versions. It must be owned by the
      same user that the versioned container due to access control verification
      - ACLs are <span class="emphasis"><em>not</em></span> taken into consideration. Those
      containers cannot be versioned by the S3 object versioning mechanism.
      Default is 'false'.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Logging Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.7.8">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.7.8.2"><span class="term">rgw log nonexistent bucket</span></dt><dd><p>
      Enables the Object Gateway to log a request for a non-existent bucket. Default is
      'false'.
     </p></dd><dt id="id-1.3.5.2.7.8.3"><span class="term">rgw log object name</span></dt><dd><p>
      The logging format for an object name. See the manual page <code class="command">man 1
      date</code> for details about format specifiers. Default is
      '%Y-%m-%d-%H-%i-%n'.
     </p></dd><dt id="id-1.3.5.2.7.8.4"><span class="term">rgw log object name utc</span></dt><dd><p>
      Whether a logged object name includes a UTC time. If set to 'false'
      (default), it uses the local time.
     </p></dd><dt id="id-1.3.5.2.7.8.5"><span class="term">rgw usage max shards</span></dt><dd><p>
      The maximum number of shards for usage logging. Default is 32.
     </p></dd><dt id="id-1.3.5.2.7.8.6"><span class="term">rgw usage max user shards</span></dt><dd><p>
      The maximum number of shards used for a single user’s usage logging.
      Default is 1.
     </p></dd><dt id="id-1.3.5.2.7.8.7"><span class="term">rgw enable ops log</span></dt><dd><p>
      Enable logging for each successful Object Gateway operation. Default is 'false'.
     </p></dd><dt id="id-1.3.5.2.7.8.8"><span class="term">rgw enable usage log</span></dt><dd><p>
      Enable the usage log. Default is 'false'.
     </p></dd><dt id="id-1.3.5.2.7.8.9"><span class="term">rgw ops log rados</span></dt><dd><p>
      Whether the operations log should be written to the Ceph Storage Cluster
      back end. Default is 'true'.
     </p></dd><dt id="id-1.3.5.2.7.8.10"><span class="term">rgw ops log socket path</span></dt><dd><p>
      The Unix domain socket for writing operations logs.
     </p></dd><dt id="id-1.3.5.2.7.8.11"><span class="term">rgw ops log data backlog</span></dt><dd><p>
      The maximum data backlog data size for operations logs written to a Unix
      domain socket. Default is 5 &lt;&lt; 20.
     </p></dd><dt id="id-1.3.5.2.7.8.12"><span class="term">rgw usage log flush threshold</span></dt><dd><p>
      The number of dirty merged entries in the usage log before flushing
      synchronously. Default is 1024.
     </p></dd><dt id="id-1.3.5.2.7.8.13"><span class="term">rgw usage log tick interval</span></dt><dd><p>
      Flush pending usage log data every 'n' seconds. Default is 30.
     </p></dd><dt id="id-1.3.5.2.7.8.14"><span class="term">rgw log http headers</span></dt><dd><p>
      Comma-delimited list of HTTP headers to include in log entries. Header
      names are case insensitive, and use the full header name with words
      separated by underscores. For example 'http_x_forwarded_for,
      http_x_special_k'.
     </p></dd><dt id="id-1.3.5.2.7.8.15"><span class="term">rgw intent log object name</span></dt><dd><p>
      The logging format for the intent log object name. See the manual page
      <code class="command">man 1 date</code> for details about format specifiers.
      Default is '%Y-%m-%d-%i-%n'.
     </p></dd><dt id="id-1.3.5.2.7.8.16"><span class="term">rgw intent log object name utc</span></dt><dd><p>
      Whether the intent log object name includes a UTC time. If set to 'false'
      (default), it uses the local time.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Keystone Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.7.9">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.7.9.2"><span class="term">rgw keystone url</span></dt><dd><p>
      The URL for the Keystone server.
     </p></dd><dt id="id-1.3.5.2.7.9.3"><span class="term">rgw keystone api version</span></dt><dd><p>
      The version (2 or 3) of OpenStack Identity API that should be used for
      communication with the Keystone server. Default is 2.
     </p></dd><dt id="id-1.3.5.2.7.9.4"><span class="term">rgw keystone admin domain</span></dt><dd><p>
      The name of the OpenStack domain with the administrator privilege when
      using OpenStack Identity API v3.
     </p></dd><dt id="id-1.3.5.2.7.9.5"><span class="term">rgw keystone admin project</span></dt><dd><p>
      The name of the OpenStack project with the administrator privilege when
      using OpenStack Identity API v3. If not set, the value of the
      <code class="command">rgw keystone admin tenant</code> will be used instead.
     </p></dd><dt id="id-1.3.5.2.7.9.6"><span class="term">rgw keystone admin token</span></dt><dd><p>
      The Keystone administrator token (shared secret). In the Object Gateway,
      authentication with the administrator token has priority over
      authentication with the administrator credentials (options <code class="option">rgw
      keystone admin user</code>, <code class="option">rgw keystone admin
      password</code>, <code class="option">rgw keystone admin tenant</code>,
      <code class="option">rgw keystone admin project</code>, and <code class="option">rgw keystone
      admin domain</code>). Administrator token feature is considered as
      deprecated.
     </p></dd><dt id="id-1.3.5.2.7.9.7"><span class="term">rgw keystone admin tenant</span></dt><dd><p>
      The name of the OpenStack tenant with the administrator privilege
      (Service Tenant) when using OpenStack Identity API v2.
     </p></dd><dt id="id-1.3.5.2.7.9.8"><span class="term">rgw keystone admin user</span></dt><dd><p>
      The name of the OpenStack user with the administrator privilege for
      Keystone authentication (Service User) when using OpenStack Identity API
      v2.
     </p></dd><dt id="id-1.3.5.2.7.9.9"><span class="term">rgw keystone admin password</span></dt><dd><p>
      The password for the OpenStack administrator user when using OpenStack
      Identity API v2.
     </p></dd><dt id="id-1.3.5.2.7.9.10"><span class="term">rgw keystone accepted roles</span></dt><dd><p>
      The roles required to serve requests. Default is 'Member, admin'.
     </p></dd><dt id="id-1.3.5.2.7.9.11"><span class="term">rgw keystone token cache size</span></dt><dd><p>
      The maximum number of entries in each Keystone token cache. Default is
      10000.
     </p></dd><dt id="id-1.3.5.2.7.9.12"><span class="term">rgw keystone revocation interval</span></dt><dd><p>
      The number of seconds between token revocation checks. Default is 15 *
      60.
     </p></dd><dt id="id-1.3.5.2.7.9.13"><span class="term">rgw keystone verify ssl</span></dt><dd><p>
      Verify SSL certificates while making token requests to keystone. Default
      is 'true'.
     </p></dd></dl></div><section class="sect2" id="sec-ceph-rgw-configuration-notes" data-id-title="Additional Notes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.1 </span><span class="title-name">Additional Notes</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-configuration-notes">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.7.10.2.1"><span class="term">rgw dns name</span></dt><dd><p>
       If the parameter <code class="literal">rgw dns name</code> is added to the
       <code class="filename">ceph.conf</code>, make sure that the S3 client is
       configured to direct requests at the endpoint specified by <code class="literal">rgw
       dns name</code>.
      </p></dd></dl></div></section></section><section class="sect1" id="ceph-rgw-access" data-id-title="Managing Object Gateway Access"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.5 </span><span class="title-name">Managing Object Gateway Access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-access">#</a></h2></div></div></div><p>
   You can communicate with Object Gateway using either S3- or Swift-compatible
   interface. S3 interface is compatible with a large subset of the Amazon S3
   RESTful API. Swift interface is compatible with a large subset of the
   OpenStack Swift API.
  </p><p>
   Both interfaces require you to create a specific user, and install the
   relevant client software to communicate with the gateway using the user's
   secret key.
  </p><section class="sect2" id="accessing-ragos-gateway" data-id-title="Accessing Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.5.1 </span><span class="title-name">Accessing Object Gateway</span> <a title="Permalink" class="permalink" href="#accessing-ragos-gateway">#</a></h3></div></div></div><section class="sect3" id="id-1.3.5.2.8.4.2" data-id-title="S3 Interface Access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.1.1 </span><span class="title-name">S3 Interface Access</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.8.4.2">#</a></h4></div></div></div><p>
     To access the S3 interface, you need a REST client.
     <code class="command">S3cmd</code> is a command line S3 client. You can find it in
     the
     <a class="link" href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd" target="_blank">OpenSUSE
     Build Service</a>. The repository contains versions for both SUSE Linux Enterprise and
     openSUSE based distributions.
    </p><p>
     If you want to test your access to the S3 interface, you can also write a
     small a Python script. The script will connect to Object Gateway, create a new
     bucket, and list all buckets. The values for
     <code class="option">aws_access_key_id</code> and
     <code class="option">aws_secret_access_key</code> are taken from the values of
     <code class="option">access_key</code> and <code class="option">secret_key</code> returned by
     the <code class="command">radosgw_admin</code> command from
     <a class="xref" href="#adding-s3-swift-users" title="13.5.2.1. Adding S3 and Swift Users">Section 13.5.2.1, “Adding S3 and Swift Users”</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install the <code class="systemitem">python-boto</code> package:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in python-boto</pre></div></li><li class="step"><p>
       Create a new Python script called <code class="filename">s3test.py</code> with
       the following content:
       
      </p><div class="verbatim-wrap"><pre class="screen">import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
  print "{name}\t{created}".format(
  name = bucket.name,
  created = bucket.creation_date,
  )</pre></div><p>
       Replace <code class="literal">{hostname}</code> with the host name of the host
       where you configured Object Gateway service, for example
       <code class="literal">gateway_host</code>.
      </p></li><li class="step"><p>
       Run the script:
      </p><div class="verbatim-wrap"><pre class="screen">python s3test.py</pre></div><p>
       The script outputs something like the following:
      </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket 2015-07-22T15:37:42.000Z</pre></div></li></ol></div></div></section><section class="sect3" id="id-1.3.5.2.8.4.3" data-id-title="Swift Interface Access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.1.2 </span><span class="title-name">Swift Interface Access</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.8.4.3">#</a></h4></div></div></div><p>
     To access Object Gateway via Swift interface, you need the <code class="command">swift</code>
     command line client. Its manual page <code class="command">man 1 swift</code> tells
     you more about its command line options.
    </p><p>
     The package is included in the 'Public Cloud' module for SUSE Linux Enterprise 12 SP3.
     Before installing the package, you need to activate the module and refresh
     the software repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>SUSEConnect -p sle-module-public-cloud/12/x86_64
sudo zypper refresh</pre></div><p>
     To install the <code class="command">swift</code> command, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in python-swiftclient</pre></div><p>
     The swift access uses the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>swift -A http://<em class="replaceable">IP_ADDRESS</em>/auth/1.0 \
-U example_user:swift -K '<em class="replaceable">swift_secret_key</em>' list</pre></div><p>
     Replace <em class="replaceable">IP_ADDRESS</em> with the IP address of the
     gateway server, and <em class="replaceable">swift_secret_key</em> with its
     value from the output of the <code class="command">radosgw-admin key create</code>
     command executed for the <code class="systemitem">swift</code> user in
     <a class="xref" href="#adding-s3-swift-users" title="13.5.2.1. Adding S3 and Swift Users">Section 13.5.2.1, “Adding S3 and Swift Users”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</pre></div><p>
     The output is:
    </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket</pre></div></section></section><section class="sect2" id="s3-swift-accounts-managment" data-id-title="Managing S3 and Swift Accounts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.5.2 </span><span class="title-name">Managing S3 and Swift Accounts</span> <a title="Permalink" class="permalink" href="#s3-swift-accounts-managment">#</a></h3></div></div></div><section class="sect3" id="adding-s3-swift-users" data-id-title="Adding S3 and Swift Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.2.1 </span><span class="title-name">Adding S3 and Swift Users</span> <a title="Permalink" class="permalink" href="#adding-s3-swift-users">#</a></h4></div></div></div><p>
     You need to create a user, access key and secret to enable end users to
     interact with the gateway. There are two types of users: a
     <span class="emphasis"><em>user</em></span> and <span class="emphasis"><em>subuser</em></span>. While
     <span class="emphasis"><em>users</em></span> are used when interacting with the S3
     interface, <span class="emphasis"><em>subusers</em></span> are users of the Swift
     interface. Each subuser is associated to a user.
    </p><p>
     Users can also be added via the DeepSea file
     <code class="filename">rgw.sls</code>. For an example, see
     <a class="xref" href="#ceph-nfsganesha-customrole-rgw-multiusers" title="16.3.1. Different Object Gateway Users for NFS Ganesha">Section 16.3.1, “Different Object Gateway Users for NFS Ganesha”</a>.
    </p><p>
     To create a Swift user, follow the steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To create a Swift user—which is a <span class="emphasis"><em>subuser</em></span>
       in our terminology—you need to create the associated
       <span class="emphasis"><em>user</em></span> first.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">username</em> \
 --display-name="<em class="replaceable">display-name</em>" --email=<em class="replaceable">email</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div></li><li class="step"><p>
       To create a subuser (Swift interface) for the user, you must specify
       the user ID (--uid=<em class="replaceable">username</em>), a subuser ID,
       and the access level for the subuser.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin subuser create --uid=<em class="replaceable">uid</em> \
 --subuser=<em class="replaceable">uid</em> \
 --access=[ <em class="replaceable">read | write | readwrite | full</em> ]</pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</pre></div></li><li class="step"><p>
       Generate a secret key for the user.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</pre></div></li><li class="step"><p>
       Both commands will output JSON-formatted data showing the user state.
       Notice the following lines, and remember the
       <code class="literal">secret_key</code> value:
      </p><div class="verbatim-wrap"><pre class="screen">"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</pre></div></li></ol></div></div><p>
     When accessing Object Gateway through the S3 interface you need to create a S3 user
     by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">username</em> \
 --display-name="<em class="replaceable">display-name</em>" --email=<em class="replaceable">email</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div><p>
     The command also creates the user's access and secret key. Check its
     output for <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     keywords and their values:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</pre></div></section><section class="sect3" id="removing-s3-swift-users" data-id-title="Removing S3 and Swift Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.2.2 </span><span class="title-name">Removing S3 and Swift Users</span> <a title="Permalink" class="permalink" href="#removing-s3-swift-users">#</a></h4></div></div></div><p>
     The procedure for deleting users is similar for S3 and Swift users. But
     in case of Swift users you may need to delete the user including its
     subusers.
    </p><p>
     To remove a S3 or Swift user (including all its subusers), specify
     <code class="option">user rm</code> and the user ID in the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user rm --uid=example_user</pre></div><p>
     To remove a subuser, specify <code class="option">subuser rm</code> and the subuser
     ID.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin subuser rm --uid=example_user:swift</pre></div><p>
     You can make use of the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.8.5.3.8.1"><span class="term">--purge-data</span></dt><dd><p>
        Purges all data associated to the user ID.
       </p></dd><dt id="id-1.3.5.2.8.5.3.8.2"><span class="term">--purge-keys</span></dt><dd><p>
        Purges all keys associated to the user ID.
       </p></dd></dl></div><div id="id-1.3.5.2.8.5.3.9" data-id-title="Removing a Subuser" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a Subuser</h6><p>
      When you remove a subuser, you are removing access to the Swift
      interface. The user will remain in the system.
     </p></div></section><section class="sect3" id="changing-s3-swift-users-password" data-id-title="Changing S3 and Swift User Access and Secret Keys"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.2.3 </span><span class="title-name">Changing S3 and Swift User Access and Secret Keys</span> <a title="Permalink" class="permalink" href="#changing-s3-swift-users-password">#</a></h4></div></div></div><p>
     The <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     parameters identify the Object Gateway user when accessing the gateway. Changing
     the existing user keys is the same as creating new ones, as the old keys
     get overwritten.
    </p><p>
     For S3 users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin key create --uid=<em class="replaceable">example_user</em> --key-type=s3 --gen-access-key --gen-secret</pre></div><p>
     For Swift users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin key create --subuser=<em class="replaceable">example_user</em>:swift --key-type=swift --gen-secret</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.8.5.4.7.1"><span class="term"><code class="option">--key-type=<em class="replaceable">type</em></code></span></dt><dd><p>
        Specifies the type of key. Either <code class="literal">swift</code> or
        <code class="literal">s3</code>.
       </p></dd><dt id="id-1.3.5.2.8.5.4.7.2"><span class="term"><code class="option">--gen-access-key</code></span></dt><dd><p>
        Generates a random access key (for S3 user by default).
       </p></dd><dt id="id-1.3.5.2.8.5.4.7.3"><span class="term"><code class="option">--gen-secret</code></span></dt><dd><p>
        Generates a random secret key.
       </p></dd><dt id="id-1.3.5.2.8.5.4.7.4"><span class="term"><code class="option">--secret=<em class="replaceable">key</em></code></span></dt><dd><p>
        Specifies a secret key, for example manually generated.
       </p></dd></dl></div></section><section class="sect3" id="user-quota-managment" data-id-title="User Quota Management"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.5.2.4 </span><span class="title-name">User Quota Management</span> <a title="Permalink" class="permalink" href="#user-quota-managment">#</a></h4></div></div></div><p>
     The Ceph Object Gateway enables you to set quotas on users and buckets owned by
     users. Quotas include the maximum number of objects in a bucket and the
     maximum storage size in megabytes.
    </p><p>
     Before you enable a user quota, you first need to set its parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin quota set --quota-scope=user --uid=<em class="replaceable">example_user</em> \
 --max-objects=1024 --max-size=1024</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.8.5.5.5.1"><span class="term"><code class="option">--max-objects</code></span></dt><dd><p>
        Specifies the maximum number of objects. A negative value disables the
        check.
       </p></dd><dt id="id-1.3.5.2.8.5.5.5.2"><span class="term"><code class="option">--max-size</code></span></dt><dd><p>
        Specifies the maximum number of bytes. A negative value disables the
        check.
       </p></dd><dt id="id-1.3.5.2.8.5.5.5.3"><span class="term"><code class="option">--quota-scope</code></span></dt><dd><p>
        Sets the scope for the quota. The options are <code class="literal">bucket</code>
        and <code class="literal">user</code>. Bucket quotas apply to buckets a user
        owns. User quotas apply to a user.
       </p></dd></dl></div><p>
     Once you set a user quota, you may enable it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin quota enable --quota-scope=user --uid=<em class="replaceable">example_user</em></pre></div><p>
     To disable a quota:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin quota disable --quota-scope=user --uid=<em class="replaceable">example_user</em></pre></div><p>
     To list quota settings:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user info --uid=<em class="replaceable">example_user</em></pre></div><p>
     To update quota statistics:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user stats --uid=<em class="replaceable">example_user</em> --sync-stats</pre></div></section></section></section><section class="sect1" id="ceph-rgw-https" data-id-title="Enabling HTTPS/SSL for Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.6 </span><span class="title-name">Enabling HTTPS/SSL for Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-https">#</a></h2></div></div></div><p>
   To enable the default Object Gateway role to communicate securely using SSL, you need
   to either have a CA-issued certificate, or create a self-signed one—
   not both. There are two ways to configure Object Gateway with HTTPS enabled: a simple
   way that makes use of the default settings, and an advanced way that lets you
   fine-tune HTTPS related settings.
  </p><section class="sect2" id="ogw-selfcert" data-id-title="Create a Self-Signed Certificate"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.1 </span><span class="title-name">Create a Self-Signed Certificate</span> <a title="Permalink" class="permalink" href="#ogw-selfcert">#</a></h3></div></div></div><p>
    By default, DeepSea expects the certificate file in
    <code class="filename">/srv/salt/ceph/rgw/cert/rgw.pem</code> on the Salt master. It
    will then distribute the certificate to
    <code class="filename">/etc/ceph/rgw.pem</code> on the Salt minion with the Object Gateway
    role, where Ceph reads it.
   </p><p>
    The following procedure describes how to generate a self-signed SSL
    certificate on the Salt master node.
   </p><div class="procedure"><div class="procedure-contents"><div id="id-1.3.5.2.9.3.4.1" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
         If you have a valid certificated signed by a CA, proceed to Step 3.
       </p></div><ol class="procedure" type="1"><li class="step"><p>
      If you need your Object Gateway to be known by additional subject identities, add
      them to the <code class="option">subjectAltName</code> option in the
      <code class="literal">[v3_req]</code> section of the
      <code class="filename">/etc/ssl/openssl.cnf</code> file:
     </p><div class="verbatim-wrap"><pre class="screen">[...]
[ v3_req ]
subjectAltName = DNS:server1.example.com DNS:server2.example.com
[...]</pre></div><div id="id-1.3.5.2.9.3.4.2.3" data-id-title="IP Addresses in subjectAltName" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: IP Addresses in <code class="option">subjectAltName</code></h6><p>
       To use IP addresses instead of domain names in the
       <code class="option">subjectAltName</code> option, replace the example line with
       the following:
      </p><div class="verbatim-wrap"><pre class="screen">subjectAltName = IP:10.0.0.10 IP:10.0.0.11</pre></div></div></li><li class="step"><p>
      Create the key and the certificate using <code class="command">openssl</code>.
      Enter all data you need to include in your certificate. We recommend
      entering the FQDN as the common name. Before signing the certificate,
      verify that 'X509v3 Subject Alternative Name:' is included in requested
      extensions, and that the resulting certificate has "X509v3 Subject
      Alternative Name:" set.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>openssl req -x509 -nodes -days 1095 \
 -newkey rsa:4096 -keyout rgw.key -out /srv/salt/ceph/rgw/cert/rgw.pem</pre></div></li><li class="step"><p>
      Append the files to the <code class="filename">rgw.pem</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cat rgw.key &gt;&gt; /srv/salt/ceph/rgw/cert/rgw.pem</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-ssl-simple" data-id-title="Simple HTTPS Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.2 </span><span class="title-name">Simple HTTPS Configuration</span> <a title="Permalink" class="permalink" href="#ogw-ssl-simple">#</a></h3></div></div></div><p>
    By default, Ceph on the Object Gateway node reads the
    <code class="filename">/etc/ceph/rgw.pem</code> certificate, and uses port 443 for
    secure SSL communication. If you do not need to change these values, follow
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_init: default-ssl</pre></div></li><li class="step"><p>
      Copy the default Object Gateway SSL configuration to the
      <code class="filename">ceph.conf.d</code> subdirectory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</pre></div></li><li class="step"><p>
      Run DeepSea Stages 2, 3, and 4 to apply the changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-ssl-advanced" data-id-title="Advanced HTTPS Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.3 </span><span class="title-name">Advanced HTTPS Configuration</span> <a title="Permalink" class="permalink" href="#ogw-ssl-advanced">#</a></h3></div></div></div><p>
    If you need to change the default values for SSL settings of the Object Gateway,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_init: default-ssl</pre></div></li><li class="step"><p>
      Copy the default Object Gateway SSL configuration to the
      <code class="filename">ceph.conf.d</code> subdirectory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</pre></div></li><li class="step"><p>
      Edit
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</code>
      and change the default options, such as port number or path to the SSL
      certificate, to reflect your setup.
     </p></li><li class="step"><p>
      Run DeepSea Stage 3 and 4 to apply the changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div><div id="rgw-civetweb-multiport" data-id-title="Binding to Multiple Ports" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Binding to Multiple Ports</h6><p>
     The CivetWeb server can bind to multiple ports. This is useful if you need
     to access a single Object Gateway instance with both SSL and non-SSL connections.
     When specifying the ports, separate their numbers by a plus sign '+'. A
     two-port configuration line example follows:
    </p><div class="verbatim-wrap"><pre class="screen">[client.{{ client }}]
rgw_frontends = civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem</pre></div></div></section></section><section class="sect1" id="ceph-rgw-sync" data-id-title="Sync Modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.7 </span><span class="title-name">Sync Modules</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync">#</a></h2></div></div></div><p>
   The <span class="emphasis"><em>multisite</em></span> functionality of Object Gateway introduced in
   Jewel allows to create multiple zones and mirror data and metadata between
   them. <span class="emphasis"><em>Sync Modules</em></span> are built atop of the multisite
   framework that allows for forwarding data and metadata to a different
   external tier. A sync module allows for a set of actions to be performed
   whenever a change in data occurs (metadata ops like bucket or user creation
   etc. are also regarded as changes in data). As the rgw multisite changes are
   eventually consistent at remote sites, changes are propagated
   asynchronously. This would allow for unlocking use cases such as backing up
   the object storage to an external cloud cluster or a custom backup solution
   using tape drives, indexing metadata in Elasticsearch etc.
  </p><section class="sect2" id="ceph-rgw-sync-zones" data-id-title="Synchronizing Zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.7.1 </span><span class="title-name">Synchronizing Zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones">#</a></h3></div></div></div><p>
    A sync module configuration is local to a zone. The sync module determines
    whether the zone exports data or can only consume data that was modified in
    another zone. As of luminous the supported sync plug-ins are
    <code class="literal">elasticsearch</code>, <code class="literal">rgw</code>, which is the
    default sync plug-in that synchronizes data between the zones and
    <code class="literal">log</code> which is a trivial sync plug-in that logs the
    metadata operation that happens in the remote zones. The following sections
    are written with the example of a zone using
    <code class="literal">elasticsearch</code> sync module. The process would be similar
    for configuring any other sync plug-in.
   </p><div id="id-1.3.5.2.10.3.3" data-id-title="Default Sync Plugin" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Default Sync Plugin</h6><p>
     <code class="literal">rgw</code> is the default sync plug-in and there is no need to
     explicitly configure this.
    </p></div><section class="sect3" id="ceph-rgw-sync-zones-req" data-id-title="Requirements and Assumptions"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.7.1.1 </span><span class="title-name">Requirements and Assumptions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-req">#</a></h4></div></div></div><p>
     Let us assume a simple multisite configuration as described in
     <a class="xref" href="#ceph-rgw-fed" title="13.11. Multisite Object Gateways">Section 13.11, “Multisite Object Gateways”</a> consisting of the 2 zones
     <code class="literal">us-east</code> and <code class="literal">us-west</code>. Now we add a
     third zone <code class="literal">us-east-es</code> which is a zone that only
     processes metadata from the other sites. This zone can be in the same or a
     different Ceph cluster than <code class="literal">us-east</code>. This zone would
     only consume metadata from other zones and Object Gateways in this zone will not
     serve any end user requests directly.
    </p></section><section class="sect3" id="ceph-rgw-sync-zones-configure" data-id-title="Configuring Sync Modules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.7.1.2 </span><span class="title-name">Configuring Sync Modules</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-configure">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the third zone similar to the ones described in
       <a class="xref" href="#ceph-rgw-fed" title="13.11. Multisite Object Gateways">Section 13.11, “Multisite Object Gateways”</a>, for example
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key={system-key} --secret={secret} --endpoints=http://rgw-es:80</pre></div></li><li class="step"><p>
       A sync module can be configured for this zone via the following
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --tier-type={tier-type} \
--tier-config={set of key=value pairs}</pre></div></li><li class="step"><p>
       For example in the <code class="literal">elasticsearch</code> sync module
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1</pre></div><p>
       For the various supported tier-config options refer to
       <a class="xref" href="#ceph-rgw-sync-elastic" title="13.7.2. Storing Metadata in Elasticsearch">Section 13.7.2, “Storing Metadata in Elasticsearch”</a>.
      </p></li><li class="step"><p>
       Finally update the period
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
       Now start the radosgw in the zone
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> start ceph-radosgw@rgw.`hostname -s`
<code class="prompt user">root # </code><code class="command">systemctl</code> enable ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div></section></section><section class="sect2" id="ceph-rgw-sync-elastic" data-id-title="Storing Metadata in Elasticsearch"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.7.2 </span><span class="title-name">Storing Metadata in Elasticsearch</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic">#</a></h3></div></div></div><p>
    This sync module writes the metadata from other zones to Elasticsearch. As
    of luminous this is JSON of data fields we currently store in
    Elasticsearch.
   </p><div class="verbatim-wrap"><pre class="screen">{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}</pre></div><section class="sect3" id="ceph-rgw-sync-elastic-config" data-id-title="Elasticsearch Tier Type Configuration Parameters"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.7.2.1 </span><span class="title-name">Elasticsearch Tier Type Configuration Parameters</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-config">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.10.4.4.2.1"><span class="term">endpoint</span></dt><dd><p>
        Specifies the Elasticsearch server endpoint to access.
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.2"><span class="term">num_shards</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of shards that Elasticsearch
        will be configured with on data sync initialization. Note that this
        cannot be changed after initialization. Any change here requires
        rebuild of the Elasticsearch index and reinitialization of the data
        sync process.
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.3"><span class="term">num_replicas</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of the replicas that
        Elasticsearch will be configured with on data sync initialization.
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.4"><span class="term">explicit_custom_meta</span></dt><dd><p>
        <span class="emphasis"><em>(true | false)</em></span> Specifies whether all user custom
        metadata will be indexed, or whether user will need to configure (at
        the bucket level) what customer metadata entries should be indexed.
        This is false by default
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.5"><span class="term">index_buckets_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty, all
        buckets will be indexed. Otherwise, only buckets specified here will be
        indexed. It is possible to provide bucket prefixes (for example
        'foo*'), or bucket suffixes (for example '*bar').
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.6"><span class="term">approved_owners_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty,
        buckets of all owners will be indexed (subject to other restrictions),
        otherwise, only buckets owned by specified owners will be indexed.
        Suffixes and prefixes can also be provided.
       </p></dd><dt id="id-1.3.5.2.10.4.4.2.7"><span class="term">override_index_path</span></dt><dd><p>
        <span class="emphasis"><em>(string)</em></span> if not empty, this string will be used as
        the Elasticsearch index path. Otherwise the index path will be
        determined and generated on sync initialization.
       </p></dd></dl></div></section><section class="sect3" id="ceph-rgw-sync-elastic-query" data-id-title="Metadata Queries"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.7.2.2 </span><span class="title-name">Metadata Queries</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-query">#</a></h4></div></div></div><p>
     Since the Elasticsearch cluster now stores object metadata, it is
     important that the Elasticsearch endpoint is not exposed to the public and
     only accessible to the cluster administrators. For exposing metadata
     queries to the end user itself this poses a problem since we'd want the
     user to only query their metadata and not of any other users, this would
     require the Elasticsearch cluster to authenticate users in a way similar
     to RGW does which poses a problem.
    </p><p>
     As of Luminous RGW in the metadata master zone can now service end user
     requests. This allows for not exposing the Elasticsearch endpoint in
     public and also solves the authentication and authorization problem since
     RGW itself can authenticate the end user requests. For this purpose RGW
     introduces a new query in the bucket APIs that can service Elasticsearch
     requests. All these requests must be sent to the metadata master zone.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.10.4.5.4.1"><span class="term">Get an Elasticsearch Query</span></dt><dd><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?query={query-expr}</pre></div><p>
        request params:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          max-keys: max number of entries to return
         </p></li><li class="listitem"><p>
          marker: pagination marker
         </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]</pre></div><p>
        op is one of the following: &lt;, &lt;=, ==, &gt;=, &gt;
       </p><p>
        For example:
       </p><div class="verbatim-wrap"><pre class="screen">GET /?query=name==foo</pre></div><p>
        Will return all the indexed keys that user has read permission to, and
        are named 'foo'. The output will be a list of keys in XML that is
        similar to the S3 list buckets response.
       </p></dd><dt id="id-1.3.5.2.10.4.5.4.2"><span class="term">Configure custom metadata fields</span></dt><dd><p>
        Define which custom metadata entries should be indexed (under the
        specified bucket), and what are the types of these keys. If explicit
        custom metadata indexing is configured, this is needed so that rgw will
        index the specified custom metadata values. Otherwise it is needed in
        cases where the indexed metadata keys are of a type other than string.
       </p><div class="verbatim-wrap"><pre class="screen">POST /<em class="replaceable">BUCKET</em>?mdsearch
x-amz-meta-search: &lt;key [; type]&gt; [, ...]</pre></div><p>
        Multiple metadata fields must be comma separated, a type can be forced
        for a field with a `;`. The currently allowed types are
        string(default), integer and date, for example, if you want to index a
        custom object metadata x-amz-meta-year as int, x-amz-meta-date as type
        date and x-amz-meta-title as string, you would do
       </p><div class="verbatim-wrap"><pre class="screen">POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string</pre></div></dd><dt id="id-1.3.5.2.10.4.5.4.3"><span class="term">Delete custom metadata configuration</span></dt><dd><p>
        Delete custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">DELETE /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd><dt id="id-1.3.5.2.10.4.5.4.4"><span class="term">Get custom metadata configuration</span></dt><dd><p>
        Retrieve custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd></dl></div></section></section></section><section class="sect1" id="ceph-rgw-ldap" data-id-title="LDAP Authentication"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.8 </span><span class="title-name">LDAP Authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap">#</a></h2></div></div></div><p>
   Apart from the default local user authentication, Object Gateway can use LDAP server
   services to authenticate users as well.
  </p><section class="sect2" id="ceph-rgw-ldap-how-works" data-id-title="Authentication Mechanism"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.8.1 </span><span class="title-name">Authentication Mechanism</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-how-works">#</a></h3></div></div></div><p>
    The Object Gateway extracts the user's LDAP credentials from a token. A search
    filter is constructed from the user name. The Object Gateway uses the configured
    service account to search the directory for a matching entry. If an entry
    is found, the Object Gateway attempts to bind to the found distinguished name with
    the password from the token. If the credentials are valid, the bind will
    succeed, and the Object Gateway grants access.
   </p><p>
    You can limit the allowed users by setting the base for the search to a
    specific organizational unit or by specifying a custom search filter, for
    example requiring specific group membership, custom object classes, or
    attributes.
   </p></section><section class="sect2" id="ceph-rgw-ldap-reqs" data-id-title="Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.8.2 </span><span class="title-name">Requirements</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-reqs">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>LDAP or Active Directory</em></span>: A running LDAP instance
      accessible by the Object Gateway.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Service account</em></span>: LDAP credentials to be used by the
      Object Gateway with search permissions.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>User account</em></span>: At least one user account in the LDAP
      directory.
     </p></li></ul></div><div id="id-1.3.5.2.11.4.3" data-id-title="Do Not Overlap LDAP and Local Users" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Overlap LDAP and Local Users</h6><p>
     You should not use the same user names for local users and for users being
     authenticated by using LDAP. The Object Gateway cannot distinguish them and it
     treats them as the same user.
    </p></div><div id="id-1.3.5.2.11.4.4" data-id-title="Sanity Checks" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sanity Checks</h6><p>
     Use the <code class="command">ldapsearch</code> utility to verify the service
     account or the LDAP connection. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</pre></div><p>
     Make sure to use the same LDAP parameters as in the Ceph configuration
     file to eliminate possible problems.
    </p></div></section><section class="sect2" id="ceph-rgw-ldap-config" data-id-title="Configure Object Gateway to Use LDAP Authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.8.3 </span><span class="title-name">Configure Object Gateway to Use LDAP Authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-config">#</a></h3></div></div></div><p>
    The following parameters in the <code class="filename">/etc/ceph/ceph.conf</code>
    configuration file are related to the LDAP authentication:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.11.5.3.1"><span class="term"><code class="option">rgw_ldap_uri</code></span></dt><dd><p>
       Specifies the LDAP server to use. Make sure to use the
       <code class="literal">ldaps://<em class="replaceable">fqdn</em>:<em class="replaceable">port</em></code>
       parameter to avoid transmitting the plain text credentials openly.
      </p></dd><dt id="id-1.3.5.2.11.5.3.2"><span class="term"><code class="option">rgw_ldap_binddn</code></span></dt><dd><p>
       The Distinguished Name (DN) of the service account used by the Object Gateway.
      </p></dd><dt id="id-1.3.5.2.11.5.3.3"><span class="term"><code class="option">rgw_ldap_secret</code></span></dt><dd><p>
       The password for the service account.
      </p></dd><dt id="id-1.3.5.2.11.5.3.4"><span class="term">rgw_ldap_searchdn</span></dt><dd><p>
       Specifies the base in the directory information tree for searching
       users. This might be your users organizational unit or some more
       specific Organizational Unit (OU).
      </p></dd><dt id="id-1.3.5.2.11.5.3.5"><span class="term"><code class="option">rgw_ldap_dnattr</code></span></dt><dd><p>
       The attribute being used in the constructed search filter to match a
       user name. Depending on your Directory Information Tree (DIT) this would
       probably be <code class="literal">uid</code> or <code class="literal">cn</code>.
      </p></dd><dt id="id-1.3.5.2.11.5.3.6"><span class="term"><code class="option">rgw_search_filter</code></span></dt><dd><p>
       If not specified, the Object Gateway automatically constructs the search filter
       with the <code class="option">rgw_ldap_dnattr</code> setting. Use this parameter to
       narrow the list of allowed users in very flexible ways. Consult
       <a class="xref" href="#ceph-rgw-ldap-filter" title="13.8.4. Using a Custom Search Filter to Limit User Access">Section 13.8.4, “Using a Custom Search Filter to Limit User Access”</a> for details.
      </p></dd></dl></div></section><section class="sect2" id="ceph-rgw-ldap-filter" data-id-title="Using a Custom Search Filter to Limit User Access"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.8.4 </span><span class="title-name">Using a Custom Search Filter to Limit User Access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-filter">#</a></h3></div></div></div><p>
    There are two ways you can use the <code class="option">rgw_search_filter</code>
    parameter.
   </p><section class="sect3" id="id-1.3.5.2.11.6.3" data-id-title="Partial Filter to Further Limit the Constructed Search Filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.8.4.1 </span><span class="title-name">Partial Filter to Further Limit the Constructed Search Filter</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.11.6.3">#</a></h4></div></div></div><p>
     An example of a partial filter:
    </p><div class="verbatim-wrap"><pre class="screen">"objectclass=inetorgperson"</pre></div><p>
     The Object Gateway will generate the search filter as usual with the user name from
     the token and the value of <code class="option">rgw_ldap_dnattr</code>. The
     constructed filter is then combined with the partial filter from the
     <code class="option">rgw_search_filter</code> attribute. Depending on the user name
     and the settings the final search filter may become:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=hari)(objectclass=inetorgperson))"</pre></div><p>
     In that case, user 'hari' will only be granted access if he is found in
     the LDAP directory, has an object class of 'inetorgperson', and did
     specify a valid password.
    </p></section><section class="sect3" id="id-1.3.5.2.11.6.4" data-id-title="Complete Filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.8.4.2 </span><span class="title-name">Complete Filter</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.11.6.4">#</a></h4></div></div></div><p>
     A complete filter must contain a <code class="option">USERNAME</code> token which
     will be substituted with the user name during the authentication attempt.
     The <code class="option">rgw_ldap_dnattr</code> parameter is not used anymore in this
     case. For example, to limit valid users to a specific group, use the
     following filter:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</pre></div><div id="id-1.3.5.2.11.6.4.4" data-id-title="memberOf Attribute" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="literal">memberOf</code> Attribute</h6><p>
      Using the <code class="literal">memberOf</code> attribute in LDAP searches requires
      server side support from you specific LDAP server implementation.
     </p></div></section></section><section class="sect2" id="ceph-rgw-ldap-token" data-id-title="Generating an Access Token for LDAP authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.8.5 </span><span class="title-name">Generating an Access Token for LDAP authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-token">#</a></h3></div></div></div><p>
    The <code class="command">radosgw-token</code> utility generates the access token
    based on the LDAP user name and password. It outputs a base-64 encoded
    string which is the actual access token. Use your favorite S3 client (refer
    to <a class="xref" href="#accessing-ragos-gateway" title="13.5.1. Accessing Object Gateway">Section 13.5.1, “Accessing Object Gateway”</a>) and specify the token as the
    access key and use an empty secret key.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>export RGW_ACCESS_KEY_ID="<em class="replaceable">username</em>"
<code class="prompt user">cephadm &gt; </code>export RGW_SECRET_ACCESS_KEY="<em class="replaceable">password</em>"
<code class="prompt user">cephadm &gt; </code>radosgw-token --encode --ttype=ldap</pre></div><div id="id-1.3.5.2.11.7.4" data-id-title="Clear Text Credentials" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Clear Text Credentials</h6><p>
     The access token is a base-64 encoded JSON structure and contains the LDAP
     credentials as a clear text.
    </p></div><div id="id-1.3.5.2.11.7.5" data-id-title="Active Directory" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Active Directory</h6><p>
     For Active Directory, use the <code class="option">--ttype=ad</code> parameter.
    </p></div></section></section><section class="sect1" id="ogw-bucket-sharding" data-id-title="Bucket Index Sharding"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.9 </span><span class="title-name">Bucket Index Sharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding">#</a></h2></div></div></div><p>
   The Object Gateway stores bucket index data in an index pool, which defaults to
   <code class="literal">.rgw.buckets.index</code>. If you put too many (hundreds of
   thousands) objects into a single bucket and the quota for maximum number of
   objects per bucket (<code class="option">rgw bucket default quota max objects</code>)
   is not set, the performance of the index pool may degrade. <span class="emphasis"><em>Bucket
   index sharding</em></span> prevents such performance decreases and allows a
   high number of objects per bucket.
  </p><section class="sect2" id="ogw-bucket-reshard" data-id-title="Bucket Index Resharding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.9.1 </span><span class="title-name">Bucket Index Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-reshard">#</a></h3></div></div></div><p>
    If a bucket has grown large and its initial configuration is not sufficient
    anymore, the bucket's index pool needs to be resharded. You can either use
    automatic online bucket index resharding (refer to
    <a class="xref" href="#ogw-bucket-sharding-dyn" title="13.9.1.1. Dynamic Resharding">Section 13.9.1.1, “Dynamic Resharding”</a>, or reshard the bucket index
    offline manually (refer to <a class="xref" href="#ogw-bucket-sharding-re" title="13.9.1.2. Manual Resharding">Section 13.9.1.2, “Manual Resharding”</a>.
   </p><section class="sect3" id="ogw-bucket-sharding-dyn" data-id-title="Dynamic Resharding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.9.1.1 </span><span class="title-name">Dynamic Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-dyn">#</a></h4></div></div></div><p>
     Since SUSE Enterprise Storage 5.5, we support online bucket resharding. It detects if
     the number of objects per bucket reaches a certain threshold, and
     automatically increases the number of shards used by the bucket index.
     This process reduces the number of entries in each bucket index shard.
    </p><p>
     The detection process runs:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       When new objects are added to the bucket.
      </p></li><li class="listitem"><p>
       In a background process that periodically scans all the buckets. This is
       needed in order to deal with existing buckets that are not being
       updated.
      </p></li></ul></div><p>
     A bucket that requires resharding is added to the
     <code class="option">reshard_log</code> queue and will be scheduled to be resharded
     later. The reshard threads run in the background and execute the scheduled
     resharding, one at a time.
    </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Configuring Dynamic Resharding </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.12.3.3.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.12.3.3.6.2"><span class="term"><code class="option">rgw_dynamic_resharding</code></span></dt><dd><p>
        Enables or disables dynamic bucket index resharding. Possible values
        are 'true' or 'false'. Defaults to 'true'.
       </p></dd><dt id="id-1.3.5.2.12.3.3.6.3"><span class="term"><code class="option">rgw_reshard_num_logs</code></span></dt><dd><p>
        Number of shards for the resharding log. Defaults to 16.
       </p></dd><dt id="id-1.3.5.2.12.3.3.6.4"><span class="term"><code class="option">rgw_reshard_bucket_lock_duration</code></span></dt><dd><p>
        Duration of lock on the bucket object during resharding. Defaults to
        120 seconds.
       </p></dd><dt id="id-1.3.5.2.12.3.3.6.5"><span class="term"><code class="option">rgw_max_objs_per_shard</code></span></dt><dd><p>
        Maximum number of objects per bucket index shard. Defaults to 100000
        objects.
       </p></dd><dt id="id-1.3.5.2.12.3.3.6.6"><span class="term"><code class="option">rgw_reshard_thread_interval</code></span></dt><dd><p>
        Maximum time between rounds of reshard thread processing. Defaults to
        600 seconds.
       </p></dd></dl></div><div id="id-1.3.5.2.12.3.3.7" data-id-title="Multisite Configurations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Multisite Configurations</h6><p>
      Dynamic resharding is not supported in multisite environment. It is
      disabled by default since Ceph 12.2.2, but we recommend you to double
      check the setting.
     </p></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Commands to Administer the Resharding Process </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.12.3.3.8">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.2.12.3.3.8.2"><span class="term">Add a bucket to the resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin reshard add \
 --bucket <em class="replaceable">BUCKET_NAME</em> \
 --num-shards <em class="replaceable">NEW_NUMBER_OF_SHARDS</em></pre></div></dd><dt id="id-1.3.5.2.12.3.3.8.3"><span class="term">List resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin reshard list</pre></div></dd><dt id="id-1.3.5.2.12.3.3.8.4"><span class="term">Process / schedule a bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin reshard process</pre></div></dd><dt id="id-1.3.5.2.12.3.3.8.5"><span class="term">Display the bucket resharding status:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin reshard status --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd><dt id="id-1.3.5.2.12.3.3.8.6"><span class="term">Cancel pending bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin reshard cancel --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd></dl></div></section><section class="sect3" id="ogw-bucket-sharding-re" data-id-title="Manual Resharding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.9.1.2 </span><span class="title-name">Manual Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-re">#</a></h4></div></div></div><p>
     Dynamic resharding mentioned in <a class="xref" href="#ogw-bucket-sharding-dyn" title="13.9.1.1. Dynamic Resharding">Section 13.9.1.1, “Dynamic Resharding”</a>
     is supported only for simple Object Gateway configurations. For multisite
     configurations, use manual resharding described in this section.
    </p><p>
     To reshard the bucket index manually offline, use the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin bucket reshard</pre></div><p>
     The <code class="command">bucket reshard</code> command performs the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creates a new set of bucket index objects for the specified object.
      </p></li><li class="listitem"><p>
       Spreads all objects entries of these index objects.
      </p></li><li class="listitem"><p>
       Creates a new bucket instance.
      </p></li><li class="listitem"><p>
       Links the new bucket instance with the bucket so that all new index
       operations go through the new bucket indexes.
      </p></li><li class="listitem"><p>
       Prints the old and the new bucket ID to the standard output.
      </p></li></ul></div><div class="procedure" id="id-1.3.5.2.12.3.4.7" data-id-title="Resharding the Bucket Index Pool"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 13.1: </span><span class="title-name">Resharding the Bucket Index Pool </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.12.3.4.7">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Make sure that all operations to the bucket are stopped.
      </p></li><li class="step"><p>
       Back up the original bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin bi list \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 &gt; <em class="replaceable">BUCKET_NAME</em>.list.backup</pre></div></li><li class="step"><p>
       Reshard the bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm &gt; </code>radosgw-admin reshard \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 --num-shards=<em class="replaceable">NEW_SHARDS_NUMBER</em></pre></div><div id="id-1.3.5.2.12.3.4.7.4.3" data-id-title="Old Bucket ID" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Old Bucket ID</h6><p>
        As part of its output, this command also prints the new and the old
        bucket ID. Note the old bucket ID down; you will need it to purge the
        old bucket index objects.
       </p></div></li><li class="step"><p>
       Verify that the objects are listed correctly by comparing the old bucket
       index listing with the new one. Then purge the old bucket index objects:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin bi purge
 --bucket=<em class="replaceable">BUCKET_NAME</em>
 --bucket-id=<em class="replaceable">OLD_BUCKET_ID</em></pre></div></li></ol></div></div></section></section><section class="sect2" id="ogw-bucket-sharding-new" data-id-title="Bucket Index Sharding for New Buckets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.9.2 </span><span class="title-name">Bucket Index Sharding for New Buckets</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-new">#</a></h3></div></div></div><p>
    There are two options that affect bucket index sharding:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Use the <code class="option">rgw_override_bucket_index_max_shards</code> option for
      simple configurations.
     </p></li><li class="listitem"><p>
      Use the <code class="option">bucket_index_max_shards</code> option for multisite
      configurations.
     </p></li></ul></div><p>
    Setting the options to <code class="literal">0</code> disables bucket index sharding.
    A value greater than <code class="literal">0</code> enables bucket index sharding and
    sets the maximum number of shards.
   </p><p>
    The following formula helps you calculate the recommended number of shards:
   </p><div class="verbatim-wrap"><pre class="screen">number_of_objects_expected_in_a_bucket / 100000</pre></div><p>
    Be aware that the maximum number of shards is 7877.
   </p><section class="sect3" id="id-1.3.5.2.12.4.8" data-id-title="Simple Configurations"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.9.2.1 </span><span class="title-name">Simple Configurations</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.12.4.8">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open the Ceph configuration file and add or modify the following
       option:
      </p><div class="verbatim-wrap"><pre class="screen">rgw_override_bucket_index_max_shards = 12</pre></div><div id="id-1.3.5.2.12.4.8.2.1.3" data-id-title="All or One Object Gateway Instances" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: All or One Object Gateway Instances</h6><p>
        To configure bucket index sharding for all instances of the Object Gateway,
        include <code class="option">rgw_override_bucket_index_max_shards</code> in the
        <code class="literal">[global]</code> section.
       </p><p>
        To configure bucket index sharding only for a particular instance of
        the Object Gateway, include
        <code class="option">rgw_override_bucket_index_max_shards</code> in the related
        instance section.
       </p></div></li><li class="step"><p>
       Restart the Object Gateway. See <a class="xref" href="#ceph-rgw-operating" title="13.3. Operating the Object Gateway Service">Section 13.3, “Operating the Object Gateway Service”</a> for more
       details.
      </p></li></ol></div></div></section><section class="sect3" id="id-1.3.5.2.12.4.9" data-id-title="Multisite Configurations"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.9.2.2 </span><span class="title-name">Multisite Configurations</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.12.4.9">#</a></h4></div></div></div><p>
     Multisite configurations can have a different index pool to manage
     failover. To configure a consistent shard count for zones in one zone
     group, set the <code class="option">bucket_index_max_shards</code> option in the zone
     group's configuration:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Export the zone group configuration to the
       <code class="filename">zonegroup.json</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup get &gt; zonegroup.json</pre></div></li><li class="step"><p>
       Edit the <code class="filename">zonegroup.json</code> file and set the
       <code class="option">bucket_index_max_shards</code> option for each named zone.
      </p></li><li class="step"><p>
       Reset the zone group:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup set &lt; zonegroup.json</pre></div></li><li class="step"><p>
       Update the period:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period update --commit</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="ogw-keystone" data-id-title="Integrating OpenStack Keystone"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.10 </span><span class="title-name">Integrating OpenStack Keystone</span> <a title="Permalink" class="permalink" href="#ogw-keystone">#</a></h2></div></div></div><p>
   OpenStack Keystone is an identity service for the OpenStack product. You can
   integrate the Object Gateway with Keystone to set up a gateway that accepts a
   Keystone authentication token. A user authorized by Keystone to access
   the gateway will be verified on the Ceph Object Gateway side and automatically created if
   needed. The Object Gateway queries Keystone periodically for a list of revoked
   tokens.
  </p><section class="sect2" id="ogw-keystone-ostack" data-id-title="Configuring OpenStack"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.10.1 </span><span class="title-name">Configuring OpenStack</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ostack">#</a></h3></div></div></div><p>
    Before configuring the Ceph Object Gateway, you need to configure the OpenStack Keystone to
    enable the Swift service and point it to the Ceph Object Gateway:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <span class="emphasis"><em>Set the Swift service.</em></span> To use OpenStack to validate
      Swift users, first create the Swift service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>openstack service create \
 --name=swift \
 --description="Swift Service" \
 object-store</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Set the endpoints.</em></span> After you create the Swift
      service, point to the Ceph Object Gateway. Replace
      <em class="replaceable">REGION_NAME</em> with the name of the gateway’s
      zone group name or region name.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>openstack endpoint create --region <em class="replaceable">REGION_NAME</em> \
 --publicurl   "http://radosgw.example.com:8080/swift/v1" \
 --adminurl    "http://radosgw.example.com:8080/swift/v1" \
 --internalurl "http://radosgw.example.com:8080/swift/v1" \
 swift</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Verify the settings.</em></span> After you create the Swift
      service and set the endpoints, show the endpoints to verify that all the
      settings are correct.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>openstack endpoint show object-store</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-keystone-ogw" data-id-title="Configuring the Ceph Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.10.2 </span><span class="title-name">Configuring the Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ogw">#</a></h3></div></div></div><section class="sect3" id="id-1.3.5.2.13.4.2" data-id-title="Configure SSL Certificates"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.10.2.1 </span><span class="title-name">Configure SSL Certificates</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.13.4.2">#</a></h4></div></div></div><p>
     The Ceph Object Gateway queries Keystone periodically for a list of revoked tokens.
     These requests are encoded and signed. Keystone may be also configured
     to provide self-signed tokens, which are also encoded and signed. You need
     to configure the gateway so that it can decode and verify these signed
     messages. Therefore, the OpenSSL certificates that Keystone uses to
     create the requests need to be converted to the 'nss db' format:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /var/ceph/nss
<code class="prompt user">root # </code>openssl x509 -in /etc/keystone/ssl/certs/ca.pem \
 -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
<code class="systemitem">root</code>openssl x509 -in /etc/keystone/ssl/certs/signing_cert.pem \
 -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"</pre></div><p>
     To allow Ceph Object Gateway to interact with OpenStack Keystone, OpenStack Keystone can use a
     self-signed SSL certificate. Either install Keystone’s SSL certificate
     on the node running the Ceph Object Gateway, or alternatively set the value of the
     option <code class="option">rgw keystone verify ssl</code> to 'false'. Setting
     <code class="option">rgw keystone verify ssl</code> to 'false' means that the gateway
     will not attempt to verify the certificate.
    </p></section><section class="sect3" id="id-1.3.5.2.13.4.3" data-id-title="Configure the Object Gateways Options"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.10.2.2 </span><span class="title-name">Configure the Object Gateway's Options</span> <a title="Permalink" class="permalink" href="#id-1.3.5.2.13.4.3">#</a></h4></div></div></div><p>
     You can configure Keystone integration using the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.13.4.3.3.1"><span class="term"><code class="option">rgw keystone api version</code></span></dt><dd><p>
        Version of the Keystone API. Valid options are 2 or 3. Defaults to 2.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.2"><span class="term"><code class="option">rgw keystone url</code></span></dt><dd><p>
        The URL and port number of the administrative RESTful API on the
        Keystone server. Follows the pattern
        <em class="replaceable">SERVER_URL:PORT_NUMBER</em>.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.3"><span class="term"><code class="option">rgw keystone admin token</code></span></dt><dd><p>
        The token or shared secret that is configured internally in Keystone
        for administrative requests.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.4"><span class="term"><code class="option">rgw keystone accepted roles</code></span></dt><dd><p>
        The roles required to serve requests. Defaults to 'Member, admin'.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.5"><span class="term"><code class="option">rgw keystone accepted admin roles</code></span></dt><dd><p>
        The list of roles allowing a user to gain administrative privileges.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.6"><span class="term"><code class="option">rgw keystone token cache size</code></span></dt><dd><p>
        The maximum number of entries in the Keystone token cache.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.7"><span class="term"><code class="option">rgw keystone revocation interval</code></span></dt><dd><p>
        The number of seconds before checking revoked tokens. Defaults to 15 *
        60.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.8"><span class="term"><code class="option">rgw keystone implicit tenants</code></span></dt><dd><p>
        Create new users in their own tenants of the same name. Defaults to
        'false'.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.9"><span class="term"><code class="option">rgw s3 auth use keystone</code></span></dt><dd><p>
        If set to 'true', the Ceph Object Gateway will authenticate users using Keystone.
        Defaults to 'false'.
       </p></dd><dt id="id-1.3.5.2.13.4.3.3.10"><span class="term"><code class="option">nss db path</code></span></dt><dd><p>
        The path to the NSS database.
       </p></dd></dl></div><p>
     It is also possible to configure the Keystone service tenant, user &amp;
     password for keystone (for v2.0 version of the OpenStack Identity API),
     similar to the way OpenStack services tend to be configured. This way you
     can avoid setting the shared secret <code class="option">rgw keystone admin
     token</code> in the configuration file, which should be disabled in
     production environments. The service tenant credentials should have admin
     privileges, for more details refer to the
     <a class="link" href="https://docs.openstack.org/keystone/latest/#setting-up-projects-users-and-roles" target="_blank">official
     OpenStack Keystone documentation</a>. The related configuration options
     follow:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.13.4.3.5.1"><span class="term"><code class="option">rgw keystone admin user</code></span></dt><dd><p>
        The Keystone administrator user name.
       </p></dd><dt id="id-1.3.5.2.13.4.3.5.2"><span class="term"><code class="option">rgw keystone admin password</code></span></dt><dd><p>
        The keystone administrator user password.
       </p></dd><dt id="id-1.3.5.2.13.4.3.5.3"><span class="term"><code class="option">rgw keystone admin tenant</code></span></dt><dd><p>
        The Keystone version 2.0 administrator user tenant.
       </p></dd></dl></div><p>
     A Ceph Object Gateway user is mapped to a Keystone tenant. A Keystone user has
     different roles assigned to it, possibly on more than a single tenant.
     When the Ceph Object Gateway gets the ticket, it looks at the tenant and the user roles
     that are assigned to that ticket, and accepts or rejects the request
     according to the setting of the <code class="option">rgw keystone accepted
     roles</code> option.
    </p><div id="id-1.3.5.2.13.4.3.7" data-id-title="Mapping to OpenStack Tenants" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mapping to OpenStack Tenants</h6><p>
      Although Swift tenants are mapped to the Object Gateway user by default, they
      can be also mapped to OpenStack tenants via the <code class="option">rgw keystone
      implicit tenants</code> option. This will make containers use the
      tenant namespace instead of the S3 like global namespace that the Object Gateway
      defaults to. We recommend deciding on the mapping method at the planning
      stage to avoid confusion. The reason is that toggling the option later
      affects only newer requests which get mapped under a tenant, while older
      buckets created before still continue to be in a global namespace.
     </p></div><p>
     For version 3 of the OpenStack Identity API, you should replace the
     <code class="option">rgw keystone admin tenant</code> option with:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.13.4.3.9.1"><span class="term"><code class="option">rgw keystone admin domain</code></span></dt><dd><p>
        The Keystone administrator user domain.
       </p></dd><dt id="id-1.3.5.2.13.4.3.9.2"><span class="term"><code class="option">rgw keystone admin project</code></span></dt><dd><p>
        The Keystone administrator user project.
       </p></dd></dl></div></section></section></section><section class="sect1" id="ceph-rgw-fed" data-id-title="Multisite Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.11 </span><span class="title-name">Multisite Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed">#</a></h2></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.2.14.2.1"><span class="term">Zone</span></dt><dd><p>
      A logical grouping of one or more Object Gateway instances. There must be one zone
      designated as the <span class="emphasis"><em>master</em></span> zone in a
      <span class="emphasis"><em>zonegroup</em></span>, which handles all bucket and user
      creation.
     </p></dd><dt id="id-1.3.5.2.14.2.2"><span class="term">Zonegroup</span></dt><dd><p>
      A zonegroup consists of multiple zones. There should be a master
      zonegroup that will handle changes to the system configuration.
     </p></dd><dt id="id-1.3.5.2.14.2.3"><span class="term">Zonegroup map</span></dt><dd><p>
      A configuration structure that holds the map of the entire system, for
      example which zonegroup is the master, relationships between different
      zone groups, and certain configuration options such as storage policies.
     </p></dd><dt id="id-1.3.5.2.14.2.4"><span class="term">Realm</span></dt><dd><p>
      A container for zone groups. This allows for separation of zone groups
      between clusters. It is possible to create multiple realms, making it
      easier to run completely different configurations in the same cluster.
     </p></dd><dt id="id-1.3.5.2.14.2.5"><span class="term">Period</span></dt><dd><p>
      A period holds the configuration structure for the current state of the
      realm. Every period contains a unique ID and an epoch. Every realm has an
      associated current period, holding the current state of configuration of
      the zone groups and storage policies. Any configuration change for a
      non-master zone will increment the period's epoch. Changing the master
      zone to a different zone will trigger the following changes:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        A new period is generated with a new period ID and epoch of 1.
       </p></li><li class="listitem"><p>
        Realm's current period is updated to point to the newly generated
        period ID.
       </p></li><li class="listitem"><p>
        Realm's epoch is incremented.
       </p></li></ul></div></dd></dl></div><p>
   You can configure each Object Gateway to participate in a federated architecture,
   working in an active zone configuration while allowing for writes to
   non-master zones.
  </p><section class="sect2" id="ceph-rgw-fed-term" data-id-title="Terminology"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.1 </span><span class="title-name">Terminology</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-term">#</a></h3></div></div></div><p>
    A description of terms specific to a federated architecture follows:
   </p></section><section class="sect2" id="ceph-rgw-fed-intro" data-id-title="Example Cluster Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.2 </span><span class="title-name">Example Cluster Setup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-intro">#</a></h3></div></div></div><p>
    In this example, we will focus on creating a single zone group with three
    separate zones, which actively synchronize their data. Two zones belong to
    the same cluster, while the third belongs to a different one. There is no
    synchronization agent involved in mirroring data changes between the
    Object Gateways. This allows for a much simpler configuration scheme and
    active-active configurations. Note that metadata operations—such as
    creating a new user—still need to go through the master zone.
    However, data operations—such as creation of buckets and
    objects—can be handled by any of the zones.
   </p></section><section class="sect2" id="ceph-rgw-fed-keys" data-id-title="System Keys"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.3 </span><span class="title-name">System Keys</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-keys">#</a></h3></div></div></div><p>
    While configuring zones, Object Gateway expects creation of an S3-compatible system
    user together with their access and secret keys. This allows another Object Gateway
    instance to pull the configuration remotely with the access and secret
    keys. For more information on creating S3 users, see
    <a class="xref" href="#adding-s3-swift-users" title="13.5.2.1. Adding S3 and Swift Users">Section 13.5.2.1, “Adding S3 and Swift Users”</a>.
   </p><div id="id-1.3.5.2.14.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     It is useful to generate the access and secret keys before the zone
     creation itself because it makes scripting and use of configuration
     management tools easier later on.
    </p></div><p>
    For the purpose of this example, let us assume that the access and secret
    keys are set in the environment variables:
   </p><div class="verbatim-wrap"><pre class="screen"># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</pre></div><p>
    Generally, access keys consist of 20 alphanumeric characters, while secret
    keys consist of 40 alphanumeric characters (they can contain +/= characters
    as well). You can generate these keys in the command line:
   </p><div class="verbatim-wrap"><pre class="screen"># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</pre></div></section><section class="sect2" id="ceph-rgw-fed-naming" data-id-title="Naming Conventions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.4 </span><span class="title-name">Naming Conventions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-naming">#</a></h3></div></div></div><p>
    This example describes the process of setting up a master zone. We will
    assume a zonegroup called <code class="literal">us</code> spanning the United States,
    which will be our master zonegroup. This will contain two zones written in
    a <em class="replaceable">zonegroup</em>-<em class="replaceable">zone</em>
    format. This is our convention only and you can choose a format that you
    prefer. In summary:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Master zonegroup: United States <code class="literal">us</code>
     </p></li><li class="listitem"><p>
      Master zone: United States, East Region 1: <code class="literal">us-east-1</code>
     </p></li><li class="listitem"><p>
      Secondary zone: United States, East Region 2:
      <code class="literal">us-east-2</code>
     </p></li><li class="listitem"><p>
      Secondary zone: United States, West Region: <code class="literal">us-west</code>
     </p></li></ul></div><p>
    This will be a part of a larger realm named <code class="literal">gold</code>. The
    <code class="literal">us-east-1</code> and <code class="literal">us-east-2</code> zones are
    part of the same Ceph cluster, <code class="literal">us-east-1</code> being the
    primary one. <code class="literal">us-west</code> is in a different Ceph cluster.
   </p></section><section class="sect2" id="ceph-rgw-fed-pools" data-id-title="Default Pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.5 </span><span class="title-name">Default Pools</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-pools">#</a></h3></div></div></div><p>
    When configured with the appropriate permissions, Object Gateway creates default
    pools on its own. The <code class="literal">pg_num</code> and
    <code class="literal">pgp_num</code> values are taken from the
    <code class="filename">ceph.conf</code> configuration file. Pools related to a zone
    by default follow the convention of
    <em class="replaceable">zone-name</em>.<em class="replaceable">pool-name</em>.
    For example for the <code class="literal">us-east-1</code> zone, it will be the
    following pools:
   </p><div class="verbatim-wrap"><pre class="screen">.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</pre></div><p>
    These pools can be created in other zones as well, by replacing
    <code class="literal">us-east-1</code> with the appropriate zone name.
   </p></section><section class="sect2" id="ceph-rgw-fed-realm" data-id-title="Creating a Realm"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.6 </span><span class="title-name">Creating a Realm</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-realm">#</a></h3></div></div></div><p>
    Configure a realm called <code class="literal">gold</code> and make it the default
    realm:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</pre></div><p>
    Note that every realm has an ID, which allows for flexibility such as
    renaming the realm later if needed. The <code class="literal">current_period</code>
    changes whenever we change anything in the master zone. The
    <code class="literal">epoch</code> is incremented when there is a change in the
    master zone's configuration which results in a change of the current
    period.
   </p></section><section class="sect2" id="ceph-rgw-fed-deldefzonegrp" data-id-title="Deleting the Default Zonegroup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.7 </span><span class="title-name">Deleting the Default Zonegroup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-deldefzonegrp">#</a></h3></div></div></div><p>
    The default installation of Object Gateway creates the default zonegroup called
    <code class="literal">default</code>. Because we no longer need the default
    zonegroup, remove it.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup delete --rgw-zonegroup=default</pre></div></section><section class="sect2" id="ceph-rgw-fed-createmasterzonegrp" data-id-title="Creating a Master Zonegroup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.8 </span><span class="title-name">Creating a Master Zonegroup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-createmasterzonegrp">#</a></h3></div></div></div><p>
    Create a master zonegroup called <code class="literal">us</code>. The zonegroup will
    manage the zonegroup map and propagate changes to the rest of the system.
    By marking the zonegroup as default, you allow explicitly mentioning the
    rgw-zonegroup switch for later commands.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</pre></div><p>
    Alternatively, you can mark a zonegroup as default with the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup default --rgw-zonegroup=us</pre></div></section><section class="sect2" id="ceph-rgw-fed-masterzone" data-id-title="Creating a Master Zone"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.9 </span><span class="title-name">Creating a Master Zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone">#</a></h3></div></div></div><p>
    Now create a default zone and add it to the default zonegroup. Note that
    you will use this zone for metadata operations such as user creation:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</pre></div><p>
    Note that the <code class="option">--rgw-zonegroup</code> and
    <code class="option">--default</code> switches add the zone to a zonegroup and make it
    the default zone. Alternatively, the same can also be done with the
    following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zone default --rgw-zone=us-east-1
<code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</pre></div><section class="sect3" id="ceph-rgw-fed-masterzone-createuser" data-id-title="Creating System Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.9.1 </span><span class="title-name">Creating System Users</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-createuser">#</a></h4></div></div></div><p>
     To access zone pools, you need to create a system user. Note that you will
     need these keys when configuring the secondary zone as well.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> \
--secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em> --system</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-updateperiod" data-id-title="Update the Period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.9.2 </span><span class="title-name">Update the Period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-updateperiod">#</a></h4></div></div></div><p>
     Because you changed the master zone configuration, you need to commit the
     changes for them to take effect in the realm configuration structure.
     Initially, the period looks like this:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</pre></div><p>
     Update the period and commit the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-startrgw" data-id-title="Start the Object Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.9.3 </span><span class="title-name">Start the Object Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-startrgw">#</a></h4></div></div></div><p>
     You need to mention the Object Gateway zone and port options in the configuration
     file before starting the Object Gateway. For more information on Object Gateway and its
     configuration, see <a class="xref" href="#cha-ceph-gw" title="Chapter 13. Ceph Object Gateway">Chapter 13, <em>Ceph Object Gateway</em></a>. The configuration
     section of Object Gateway should look similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</pre></div><p>
     Start the Object Gateway:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-radosgw@rgw.us-east-1</pre></div></section></section><section class="sect2" id="ceph-rgw-fed-secondaryzone" data-id-title="Creating a Secondary Zone"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.10 </span><span class="title-name">Creating a Secondary Zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondaryzone">#</a></h3></div></div></div><p>
    In the same cluster, create and configure the secondary zone named
    <code class="literal">us-east-2</code>. You can execute all the following commands in
    the node hosting the master zone itself.
   </p><p>
    To create the secondary zone, use the same command as when you created the
    primary zone, except dropping the master flag:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</pre></div><section class="sect3" id="ceph-rgw-fed-secondzone-updateperiod" data-id-title="Update the Period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.10.1 </span><span class="title-name">Update the Period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-updateperiod">#</a></h4></div></div></div><p>
     Inform all the gateways of the new change in the system map by doing a
     period update and committing the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</pre></div></section><section class="sect3" id="ceph-rgw-fed-secondzone-startrgw" data-id-title="Start the Object Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.10.2 </span><span class="title-name">Start the Object Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-startrgw">#</a></h4></div></div></div><p>
     Adjust the configuration of the Object Gateway for the secondary zone, and start
     it:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>sudo systemctl start ceph-radosgw@rgw.us-east-2</pre></div></section></section><section class="sect2" id="ceph-rgw-fed-seccluster" data-id-title="Adding Object Gateway to the Second Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.11 </span><span class="title-name">Adding Object Gateway to the Second Cluster</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-seccluster">#</a></h3></div></div></div><p>
    The second Ceph cluster belongs to the same zonegroup as the initial one,
    but may be geographically located elsewhere.
   </p><section class="sect3" id="ceph-rgw-fed-seccluster-realm" data-id-title="Default Realm and Zonegroup"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.11.1 </span><span class="title-name">Default Realm and Zonegroup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-seccluster-realm">#</a></h4></div></div></div><p>
     Since you already created the realm for the first gateway, pull the realm
     here and make it the default here:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
<code class="prompt user">cephadm &gt; </code>radosgw-admin realm default --rgw-realm=gold</pre></div><p>
     Get the configuration from the master zone by pulling the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em></pre></div><p>
     Set the default zonegroup to the already created <code class="literal">us</code>
     zonegroup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zonegroup default --rgw-zonegroup=us</pre></div></section><section class="sect3" id="ceph-rgw-fed-seccluster-seczone" data-id-title="Secondary Zone Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.11.2 </span><span class="title-name">Secondary Zone Configuration</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-seccluster-seczone">#</a></h4></div></div></div><p>
     Create a new zone named <code class="literal">us-west</code> with the same system
     keys:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<em class="replaceable">$SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">$SYSTEM_SECRET_KEY</em> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</pre></div></section><section class="sect3" id="ceph-rgw-fed-seccluster-period" data-id-title="Update the Period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.11.3 </span><span class="title-name">Update the Period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-seccluster-period">#</a></h4></div></div></div><p>
     To propagate the zonegroup map changes, we update and commit the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</pre></div><p>
     Note that the period epoch number has incremented, indicating a change in
     the configuration.
    </p></section><section class="sect3" id="ceph-rgw-fed-seccluster-rgwstart" data-id-title="Start the Object Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.11.11.4 </span><span class="title-name">Start the Object Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-seccluster-rgwstart">#</a></h4></div></div></div><p>
     This is similar to starting the Object Gateway in the first zone. The only
     difference is that the Object Gateway zone configuration should reflect the
     <code class="literal">us-west</code> zone name:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</pre></div><p>
     Start the second Object Gateway:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-radosgw@rgw.us-west</pre></div></section></section><section class="sect2" id="ceph-rgw-fed-failover" data-id-title="Failover and Disaster Recovery"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.11.12 </span><span class="title-name">Failover and Disaster Recovery</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-failover">#</a></h3></div></div></div><p>
    If the master zone should fail, failover to the secondary zone for disaster
    recovery.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make the secondary zone the master and default zone. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --master --default</pre></div><p>
      By default, Ceph Object Gateway will run in an active-active
      configuration. If the cluster was configured to run in an active-passive
      configuration, the secondary zone is a read-only zone. Remove the
      --read-only status to allow the zone to receive write operations. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --master --default \
--read-only=False</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
      Finally, restart the Ceph Object Gateway.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> restart ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div><p>
    If the former master zone recovers, revert the operation.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      From the recovered zone, pull the period from the current master zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> period pull --url={url-to-master-zone-gateway} \
--access-key={access-key} --secret={secret}</pre></div></li><li class="step"><p>
      Make the recovered zone the master and default zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --master --default</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
      Then, restart the Ceph Object Gateway in the recovered zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> restart ceph-radosgw@rgw.`hostname -s`</pre></div></li><li class="step"><p>
      If the secondary zone needs to be a read-only configuration, update the
      secondary zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone={zone-name} --read-only</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
      Finally, restart the Ceph Object Gateway in the secondary zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> restart ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div></section></section><section class="sect1" id="ogw-haproxy" data-id-title="Load Balancing the Object Gateway Servers with HAProxy"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.12 </span><span class="title-name">Load Balancing the Object Gateway Servers with HAProxy</span> <a title="Permalink" class="permalink" href="#ogw-haproxy">#</a></h2></div></div></div><p>
   You can use the HAProxy load balancer to distribute all requests across
   multiple Object Gateway back-end servers. Refer to
   <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-lb-haproxy" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-lb-haproxy</a>
   for more details on configuring HAProxy.
  </p><p>
   Following is a simple configuration of HAProxy for balancing Object Gateway nodes
   using round robin as the balancing algorithm:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>cat /etc/haproxy/haproxy.cfg
[...]
frontend <em class="replaceable">https_frontend</em>
bind *:443 crt <em class="replaceable">path-to-cert.pem</em> [ciphers: ... ]
default_backend rgw

backend rgw
mode http
balance roundrobin
server rgw_server1 <em class="replaceable">rgw-endpoint1</em> weight 1 maxconn 100 check
server rgw_server2 <em class="replaceable">rgw-endpoint2</em> weight 1 maxconn 100 check
[...]</pre></div></section></section><section class="chapter" id="cha-ceph-iscsi" data-id-title="Ceph iSCSI Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14 </span><span class="title-name">Ceph iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-iscsi">#</a></h2></div></div></div><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span>.
  
 </p><section class="sect1" id="ceph-iscsi-connect" data-id-title="Connecting to lrbd-managed Targets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.1 </span><span class="title-name">Connecting to lrbd-managed Targets</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect">#</a></h2></div></div></div><p>
   This chapter describes how to connect to lrdb-managed targets from clients
   running Linux, Microsoft Windows, or VMware.
  </p><section class="sect2" id="ceph-iscsi-connect-linux" data-id-title="Linux (open-iscsi)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.1 </span><span class="title-name">Linux (<code class="systemitem">open-iscsi</code>)</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux">#</a></h3></div></div></div><p>
    Connecting to lrbd-backed iSCSI targets with
    <code class="systemitem">open-iscsi</code> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </p><p>
    Both steps require that the <code class="systemitem">open-iscsi</code> daemon is
    running. The way you start the <code class="systemitem">open-iscsi</code> daemon
    is dependent on your Linux distribution:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      On SUSE Linux Enterprise Server (SLES); and Red Hat Enterprise Linux (RHEL) hosts, run <code class="command">systemctl start
      iscsid</code> (or <code class="command">service iscsid start</code> if
      <code class="command">systemctl</code> is not available).
     </p></li><li class="listitem"><p>
      On Debian and Ubuntu hosts, run <code class="command">systemctl start
      open-iscsi</code> (or <code class="command">service open-iscsi start</code>).
     </p></li></ul></div><p>
    If your initiator host runs SUSE Linux Enterprise Server, refer to
    <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#sec-iscsi-initiator" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#sec-iscsi-initiator</a>
    for details on how to connect to an iSCSI target.
   </p><p>
    For any other Linux distribution supporting
    <code class="systemitem">open-iscsi</code>, proceed to discover targets on your
    <code class="systemitem">lrbd</code> gateway (this example uses iscsi1.example.com
    as the portal address; for multipath access repeat these steps with
    iscsi2.example.com):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</pre></div><p>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    Repeat this process for other portal IP addresses or hosts.
   </p><p>
    If your system has the <code class="systemitem">lsscsi</code> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </p><div class="verbatim-wrap"><pre class="screen">lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</pre></div><p>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <code class="systemitem">multipath</code> utility:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</pre></div><p>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div><p>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </p><p>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </p><section class="sect3" id="ceph-iscsi-connect-linux-multipath" data-id-title="Multipath Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">14.1.1.1 </span><span class="title-name">Multipath Configuration</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux-multipath">#</a></h4></div></div></div><p>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <code class="systemitem">lrbd</code> configuration. Select
     a strategy prior to using block storage. After editing the
     <code class="filename">/etc/multipath.conf</code>, restart
     <code class="systemitem">multipathd</code> with
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart multipathd</pre></div><p>
     For an active-passive configuration with friendly names, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. After connecting to your
     targets successfully, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</pre></div><p>
     Note the status of each link. For an active-active configuration, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. Restart
     <code class="systemitem">multipathd</code> and run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</pre></div></section></section><section class="sect2" id="ceph-iscsi-connect-win" data-id-title="Microsoft Windows (Microsoft iSCSI initiator)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.2 </span><span class="title-name">Microsoft Windows (Microsoft iSCSI initiator)</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-win">#</a></h3></div></div></div><p>
    To connect to a SUSE Enterprise Storage 5.5 iSCSI target from a Windows 2012 server, follow
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open Windows Server Manager. From the Dashboard, select
      <span class="guimenu">Tools</span> / <span class="guimenu">iSCSI
      Initiator</span>. The <span class="guimenu">iSCSI Initiator
      Properties</span> dialog appears. Select the
      <span class="guimenu">Discovery</span> tab:
     </p><div class="figure" id="id-1.3.5.3.4.4.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-initiator-props.png" target="_blank"><img src="images/iscsi-initiator-props.png" width="" alt="iSCSI Initiator Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.1: </span><span class="title-name">iSCSI Initiator Properties </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">Discover Target Portal</span> dialog, enter the
      target's host name or IP address in the <span class="guimenu">Target</span> field
      and click <span class="guimenu">OK</span>:
     </p><div class="figure" id="id-1.3.5.3.4.4.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip.png" target="_blank"><img src="images/iscsi-target-ip.png" width="" alt="Discover Target Portal"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.2: </span><span class="title-name">Discover Target Portal </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <span class="guimenu">Target Portals</span> list:
     </p><div class="figure" id="id-1.3.5.3.4.4.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip-list.png" target="_blank"><img src="images/iscsi-target-ip-list.png" width="" alt="Target Portals"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.3: </span><span class="title-name">Target Portals </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.3.2">#</a></h6></div></div></li><li class="step"><p>
      Next, switch to the <span class="guimenu">Targets</span> tab and review your
      discovered target(s).
     </p><div class="figure" id="id-1.3.5.3.4.4.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-targets.png" target="_blank"><img src="images/iscsi-targets.png" width="" alt="Targets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.4: </span><span class="title-name">Targets </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.4.2">#</a></h6></div></div></li><li class="step"><p>
      Click <span class="guimenu">Connect</span> in the <span class="guimenu">Targets</span> tab.
      The <span class="guimenu">Connect To Target</span> dialog appears. Select the
      <span class="guimenu">Enable Multi-path</span> check box to enable multipath I/O
      (MPIO), then click <span class="guimenu">OK</span>:
     </p></li><li class="step"><p>
      When the <span class="guimenu">Connect to Target</span> dialog closes, select
      <span class="guimenu">Properties</span> to review the target's properties:
     </p><div class="figure" id="id-1.3.5.3.4.4.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-properties.png" target="_blank"><img src="images/iscsi-target-properties.png" width="" alt="iSCSI Target Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.5: </span><span class="title-name">iSCSI Target Properties </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.6.2">#</a></h6></div></div></li><li class="step"><p>
      Select <span class="guimenu">Devices</span>, and click <span class="guimenu">MPIO</span> to
      review the multipath I/O configuration:
     </p><div class="figure" id="id-1.3.5.3.4.4.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-device-details.png" target="_blank"><img src="images/iscsi-device-details.png" width="" alt="Device Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.6: </span><span class="title-name">Device Details </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.3.7.2">#</a></h6></div></div><p>
      The default <span class="guimenu">Load Balance policy</span> is <span class="guimenu">Round
      Robin With Subset</span>. If you prefer a pure fail-over
      configuration, change it to <span class="guimenu">Fail Over Only</span>.
     </p></li></ol></div></div><p>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <span class="guimenu">OK</span> to close the <span class="guimenu">iSCSI
    Initiator Properties</span> dialog, and proceed with the<span class="guimenu"> File
    and Storage Services</span> role from the <span class="guimenu">Server
    Manager</span> dashboard.
   </p><p>
    Observe the newly connected volume. It identifies as <span class="emphasis"><em>SUSE RBD
    SCSI Multi-Path Drive</em></span> on the iSCSI bus, and is initially marked
    with an <span class="emphasis"><em>Offline</em></span> status and a partition table type of
    <span class="emphasis"><em>Unknown</em></span>. If the new volume does not appear
    immediately, select <span class="guimenu">Rescan Storage</span> from the
    <span class="guimenu">Tasks</span> drop-down box to rescan the iSCSI bus.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Right-click on the iSCSI volume and select <span class="guimenu">New Volume</span>
      from the context menu. The <span class="guimenu">New Volume Wizard</span> appears.
      Click <span class="guimenu">Next</span>, highlight the newly connected iSCSI volume
      and click <span class="guimenu">Next</span> to begin.
     </p><div class="figure" id="id-1.3.5.3.4.4.6.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-wizard.png" target="_blank"><img src="images/iscsi-volume-wizard.png" width="" alt="New Volume Wizard"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.7: </span><span class="title-name">New Volume Wizard </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.6.1.2">#</a></h6></div></div></li><li class="step"><p>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </p><div class="figure" id="id-1.3.5.3.4.4.6.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-win-prompt1.png" target="_blank"><img src="images/iscsi-win-prompt1.png" width="" alt="Offline Disk Prompt"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.8: </span><span class="title-name">Offline Disk Prompt </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.6.2.2">#</a></h6></div></div></li><li class="step"><p>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or directory name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <span class="guimenu">Create</span> to finish creating the volume:
     </p><div class="figure" id="id-1.3.5.3.4.4.6.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-confirm.png" target="_blank"><img src="images/iscsi-volume-confirm.png" width="" alt="Confirm Volume Selections"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.9: </span><span class="title-name">Confirm Volume Selections </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.4.6.3.2">#</a></h6></div></div><p>
      When the process finishes, review the results, then
      <span class="guimenu">Close</span> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-iscsi-connect-vmware" data-id-title="VMware"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.3 </span><span class="title-name">VMware</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-vmware">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To connect to <code class="systemitem">lrbd</code> managed iSCSI volumes you
      need a configured iSCSI software adapter. If no such adapter is available
      in your vSphere configuration, create one by selecting
      <span class="guimenu">Configuration</span> / <span class="guimenu">Storage
      Adapters</span> / <span class="guimenu">Add</span> / <span class="guimenu">iSCSI Software
      initiator</span>.
     </p></li><li class="step"><p>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <span class="guimenu">Properties</span> from the context
      menu:
     </p><div class="figure" id="id-1.3.5.3.4.5.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi_vmware_adapter_props.png" target="_blank"><img src="images/iscsi_vmware_adapter_props.png" width="" alt="iSCSI Initiator Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.10: </span><span class="title-name">iSCSI Initiator Properties </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">iSCSI Software Initiator</span> dialog, click the
      <span class="guimenu">Configure</span> button. Then go to the <span class="guimenu">Dynamic
      Discovery</span> tab and select <span class="guimenu">Add</span>.
     </p></li><li class="step"><p>
      Enter the IP address or host name of your <code class="systemitem">lrbd</code>
      iSCSI gateway. If you run multiple iSCSI gateways in a failover
      configuration, repeat this step for as many gateways as you operate.
     </p><div class="figure" id="id-1.3.5.3.4.5.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-target.png" target="_blank"><img src="images/iscsi-vmware-add-target.png" width="" alt="Add Target Server"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.11: </span><span class="title-name">Add Target Server </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.4.2">#</a></h6></div></div><p>
      When you have entered all iSCSI gateways, click <span class="guimenu">OK</span> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </p></li><li class="step"><p>
      When the rescan completes, the new iSCSI device appears below the
      <span class="guimenu">Storage Adapters</span> list in the
      <span class="guimenu">Details</span> pane. For multipath devices, you can now
      right-click on the adapter and select <span class="guimenu">Manage Paths</span>
      from the context menu:
     </p><div class="figure" id="id-1.3.5.3.4.5.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-multipath.png" target="_blank"><img src="images/iscsi-vmware-multipath.png" width="" alt="Manage Multipath Devices"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.12: </span><span class="title-name">Manage Multipath Devices </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.5.2">#</a></h6></div></div><p>
      You should now see all paths with a green light under
      <span class="guimenu">Status</span>. One of your paths should be marked
      <span class="guimenu">Active (I/O)</span> and all others simply
      <span class="guimenu">Active</span>:
     </p><div class="figure" id="id-1.3.5.3.4.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-paths.png" target="_blank"><img src="images/iscsi-vmware-paths.png" width="" alt="Paths Listing for Multipath"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.13: </span><span class="title-name">Paths Listing for Multipath </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.5.4">#</a></h6></div></div></li><li class="step"><p>
      You can now switch from <span class="guimenu">Storage Adapters</span> to the item
      labeled <span class="guimenu">Storage</span>. Select <span class="guimenu">Add
      Storage...</span> in the top-right corner of the pane to bring up the
      <span class="guimenu">Add Storage</span> dialog. Then, select
      <span class="guimenu">Disk/LUN</span> and click <span class="guimenu">Next</span>. The newly
      added iSCSI device appears in the <span class="guimenu">Select Disk/LUN</span>
      list. Select it, then click <span class="guimenu">Next</span> to proceed:
     </p><div class="figure" id="id-1.3.5.3.4.5.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-storage-dialog.png" target="_blank"><img src="images/iscsi-vmware-add-storage-dialog.png" width="" alt="Add Storage Dialog"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.14: </span><span class="title-name">Add Storage Dialog </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.6.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Next</span> to accept the default disk layout.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Properties</span> pane, assign a name to the new
      datastore, and click <span class="guimenu">Next</span>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <span class="guimenu">Custom Space Setting</span> for a smaller datastore:
     </p><div class="figure" id="id-1.3.5.3.4.5.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-custom-datastore.png" target="_blank"><img src="images/iscsi-vmware-custom-datastore.png" width="" alt="Custom Space Setting"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.15: </span><span class="title-name">Custom Space Setting </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.7.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Finish</span> to complete the datastore creation.
     </p><p>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the
      <code class="systemitem">lrbd</code>-backed iSCSI volume like any other vSphere
      datastore.
     </p><div class="figure" id="id-1.3.5.3.4.5.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-overview.png" target="_blank"><img src="images/iscsi-vmware-overview.png" width="" alt="iSCSI Datastore Overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 14.16: </span><span class="title-name">iSCSI Datastore Overview </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.4.5.3.7.5">#</a></h6></div></div></li></ol></div></div></section></section><section class="sect1" id="ceph-iscsi-conclude" data-id-title="Conclusion"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.2 </span><span class="title-name">Conclusion</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-conclude">#</a></h2></div></div></div><p>
   <code class="systemitem">lrbd</code> is a key component of SUSE Enterprise Storage 5.5 that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using
   <code class="systemitem">lrbd</code> on one or more iSCSI gateway hosts, Ceph RBD
   images become available as Logical Units (LUs) associated with iSCSI
   targets, which can be accessed in an optionally load-balanced, highly
   available fashion.
  </p><p>
   Since all of <code class="systemitem">lrbd</code>'s configuration is stored in the
   Ceph RADOS object store, <code class="systemitem">lrbd</code> gateway hosts are
   inherently without persistent state and thus can be replaced, augmented, or
   reduced at will. As a result, SUSE Enterprise Storage 5.5 enables SUSE customers to run a
   truly distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open source
   platform.
  </p></section></section><section class="chapter" id="cha-ceph-cephfs" data-id-title="Clustered File System"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15 </span><span class="title-name">Clustered File System</span> <a title="Permalink" class="permalink" href="#cha-ceph-cephfs">#</a></h2></div></div></div><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
 </p><section class="sect1" id="ceph-cephfs-cephfs-mount" data-id-title="Mounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.1 </span><span class="title-name">Mounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-mount">#</a></h2></div></div></div><p>
   When the file system is created and the MDS is active, you are ready to
   mount the file system from a client host.
  </p><section class="sect2" id="cephfs-client-preparation" data-id-title="Client Preparation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.1 </span><span class="title-name">Client Preparation</span> <a title="Permalink" class="permalink" href="#cephfs-client-preparation">#</a></h3></div></div></div><p>
    If the client host is running SUSE Linux Enterprise 12 SP2 or SP3, you can skip this
    section as the system is ready to mount CephFS 'out of the box'.
   </p><p>
    If the client host is running SUSE Linux Enterprise 12 SP1, you need to apply all the
    latest patches before mounting CephFS.
   </p><p>
    In any case, everything needed to mount CephFS is included in SUSE Linux Enterprise. The
    SUSE Enterprise Storage 5.5 product is not needed.
   </p><p>
    To support the full <code class="command">mount</code> syntax, the
    <span class="package">ceph-common</span> package (which is shipped with SUSE Linux Enterprise) should
    be installed before trying to mount CephFS.
   </p></section><section class="sect2" id="Creating-Secret-File" data-id-title="Create a Secret File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.2 </span><span class="title-name">Create a Secret File</span> <a title="Permalink" class="permalink" href="#Creating-Secret-File">#</a></h3></div></div></div><p>
    The Ceph cluster runs with authentication turned on by default. You
    should create a file that stores your secret key (not the keyring itself).
    To obtain the secret key for a particular user and then create the file, do
    the following:
   </p><div class="procedure" id="id-1.3.5.4.4.4.3" data-id-title="Creating a Secret Key"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 15.1: </span><span class="title-name">Creating a Secret Key </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.4.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      View the key for the particular user in a keyring file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>cat /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
      Copy the key of the user who will be using the mounted Ceph FS file
      system. Usually, the key looks similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen">AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</pre></div></li><li class="step"><p>
      Create a file with the user name as a file name part, for example
      <code class="filename">/etc/ceph/admin.secret</code> for the user
      <span class="emphasis"><em>admin</em></span>.
     </p></li><li class="step"><p>
      Paste the key value to the file created in the previous step.
     </p></li><li class="step"><p>
      Set proper access rights to the file. The user should be the only one who
      can read the file—others may not have any access rights.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-cephfs-krnldrv" data-id-title="Mount CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.3 </span><span class="title-name">Mount CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-krnldrv">#</a></h3></div></div></div><p>
    You can mount CephFS with the <code class="command">mount</code> command. You need
    to specify the monitor host name or IP address. Because the
    <code class="systemitem">cephx</code> authentication is enabled by default in
    SUSE Enterprise Storage 5.5, you need to specify a user name and their related secret as
    well:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><p>
    As the previous command remains in the shell history, a more secure
    approach is to read the secret from a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    Note that the secret file should only contain the actual keyring secret. In
    our example, the file will then contain only the following line:
   </p><div class="verbatim-wrap"><pre class="screen">AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><div id="id-1.3.5.4.4.5.8" data-id-title="Specify Multiple Monitors" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Specify Multiple Monitors</h6><p>
     It is a good idea to specify multiple monitors separated by commas on the
     <code class="command">mount</code> command line in case one monitor happens to be
     down at the time of mount. Each monitor address takes the form
     <code class="literal">host[:port]</code>. If the port is not specified, it defaults
     to 6789.
    </p></div><p>
    Create the mount point on the local host:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /mnt/cephfs</pre></div><p>
    Mount the CephFS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    A subdirectory <code class="filename">subdir</code> may be specified if a subset of
    the file system is to be mounted:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    You can specify more than one monitor host in the <code class="command">mount</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><div id="id-1.3.5.4.4.5.17" data-id-title="Read Access to the Root Directory" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Read Access to the Root Directory</h6><p>
     If clients with path restriction are used, the MDS capabilities need to
     include read access to the root directory. For example, a keyring may look
     as follows:
    </p><div class="verbatim-wrap"><pre class="screen">client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</pre></div><p>
     The <code class="literal">allow r path=/</code> part means that path-restricted
     clients are able to see the root volume, but cannot write to it. This may
     be an issue for use cases where complete isolation is a requirement.
    </p></div></section></section><section class="sect1" id="ceph-cephfs-cephfs-unmount" data-id-title="Unmounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.2 </span><span class="title-name">Unmounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-unmount">#</a></h2></div></div></div><p>
   To unmount the CephFS, use the <code class="command">umount</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>umount /mnt/cephfs</pre></div></section><section class="sect1" id="ceph-cephfs-cephfs-fstab" data-id-title="CephFS in /etc/fstab"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-fstab">#</a></h2></div></div></div><p>
   To mount CephFS automatically upon client start-up, insert the
   corresponding line in its file systems table
   <code class="filename">/etc/fstab</code>:
  </p><div class="verbatim-wrap"><pre class="screen">mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</pre></div></section><section class="sect1" id="ceph-cephfs-activeactive" data-id-title="Multiple Active MDS Daemons (Active-Active MDS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-activeactive">#</a></h2></div></div></div><p>
   CephFS is configured for a single active MDS daemon by default. To scale
   metadata performance for large-scale systems, you can enable multiple active
   MDS daemons, which will share the metadata workload with one another.
  </p><section class="sect2" id="id-1.3.5.4.7.3" data-id-title="When to Use Active-Active MDS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.4.1 </span><span class="title-name">When to Use Active-Active MDS</span> <a title="Permalink" class="permalink" href="#id-1.3.5.4.7.3">#</a></h3></div></div></div><p>
    Consider using multiple active MDS daemons when your metadata performance
    is bottlenecked on the default single MDS.
   </p><p>
    Adding more daemons does not increase performance on all workload types.
    For example, a single application running on a single client will not
    benefit from an increased number of MDS daemons unless the application is
    doing a lot of metadata operations in parallel.
   </p><p>
    Workloads that typically benefit from a larger number of active MDS daemons
    are those with many clients, perhaps working on many separate directories.
   </p></section><section class="sect2" id="cephfs-activeactive-increase" data-id-title="Increasing the MDS Active Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.4.2 </span><span class="title-name">Increasing the MDS Active Cluster Size</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-increase">#</a></h3></div></div></div><p>
    Each CephFS file system has a <code class="option">max_mds</code> setting, which
    controls how many ranks will be created. The actual number of ranks in the
    file system will only be increased if a spare daemon is available to take
    on the new rank. For example, if there is only one MDS daemon running and
    <code class="option">max_mds</code> is set to two, no second rank will be created.
   </p><p>
    In the following example, we set the <code class="option">max_mds</code> option to 2
    to create a new rank apart from the default one. To see the changes, run
    <code class="command">ceph status</code> before and after you set
    <code class="option">max_mds</code>, and watch the line containing
    <code class="literal">fsmap</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> mds set max_mds 2
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</pre></div><p>
    The newly created rank (1) passes through the 'creating' state and then
    enter its 'active' state.
   </p><div id="id-1.3.5.4.7.4.6" data-id-title="Standby Daemons" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Standby Daemons</h6><p>
     Even with multiple active MDS daemons, a highly available system still
     requires standby daemons to take over if any of the servers running an
     active daemon fail.
    </p><p>
     Consequently, the practical maximum of <code class="option">max_mds</code> for highly
     available systems is one less than the total number of MDS servers in your
     system. To remain available in the event of multiple server failures,
     increase the number of standby daemons in the system to match the number
     of server failures you need to survive.
    </p></div></section><section class="sect2" id="cephfs-activeactive-decrease" data-id-title="Decreasing the Number of Ranks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.4.3 </span><span class="title-name">Decreasing the Number of Ranks</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-decrease">#</a></h3></div></div></div><p>
    All ranks—including the ranks to be removed—must first be
    active. This means that you need to have at least <code class="option">max_mds</code>
    MDS daemons available.
   </p><p>
    First, set <code class="option">max_mds</code> to a lower number. For example, go back
    to having a single active MDS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> mds set max_mds 1
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</pre></div><p>
    Note that we still have two active MDSs. The ranks still exist even though
    we have decreased <code class="option">max_mds</code>, because
    <code class="option">max_mds</code> only restricts the creation of new ranks.
   </p><p>
    Next, use the <code class="command">ceph mds deactivate
    <em class="replaceable">rank</em></code> command to remove the unneeded
    rank:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</pre></div><p>
    The deactivated rank will first enter the stopping state, for a period of
    time while it hands off its share of the metadata to the remaining active
    daemons. This phase can take from seconds to minutes. If the MDS appears to
    be stuck in the stopping state then that should be investigated as a
    possible bug.
   </p><p>
    If an MDS daemon crashes or is terminated while in the 'stopping' state, a
    standby will take over and the rank will go back to 'active'. You can try
    to deactivate it again when it has come back up.
   </p><p>
    When a daemon finishes stopping, it will start again and go back to being a
    standby.
   </p></section><section class="sect2" id="cephfs-activeactive-pinning" data-id-title="Manually Pinning Directory Trees to a Rank"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.4.4 </span><span class="title-name">Manually Pinning Directory Trees to a Rank</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-pinning">#</a></h3></div></div></div><p>
    In multiple active metadata server configurations, a balancer runs, which
    works to spread metadata load evenly across the cluster. This usually works
    well enough for most users, but sometimes it is desirable to override the
    dynamic balancer with explicit mappings of metadata to particular ranks.
    This can allow the administrator or users to evenly spread application load
    or limit impact of users' metadata requests on the entire cluster.
   </p><p>
    The mechanism provided for this purpose is called an 'export pin'. It is an
    extended attribute of directories. The name of this extended attribute is
    <code class="literal">ceph.dir.pin</code>. Users can set this attribute using
    standard commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>setfattr -n ceph.dir.pin -v 2 <em class="replaceable">/path/to/dir</em></pre></div><p>
    The value (<code class="option">-v</code>) of the extended attribute is the rank to
    assign the directory sub-tree to. A default value of -1 indicates that the
    directory is not pinned.
   </p><p>
    A directory export pin is inherited from its closest parent with a set
    export pin. Therefore, setting the export pin on a directory affects all of
    its children. However, the parent's pin can be overridden by setting the
    child directory export pin. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</pre></div></section></section><section class="sect1" id="ceph-cephfs-failover" data-id-title="Managing Failover"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.5 </span><span class="title-name">Managing Failover</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover">#</a></h2></div></div></div><p>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <code class="option">mds_beacon_grace</code> seconds (default 15 seconds) before
   marking the daemon as <span class="emphasis"><em>laggy</em></span>. You can configure one or
   more 'standby' daemons that will take over during the MDS daemon failover.
  </p><section class="sect2" id="ceph-cephfs-failover-standby" data-id-title="Configuring Standby Daemons"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.5.1 </span><span class="title-name">Configuring Standby Daemons</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover-standby">#</a></h3></div></div></div><p>
    There are several configuration settings that control how a daemon will
    behave while in standby. You can specify them in the
    <code class="filename">ceph.conf</code> on the host where the MDS daemon runs. The
    daemon loads these settings when it starts, and sends them to the monitor.
   </p><p>
    By default, if none of these settings are used, all MDS daemons which do
    not hold a rank will be used as 'standbys' for any rank.
   </p><p>
    The settings which associate a standby daemon with a particular name or
    rank do not guarantee that the daemon will only be used for that rank. They
    mean that when several standbys are available, the associated standby
    daemon will be used. If a rank is failed, and a standby is available, it
    will be used even if it is associated with a different rank or named
    daemon.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.8.3.5.1"><span class="term">mds_standby_replay</span></dt><dd><p>
       If set to true, then the standby daemon will continuously read the
       metadata journal of an up rank. This will give it a warm metadata cache,
       and speed up the process of failing over if the daemon serving the rank
       fails.
      </p><p>
       An up rank may only have one standby replay, daemon assigned to it. If
       two daemons are both set to be standby replay then one of them will
       arbitrarily win, and the other will become a normal non-replay standby.
      </p><p>
       When a daemon has entered the standby replay state, it will only be used
       as a standby for the rank that it is following. If another rank fails,
       this standby replay daemon will not be used as a replacement, even if no
       other standbys are available.
      </p></dd><dt id="id-1.3.5.4.8.3.5.2"><span class="term">mds_standby_for_name</span></dt><dd><p>
       Set this to make the standby daemon only take over a failed rank if the
       last daemon to hold it matches this name.
      </p></dd><dt id="id-1.3.5.4.8.3.5.3"><span class="term">mds_standby_for_rank</span></dt><dd><p>
       Set this to make the standby daemon only take over the specified rank.
       If another rank fails, this daemon will not be used to replace it.
      </p><p>
       Use in conjunction with<code class="option">mds_standby_for_fscid</code> to be
       specific about which file system's rank you are targeting in case of
       multiple file systems.
      </p></dd><dt id="id-1.3.5.4.8.3.5.4"><span class="term">mds_standby_for_fscid</span></dt><dd><p>
       If <code class="option">mds_standby_for_rank</code> is set, this is simply a
       qualifier to say which file system's rank is being referred to.
      </p><p>
       If <code class="option">mds_standby_for_rank</code> is not set, then setting FSCID
       will cause this daemon to target any rank in the specified FSCID. Use
       this if you have a daemon that you want to use for any rank, but only
       within a particular file system.
      </p></dd><dt id="id-1.3.5.4.8.3.5.5"><span class="term">mon_force_standby_active</span></dt><dd><p>
       This setting is used on monitor hosts. It defaults to true.
      </p><p>
       If it is false, then daemons configured with
       <code class="option">standby_replay=true</code> will only become active if the
       rank/name that they have been configured to follow fails. On the other
       hand, if this setting is true, then a daemon configured with
       <code class="option">standby_replay=true</code> may be assigned some other rank.
      </p></dd></dl></div></section><section class="sect2" id="ceph-cephfs-failover-examples" data-id-title="Examples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.5.2 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover-examples">#</a></h3></div></div></div><p>
    Several example <code class="filename">ceph.conf</code> configurations follow. You
    can either copy a <code class="filename">ceph.conf</code> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains that server's daemon configuration.
   </p><section class="sect3" id="id-1.3.5.4.8.4.3" data-id-title="Simple Pair"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.5.2.1 </span><span class="title-name">Simple Pair</span> <a title="Permalink" class="permalink" href="#id-1.3.5.4.8.4.3">#</a></h4></div></div></div><p>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </p><div class="verbatim-wrap"><pre class="screen">[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-nfsganesha" data-id-title="NFS Ganesha: Export Ceph Data via NFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span> <a title="Permalink" class="permalink" href="#cha-ceph-nfsganesha">#</a></h2></div></div></div><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-nfs" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p><p>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/<em class="replaceable">USERNAME</em>/<em class="replaceable">BUCKETNAME</em></code>.
 </p><p>
  A CephFS is exported by default via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/cephfs</code>.
 </p><div id="id-1.3.5.5.6" data-id-title="NFS Ganesha Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha Performance</h6><p>
   Due to increased protocol overhead and additional latency caused by extra
   network hops between the client and the storage, accessing Ceph via an NFS
   Gateway may significantly reduce application performance when compared to
   native CephFS or Object Gateway clients.
  </p></div><section class="sect1" id="ceph-nfsganesha-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-install">#</a></h2></div></div></div><p>
   For installation instructions, see <span class="intraxref">Book “Deployment Guide”, Chapter 12 “Installation of NFS Ganesha”</span>.
  </p></section><section class="sect1" id="ceph-nfsganesha-config" data-id-title="Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config">#</a></h2></div></div></div><p>
   For a list of all parameters available within the configuration file, see:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">man ganesha-config</code>
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-ceph-config</code> for CephFS File System
     Abstraction Layer (FSAL) options.
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-rgw-config</code> for Object Gateway FSAL options.
    </p></li></ul></div><p>
   This section includes information to help you configure the NFS Ganesha server
   to export the cluster data accessible via Object Gateway and CephFS.
  </p><p>
   NFS Ganesha configuration is controlled by
   <code class="filename">/etc/ganesha/ganesha.conf</code>. Note that changes to this
   file are overwritten when DeepSea Stage 4 is executed. To persistently
   change the settings, edit the file
   <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> located on
   the Salt master.
  </p><section class="sect2" id="ceph-nfsganesha-config-general" data-id-title="Export Section"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.1 </span><span class="title-name">Export Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-general">#</a></h3></div></div></div><p>
    This section describes how to configure the <code class="literal">EXPORT</code>
    sections in the <code class="filename">ganesha.conf</code>.
   </p><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><section class="sect3" id="ceph-nfsganesha-config-general-export" data-id-title="Export Main Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">16.2.1.1 </span><span class="title-name">Export Main Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-general-export">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.6.4.2.1"><span class="term">Export_Id</span></dt><dd><p>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.2"><span class="term">Path</span></dt><dd><p>
        Export path in the related CephFS pool (mandatory). This allows
        subdirectories to be exported from the CephFS.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.3"><span class="term">Pseudo</span></dt><dd><p>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </p><p>
        Example: with the value <code class="literal">/cephfs/</code> and after executing
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount <em class="replaceable">GANESHA_IP</em>:/cephfs/ /mnt/</pre></div><p>
        The CephFS data is available in the directory
        <code class="filename">/mnt/cephfs/</code> on the client.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.4"><span class="term">Access_Type</span></dt><dd><p>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </p><div id="id-1.3.5.5.8.6.4.2.4.2.2" data-id-title="Limit Access to Clients" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Limit Access to Clients</h6><p>
         If you leave <code class="literal">Access_Type = RW</code> in the main
         <code class="literal">EXPORT</code> section and limit access to a specific
         client in the <code class="literal">CLIENT</code> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <code class="literal">Access_Type =
         None</code> in the <code class="literal">EXPORT</code> section and then
         specify less restrictive access mode for one or more clients in the
         <code class="literal">CLIENT</code> section:
        </p><div class="verbatim-wrap"><pre class="screen">EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}</pre></div></div></dd><dt id="id-1.3.5.5.8.6.4.2.5"><span class="term">Squash</span></dt><dd><p>
        NFS squash option.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.6"><span class="term">FSAL</span></dt><dd><p>
        Exporting 'File System Abstraction Layer'. See
        <a class="xref" href="#ceph-nfsganesha-config-general-fsal" title="16.2.1.2. FSAL Subsection">Section 16.2.1.2, “FSAL Subsection”</a>.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-general-fsal" data-id-title="FSAL Subsection"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">16.2.1.2 </span><span class="title-name">FSAL Subsection</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-general-fsal">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.6.5.3.1"><span class="term">Name</span></dt><dd><p>
        Defines which back-end NFS Ganesha uses. Allowed values are
        <code class="literal">CEPH</code> for CephFS or <code class="literal">RGW</code> for
        Object Gateway. Depending on the choice, a <code class="literal">role-mds</code> or
        <code class="literal">role-rgw</code> must be defined in the
        <code class="filename">policy.cfg</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="ceph-nfsganesha-config-rgw" data-id-title="RGW Section"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.2 </span><span class="title-name">RGW Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-rgw">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.7.3.1"><span class="term">ceph_conf</span></dt><dd><p>
       Points to the <code class="filename">ceph.conf</code> file. When deploying with
       DeepSea, it is not necessary to change this value.
      </p></dd><dt id="id-1.3.5.5.8.7.3.2"><span class="term">name</span></dt><dd><p>
       The name of the Ceph client user used by NFS Ganesha.
      </p></dd><dt id="id-1.3.5.5.8.7.3.3"><span class="term">cluster</span></dt><dd><p>
       Name of the Ceph cluster. SUSE Enterprise Storage 5.5 currently only supports one
       cluster name, which is <code class="literal">ceph</code> by default.
      </p></dd></dl></div></section><section class="sect2" id="ganesha-nfsport" data-id-title="Changing Default NFS Ganesha Ports"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.3 </span><span class="title-name">Changing Default NFS Ganesha Ports</span> <a title="Permalink" class="permalink" href="#ganesha-nfsport">#</a></h3></div></div></div><p>
    NFS Ganesha uses the port 2049 for NFS and 875 for the rquota support by
    default. To change the default port numbers, use the
    <code class="option">NFS_Port</code> and <code class="option">RQUOTA_Port</code> options inside
    the <code class="literal">NFS_CORE_PARAM</code> section, for example:
   </p><div class="verbatim-wrap"><pre class="screen">NFS_CORE_PARAM
{
 NFS_Port = 2060;
 RQUOTA_Port = 876;
}</pre></div></section></section><section class="sect1" id="ceph-nfsganesha-customrole" data-id-title="Custom NFS Ganesha Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.3 </span><span class="title-name">Custom NFS Ganesha Roles</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole">#</a></h2></div></div></div><p>
   Custom NFS Ganesha roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <code class="filename">policy.cfg</code>. The roles
   allow for:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Separated NFS Ganesha nodes for accessing Object Gateway and CephFS.
    </p></li><li class="listitem"><p>
     Assigning different Object Gateway users to NFS Ganesha nodes.
    </p></li></ul></div><p>
   Having different Object Gateway users enables NFS Ganesha nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with Ceph buckets used in the CRUSH Map.
  </p><section class="sect2" id="ceph-nfsganesha-customrole-rgw-multiusers" data-id-title="Different Object Gateway Users for NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.1 </span><span class="title-name">Different Object Gateway Users for NFS Ganesha</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole-rgw-multiusers">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create two
    NFS Ganesha roles with different Object Gateway users. In this example, the roles
    <code class="literal">gold</code> and <code class="literal">silver</code> are used, for which
    DeepSea already provides example configuration files.
   </p><div class="procedure" id="proc-ceph-nfsganesha-rgw-multiusers"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> with
      the editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/gold.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</pre></div><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/silver.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</pre></div></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 silver.conf.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 gold.conf.j2</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 silver.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 gold.j2</pre></div></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/rgw/files/
<code class="prompt user">root # </code><code class="command">cp</code> rgw.j2 silver.j2
<code class="prompt user">root # </code><code class="command">cp</code> rgw.j2 gold.j2</pre></div></li><li class="step"><p>
      Object Gateway also needs the configuration for the different roles:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/configuration/files/
<code class="prompt user">root # </code><code class="command">cp</code> ceph.conf.rgw silver.conf
<code class="prompt user">root # </code><code class="command">cp</code> ceph.conf.rgw gold.conf</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-silver/cluster/<em class="replaceable">NODE1</em>.sls
role-gold/cluster/<em class="replaceable">NODE2</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-nfsganesha-customrole-rgw-cephfs" data-id-title="Separating CephFS and Object Gateway FSAL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.2 </span><span class="title-name">Separating CephFS and Object Gateway FSAL</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole-rgw-cephfs">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create 2 new
    different roles that use CephFS and Object Gateway:
   </p><div class="procedure" id="proc-ceph-nfsganesha-customrole"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/rgw.sls</code> with the
      editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_rgw.conf.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_cfs.conf.j2</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_rgw.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_cfs.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 ganesha_rgw.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 ganesha_cfs.j2</pre></div><p>
      The line <code class="literal">caps mds = "allow *"</code> can be removed from the
      <code class="filename">ganesha_rgw.j2</code>.
     </p></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</pre></div></li><li class="step"><p>
      Object Gateway needs the configuration for the new role:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-ganesha_rgw/cluster/<em class="replaceable">NODE1</em>.sls
role-ganesha_cfs/cluster/<em class="replaceable">NODE1</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ganesha-rgw-supported-operations" data-id-title="Supported Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.3 </span><span class="title-name">Supported Operations</span> <a title="Permalink" class="permalink" href="#ganesha-rgw-supported-operations">#</a></h3></div></div></div><p>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Links including symbolic links are not supported.</em></span>
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>NFS access control lists (ACLs) are not supported.</em></span>
      Unix user and group ownership and permissions <span class="emphasis"><em>are</em></span>
      supported.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Directories may not be moved or renamed.</em></span> You
      <span class="emphasis"><em>may</em></span> move files between directories.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Only full, sequential write I/O is supported.</em></span>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example some versions of GNU
      <code class="command">tar</code>), but may fail due to infrequent non-sequential
      stores. When mounting via NFS, application's sequential I/O can generally
      be forced to sequential writes to the NFS server via synchronous mounting
      (the <code class="option">-o sync</code> option). NFS clients that cannot mount
      synchronously (for example Microsoft Windows*) will not be able to upload
      files.
     </p></li><li class="listitem"><p>
      NFS RGW supports read-write operations only for block size smaller than
      4MB.
     </p></li></ul></div></section></section><section class="sect1" id="ceph-nfsganesha-services" data-id-title="Starting or Restarting NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-services">#</a></h2></div></div></div><p>
   To enable and start the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> enable nfs-ganesha
<code class="prompt user">root # </code><code class="command">systemctl</code> start nfs-ganesha</pre></div><p>
   Restart NFS Ganesha with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> restart nfs-ganesha</pre></div><p>
   When NFS Ganesha is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </p></section><section class="sect1" id="ceph-nfsganesha-loglevel" data-id-title="Setting the Log Level"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.5 </span><span class="title-name">Setting the Log Level</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-loglevel">#</a></h2></div></div></div><p>
   You change the default debug level <code class="literal">NIV_EVENT</code> by editing
   the file <code class="filename">/etc/sysconfig/nfs-ganesha</code>. Replace
   <code class="literal">NIV_EVENT</code> with <code class="literal">NIV_DEBUG</code> or
   <code class="literal">NIV_FULL_DEBUG</code>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</pre></div><p>
   A restart of the service is required when changing the log level.
  </p><div id="id-1.3.5.5.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>NFS Ganesha uses Ceph client libraries to connect to the Ceph
      cluster. By default, client libraries do not log errors or any other
      output. To see more details about NFS Ganesha interacting with the
      Ceph cluster (for example, connection issues details) logging needs
      to be explicitly defined in the <code class="filename">ceph.conf</code> configuration file
      under the <code class="literal">[client]</code> section. For example:</p><div class="verbatim-wrap"><pre class="screen">[client]
	log_file = "/var/log/ceph/ceph-client.log"</pre></div></div></section><section class="sect1" id="ceph-nfsganesha-verify" data-id-title="Verifying the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.6 </span><span class="title-name">Verifying the Exported NFS Share</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-verify">#</a></h2></div></div></div><p>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   NFS Ganesha server node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">showmount</code> -e
/ (everything)</pre></div></section><section class="sect1" id="ceph-nfsganesha-mount" data-id-title="Mounting the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.7 </span><span class="title-name">Mounting the Exported NFS Share</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-mount">#</a></h2></div></div></div><p>
   To mount the exported NFS share (as configured in
   <a class="xref" href="#ceph-nfsganesha-config" title="16.2. Configuration">Section 16.2, “Configuration”</a>) on a client host, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -t nfs -o rw,noatime,sync \
 <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em></pre></div></section><section class="sect1" id="ceph-nfsganesha-more" data-id-title="Additional Resources"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.8 </span><span class="title-name">Additional Resources</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-more">#</a></h2></div></div></div><p>
   The original NFS Ganesha documentation can be found at
   <a class="link" href="https://github.com/nfs-ganesha/nfs-ganesha/wiki/Docs" target="_blank">https://github.com/nfs-ganesha/nfs-ganesha/wiki/Docs</a>.
  </p></section></section></div><div class="part" id="part-gui" data-id-title="Managing Cluster with GUI Tools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part IV </span><span class="title-name">Managing Cluster with GUI Tools </span><a title="Permalink" class="permalink" href="#part-gui">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#ceph-oa"><span class="title-number">17 </span><span class="title-name">openATTIC</span></a></span></li><dd class="toc-abstract"><p>
   Calamari used to be the preferred Web UI application for managing and
   monitoring the Ceph cluster. Since SUSE Enterprise Storage 5.5, Calamari has been removed
   in favor of the more advanced openATTIC.
  </p></dd></ul></div><section class="chapter" id="ceph-oa" data-id-title="openATTIC"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17 </span><span class="title-name">openATTIC</span> <a title="Permalink" class="permalink" href="#ceph-oa">#</a></h2></div></div></div><div id="id-1.3.6.2.3" data-id-title="Calamari Removed" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Calamari Removed</h6><p>
   Calamari used to be the preferred Web UI application for managing and
   monitoring the Ceph cluster. Since SUSE Enterprise Storage 5.5, Calamari has been removed
   in favor of the more advanced openATTIC.
  </p></div><p>
  openATTIC is a central storage management system which supports Ceph storage
  cluster. With openATTIC, you can control everything from a central management
  interface. It is no longer necessary to be familiar with the inner workings
  of the Ceph storage tools. Cluster management tasks can be carried out
  either by using openATTIC's intuitive Web interface, or via its REST API.
 </p><section class="sect1" id="ceph-oa-installation" data-id-title="openATTIC Deployment and Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.1 </span><span class="title-name">openATTIC Deployment and Configuration</span> <a title="Permalink" class="permalink" href="#ceph-oa-installation">#</a></h2></div></div></div><p>
   This section introduces steps to deploy and configure openATTIC and its supported
   features so that you can administer your Ceph cluster using a
   user-friendly Web interface.
  </p><section class="sect2" id="oa-ssl" data-id-title="Enabling Secure Access to openATTIC using SSL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.1 </span><span class="title-name">Enabling Secure Access to openATTIC using SSL</span> <a title="Permalink" class="permalink" href="#oa-ssl">#</a></h3></div></div></div><p>
    Access to the openATTIC Web application uses non-secure HTTP protocol by
    default. To enable secure access to openATTIC, you need to configure the Apache
    Web server manually:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you do not have an SSL certificate signed by a well known certificate
      authority (CA), create a self-signed SSL certificate and copy its files
      to the directory where the Web server expects it, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 \
 -keyout key.pem -out cert.pem
<code class="prompt user">root # </code>cp cert.pem /etc/ssl/certs/servercert.pem
<code class="prompt user">root # </code>cp key.pem /etc/ssl/certs/serverkey.pem</pre></div><p>
      Refer to
      <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-apache2-ssl" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-apache2-ssl</a>
      for more details on creating SSL certificates.
     </p></li><li class="step"><p>
      Add 'SSL' to the <code class="option">APACHE_SERVER_FLAGS</code> option in the
      <code class="filename">/etc/sysconfig/apache2</code> configuration file. You can
      do it manually, or run the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>a2enmod ssl
<code class="prompt user">root # </code>a2enflag SSL</pre></div></li><li class="step"><p>
      Create <code class="filename">/etc/apache2/vhosts.d/vhost-ssl.conf</code> for a
      new Apache virtual host with the following content:
     </p><div class="verbatim-wrap"><pre class="screen">&lt;IfDefine SSL&gt;
&lt;IfDefine !NOSSL&gt;
&lt;VirtualHost *:80&gt;
 ServerName <em class="replaceable">OA_HOST_NAME</em>
 Redirect "/" "https://<em class="replaceable">OA_HOST_NAME</em>/"
&lt;/VirtualHost&gt;
&lt;VirtualHost _default_:443&gt;
 ServerName <em class="replaceable">OA_HOST_NAME</em>
 DocumentRoot "/srv/www/htdocs"
 ErrorLog /var/log/apache2/error_log
 TransferLog /var/log/apache2/access_log
 SSLEngine on
 SSLCertificateFile /etc/ssl/certs/servercert.pem
 SSLCertificateKeyFile /etc/ssl/certs/serverkey.pem
 CustomLog /var/log/apache2/ssl_request_log ssl_combined
&lt;/VirtualHost&gt;
&lt;/IfDefine&gt;
&lt;/IfDefine&gt;</pre></div></li><li class="step"><p>
      Restart the Web server to reload the new virtual host definition together
      with the certificate files:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart apache2.service</pre></div></li></ol></div></div></section><section class="sect2" id="oa-deploy" data-id-title="Deploying openATTIC"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.2 </span><span class="title-name">Deploying openATTIC</span> <a title="Permalink" class="permalink" href="#oa-deploy">#</a></h3></div></div></div><p>
    Since SUSE Enterprise Storage 5.5, openATTIC has been deployed as a DeepSea role. Refer to
    <a class="xref" href="#storage-salt-cluster" title="Chapter 1. Salt Cluster Administration">Chapter 1, <em>Salt Cluster Administration</em></a> for a general procedure.
   </p></section><section class="sect2" id="ceph-oa-install-oa" data-id-title="openATTIC Initial Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.3 </span><span class="title-name">openATTIC Initial Setup</span> <a title="Permalink" class="permalink" href="#ceph-oa-install-oa">#</a></h3></div></div></div><p>
    By default, <code class="command">oaconfig</code> creates an administrative user
    account, <code class="systemitem">openattic</code>, with the same password as the
    user name. As a security precaution, we strongly recommend changing this
    password immediately:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>oaconfig changepassword openattic
Changing password for user 'openattic'
Password: &lt;enter password&gt;
Password (again): &lt;re-enter password&gt;
Password changed successfully for user 'openattic'</pre></div></section><section class="sect2" id="oa-ds" data-id-title="DeepSea Integration in openATTIC"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.4 </span><span class="title-name">DeepSea Integration in openATTIC</span> <a title="Permalink" class="permalink" href="#oa-ds">#</a></h3></div></div></div><p>
    Some openATTIC features, such as iSCSI Gateway and Object Gateway management, make use of the
    DeepSea REST API. It is enabled and configured by default. If you need to
    override its default settings for debugging purposes, edit
    <code class="filename">/etc/sysconfig/openattic</code> and add or change the
    following lines:
   </p><div class="verbatim-wrap"><pre class="screen">SALT_API_HOST="<em class="replaceable">salt_api_host</em>"
SALT_API_PORT=8001
SALT_API_USERNAME="example_user"
SALT_API_PASSWORD="password"</pre></div><div id="id-1.3.6.2.5.6.4" data-id-title="oaconfig restart" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: <code class="command">oaconfig restart</code></h6><p>
     Remember to run <code class="command">oaconfig restart</code> after you make changes
     to the <code class="filename">/etc/sysconfig/openattic</code> file.
    </p></div><div id="id-1.3.6.2.5.6.5" data-id-title="File Syntax" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: File Syntax</h6><p>
     <code class="filename">/etc/sysconfig/openattic</code> is used in Python as well as
     Bash. Therefore, the files need to be in a format which Bash can
     understand, and it is not possible to have spaces before or after the
     'equals' signs.
    </p></div></section><section class="sect2" id="oa-rgw" data-id-title="Object Gateway Management"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.5 </span><span class="title-name">Object Gateway Management</span> <a title="Permalink" class="permalink" href="#oa-rgw">#</a></h3></div></div></div><p>
    Object Gateway management features in openATTIC are enabled by default. If you need to
    override the default values for Object Gateway API as discovered from DeepSea,
    include the following options with relevant values in
    <code class="filename">/etc/sysconfig/openattic</code>. For example:
   </p><div class="verbatim-wrap"><pre class="screen">RGW_API_HOST="<em class="replaceable">rgw_api_host</em>"
RGW_API_PORT=80
RGW_API_SCHEME="http"
RGW_API_ACCESS_KEY="VFEG733GBY0DJCIV6NK0"
RGW_API_SECRET_KEY="lJzPbZYZTv8FzmJS5eiiZPHxlT2LMGOMW8ZAeOAq"</pre></div><div id="id-1.3.6.2.5.7.4" data-id-title="Default Resource for Object Gateway" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Default Resource for Object Gateway</h6><p>
     If your Object Gateway admin resource is not configured to use the default value
     'admin' as used in 'http://rgw_host:80/admin', you need to also set the
     <code class="option">RGW_API_ADMIN_RESOURCE</code> option appropriately.
    </p></div><p>
    To obtain Object Gateway credentials, use the <code class="command">radosgw-admin</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>radosgw-admin user info --uid=admin</pre></div></section><section class="sect2" id="oa-igw" data-id-title="iSCSI Gateway Management"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.6 </span><span class="title-name">iSCSI Gateway Management</span> <a title="Permalink" class="permalink" href="#oa-igw">#</a></h3></div></div></div><p>
    iSCSI Gateway management features in openATTIC are enabled by default. If you need
    override the default Salt API host name, change the
    <code class="option">SALT_API_HOST</code> value as described in
    <a class="xref" href="#oa-ds" title="17.1.4. DeepSea Integration in openATTIC">Section 17.1.4, “DeepSea Integration in openATTIC”</a>.
   </p></section></section><section class="sect1" id="ceph-oa-install-webui" data-id-title="openATTIC Web User Interface"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.2 </span><span class="title-name">openATTIC Web User Interface</span> <a title="Permalink" class="permalink" href="#ceph-oa-install-webui">#</a></h2></div></div></div><p>
   openATTIC can be managed using a Web user interface. Open a Web browser and
   navigate to http://<em class="replaceable">SERVER_HOST</em>/openattic. To log
   in, use the default user name <span class="emphasis"><em>openattic</em></span> and the
   corresponding password.
  </p><div class="figure" id="id-1.3.6.2.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_login.png" target="_blank"><img src="images/oa_login.png" width="" alt="openATTIC Login Screen"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.1: </span><span class="title-name">openATTIC Login Screen </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.6.3">#</a></h6></div></div><p>
   The openATTIC user interface is graphically divided into a top menu pane and a
   content pane.
  </p><p>
   The right part of the top pane includes a link to the current user settings,
   and a <span class="guimenu">Logout</span> link, and links to the list of existing
   <span class="guimenu">Background tasks</span> and system
   <span class="guimenu">Notifications</span>. The rest of the top pane includes the main
   openATTIC menu.
  </p><p>
   The content pane changes depending on which item menu is activated. By
   default, a <span class="guimenu">Dashboard</span> is displayed showing a number
   widgets to inform you about the status of the cluster.
  </p><div class="figure" id="id-1.3.6.2.6.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_dashboard.png" target="_blank"><img src="images/oa_dashboard.png" width="" alt="openATTIC Dashboard"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.2: </span><span class="title-name">openATTIC Dashboard </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.6.7">#</a></h6></div></div></section><section class="sect1" id="ceph-oa-webui-dash" data-id-title="Dashboard"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.3 </span><span class="title-name">Dashboard</span> <a title="Permalink" class="permalink" href="#ceph-oa-webui-dash">#</a></h2></div></div></div><p>
   Each <span class="guimenu">Dashboard</span> widget shows specific status information
   related to the running Ceph cluster. After clicking the title of a widget,
   the widget spreads across the whole content pane, possibly showing more
   details. A list of several widgets follows:
  </p><p>
   The <span class="guimenu">Status</span> widget tells whether the cluster is operating
   correctly. In case a problem is detected, you can view the detailed error
   message by clicking the subtitle inside the widget.
  </p><p>
   The <span class="guimenu">Monitors in Quorum</span>, <span class="guimenu">Pools</span>,
   <span class="guimenu">OSDs In</span>, <span class="guimenu">OSDs Out</span>, <span class="guimenu">OSDs
   Up</span>, <span class="guimenu">OSDs Down</span>, and <span class="guimenu">Average PGs per
   OSD</span> widgets simply show the related numbers.
  </p><div class="figure" id="id-1.3.6.2.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_basic_widgets.png" target="_blank"><img src="images/oa_basic_widgets.png" width="" alt="Basic Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.3: </span><span class="title-name">Basic Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.7.5">#</a></h6></div></div><p>
   The following widgets deal with total and available storage capacity:
   <span class="guimenu">Cluster Capacity</span>, <span class="guimenu">Available Capacity</span>,
   <span class="guimenu">Used Capacity</span>, and <span class="guimenu">Capacity</span>.
  </p><div class="figure" id="id-1.3.6.2.7.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_capacity_widgets.png" target="_blank"><img src="images/oa_capacity_widgets.png" width="" alt="Capacity Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.4: </span><span class="title-name">Capacity Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.7.7">#</a></h6></div></div><p>
   The following widgets deal with OSD and monitor node latency:
   <span class="guimenu">Average OSD Apply Latency</span>, <span class="guimenu">Average OSD Commit
   Latency</span>, and <span class="guimenu">Average Monitor Latency</span>:
  </p><div class="figure" id="id-1.3.6.2.7.9"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_latency_widgets.png" target="_blank"><img src="images/oa_latency_widgets.png" width="" alt="Latency Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.5: </span><span class="title-name">Latency Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.7.9">#</a></h6></div></div><p>
   The <span class="guimenu">Throughput</span> widget shows the read and write per second
   statistics in time.
  </p><div class="figure" id="id-1.3.6.2.7.11"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_io.png" target="_blank"><img src="images/oa_io.png" width="" alt="Throughput"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.6: </span><span class="title-name">Throughput </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.7.11">#</a></h6></div></div><div id="id-1.3.6.2.7.12" data-id-title="More Details on Mouse Over" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Details on Mouse Over</h6><p>
    If you move the mouse pointer over any of the displayed charts, you will be
    shown more details related to the date and time pointed at in a pop-up
    window.
   </p><p>
    If you click in the chart area and then drag the mouse pointer to the left
    or right along the time axis, the time interval on the axis will be zoomed
    in to the interval you marked by moving the mouse. To zoom out back to the
    original scale, double-click the chart.
   </p></div><p>Within openATTIC there are options to display graphs for longer than
    15 days. However, by default Prometheus only stores history for 15 days.
    You can adjust this behavior in <code class="filename">/etc/systemd/system/multi-user.target.wants/prometheus.service</code>.</p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Open <code class="filename">/etc/systemd/system/multi-user.target.wants/prometheus.service</code>.
      </p></li><li class="step"><p>
        This file should reference the following:
      </p><div class="verbatim-wrap"><pre class="screen">        EnvironmentFile=-/etc/sysconfig/prometheus
        ExecStart=/usr/bin/prometheus $ARGS</pre></div><p>If not does not, add the above two lines and include
        the following:</p><div class="verbatim-wrap"><pre class="screen">        ARGS="--storage.tsdb.retention=90d" \
              --log.level=warn"</pre></div><div id="id-1.3.6.2.7.14.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>Ensure <code class="literal">ARGS</code> is a multiline bash string.
        This enables Prometheus to store up to 90 days of data.</p><p>If you want other time options, the format is as follows:
          <em class="replaceable">number</em> X <em class="replaceable">time multiplier</em>
          (where <em class="replaceable">time multiplier</em> can be h[ours], d[ays], w[eeks],
          y[ears]).</p></div></li><li class="step"><p>
        Restart the Prometheus service.
      </p></li></ol></div></div></section><section class="sect1" id="ceph-oa-webui-ceph" data-id-title="Ceph Related Tasks"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.4 </span><span class="title-name">Ceph Related Tasks</span> <a title="Permalink" class="permalink" href="#ceph-oa-webui-ceph">#</a></h2></div></div></div><p>
   openATTIC's main menu lists Ceph related tasks. Currently, the following tasks
   are relevant: <span class="guimenu">OSDs</span>, <span class="guimenu">RBDs</span>,
   <span class="guimenu">Pools</span>, <span class="guimenu">Nodes</span>,
   <span class="guimenu">iSCSI</span>, <span class="guimenu">NFS</span>, <span class="guimenu">CRUSH
   Map</span>, and <span class="guimenu">Object Gateway</span>.
  </p><section class="sect2" id="id-1.3.6.2.8.3" data-id-title="Common Web UI Features"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.1 </span><span class="title-name">Common Web UI Features</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.3">#</a></h3></div></div></div><p>
    In openATTIC, you often work with <span class="emphasis"><em>lists</em></span>—for example,
    lists of pools, OSD nodes, or RBD devices. The following common widgets
    help you manage or adjust these list:
   </p><p>
    Click <span class="inlinemediaobject"><img src="images/oa_widget_reload.png" width=""/></span> to refresh the list of items.
   </p><p>
    Click <span class="inlinemediaobject"><img src="images/oa_widget_columns.png" width=""/></span> to display or hide individual table columns.
   </p><p>
    Click <span class="inlinemediaobject"><img src="images/oa_widget_rows.png" width=""/></span> and select how many rows to display on a single page.
   </p><p>
    Click inside <span class="inlinemediaobject"><img src="images/oa_widget_search.png" width=""/></span> and filter the rows by typing the string to search
    for.
   </p><p>
    Use <span class="inlinemediaobject"><img src="images/oa_widget_pager.png" width=""/></span> to change the currently displayed page if the list
    spans across multiple pages.
   </p></section><section class="sect2" id="id-1.3.6.2.8.4" data-id-title="Listing OSD Nodes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.2 </span><span class="title-name">Listing OSD Nodes</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.4">#</a></h3></div></div></div><p>
    To list all available OSD nodes, click <span class="guimenu">OSDs</span> from the
    main menu.
   </p><p>
    The list shows each OSD's name, host name, status, weight, and storage
    back-end.
   </p><div class="figure" id="id-1.3.6.2.8.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_osds.png" target="_blank"><img src="images/oa_osds.png" width="" alt="List of OSD nodes"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.7: </span><span class="title-name">List of OSD nodes </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.4.4">#</a></h6></div></div></section><section class="sect2" id="oa-rbds" data-id-title="Managing RADOS Block Devices (RBDs)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.3 </span><span class="title-name">Managing RADOS Block Devices (RBDs)</span> <a title="Permalink" class="permalink" href="#oa-rbds">#</a></h3></div></div></div><p>
    To list all available RADOS Block Devices, click <span class="guimenu">RBDs</span> from the main
    menu.
   </p><p>
    The list shows each device's name, the related pool name, size of the
    device, and, if 'fast-diff' was enabled during the RADOS Block Device creation, the
    percentage that is already occupied.
   </p><div class="figure" id="id-1.3.6.2.8.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbds.png" target="_blank"><img src="images/oa_rbds.png" width="" alt="List of RBDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.8: </span><span class="title-name">List of RBDs </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.4">#</a></h6></div></div><section class="sect3" id="id-1.3.6.2.8.5.5" data-id-title="Status Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.3.1 </span><span class="title-name">Status Information</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.5">#</a></h4></div></div></div><p>
     To view more detailed information about a device, activate its check box
     in the very left column:
    </p><div class="figure" id="id-1.3.6.2.8.5.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_status.png" target="_blank"><img src="images/oa_rbd_status.png" width="" alt="RBD Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.9: </span><span class="title-name">RBD Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.5.3">#</a></h6></div></div></section><section class="sect3" id="id-1.3.6.2.8.5.6" data-id-title="Statistics"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.3.2 </span><span class="title-name">Statistics</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.6">#</a></h4></div></div></div><p>
     Click the <span class="guimenu">Statistics</span> tab of an RADOS Block Device to view the
     statistics of transferred data. You can zoom in and out the time range
     either by highlighting the time range with a mouse, or by selecting it
     after clicking the date in the top left corner of the tab.
    </p></section><section class="sect3" id="id-1.3.6.2.8.5.7" data-id-title="RADOS Block Device Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.3.3 </span><span class="title-name">RADOS Block Device Snapshots</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.7">#</a></h4></div></div></div><p>
     To create an RADOS Block Device snapshot, click its <span class="guimenu">Snapshots</span> tab
     and select <span class="guimenu">Create</span> from the left top drop-down box.
    </p><p>
     After selecting a snapshot, you can rename, protect, clone, or delete it.
     Deletion also works if you select multiple snapshots.
     <span class="guimenu">Rollback</span> restores the device's state from the current
     snapshot.
    </p><div class="figure" id="id-1.3.6.2.8.5.7.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_snapshots.png" target="_blank"><img src="images/oa_rbd_snapshots.png" width="" alt="RBD Snapshots"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.10: </span><span class="title-name">RBD Snapshots </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.7.4">#</a></h6></div></div></section><section class="sect3" id="id-1.3.6.2.8.5.8" data-id-title="Deleting RBDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.3.4 </span><span class="title-name">Deleting RBDs</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.8">#</a></h4></div></div></div><p>
     To delete a device or a group of devices, activate their check boxes in
     the very left column and click <span class="guimenu">Delete</span> in the top-left
     of the RBDs table:
    </p><div class="figure" id="id-1.3.6.2.8.5.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_delete.png" target="_blank"><img src="images/oa_rbd_delete.png" width="" alt="Deleting RBD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.11: </span><span class="title-name">Deleting RBD </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.8.3">#</a></h6></div></div></section><section class="sect3" id="id-1.3.6.2.8.5.9" data-id-title="Adding RBDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.3.5 </span><span class="title-name">Adding RBDs</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.9">#</a></h4></div></div></div><p>
     To add a new device, click <span class="guimenu">Add</span> in the top left of the
     RBDs table and do the following on the <span class="guimenu">Create RBD</span>
     screen:
    </p><div class="figure" id="id-1.3.6.2.8.5.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_add.png" target="_blank"><img src="images/oa_rbd_add.png" width="" alt="Adding a New RBD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.12: </span><span class="title-name">Adding a New RBD </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.5.9.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Enter the name of the new device. Refer to
       <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware Requirements and Recommendations”, Section 2.8 “Naming Limitations”</span> for naming limitations.
      </p></li><li class="step"><p>
       Select the cluster that will store the new pool.
      </p></li><li class="step"><p>
       Select the pool from which the new RBD device will be created.
      </p></li><li class="step"><p>
       Specify the size of the new device. If you click the <span class="guimenu">use
       max</span> link above, the maximum pool size is populated.
      </p></li><li class="step"><p>
       To fine-tune the device parameters, click <span class="guimenu">Expert
       settings</span> and activate or deactivate the displayed options.
      </p></li><li class="step"><p>
       Confirm with <span class="guimenu">Create</span>.
      </p></li></ol></div></div></section></section><section class="sect2" id="id-1.3.6.2.8.6" data-id-title="Managing Pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.4 </span><span class="title-name">Managing Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6">#</a></h3></div></div></div><div id="id-1.3.6.2.8.6.2" data-id-title="More Information on Pools" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on Pools</h6><p>
     For more general information about Ceph pools, refer to
     <a class="xref" href="#ceph-pools" title="Chapter 8. Managing Storage Pools">Chapter 8, <em>Managing Storage Pools</em></a>. For information specific to erasure coded
     pools, refer to <a class="xref" href="#cha-ceph-erasure" title="Chapter 10. Erasure Coded Pools">Chapter 10, <em>Erasure Coded Pools</em></a>.
    </p></div><p>
    To list all available pools, click <span class="guimenu">Pools</span> from the main
    menu.
   </p><p>
    The list shows each pool's name, ID, the percentage of used space, the
    number of placement groups, replica size, type ('replicated' or 'erasure'),
    erasure code profile, and the CRUSH ruleset.
   </p><div class="figure" id="id-1.3.6.2.8.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools.png" target="_blank"><img src="images/oa_pools.png" width="" alt="List of Pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.13: </span><span class="title-name">List of Pools </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.5">#</a></h6></div></div><p>
    To view more detailed information about a pool, activate its check box in
    the very left column:
   </p><div class="figure" id="id-1.3.6.2.8.6.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools_status.png" target="_blank"><img src="images/oa_pools_status.png" width="" alt="Pool Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.14: </span><span class="title-name">Pool Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.7">#</a></h6></div></div><section class="sect3" id="id-1.3.6.2.8.6.8" data-id-title="Deleting Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.4.1 </span><span class="title-name">Deleting Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.8">#</a></h4></div></div></div><p>
     To delete a pool or a group of pools, activate their check boxes in the
     very left column and click <span class="guimenu">Delete</span> in the top left of
     the pools table:
    </p><div class="figure" id="id-1.3.6.2.8.6.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools_delete.png" target="_blank"><img src="images/oa_pools_delete.png" width="" alt="Deleting Pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.15: </span><span class="title-name">Deleting Pools </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.8.3">#</a></h6></div></div></section><section class="sect3" id="id-1.3.6.2.8.6.9" data-id-title="Adding Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.4.2 </span><span class="title-name">Adding Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.9">#</a></h4></div></div></div><p>
     To add a new pool, click <span class="guimenu">Add</span> in the top left of the
     pools table and do the following on the <span class="guimenu">Create Ceph
     pool</span> screen:
    </p><div class="figure" id="id-1.3.6.2.8.6.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools_add.png" target="_blank"><img src="images/oa_pools_add.png" width="" alt="Adding a New Pool"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.16: </span><span class="title-name">Adding a New Pool </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.6.9.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Enter the name of the new pool. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware Requirements and Recommendations”, Section 2.8 “Naming Limitations”</span>
       for naming limitations.
      </p></li><li class="step"><p>
       Select the cluster that will store the new pool.
      </p></li><li class="step"><p>
       Select the pool type. Pools can be either replicated or erasure coded.
      </p></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
         For a replicated pool, specify the replica size and the number of
         placement groups.
        </p></li><li class="step"><p>
         For an erasure code pool, specify the number of placement groups and
         erasure code profile. You can add your custom profile by clicking the
         plus '+' sign and specifying the profile name, data and coding chunks,
         and a ruleset failure domain.
        </p></li></ol></li><li class="step"><p>
       Confirm with <span class="guimenu">Create</span>.
      </p></li></ol></div></div></section></section><section class="sect2" id="ceph-oa-minions" data-id-title="Listing Nodes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.5 </span><span class="title-name">Listing Nodes</span> <a title="Permalink" class="permalink" href="#ceph-oa-minions">#</a></h3></div></div></div><p>
    Click <span class="guimenu">Nodes</span> from the main menu to view the list of nodes
    available on the cluster.
   </p><div class="figure" id="id-1.3.6.2.8.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_minion.png" target="_blank"><img src="images/oa_minion.png" width="" alt="List of Nodes"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.17: </span><span class="title-name">List of Nodes </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.7.3">#</a></h6></div></div><p>
    Each node is represented by its host name, public IP address, cluster ID it
    belongs to, node role (for example, 'admin', 'storage', or 'master'), and
    key acceptance status.
   </p></section><section class="sect2" id="oa-webui-nfs" data-id-title="Managing NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.6 </span><span class="title-name">Managing NFS Ganesha</span> <a title="Permalink" class="permalink" href="#oa-webui-nfs">#</a></h3></div></div></div><div id="id-1.3.6.2.8.8.2" data-id-title="More Information on NFS Ganesha" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on NFS Ganesha</h6><p>
     For more general information about NFS Ganesha, refer to
     <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 16. NFS Ganesha: Export Ceph Data via NFS">Chapter 16, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>.
    </p></div><p>
    To list all available NFS exports, click <span class="guimenu">NFS</span> from the
    main menu.
   </p><p>
    The list shows each export's directory, host name, status, type of storage
    back-end, and access type.
   </p><div class="figure" id="id-1.3.6.2.8.8.5"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs.png" target="_blank"><img src="images/oa_nfs.png" width="" alt="List of NFS Exports"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.18: </span><span class="title-name">List of NFS Exports </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.5">#</a></h6></div></div><p>
    To view more detailed information about an NFS export, activate its check
    box in the very left column:
   </p><div class="figure" id="id-1.3.6.2.8.8.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_status.png" target="_blank"><img src="images/oa_nfs_status.png" width="" alt="NFS Export Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.19: </span><span class="title-name">NFS Export Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.7">#</a></h6></div></div><div id="id-1.3.6.2.8.8.8" data-id-title="NFS Mount Command" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: NFS Mount Command</h6><p>
     At the bottom of the export detailed view, there is a mount command for
     you to be able to easily mount the related NFS export from a client
     machine.
    </p></div><section class="sect3" id="id-1.3.6.2.8.8.9" data-id-title="Adding NFS Exports"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.6.1 </span><span class="title-name">Adding NFS Exports</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.9">#</a></h4></div></div></div><p>
     To add a new NFS export, click <span class="guimenu">Add</span> in the top left of
     the exports table and enter the required information.
    </p><div class="figure" id="id-1.3.6.2.8.8.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_add.png" target="_blank"><img src="images/oa_nfs_add.png" width="" alt="Adding a New NFS Export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.20: </span><span class="title-name">Adding a New NFS Export </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.9.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Select a server host for the NFS export.
      </p></li><li class="step"><p>
       Select a storage back-end—either <span class="guimenu">CephFS</span> or
       <span class="guimenu">Object Gateway</span>.
      </p></li><li class="step"><p>
       Enter the directory path for the NFS export. If the directory does not
       exist on the server, it will be created.
      </p></li><li class="step"><p>
       Specify other NFS related options, such as supported NFS protocol
       version, access type, squashing, or transport protocol.
      </p></li><li class="step"><p>
       If you need to limit access to specific clients only, click <span class="guimenu">Add
       clients</span> and add their IP addresses together with access type
       and squashing options.
      </p></li><li class="step"><p>
       Confirm with <span class="guimenu">Submit</span>.
      </p></li></ol></div></div></section><section class="sect3" id="id-1.3.6.2.8.8.10" data-id-title="Cloning and Deleting NFS Exports"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.6.2 </span><span class="title-name">Cloning and Deleting NFS Exports</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.10">#</a></h4></div></div></div><p>
     To delete an export or a group of exports, activate their check boxes in
     the very left column and select <span class="guimenu">Delete</span> in the top left
     of the exports table.
    </p><p>
     Similarly, you can select <span class="guimenu">Clone</span> to clone the activated
     gateway.
    </p></section><section class="sect3" id="id-1.3.6.2.8.8.11" data-id-title="Editing NFS Exports"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.6.3 </span><span class="title-name">Editing NFS Exports</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.11">#</a></h4></div></div></div><p>
     To edit an existing export, either click its name in the exports table, or
     activate its check box and click <span class="guimenu">Edit</span> in the top left
     of the exports table.
    </p><p>
     You can then adjust all the details of the NFS export.
    </p><div class="figure" id="id-1.3.6.2.8.8.11.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_edit.png" target="_blank"><img src="images/oa_nfs_edit.png" width="" alt="Editing an NFS Export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.21: </span><span class="title-name">Editing an NFS Export </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.8.11.4">#</a></h6></div></div></section></section><section class="sect2" id="oa-webui-iscsi" data-id-title="Managing iSCSI Gateways"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.7 </span><span class="title-name">Managing iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#oa-webui-iscsi">#</a></h3></div></div></div><div id="id-1.3.6.2.8.9.2" data-id-title="More Information on iSCSI Gateways" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on iSCSI Gateways</h6><p>
     For more general information about iSCSI Gateways, refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span> and <a class="xref" href="#cha-ceph-iscsi" title="Chapter 14. Ceph iSCSI Gateway">Chapter 14, <em>Ceph iSCSI Gateway</em></a>.
    </p></div><p>
    To list all available gateways, click <span class="guimenu">iSCSI</span> from the
    main menu.
   </p><p>
    The list shows each gateway's target, state, and related portals and RBD
    images.
   </p><div class="figure" id="id-1.3.6.2.8.9.5"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_igws.png" target="_blank"><img src="images/oa_igws.png" width="" alt="List of iSCSI Gateways"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.22: </span><span class="title-name">List of iSCSI Gateways </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.5">#</a></h6></div></div><p>
    To view more detailed information about a gateway, activate its check box
    in the very left column:
   </p><div class="figure" id="id-1.3.6.2.8.9.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_igws_status.png" target="_blank"><img src="images/oa_igws_status.png" width="" alt="Gateway Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.23: </span><span class="title-name">Gateway Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.7">#</a></h6></div></div><section class="sect3" id="id-1.3.6.2.8.9.8" data-id-title="Adding iSCSI Gateways"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.7.1 </span><span class="title-name">Adding iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.8">#</a></h4></div></div></div><p>
     To add a new iSCSI Gateway, click <span class="guimenu">Add</span> in the top left of the
     gateways table and enter the required information.
    </p><div class="figure" id="id-1.3.6.2.8.9.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_igws_add.png" target="_blank"><img src="images/oa_igws_add.png" width="" alt="Adding a New iSCSI Gateway"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.24: </span><span class="title-name">Adding a New iSCSI Gateway </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.8.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Enter the target address of the new gateway.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Add portal</span> and select one or multiple iSCSI
       portals from the list.
      </p></li><li class="step"><p>
       Click <span class="guimenu">Add image</span> and select one or multiple RBD images
       for the gateway.
      </p></li><li class="step"><p>
       If you need to use authentication to access the gateway, activate the
       <span class="guimenu">Authentication</span> check box and enter the credentials.
       You can find more advanced authentication options after activating
       <span class="guimenu">Mutual authentication</span> and <span class="guimenu">Discovery
       authentication</span>.
      </p></li><li class="step"><p>
       Confirm with <span class="guimenu">Submit</span>.
      </p></li></ol></div></div></section><section class="sect3" id="id-1.3.6.2.8.9.9" data-id-title="Editing iSCSI Gateways"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.7.2 </span><span class="title-name">Editing iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.9">#</a></h4></div></div></div><p>
     To edit an existing iSCSI Gateway, either click its name in the gateways table, or
     activate its check box and click <span class="guimenu">Edit</span> in the top left
     of the gateways table.
    </p><p>
     You can then modify the iSCSI target, add or delete portals, and add or
     delete related RBD images. You can also adjust authentication information
     for the gateway.
    </p></section><section class="sect3" id="id-1.3.6.2.8.9.10" data-id-title="Cloning and Deleting iSCSI Gateways"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.7.3 </span><span class="title-name">Cloning and Deleting iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.10">#</a></h4></div></div></div><p>
     To delete a gateway or a group of gateways, activate their check boxes in
     the very left column and select <span class="guimenu">Delete</span> in the top left
     of the gateways table.
    </p><p>
     Similarly, you can select <span class="guimenu">Clone</span> to clone the activated
     gateway.
    </p></section><section class="sect3" id="id-1.3.6.2.8.9.11" data-id-title="Starting and Stopping iSCSI Gateways"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.7.4 </span><span class="title-name">Starting and Stopping iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.9.11">#</a></h4></div></div></div><p>
     To start all gateways, select <span class="guimenu">Start all</span> in the top left
     of the gateways table. To stop all gateways, select <span class="guimenu">Stop
     all</span>.
    </p></section></section><section class="sect2" id="ceph-oa-crushmap" data-id-title="Viewing the Cluster CRUSH Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.8 </span><span class="title-name">Viewing the Cluster CRUSH Map</span> <a title="Permalink" class="permalink" href="#ceph-oa-crushmap">#</a></h3></div></div></div><p>
    Click <span class="guimenu">CRUSH Map</span> from the main menu to view cluster
    CRUSH Map.
   </p><div class="figure" id="id-1.3.6.2.8.10.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_crush.png" target="_blank"><img src="images/oa_crush.png" width="" alt="CRUSH Map"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.25: </span><span class="title-name">CRUSH Map </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.10.3">#</a></h6></div></div><p>
    In the <span class="guimenu">Physical setup</span> pane, you can see the structure of
    the cluster as described by the CRUSH Map.
   </p><p>
    In the <span class="guimenu">Replication rules</span> pane, you can view individual
    rulesets after selecting one of them from the <span class="guimenu">Content</span>
    drop-down box.
   </p><div class="figure" id="id-1.3.6.2.8.10.6"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_crush_rulesets.png" target="_blank"><img src="images/oa_crush_rulesets.png" width="" alt="Replication rules"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.26: </span><span class="title-name">Replication rules </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.10.6">#</a></h6></div></div></section><section class="sect2" id="oa-webui-rgw-users" data-id-title="Managing Object Gateway Users and Buckets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.9 </span><span class="title-name">Managing Object Gateway Users and Buckets</span> <a title="Permalink" class="permalink" href="#oa-webui-rgw-users">#</a></h3></div></div></div><div id="id-1.3.6.2.8.11.2" data-id-title="More Information on Object Gateways" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on Object Gateways</h6><p>
     For more general information about Object Gateways, refer to
     <a class="xref" href="#cha-ceph-gw" title="Chapter 13. Ceph Object Gateway">Chapter 13, <em>Ceph Object Gateway</em></a>.
    </p></div><p>
    To list Object Gateway users, select <span class="guimenu">Object
    Gateway</span> / <span class="guimenu">Users</span> from the main menu.
   </p><p>
    The list shows each user's ID, display name, e-mail address, if the user is
    suspended, and the maximum number of buckets for the user.
   </p><div class="figure" id="id-1.3.6.2.8.11.5"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_users_list.png" target="_blank"><img src="images/oa_rgw_users_list.png" width="" alt="List of Object Gateway Users"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.27: </span><span class="title-name">List of Object Gateway Users </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.5">#</a></h6></div></div><section class="sect3" id="oa-rgw-user-add" data-id-title="Adding a New Object Gateway User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.1 </span><span class="title-name">Adding a New Object Gateway User</span> <a title="Permalink" class="permalink" href="#oa-rgw-user-add">#</a></h4></div></div></div><p>
     To add a new Object Gateway user, click <span class="guimenu">Add</span> in the top left of
     the users' table and enter the relevant information.
    </p><div id="id-1.3.6.2.8.11.6.3" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
      Find more information about Object Gateway user accounts in
      <a class="xref" href="#s3-swift-accounts-managment" title="13.5.2. Managing S3 and Swift Accounts">Section 13.5.2, “Managing S3 and Swift Accounts”</a>.
     </p></div><div class="figure" id="id-1.3.6.2.8.11.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_users_add.png" target="_blank"><img src="images/oa_rgw_users_add.png" width="" alt="Adding a New Object Gateway User"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.28: </span><span class="title-name">Adding a New Object Gateway User </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.6.4">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Enter the user name, full name, and optionally an e-mail address and the
       maximum number of buckets for the user.
      </p></li><li class="step"><p>
       If the user should be initially suspended, activate the
       <span class="guimenu">Suspended</span> check box.
      </p></li><li class="step"><p>
       Specify the access and secret keys for the S3 authentication. If you
       want openATTIC to generate the keys for you, activate <span class="guimenu">Generate
       key</span>.
      </p></li><li class="step"><p>
       In the <span class="guimenu">User quota</span> section, set quota limits for the
       current user.
      </p><p>
       Check <span class="guimenu">Enabled</span> to activate the user quota limits. You
       can either specify the <span class="guimenu">Maximum size</span> of the disk space
       the user can use within the cluster, or check <span class="guimenu">Unlimited
       size</span> for no size limit.
      </p><p>
       Similarly, specify <span class="guimenu">Maximum objects</span> that the user can
       store on the cluster storage, or <span class="guimenu">Unlimited objects</span> if
       the user may store any number of objects.
      </p><div class="figure" id="id-1.3.6.2.8.11.6.5.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_userquota.png" target="_blank"><img src="images/oa_rgw_userquota.png" width="" alt="User quota"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.29: </span><span class="title-name">User quota </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.6.5.4.4">#</a></h6></div></div></li><li class="step"><p>
       In the <span class="guimenu">Bucket Quota</span> section, set the bucket quota
       limits for the current user.
      </p><div class="figure" id="id-1.3.6.2.8.11.6.5.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_bucketquota.png" target="_blank"><img src="images/oa_rgw_bucketquota.png" width="" alt="Bucket Quota"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.30: </span><span class="title-name">Bucket Quota </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.6.5.5.2">#</a></h6></div></div></li><li class="step"><p>
       Confirm with <span class="guimenu">Submit</span>.
      </p></li></ol></div></div></section><section class="sect3" id="id-1.3.6.2.8.11.7" data-id-title="Deleting Object Gateway Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.2 </span><span class="title-name">Deleting Object Gateway Users</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.7">#</a></h4></div></div></div><p>
     To delete one or more Object Gateway users, activate their check boxes in the very
     left column and select <span class="guimenu">Delete</span> in the top left of the
     users table.
    </p></section><section class="sect3" id="oa-user-edit" data-id-title="Editing Object Gateway Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.3 </span><span class="title-name">Editing Object Gateway Users</span> <a title="Permalink" class="permalink" href="#oa-user-edit">#</a></h4></div></div></div><p>
     To edit the user information of an Object Gateway user, either activate their check
     box in the very left column and select <span class="guimenu">Edit</span> in the top
     left of the users table, or click their ID. You can change the information
     you entered when adding the user in <a class="xref" href="#oa-rgw-user-add" title="17.4.9.1. Adding a New Object Gateway User">Section 17.4.9.1, “Adding a New Object Gateway User”</a>,
     plus the following additional information:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.8.11.8.3.1"><span class="term">Subusers</span></dt><dd><p>
        Add, remove, or edit subusers of the currently edited user.
       </p><div class="figure" id="id-1.3.6.2.8.11.8.3.1.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_subuser_add.png" target="_blank"><img src="images/oa_rgw_subuser_add.png" width="" alt="Adding a Subuser"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.31: </span><span class="title-name">Adding a Subuser </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.8.3.1.2.2">#</a></h6></div></div></dd><dt id="id-1.3.6.2.8.11.8.3.2"><span class="term">Keys</span></dt><dd><p>
        Add, remove, or view access and secret keys of the currently edited
        user.
       </p><p>
        You can add S3 keys for the currently edited user, or view Swift keys
        for their subusers.
       </p><div class="figure" id="id-1.3.6.2.8.11.8.3.2.2.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_user_s3key_edit.png" target="_blank"><img src="images/oa_rgw_user_s3key_edit.png" width="" alt="View S3 keys"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.32: </span><span class="title-name">View S3 keys </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.8.3.2.2.3">#</a></h6></div></div></dd><dt id="id-1.3.6.2.8.11.8.3.3"><span class="term">Capabilities</span></dt><dd><p>
        Add or remove user's capabilities. The capabilities apply to
        <span class="guimenu">buckets</span>, <span class="guimenu">zone</span>,
        <span class="guimenu">users</span>, <span class="guimenu">metadata</span>, and
        <span class="guimenu">usage</span>. Each capability value can be one of 'read',
        'write', or '*' for read and write privilege.
       </p><div class="figure" id="id-1.3.6.2.8.11.8.3.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rgw_user_capabs.png" target="_blank"><img src="images/oa_rgw_user_capabs.png" width="" alt="Capabilities"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.33: </span><span class="title-name">Capabilities </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.8.3.3.2.2">#</a></h6></div></div></dd></dl></div></section><section class="sect3" id="oa-ogw-bucket-list" data-id-title="Listing Buckets for Object Gateway Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.4 </span><span class="title-name">Listing Buckets for Object Gateway Users</span> <a title="Permalink" class="permalink" href="#oa-ogw-bucket-list">#</a></h4></div></div></div><div id="id-1.3.6.2.8.11.9.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      A bucket is a mechanism for storing data objects. A user account may have
      many buckets, but bucket names must be unique. Although the term 'bucket'
      is normally used within the Amazon S3 API, the term 'container' is used
      in the OpenStack Swift API context.
     </p></div><p>
     Click <span class="guimenu">Object
     Gateway</span> / <span class="guimenu">Buckets</span> to list all
     available Object Gateway buckets.
    </p><div class="figure" id="id-1.3.6.2.8.11.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/os_ogw_bucket_list.png" target="_blank"><img src="images/os_ogw_bucket_list.png" width="" alt="Object Gateway Buckets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.34: </span><span class="title-name">Object Gateway Buckets </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.9.4">#</a></h6></div></div></section><section class="sect3" id="oa-ogw-bucket-add" data-id-title="Adding Buckets for Object Gateway Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.5 </span><span class="title-name">Adding Buckets for Object Gateway Users</span> <a title="Permalink" class="permalink" href="#oa-ogw-bucket-add">#</a></h4></div></div></div><p>
     To add a new bucket, click <span class="guimenu">Add</span> in the top left of the
     buckets table and enter the new bucket name and the related Object Gateway user.
     Confirm with <span class="guimenu">Submit</span>.
    </p><div class="figure" id="id-1.3.6.2.8.11.10.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_ogw_bucket_add.png" target="_blank"><img src="images/oa_ogw_bucket_add.png" width="" alt="Adding a New Bucket"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.35: </span><span class="title-name">Adding a New Bucket </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.10.3">#</a></h6></div></div></section><section class="sect3" id="oa-ogw-bucket-view" data-id-title="Viewing Bucket Details"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.6 </span><span class="title-name">Viewing Bucket Details</span> <a title="Permalink" class="permalink" href="#oa-ogw-bucket-view">#</a></h4></div></div></div><p>
     To view detailed information about an Object Gateway bucket, activate its check box
     in the very left column of the buckets table.
    </p><div class="figure" id="id-1.3.6.2.8.11.11.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_ogw_bucket_view.png" target="_blank"><img src="images/oa_ogw_bucket_view.png" width="" alt="Bucket Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.36: </span><span class="title-name">Bucket Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.11.3">#</a></h6></div></div></section><section class="sect3" id="oa-ogw-bucket-edit" data-id-title="Editing Buckets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.7 </span><span class="title-name">Editing Buckets</span> <a title="Permalink" class="permalink" href="#oa-ogw-bucket-edit">#</a></h4></div></div></div><p>
     To edit a bucket, either activate its check box in the very left column
     and select <span class="guimenu">Edit</span> in the top left of the buckets table,
     or click its name.
    </p><div class="figure" id="id-1.3.6.2.8.11.12.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_ogw_bucket_edit.png" target="_blank"><img src="images/oa_ogw_bucket_edit.png" width="" alt="Editing an Object Gateway Bucket"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.37: </span><span class="title-name">Editing an Object Gateway Bucket </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.12.3">#</a></h6></div></div><p>
     On the edit screen, you can change the user to which the bucket belongs.
    </p></section><section class="sect3" id="oa-ogw-bucket-delete" data-id-title="Deleting Buckets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.9.8 </span><span class="title-name">Deleting Buckets</span> <a title="Permalink" class="permalink" href="#oa-ogw-bucket-delete">#</a></h4></div></div></div><p>
     To delete one or more Object Gateway buckets, activate their check boxes in the
     very left column of the buckets table, and select
     <span class="guimenu">Delete</span> in the top left of the table.
    </p><div class="figure" id="id-1.3.6.2.8.11.13.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_ogw_buckets_delete.png" target="_blank"><img src="images/oa_ogw_buckets_delete.png" width="" alt="Deleting Buckets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.38: </span><span class="title-name">Deleting Buckets </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.8.11.13.3">#</a></h6></div></div><p>
     To confirm the deletion, type 'yes' in the <span class="guimenu">Delete
     buckets</span> pop-up window, and click <span class="guimenu">Delete</span>.
    </p><div id="id-1.3.6.2.8.11.13.5" data-id-title="Careful Deletion" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Careful Deletion</h6><p>
      When deleting an Object Gateway bucket, it is currently not verified if the bucket
      is actually in use, for example by NFS Ganesha via the S3 storage back-end.
     </p></div></section></section></section></section></div><div class="part" id="part-virt" data-id-title="Integration with Virtualization Tools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part V </span><span class="title-name">Integration with Virtualization Tools </span><a title="Permalink" class="permalink" href="#part-virt">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">18 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></span></li><dd class="toc-abstract"><p>The libvirt library creates a virtual machine abstraction layer between hypervisor interfaces and the software applications that use them. With libvirt, developers and system administrators can focus on a common management framework, common API, and common shell interface (virsh) to many different h…</p></dd><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">19 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></span></li><dd class="toc-abstract"><p>The most frequent Ceph use case involves providing block device images to virtual machines. For example, a user may create a 'golden' image with an OS and any relevant software in an ideal configuration. Then, the user takes a snapshot of the image. Finally, the user clones the snapshot (usually man…</p></dd></ul></div><section class="chapter" id="cha-ceph-libvirt" data-id-title="Using libvirt with Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span> <a title="Permalink" class="permalink" href="#cha-ceph-libvirt">#</a></h2></div></div></div><p>
  The <code class="systemitem">libvirt</code> library creates a virtual machine abstraction layer between
  hypervisor interfaces and the software applications that use them. With
  <code class="systemitem">libvirt</code>, developers and system administrators can focus on a common
  management framework, common API, and common shell interface
  (<code class="command">virsh</code>) to many different hypervisors, including
  QEMU/KVM, Xen, LXC, or VirtualBox.
 </p><p>
  Ceph block devices support QEMU/KVM. You can use Ceph block devices
  with software that interfaces with <code class="systemitem">libvirt</code>. The cloud solution uses
  <code class="systemitem">libvirt</code> to interact with QEMU/KVM, and QEMU/KVM interacts with Ceph
  block devices via <code class="systemitem">librbd</code>.
 </p><p>
  To create VMs that use Ceph block devices, use the procedures in the
  following sections. In the examples, we have used
  <code class="literal">libvirt-pool</code> for the pool name,
  <code class="literal">client.libvirt</code> for the user name, and
  <code class="literal">new-libvirt-image</code> for the image name. You may use any
  value you like, but ensure you replace those values when executing commands
  in the subsequent procedures.
 </p><section class="sect1" id="ceph-libvirt-cfg-ceph" data-id-title="Configuring Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.1 </span><span class="title-name">Configuring Ceph</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-ceph">#</a></h2></div></div></div><p>
   To configure Ceph for use with <code class="systemitem">libvirt</code>, perform the following steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a pool. The following example uses the pool name
     <code class="literal">libvirt-pool</code> with 128 placement groups.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create libvirt-pool 128 128</pre></div><p>
     Verify that the pool exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd lspools</pre></div></li><li class="step"><p>
     Create a Ceph User. The following example uses the Ceph user name
     <code class="literal">client.libvirt</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get-or-create client.libvirt mon 'profile rbd' osd \
 'profile rbd pool=libvirt-pool'</pre></div><p>
     Verify the name exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth list</pre></div><div id="id-1.3.7.2.6.3.2.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      <code class="systemitem">libvirt</code> will access Ceph using the ID <code class="literal">libvirt</code>, not
      the Ceph name <code class="literal">client.libvirt</code>. See
      <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/user-management/#user" target="_blank">http://docs.ceph.com/docs/master/rados/operations/user-management/#user</a>
      for a detailed explanation of the difference between ID and name.
     </p></div></li><li class="step"><p>
     Use QEMU to create an image in your RBD pool. The following example uses
     the image name <code class="literal">new-libvirt-image</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div id="id-1.3.7.2.6.3.3.2" data-id-title="Keyring File Location" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Keyring File Location</h6><p>
      The <code class="systemitem">libvirt</code> user key is stored in a keyring file placed in the
      <code class="filename">/etc/ceph</code> directory. The keyring file needs to have
      a appropriate name that includes the name of the Ceph cluster it
      belongs to. For the default cluster name 'ceph', the keyring file name is
      <code class="filename">/etc/ceph/ceph.client.libvirt.keyring</code>.
     </p><p>
      If the keyring does not exist, create it with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get client.libvirt &gt; /etc/ceph/ceph.client.libvirt.keyring</pre></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu-img create -f raw rbd:libvirt-pool/new-libvirt-image:id=libvirt 2G</pre></div><p>
     Verify the image exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd -p libvirt-pool ls</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-virt-manager" data-id-title="Preparing the VM Manager"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.2 </span><span class="title-name">Preparing the VM Manager</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-virt-manager">#</a></h2></div></div></div><p>
   You may use <code class="systemitem">libvirt</code> without a VM manager, but you may find it simpler to
   create your first domain with <code class="command">virt-manager</code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install a virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in virt-manager</pre></div></li><li class="step"><p>
     Prepare/download an OS image of the system you want to run virtualized.
    </p></li><li class="step"><p>
     Launch the virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen">virt-manager</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-create-vm" data-id-title="Creating a VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.3 </span><span class="title-name">Creating a VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-create-vm">#</a></h2></div></div></div><p>
   To create a VM with <code class="command">virt-manager</code>, perform the following
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Choose the connection from the list, right-click it, and select
     <span class="guimenu">New</span>.
    </p></li><li class="step"><p>
     <span class="guimenu">Import existing disk image</span> by providing the path to the
     existing storage. Specify OS type, memory settings, and
     <span class="guimenu">Name</span> the virtual machine, for example
     <code class="literal">libvirt-virtual-machine</code>.
    </p></li><li class="step"><p>
     Finish the configuration and start the VM.
    </p></li><li class="step"><p>
     Verify that the newly created domain exists with <code class="command">sudo virsh
     list</code>. If needed, specify the connection string, such as
    </p><div class="verbatim-wrap"><pre class="screen"><code class="command">virsh -c qemu+ssh://root@vm_host_hostname/system list</code>
Id    Name                           State
-----------------------------------------------
[...]
 9     libvirt-virtual-machine       running</pre></div></li><li class="step"><p>
     Log in to the VM and stop it before configuring it for use with Ceph.
    </p></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-cfg-vm" data-id-title="Configuring the VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.4 </span><span class="title-name">Configuring the VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-vm">#</a></h2></div></div></div><p>
   In this chapter, we focus on configuring VMs for integration with Ceph
   using <code class="command">virsh</code>. <code class="command">virsh</code> commands often
   require root privileges (<code class="command">sudo</code>) and will not return
   appropriate results or notify you that root privileges are required. For a
   reference of <code class="command">virsh</code> commands, refer to
   <a class="link" href="http://www.libvirt.org/virshcmdref.html" target="_blank">Virsh Command
   Reference</a>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open the configuration file with <code class="command">virsh edit</code>
     <em class="replaceable">vm-domain-name</em>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh edit libvirt-virtual-machine</pre></div></li><li class="step"><p>
     Under &lt;devices&gt; there should be a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;source file='/path/to/image/recent-linux.img'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;address type='drive' controller='0' bus='0' unit='0'/&gt;
    &lt;/disk&gt;</pre></div><p>
     Replace <code class="filename">/path/to/image/recent-linux.img</code> with the path
     to the OS image.
    </p><div id="id-1.3.7.2.9.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Use <code class="command">sudo virsh edit</code> instead of a text editor. If you
      edit the configuration file under <code class="filename">/etc/libvirt/qemu</code>
      with a text editor, <code class="systemitem">libvirt</code> may not recognize the change. If there is a
      discrepancy between the contents of the XML file under
      <code class="filename">/etc/libvirt/qemu</code> and the result of <code class="command">sudo
      virsh dumpxml</code> <em class="replaceable">vm-domain-name</em>, then
      your VM may not work properly.
     </p></div></li><li class="step"><p>
     Add the Ceph RBD image you previously created as a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='network' device='disk'&gt;
        &lt;source protocol='rbd' name='libvirt-pool/new-libvirt-image'&gt;
                &lt;host name='<em class="replaceable">monitor-host</em>' port='6789'/&gt;
        &lt;/source&gt;
        &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><p>
     Replace <em class="replaceable">monitor-host</em> with the name of your
     host, and replace the pool and/or image name as necessary. You may add
     multiple &lt;host&gt; entries for your Ceph monitors. The
     <code class="literal">dev</code> attribute is the logical device name that will
     appear under the <code class="filename">/dev</code> directory of your VM. The
     optional bus attribute indicates the type of disk device to emulate. The
     valid settings are driver specific (for example ide, scsi, virtio, xen,
     usb or sata). See
     <a class="link" href="http://www.libvirt.org/formatdomain.html#elementsDisks" target="_blank">Disks</a>
     for details of the &lt;disk&gt; element, and its child elements and
     attributes.
    </p></li><li class="step"><p>
     Save the file.
    </p></li><li class="step"><p>
     If your Ceph cluster has authentication enabled (it does by default),
     you must generate a secret. Open an editor of your choice and create a
     file called <code class="filename">secret.xml</code> with the following content:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;secret ephemeral='no' private='no'&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.libvirt secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;</pre></div></li><li class="step"><p>
     Define the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;</pre></div></li><li class="step"><p>
     Get the <code class="literal">client.libvirt</code> key and save the key string to a
     file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</pre></div></li><li class="step"><p>
     Set the UUID of the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh secret-set-value --secret <em class="replaceable">uuid of secret</em> \
--base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</pre></div><p>
     You must also set the secret manually by adding the following
     <code class="literal">&lt;auth&gt;</code> entry to the
     <code class="literal">&lt;disk&gt;</code> element you entered earlier (replacing the
     uuid value with the result from the command line example above).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh edit libvirt-virtual-machine</pre></div><p>
     Then, add <code class="literal">&lt;auth&gt;&lt;/auth&gt;</code> element to the
     domain configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">...
&lt;/source&gt;
&lt;auth username='libvirt'&gt;
        &lt;secret type='ceph' uuid='9ec59067-fdbc-a6c0-03ff-df165c0587b8'/&gt;
&lt;/auth&gt;
&lt;target ...</pre></div><div id="id-1.3.7.2.9.3.8.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The exemplary ID is <code class="literal">libvirt</code>, not the Ceph name
      <code class="literal">client.libvirt</code> as generated at step 2 of
      <a class="xref" href="#ceph-libvirt-cfg-ceph" title="18.1. Configuring Ceph">Section 18.1, “Configuring Ceph”</a>. Ensure you use the ID component
      of the Ceph name you generated. If for some reason you need to regenerate
      the secret, you will need to execute <code class="command">sudo virsh
      secret-undefine</code> <em class="replaceable">uuid</em> before
      executing <code class="command">sudo virsh secret-set-value</code> again.
     </p></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-summary" data-id-title="Summary"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.5 </span><span class="title-name">Summary</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-summary">#</a></h2></div></div></div><p>
   Once you have configured the VM for use with Ceph, you can start the VM.
   To verify that the VM and Ceph are communicating, you may perform the
   following procedures.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check to see if Ceph is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph health</pre></div></li><li class="step"><p>
     Check to see if the VM is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh list</pre></div></li><li class="step"><p>
     Check to see if the VM is communicating with Ceph. Replace
     <em class="replaceable">vm-domain-name</em> with the name of your VM domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh qemu-monitor-command --hmp <em class="replaceable">vm-domain-name</em> 'info block'</pre></div></li><li class="step"><p>
     Check to see if the device from <code class="literal">&amp;target dev='hdb'
     bus='ide'/&gt;</code> appears under <code class="filename">/dev</code> or under
     <code class="filename">/proc/partitions</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ls /dev
<code class="prompt user">cephadm &gt; </code>cat /proc/partitions</pre></div></li></ol></div></div></section></section><section class="chapter" id="cha-ceph-kvm" data-id-title="Ceph as a Back-end for QEMU KVM Instance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span> <a title="Permalink" class="permalink" href="#cha-ceph-kvm">#</a></h2></div></div></div><p>
  The most frequent Ceph use case involves providing block device images to
  virtual machines. For example, a user may create a 'golden' image with an OS
  and any relevant software in an ideal configuration. Then, the user takes a
  snapshot of the image. Finally, the user clones the snapshot (usually many
  times, see <a class="xref" href="#cha-ceph-snapshots-rbd" title="9.3. Snapshots">Section 9.3, “Snapshots”</a> for details). The ability to
  make copy-on-write clones of a snapshot means that Ceph can provision block
  device images to virtual machines quickly, because the client does not need
  to download an entire image each time it spins up a new virtual machine.
 </p><p>
  Ceph block devices can integrate with the QEMU virtual machines. For more
  information on QEMU KVM, see
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#part-virt-qemu" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#part-virt-qemu</a>.
 </p><section class="sect1" id="ceph-kvm-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#ceph-kvm-install">#</a></h2></div></div></div><p>
   In order to use Ceph block devices, QEMU needs to have the appropriate
   driver installed. Check whether the <code class="systemitem">qemu-block-rbd</code>
   package is installed, and install it if needed:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install qemu-block-rbd</pre></div></section><section class="sect1" id="ceph-kvm-usage" data-id-title="Usage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.2 </span><span class="title-name">Usage</span> <a title="Permalink" class="permalink" href="#ceph-kvm-usage">#</a></h2></div></div></div><p>
   The QEMU command line expects you to specify the pool name and image name.
   You may also specify a snapshot name.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em><em class="replaceable">:option1=value1</em><em class="replaceable">:option2=value2...</em></pre></div><p>
   For example, specifying the <em class="replaceable">id</em> and
   <em class="replaceable">conf</em> options might look like the following:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>:<code class="option">id=glance:conf=/etc/ceph/ceph.conf</code></pre></div></section><section class="sect1" id="id-1.3.7.3.7" data-id-title="Creating Images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.3 </span><span class="title-name">Creating Images with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.7">#</a></h2></div></div></div><p>
   You can create a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   create. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:pool1/image1 10G
Formatting 'rbd:pool1/image1', fmt=raw size=10737418240 nocow=off cluster_size=0</pre></div><div id="id-1.3.7.3.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The <code class="literal">raw</code> data format is really the only sensible format
    option to use with RBD. Technically, you could use other QEMU-supported
    formats such as <code class="literal">qcow2</code>, but doing so would add additional
    overhead, and would also render the volume unsafe for virtual machine live
    migration when caching is enabled.
   </p></div></section><section class="sect1" id="id-1.3.7.3.8" data-id-title="Resizing Images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.4 </span><span class="title-name">Resizing Images with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.8">#</a></h2></div></div></div><p>
   You can resize a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   resize. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:pool1/image1 9G
Image resized.</pre></div></section><section class="sect1" id="id-1.3.7.3.9" data-id-title="Retrieving Image Info with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.5 </span><span class="title-name">Retrieving Image Info with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.9">#</a></h2></div></div></div><p>
   You can retrieve block device image information from QEMU. You must
   specify <code class="literal">rbd</code>, the pool name, and the name of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:pool1/image1
image: rbd:pool1/image1
file format: raw
virtual size: 9.0G (9663676416 bytes)
disk size: unavailable
cluster_size: 4194304</pre></div></section><section class="sect1" id="id-1.3.7.3.10" data-id-title="Running QEMU with RBD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.6 </span><span class="title-name">Running QEMU with RBD</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.10">#</a></h2></div></div></div><p>
   QEMU can access an image as a virtual block device directly via
   <code class="systemitem">librbd</code>. This avoids an additional context switch,
   and can take advantage of RBD caching.
  </p><p>
   You can use <code class="command">qemu-img</code> to convert existing virtual machine
   images to Ceph block device images. For example, if you have a qcow2
   image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img convert -f qcow2 -O raw sles12.qcow2 rbd:pool1/sles12</pre></div><p>
   To run a virtual machine booting from that image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12</pre></div><p>
   <a class="link" href="http://ceph.com/docs/master/rbd/rbd-config-ref/#cache-settings" target="_blank">RBD
   caching</a> can significantly improve performance. QEMU’s cache
   options control <code class="systemitem">librbd</code> caching:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=rbd,file=rbd:pool1/sles12,cache=writeback</pre></div></section><section class="sect1" id="id-1.3.7.3.11" data-id-title="Enabling Discard/TRIM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.7 </span><span class="title-name">Enabling Discard/TRIM</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.11">#</a></h2></div></div></div><p>
   Ceph block devices support the discard operation. This means that a guest
   can send TRIM requests to let a Ceph block device reclaim unused space.
   This can be enabled in the guest by mounting <code class="systemitem">XFS</code>
   with the discard option.
  </p><p>
   For this to be available to the guest, it must be explicitly enabled for the
   block device. To do this, you must specify a
   <code class="option">discard_granularity</code> associated with the drive:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12,id=drive1,if=none \
-device driver=ide-hd,drive=drive1,discard_granularity=512</pre></div><div id="id-1.3.7.3.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The above example uses the IDE driver. The virtio driver does not support
    discard.
   </p></div><p>
   If using <code class="systemitem">libvirt</code>, edit your libvirt domain’s
   configuration file using <code class="command">virsh edit</code> to include the
   <code class="literal">xmlns:qemu</code> value. Then, add a <code class="literal">qemu:commandline
   block</code> as a child of that domain. The following example shows how
   to set two devices with <code class="literal">qemu id=</code> to different
   <code class="literal">discard_granularity</code> values.
  </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
 &lt;qemu:commandline&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-0.discard_granularity=4096'/&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-1.discard_granularity=65536'/&gt;
 &lt;/qemu:commandline&gt;
&lt;/domain&gt;</pre></div></section><section class="sect1" id="id-1.3.7.3.12" data-id-title="QEMU Cache Options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.8 </span><span class="title-name">QEMU Cache Options</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.12">#</a></h2></div></div></div><p>
   QEMU’s cache options correspond to the following Ceph RBD Cache
   settings.
  </p><p>
   Writeback:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true</pre></div><p>
   Writethrough:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true
rbd_cache_max_dirty = 0</pre></div><p>
   None:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = false</pre></div><p>
   QEMU’s cache settings override Ceph’s default settings (settings
   that are not explicitly set in the Ceph configuration file). If you
   explicitly set
   <a class="link" href="http://ceph.com/docs/master/rbd/rbd-config-ref/#cache-settings" target="_blank">RBD
   Cache</a> settings in your Ceph configuration file, your Ceph
   settings override the QEMU cache settings. If you set cache settings on
   the QEMU command line, the QEMU command line settings override the
   Ceph configuration file settings.
  </p></section></section></div><div class="part" id="part-troubleshooting" data-id-title="FAQs, Tips and Troubleshooting"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part VI </span><span class="title-name">FAQs, Tips and Troubleshooting </span><a title="Permalink" class="permalink" href="#part-troubleshooting">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#storage-tips"><span class="title-number">20 </span><span class="title-name">Hints and Tips</span></a></span></li><dd class="toc-abstract"><p>
  The chapter provides information to help you enhance performance of your
  Ceph cluster and provides tips how to set the cluster up.
 </p></dd><li><span class="chapter"><a href="#storage-faqs"><span class="title-number">21 </span><span class="title-name">Frequently Asked Questions</span></a></span></li><dd class="toc-abstract"><p/></dd><li><span class="chapter"><a href="#storage-troubleshooting"><span class="title-number">22 </span><span class="title-name">Troubleshooting</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes several issues that you may face when you operate a
  Ceph cluster.
 </p></dd></ul></div><section class="chapter" id="storage-tips" data-id-title="Hints and Tips"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20 </span><span class="title-name">Hints and Tips</span> <a title="Permalink" class="permalink" href="#storage-tips">#</a></h2></div></div></div><p>
  The chapter provides information to help you enhance performance of your
  Ceph cluster and provides tips how to set the cluster up.
 </p><section class="sect1" id="tips-orphaned-partitions" data-id-title="Identifying Orphaned Partitions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.1 </span><span class="title-name">Identifying Orphaned Partitions</span> <a title="Permalink" class="permalink" href="#tips-orphaned-partitions">#</a></h2></div></div></div><p>
   To identify possibly orphaned journal/WAL/DB devices, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Pick the device that may have orphaned partitions and save the list of its
     partitions to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ls /dev/sdd?* &gt; /tmp/partitions</pre></div></li><li class="step"><p>
     Run <code class="command">readlink</code> against all block.wal, block.db, and
     journal devices, and compare the output to the previously saved list of
     partitions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -</pre></div><p>
     The output is the list of partitions that are <span class="emphasis"><em>not</em></span>
     used by Ceph.
    </p></li><li class="step"><p>
     Remove the orphaned partitions that do not belong to Ceph with your
     preferred command (for example <code class="command">fdisk</code>,
     <code class="command">parted</code>, or <code class="command">sgdisk</code>).
    </p></li></ol></div></div></section><section class="sect1" id="tips-scrubbing" data-id-title="Adjusting Scrubbing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.2 </span><span class="title-name">Adjusting Scrubbing</span> <a title="Permalink" class="permalink" href="#tips-scrubbing">#</a></h2></div></div></div><p>
   By default, Ceph performs light scrubbing (find more details in
   <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>) daily and deep scrubbing weekly.
   <span class="emphasis"><em>Light</em></span> scrubbing checks object sizes and checksums to
   ensure that placement groups are storing the same object data.
   <span class="emphasis"><em>Deep</em></span> scrubbing checks an object’s content with that
   of its replicas to ensure that the actual contents are the same. The price
   for checking data integrity is increased I/O load on the cluster during the
   scrubbing procedure.
  </p><p>
   The default settings allow Ceph OSDs to initiate scrubbing at inappropriate
   times, such as during periods of heavy loads. Customers may experience
   latency and poor performance when scrubbing operations conflict with their
   operations. Ceph provides several scrubbing settings that can limit
   scrubbing to periods with lower loads or during off-peak hours.
  </p><p>
   If the cluster experiences high loads during the day and low loads late at
   night, consider restricting scrubbing to night time hours, such as 11pm till
   6am:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6</pre></div><p>
   If time restriction is not an effective method of determining a scrubbing
   schedule, consider using the <code class="option">osd_scrub_load_threshold</code>
   option. The default value is 0.5, but it could be modified for low load
   conditions:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_load_threshold = 0.25</pre></div></section><section class="sect1" id="tips-stopping-osd-without-rebalancing" data-id-title="Stopping OSDs without Rebalancing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span> <a title="Permalink" class="permalink" href="#tips-stopping-osd-without-rebalancing">#</a></h2></div></div></div><p>
   You may need to stop OSDs for maintenance periodically. If you do not want
   CRUSH to automatically rebalance the cluster in order to avoid huge data
   transfers, set the cluster to <code class="literal">noout</code> first:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ceph osd set noout</pre></div><p>
   When the cluster is set to <code class="literal">noout</code>, you can begin stopping
   the OSDs within the failure domain that requires maintenance work:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl stop ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   Find more information in
   <a class="xref" href="#ceph-operating-services-individual" title="3.1.2. Starting, Stopping, and Restarting Individual Services">Section 3.1.2, “Starting, Stopping, and Restarting Individual Services”</a>.
  </p><p>
   After you complete the maintenance, start OSDs again:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl start ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   After OSD services are started, unset the cluster from
   <code class="literal">noout</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd unset noout</pre></div></section><section class="sect1" id="Cluster-Time-Setting" data-id-title="Time Synchronization of Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.4 </span><span class="title-name">Time Synchronization of Nodes</span> <a title="Permalink" class="permalink" href="#Cluster-Time-Setting">#</a></h2></div></div></div><p>
   Ceph requires precise time synchronization between all nodes.
  </p><p>
   We recommend synchronizing all Ceph cluster nodes with at least three
   reliable time sources that are located on the internal network. The internal
   time sources can point to a public time server or have their own time
   source.
  </p><div id="id-1.3.8.2.7.4" data-id-title="Public Time Servers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Public Time Servers</h6><p>
    Do not synchronize all Ceph cluster nodes directly with remote public
    time servers. With such a configuration, each node in the cluster has its
    own NTP daemon that communicates continually over the Internet with a set
    of three or four time servers that may provide slightly different time.
    This solution introduces a large degree of latency variability that makes
    it difficult or impossible to keep the clock drift under 0.05 seconds which
    is what the Ceph monitors require.
   </p></div><p>
   For details how to set up the NTP server refer to
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-netz-xntp" target="_blank">SUSE Linux Enterprise Server
   Administration Guide</a>.
  </p><p>
   Then to change the time on your cluster, do the following:
  </p><div id="id-1.3.8.2.7.7" data-id-title="Setting Time" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Setting Time</h6><p>
    You may face a situation when you need to set the time back, for example if
    the time changes from the summer to the standard time. We do not recommend
    to move the time backward for a longer period than the cluster is down.
    Moving the time forward does not cause any trouble.
   </p></div><div class="procedure" id="id-1.3.8.2.7.8" data-id-title="Time Synchronization on the Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 20.1: </span><span class="title-name">Time Synchronization on the Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.7.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop all clients accessing the Ceph cluster, especially those using
     iSCSI.
    </p></li><li class="step"><p>
     Shut down your Ceph cluster. On each node run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph.target</pre></div><div id="id-1.3.8.2.7.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If you use Ceph and SUSE OpenStack Cloud, stop also the SUSE OpenStack Cloud.
     </p></div></li><li class="step"><p>
     Verify that your NTP server is set up correctly—all ntpd daemons get
     their time from a source or sources in the local network.
    </p></li><li class="step"><p>
     Set the correct time on your NTP server.
    </p></li><li class="step"><p>
     Verify that NTP is running and working properly, on all nodes run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl status ntpd.service</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ntpq -p</pre></div></li><li class="step"><p>
     Start all monitoring nodes and verify that there is no clock skew:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start <em class="replaceable">target</em></pre></div></li><li class="step"><p>
     Start all OSD nodes.
    </p></li><li class="step"><p>
     Start other Ceph services.
    </p></li><li class="step"><p>
     Start the SUSE OpenStack Cloud if you have it.
    </p></li></ol></div></div></section><section class="sect1" id="storage-bp-cluster-mntc-unbalanced" data-id-title="Checking for Unbalanced Data Writing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span> <a title="Permalink" class="permalink" href="#storage-bp-cluster-mntc-unbalanced">#</a></h2></div></div></div><p>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <span class="emphasis"><em>weight</em></span>. The
   weight is a relative number and tells Ceph how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </p><p>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </p><p>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given rule set, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written—you can increase their weight to
   have Ceph write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </p><div class="verbatim-wrap"><pre class="screen">$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</pre></div><div id="id-1.3.8.2.8.7" data-id-title="OSD Reweight by Utilization" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: OSD Reweight by Utilization</h6><p>
    The <code class="command">ceph osd reweight-by-utilization</code>
    <em class="replaceable">threshold</em> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </p></div></section><section class="sect1" id="storage-tips-ceph-btrfs-subvol" data-id-title="Btrfs Sub-volume for /var/lib/ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.6 </span><span class="title-name">Btrfs Sub-volume for /var/lib/ceph</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol">#</a></h2></div></div></div><p>
   SUSE Linux Enterprise by default is installed on a Btrfs partition. The directory
   <code class="filename">/var/lib/ceph</code> should be excluded from Btrfs snapshots
   and rollbacks, especially when a MON is running on the node. DeepSea
   provides the <code class="literal">fs</code> runner that can set up a sub-volume for
   this path.
  </p><section class="sect2" id="storage-tips-ceph-btrfs-subvol-req-new" data-id-title="Requirements for new Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.6.1 </span><span class="title-name">Requirements for new Installation</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-req-new">#</a></h3></div></div></div><p>
    If you are setting up the cluster the first time, the following
    requirements must be met before you can use the DeepSea runner:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Salt and DeepSea are properly installed and working according to this
      documentation.
     </p></li><li class="listitem"><p>
      <code class="command">salt-run state.orch ceph.stage.0</code> has been invoked to
      synchronize all the Salt and DeepSea modules to the minions.
     </p></li><li class="listitem"><p>
      Ceph is not yet installed, thus ceph.stage.3 has not yet been run and
      <code class="filename">/var/lib/ceph</code> does not yet exist.
     </p></li></ul></div></section><section class="sect2" id="storage-tips-ceph-btrfs-subvol-req-existing" data-id-title="Requirements for Existing Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.6.2 </span><span class="title-name">Requirements for Existing Installation</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-req-existing">#</a></h3></div></div></div><p>
    If your cluster is already installed, the following requirements must be
    met before you can use the DeepSea runner:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Nodes are upgraded to SUSE Enterprise Storage 5.5 and cluster is under
      DeepSea control.
     </p></li><li class="listitem"><p>
      Ceph cluster is up and healthy.
     </p></li><li class="listitem"><p>
      Upgrade process has synchronized Salt and DeepSea modules to all
      minion nodes.
     </p></li></ul></div></section><section class="sect2" id="storage-tips-ceph-btrfs-subvol-automatic" data-id-title="Automatic Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.6.3 </span><span class="title-name">Automatic Setup</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-automatic">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ul class="procedure"><li class="step"><p>
      On the Salt master run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.migrate.subvolume</pre></div><p>
      On nodes without an existing <code class="filename">/var/lib/ceph</code>
      directory, this will, one node at a time:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        create <code class="filename">/var/lib/ceph</code> as a
        <code class="literal">@/var/lib/ceph</code> Btrfs sub-volume.
       </p></li><li class="listitem"><p>
        mount the new sub-volume and update <code class="filename">/etc/fstab</code>
        appropriately.
       </p></li><li class="listitem"><p>
        disable copy-on-write for <code class="filename">/var/lib/ceph</code>.
       </p></li></ul></div><p>
      On nodes with an existing Ceph installation, this will, one node at a
      time:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        terminate running Ceph processes.
       </p></li><li class="listitem"><p>
        unmount OSDs on the node.
       </p></li><li class="listitem"><p>
        create <code class="literal">@/var/lib/ceph</code> Btrfs sub-volume and migrate
        existing <code class="filename">/var/lib/ceph</code> data.
       </p></li><li class="listitem"><p>
        mount the new sub-volume and update <code class="filename">/etc/fstab</code>
        appropriately.
       </p></li><li class="listitem"><p>
        disable copy-on-write for <code class="filename">/var/lib/ceph/*</code>,
        omitting <code class="filename">/var/lib/ceph/osd/*</code>.
       </p></li><li class="listitem"><p>
        re-mount OSDs.
       </p></li><li class="listitem"><p>
        re-start Ceph daemons.
       </p></li></ul></div></li></ul></div></div></section><section class="sect2" id="storage-tips-ceph-btrfs-subvol-manually" data-id-title="Manual Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.6.4 </span><span class="title-name">Manual Setup</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-manually">#</a></h3></div></div></div><p>
    This uses the new <code class="literal">fs</code> runner.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Inspects the state of <code class="filename">/var/lib/ceph</code> on all nodes and
      print suggestions about how to proceed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> fs.inspect_var</pre></div><p>
      This will return one of the following commands:
     </p><div class="verbatim-wrap"><pre class="screen">salt-run fs.create_var
salt-run fs.migrate_var
salt-run fs.correct_var_attrs</pre></div></li><li class="step"><p>
      Run the command that was returned in the previous step.
     </p><p>
      If an error occurs on one of the nodes, the execution for other nodes
      will stop as well and the runner will try to revert performed changes.
      Consult the log files on the minions with the problem to determine the
      problem. The runner can be re-run after the problem has been solved.
     </p></li></ol></div></div><p>
    The command <code class="command">salt-run fs.help</code> provides a list of all
    runner and module commands for the <code class="literal">fs</code> module.
   </p></section></section><section class="sect1" id="storage-bp-srv-maint-fds-inc" data-id-title="Increasing File Descriptors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.7 </span><span class="title-name">Increasing File Descriptors</span> <a title="Permalink" class="permalink" href="#storage-bp-srv-maint-fds-inc">#</a></h2></div></div></div><p>
   For OSD daemons, the read/write operations are critical to keep the Ceph
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </p><p>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <code class="filename">/etc/ceph/ceph.conf</code>, for example:
  </p><div class="verbatim-wrap"><pre class="screen">max_open_files = 131072</pre></div><p>
   After you change <code class="option">max_open_files</code>, you need to restart the
   OSD service on the relevant Ceph node.
  </p></section><section class="sect1" id="bp-osd-on-exisitng-partitions" data-id-title="How to Use Existing Partitions for OSDs Including OSD Journals"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.8 </span><span class="title-name">How to Use Existing Partitions for OSDs Including OSD Journals</span> <a title="Permalink" class="permalink" href="#bp-osd-on-exisitng-partitions">#</a></h2></div></div></div><div id="id-1.3.8.2.11.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    This section describes an advanced topic that only storage experts and
    developers should examine. It is mostly needed when using non-standard OSD
    journal sizes. If the OSD partition's size is less than 10GB, its initial
    weight is rounded to 0 and because no data are therefore placed on it, you
    should increase its weight. We take no responsibility for overfilled
    journals.
   </p></div><p>
   If you need to use existing disk partitions as an OSD node, the OSD journal
   and data partitions need to be in a GPT partition table.
  </p><p>
   You need to set the correct partition types to the OSD partitions so that
   <code class="systemitem">udev</code> recognizes them correctly and sets their
   ownership to <code class="literal">ceph:ceph</code>.
  </p><p>
   For example, to set the partition type for the journal partition
   <code class="filename">/dev/vdb1</code> and data partition
   <code class="filename">/dev/vdb2</code>, run the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/vdb
<code class="prompt user">root # </code>sgdisk --typecode=2:4fbd7e29-9d25-41b8-afd0-062c0ceff05d /dev/vdb</pre></div><div id="id-1.3.8.2.11.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    The Ceph partition table types are listed in
    <code class="filename">/usr/lib/udev/rules.d/95-ceph-osd.rules</code>:
   </p><div class="verbatim-wrap"><pre class="screen">cat /usr/lib/udev/rules.d/95-ceph-osd.rules
# OSD_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER="ceph", GROUP="ceph", MODE="660"

# JOURNAL_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER="ceph", GROUP="ceph", MODE="660"
[...]</pre></div></div></section><section class="sect1" id="storage-admin-integration" data-id-title="Integration with Virtualization Software"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.9 </span><span class="title-name">Integration with Virtualization Software</span> <a title="Permalink" class="permalink" href="#storage-admin-integration">#</a></h2></div></div></div><section class="sect2" id="storage-bp-integration-kvm" data-id-title="Storing KVM Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.9.1 </span><span class="title-name">Storing KVM Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-kvm">#</a></h3></div></div></div><p>
    You can create a disk image for KVM-driven virtual machine, store it in a
    Ceph pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <code class="command">qemu-kvm</code> making use of
    the disk image stored in the cluster. For more detailed information, see
    <a class="xref" href="#cha-ceph-kvm" title="Chapter 19. Ceph as a Back-end for QEMU KVM Instance">Chapter 19, <em>Ceph as a Back-end for QEMU KVM Instance</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-libvirt" data-id-title="Storing libvirt Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.9.2 </span><span class="title-name">Storing <code class="systemitem">libvirt</code> Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-libvirt">#</a></h3></div></div></div><p>
    Similar to KVM (see <a class="xref" href="#storage-bp-integration-kvm" title="20.9.1. Storing KVM Disks in Ceph Cluster">Section 20.9.1, “Storing KVM Disks in Ceph Cluster”</a>), you
    can use Ceph to store virtual machines driven by <code class="systemitem">libvirt</code>. The advantage
    is that you can run any <code class="systemitem">libvirt</code>-supported virtualization solution, such
    as KVM, Xen, or LXC. For more information, see
    <a class="xref" href="#cha-ceph-libvirt" title="Chapter 18. Using libvirt with Ceph">Chapter 18, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-xen" data-id-title="Storing Xen Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.9.3 </span><span class="title-name">Storing Xen Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-xen">#</a></h3></div></div></div><p>
    One way to use Ceph for storing Xen disks is to make use of <code class="systemitem">libvirt</code>
    as described in <a class="xref" href="#cha-ceph-libvirt" title="Chapter 18. Using libvirt with Ceph">Chapter 18, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p><p>
    Another option is to make Xen talk to the <code class="systemitem">rbd</code>
    block device driver directly:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you have no disk image prepared for Xen, create a new one:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd create myimage --size 8000 --pool mypool</pre></div></li><li class="step"><p>
      List images in the pool <code class="literal">mypool</code> and check if your new
      image is there:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
      Create a new block device by mapping the <code class="literal">myimage</code> image
      to the <code class="systemitem">rbd</code> kernel module:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.8.2.12.4.4.3.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
       To specify a user name, use <code class="option">--id
       <em class="replaceable">user-name</em></code>. Moreover, if you use
       <code class="systemitem">cephx</code> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="systemitem">cephadm</code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
      List all mapped devices:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="command">rbd showmapped</code>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div></li><li class="step"><p>
      Now you can configure Xen to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <code class="command">xl</code>-style domain configuration file:
     </p><div class="verbatim-wrap"><pre class="screen">disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</pre></div></li></ol></div></div></section></section><section class="sect1" id="storage-bp-net-firewall" data-id-title="Firewall Settings for Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.10 </span><span class="title-name">Firewall Settings for Ceph</span> <a title="Permalink" class="permalink" href="#storage-bp-net-firewall">#</a></h2></div></div></div><div id="id-1.3.8.2.13.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
    DeepSea deployment stages fail when firewall is active (and even
    configured). To pass the stages correctly, you need to either turn the
    firewall off by running
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop SuSEfirewall2.service</pre></div><p>
    or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div><p>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <span class="guimenu">YaST</span> / <span class="guimenu">Security and
   Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
   Services</span>.
  </p><p>
   Following is a list of Ceph related services and numbers of the ports that
   they normally use:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.8.2.13.5.1"><span class="term">Ceph Monitor</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph MON</span> service or port 6789 (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.2"><span class="term">Ceph OSD or Metadata Server</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph OSD/MDS</span> service, or ports 6800-7300
      (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.3"><span class="term">iSCSI Gateway</span></dt><dd><p>
      Open port 3260 (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.4"><span class="term">Object Gateway</span></dt><dd><p>
      Open the port where Object Gateway communication occurs. It is set in
      <code class="filename">/etc/ceph.conf</code> on the line starting with
      <code class="literal">rgw frontends =</code>. Default is 80 for HTTP and 443 for
      HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.5"><span class="term">NFS Ganesha</span></dt><dd><p>
      By default, NFS Ganesha uses ports 2049 (NFS service, TCP) and 875 (rquota
      support, TCP). Refer to <a class="xref" href="#ganesha-nfsport" title="16.2.3. Changing Default NFS Ganesha Ports">Section 16.2.3, “Changing Default NFS Ganesha Ports”</a> for more
      information on changing the default NFS Ganesha ports.
     </p></dd><dt id="id-1.3.8.2.13.5.6"><span class="term">Apache based services, such as openATTIC, SMT, or SUSE Manager</span></dt><dd><p>
      Open ports 80 for HTTP and 443 for HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.7"><span class="term">SSH</span></dt><dd><p>
      Open port 22 (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.8"><span class="term">NTP</span></dt><dd><p>
      Open port 123 (UDP).
     </p></dd><dt id="id-1.3.8.2.13.5.9"><span class="term">Salt</span></dt><dd><p>
      Open ports 4505 and 4506 (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.10"><span class="term">Grafana</span></dt><dd><p>
      Open port 3000 (TCP).
     </p></dd><dt id="id-1.3.8.2.13.5.11"><span class="term">Prometheus</span></dt><dd><p>
      Open port 9100 (TCP).
     </p></dd></dl></div></section><section class="sect1" id="storage-bp-network-test" data-id-title="Testing Network Performance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.11 </span><span class="title-name">Testing Network Performance</span> <a title="Permalink" class="permalink" href="#storage-bp-network-test">#</a></h2></div></div></div><p>
   To test the network performance the DeepSea <code class="literal">net</code> runner
   provides the following commands.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A simple ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</pre></div></li><li class="listitem"><p>
     A jumbo ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</pre></div></li><li class="listitem"><p>
     A bandwidth test:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</pre></div><div id="id-1.3.8.2.14.3.3.3" data-id-title="Stop iperf3 Processes Manually" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Stop 'iperf3' Processes Manually</h6><p>
      When running a test using the <code class="command">net.iperf</code> runner, the
      'iperf3' server processes that are started do not stop automatically when
      a test is completed. To stop the processes, use the following runner:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' multi.kill_iperf_cmd</pre></div></div></li></ul></div></section><section class="sect1" id="storage-bd-replacing-disk" data-id-title="Replacing Storage Disk"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.12 </span><span class="title-name">Replacing Storage Disk</span> <a title="Permalink" class="permalink" href="#storage-bd-replacing-disk">#</a></h2></div></div></div><p>
   If you need to replace a storage disk in a Ceph cluster, you can do so
   during the cluster's full operation. The replacement will cause temporary
   increase in data transfer.
  </p><p>
   If the disk fails entirely, Ceph needs to rewrite at least the same amount
   of data as the capacity of the failed disk is. If the disk is properly
   evacuated and then re-added to avoid loss of redundancy during the process,
   the amount of rewritten data will be twice as big. If the new disk has a
   different size as the replaced one, it will cause some additional data to be
   redistributed to even out the usage of all OSDs.
  </p></section></section><section class="chapter" id="storage-faqs" data-id-title="Frequently Asked Questions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21 </span><span class="title-name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="#storage-faqs">#</a></h2></div></div></div><section class="sect1" id="storage-bp-tuneups-pg-num" data-id-title="How Does the Number of Placement Groups Affect the Cluster Performance?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.1 </span><span class="title-name">How Does the Number of Placement Groups Affect the Cluster Performance?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-pg-num">#</a></h2></div></div></div><p>
   When your cluster is becoming 70% to 80% full, it is time to add more OSDs
   to it. When you increase the number of OSDs, you may consider increasing the
   number of placement groups as well.
  </p><div id="id-1.3.8.3.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Changing the number of PGs causes a lot of data transfer within the
    cluster.
   </p></div><p>
   To calculate the optimal value for your newly-resized cluster is a complex
   task.
  </p><p>
   A high number of PGs creates small chunks of data. This speeds up recovery
   after an OSD failure, but puts a lot of load on the monitor nodes as they
   are responsible for calculating the data location.
  </p><p>
   On the other hand, a low number of PGs takes more time and data transfer to
   recover from an OSD failure, but does not impose that much load on monitor
   nodes as they need to calculate locations for less (but larger) data chunks.
  </p></section><section class="sect1" id="storage-bp-tuneups-mix-ssd" data-id-title="Can I Use SSDs and Hard Disks on the Same Cluster?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.2 </span><span class="title-name">Can I Use SSDs and Hard Disks on the Same Cluster?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-mix-ssd">#</a></h2></div></div></div><p>
   Solid-state drives (SSD) are generally faster than hard disks. If you mix
   the two types of disks for the same write operation, the data writing to the
   SSD disk will be slowed down by the hard disk performance. Thus, you should
   <span class="emphasis"><em>never mix SSDs and hard disks</em></span> for data writing
   following <span class="emphasis"><em>the same rule</em></span> (see
   <a class="xref" href="#datamgm-rules" title="7.3. Rule Sets">Section 7.3, “Rule Sets”</a> for more information on rules for storing
   data).
  </p><p>
   There are generally 2 cases where using SSD and hard disk on the same
   cluster makes sense:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Use each disk type for writing data following different rules. Then you
     need to have a separate rule for the SSD disk, and another rule for the
     hard disk.
    </p></li><li class="listitem"><p>
     Use each disk type for a specific purpose. For example the SSD disk for
     journal, and the hard disk for storing data.
    </p></li></ol></div></section><section class="sect1" id="storage-bp-tuneups-ssd-tradeoffs" data-id-title="What are the Trade-offs of Using a Journal on SSD?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.3 </span><span class="title-name">What are the Trade-offs of Using a Journal on SSD?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-ssd-tradeoffs">#</a></h2></div></div></div><p>
   Using SSDs for OSD journal(s) is better for performance as the journal is
   usually the bottleneck of hard disk-only OSDs. SSDs are often used to share
   journals of several OSDs.
  </p><p>
   Following is a list of potential disadvantages of using SSDs for OSD
   journal:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     SSD disks are more expensive than hard disks. But as one OSD journal
     requires up to 6GB of disk space only, the price may not be so crucial.
    </p></li><li class="listitem"><p>
     SSD disk consumes storage slots which can be otherwise used by a large
     hard disk to extend the cluster capacity.
    </p></li><li class="listitem"><p>
     SSD disks have reduced write cycles compared to hard disks, but modern
     technologies are beginning to eliminate the problem.
    </p></li><li class="listitem"><p>
     If you share more journals on the same SSD disk, you risk losing all the
     related OSDs after the SSD disk fails. This will require a lot of data to
     be moved to rebalance the cluster.
    </p></li><li class="listitem"><p>
     Hotplugging disks becomes more complex as the data mapping is not 1:1 the
     failed OSD and the journal disk.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-diskfails" data-id-title="What Happens When a Disk Fails?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.4 </span><span class="title-name">What Happens When a Disk Fails?</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-diskfails">#</a></h2></div></div></div><p>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The related OSD crashed and is automatically removed from the cluster.
    </p></li><li class="listitem"><p>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </p></li><li class="listitem"><p>
     Then you should remove the disk from the cluster CRUSH Map, and
     physically from the host hardware.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-journalfails" data-id-title="What Happens When a Journal Disk Fails?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.5 </span><span class="title-name">What Happens When a Journal Disk Fails?</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-journalfails">#</a></h2></div></div></div><p>
   Ceph can be configured to store journals or write ahead logs on devices
   separate from the OSDs.  When a disk dedicated to a journal fails, the
   related OSD(s) fail as well
    (see
   <a class="xref" href="#storage-bp-monitoring-diskfails" title="21.4. What Happens When a Disk Fails?">Section 21.4, “What Happens When a Disk Fails?”</a>).
  </p><div id="id-1.3.8.3.8.3" data-id-title="Hosting Multiple Journals on One Disk" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Hosting Multiple Journals on One Disk</h6><p>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </p></div></section></section><section class="chapter" id="storage-troubleshooting" data-id-title="Troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22 </span><span class="title-name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#storage-troubleshooting">#</a></h2></div></div></div><p>
  This chapter describes several issues that you may face when you operate a
  Ceph cluster.
 </p><section class="sect1" id="storage-bp-report-bug" data-id-title="Reporting Software Problems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.1 </span><span class="title-name">Reporting Software Problems</span> <a title="Permalink" class="permalink" href="#storage-bp-report-bug">#</a></h2></div></div></div><p>
   If you come across a problem when running SUSE Enterprise Storage 5.5 related to some of its
   components, such as Ceph or Object Gateway, report the problem to SUSE Technical
   Support. The recommended way is with the <code class="command">supportconfig</code>
   utility.
  </p><div id="id-1.3.8.4.4.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    Because <code class="command">supportconfig</code> is modular software, make sure
    that the <code class="systemitem">supportutils-plugin-ses</code> package is
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rpm -q supportutils-plugin-ses</pre></div><p>
    If it is missing on the Ceph server, install it with
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</pre></div></div><p>
   Although you can use <code class="command">supportconfig</code> on the command line,
   we recommend using the related YaST module. Find more information about
   <code class="command">supportconfig</code> in
   <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-admsupport-supportconfig" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-admsupport-supportconfig</a>.
  </p></section><section class="sect1" id="storage-bp-cluster-mntc-rados-striping" data-id-title="Sending Large Objects with rados Fails with Full OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.2 </span><span class="title-name">Sending Large Objects with <code class="command">rados</code> Fails with Full OSD</span> <a title="Permalink" class="permalink" href="#storage-bp-cluster-mntc-rados-striping">#</a></h2></div></div></div><p>
   <code class="command">rados</code> is a command line utility to manage RADOS object
   storage. For more information, see <code class="command">man 8 rados</code>.
  </p><p>
   If you send a large object to a Ceph cluster with the
   <code class="command">rados</code> utility, such as
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados -p mypool put myobject /file/to/send</pre></div><p>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance.
  </p></section><section class="sect1" id="ceph-xfs-corruption" data-id-title="Corrupted XFS File system"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.3 </span><span class="title-name">Corrupted XFS File system</span> <a title="Permalink" class="permalink" href="#ceph-xfs-corruption">#</a></h2></div></div></div><p>
   In rare circumstances like kernel bug or broken/misconfigured hardware, the
   underlying file system (XFS) in which an OSD stores its data might be
   damaged and unmountable.
  </p><p>
   If you are sure there is no problem with your hardware and the system is
   configured properly, raise a bug against the XFS subsystem of the SUSE Linux Enterprise Server
   kernel and mark the particular OSD as down:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd down <em class="replaceable">OSD identification</em></pre></div><div id="id-1.3.8.4.6.5" data-id-title="Do Not Format or Otherwise Modify the Damaged Device" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Do Not Format or Otherwise Modify the Damaged Device</h6><p>
    Even though using <code class="command">xfs_repair</code> to fix the problem in the
    file system may seem reasonable, do not use it as the command modifies the
    file system. The OSD may start but its functioning may be influenced.
   </p></div><p>
   Now zap the underlying disk and re-create the OSD by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-disk prepare --zap $OSD_DISK_DEVICE $OSD_JOURNAL_DEVICE"</pre></div><p>
   for example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-disk prepare --zap /dev/sdb /dev/sdd2</pre></div></section><section class="sect1" id="storage-bp-recover-toomanypgs" data-id-title="Too Many PGs per OSD Status Message"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.4 </span><span class="title-name">'Too Many PGs per OSD' Status Message</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-toomanypgs">#</a></h2></div></div></div><p>
   If you receive a <code class="literal">Too Many PGs per OSD</code> message after
   running <code class="command">ceph status</code>, it means that the
   <code class="option">mon_pg_warn_max_per_osd</code> value (300 by default) was
   exceeded. This value is compared to the number of PGs per OSD ratio. This
   means that the cluster setup is not optimal.
  </p><p>
   The number of PGs cannot be reduced after the pool is created. Pools that do
   not yet contain any data can safely be deleted and then re-created with a
   lower number of PGs. Where pools already contain data, the only solution is
   to add OSDs to the cluster so that the ratio of PGs per OSD becomes lower.
  </p></section><section class="sect1" id="storage-bp-recover-stuckinactive" data-id-title="nn pg stuck inactive Status Message"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.5 </span><span class="title-name">'<span class="emphasis"><em>nn</em></span> pg stuck inactive' Status Message</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-stuckinactive">#</a></h2></div></div></div><p>
   If you receive a <code class="literal">stuck inactive</code> status message after
   running <code class="command">ceph status</code>, it means that Ceph does not know
   where to replicate the stored data to fulfill the replication rules. It can
   happen shortly after the initial Ceph setup and fix itself automatically.
   In other cases, this may require a manual interaction, such as bringing up a
   broken OSD, or adding a new OSD to the cluster. In very rare cases, reducing
   the replication level may help.
  </p><p>
   If the placement groups are stuck perpetually, you need to check the output
   of <code class="command">ceph osd tree</code>. The output should look tree-structured,
   similar to the example in <a class="xref" href="#storage-bp-recover-osddown" title="22.7. OSD is Down">Section 22.7, “OSD is Down”</a>.
  </p><p>
   If the output of <code class="command">ceph osd tree</code> is rather flat as in the
   following example
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</pre></div><p>
   You should check that the related CRUSH map has a tree structure. If it is
   also flat, or with no hosts as in the above example, it may mean that host
   name resolution is not working correctly across the cluster.
  </p><p>
   If the hierarchy is incorrect—for example the root contains hosts, but
   the OSDs are at the top level and are not themselves assigned to
   hosts—you will need to move the OSDs to the correct place in the
   hierarchy. This can be done using the <code class="command">ceph osd crush move</code>
   and/or <code class="command">ceph osd crush set</code> commands. For further details
   see <a class="xref" href="#op-crush" title="7.4. CRUSH Map Manipulation">Section 7.4, “CRUSH Map Manipulation”</a>.
  </p></section><section class="sect1" id="storage-bp-recover-osdweight" data-id-title="OSD Weight is 0"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.6 </span><span class="title-name">OSD Weight is 0</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-osdweight">#</a></h2></div></div></div><p>
   When OSD starts, it is assigned a weight. The higher the weight, the bigger
   the chance that the cluster writes data to the OSD. The weight is either
   specified in a cluster CRUSH Map, or calculated by the OSDs' start-up
   script.
  </p><p>
   In some cases, the calculated value for OSDs' weight may be rounded down to
   zero. It means that the OSD is not scheduled to store data, and no data is
   written to it. The reason is usually that the disk is too small (smaller
   than 15GB) and should be replaced with a bigger one.
  </p></section><section class="sect1" id="storage-bp-recover-osddown" data-id-title="OSD is Down"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.7 </span><span class="title-name">OSD is Down</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-osddown">#</a></h2></div></div></div><p>
   OSD daemon is either running, or stopped/down. There are 3 general reasons
   why an OSD is down:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hard disk failure.
    </p></li><li class="listitem"><p>
     The OSD crashed.
    </p></li><li class="listitem"><p>
     The server crashed.
    </p></li></ul></div><p>
   You can see the detailed status of OSDs by running
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</pre></div><p>
   The example listing shows that the <code class="literal">osd.2</code> is down. Then
   you may check if the disk where the OSD is located is mounted:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</pre></div><p>
   You can track the reason why the OSD is down by inspecting its log file
   <code class="filename">/var/log/ceph/ceph-osd.2.log</code>. After you find and fix
   the reason why the OSD is not running, start it with
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd@2.service</pre></div><p>
   Do not forget to replace <code class="literal">2</code> with the actual number of your
   stopped OSD.
  </p></section><section class="sect1" id="storage-bp-performance-slowosd" data-id-title="Finding Slow OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.8 </span><span class="title-name">Finding Slow OSDs</span> <a title="Permalink" class="permalink" href="#storage-bp-performance-slowosd">#</a></h2></div></div></div><p>
   When tuning the cluster performance, it is very important to identify slow
   storage/OSDs within the cluster. The reason is that if the data is written
   to the slow(est) disk, the complete write operation slows down as it always
   waits until it is finished on all the related disks.
  </p><p>
   It is not trivial to locate the storage bottleneck. You need to examine each
   and every OSD to find out the ones slowing down the write process. To do a
   benchmark on a single OSD, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="command">ceph tell</code> osd.<em class="replaceable">OSD_ID_NUMBER</em> bench</pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</pre></div><p>
   Then you need to run this command on each OSD and compare the
   <code class="literal">bytes_per_sec</code> value to get the slow(est) OSDs.
  </p></section><section class="sect1" id="storage-bp-recover-clockskew" data-id-title="Fixing Clock Skew Warnings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.9 </span><span class="title-name">Fixing Clock Skew Warnings</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-clockskew">#</a></h2></div></div></div><p>
   The time information in all cluster nodes must be synchronized. If a node's
   time is not fully synchronized, you may get clock skew warnings when
   checking the state of the cluster.
  </p><p>
   Time synchronization is managed with NTP (see
   <a class="link" href="http://en.wikipedia.org/wiki/Network_Time_Protocol" target="_blank">http://en.wikipedia.org/wiki/Network_Time_Protocol</a>).
   Set each node to synchronize its time with one or more NTP servers,
   preferably to the same group of NTP servers. If the time skew still occurs
   on a node, follow these steps to fix it:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ntpd.service
<code class="prompt user">root # </code>systemctl stop ceph-mon.target
<code class="prompt user">root # </code>systemctl start ntpd.service
<code class="prompt user">root # </code>systemctl start ceph-mon.target</pre></div><p>
   You can then query the NTP peers and check the time offset with
   <code class="command">sudo ntpq -p</code>.
  </p><p>
   The Ceph monitors need to have their clocks synchronized to within 0.05
   seconds of each other. Refer to <a class="xref" href="#Cluster-Time-Setting" title="20.4. Time Synchronization of Nodes">Section 20.4, “Time Synchronization of Nodes”</a> for
   more information.
  </p></section><section class="sect1" id="storage-bp-performance-net-issues" data-id-title="Poor Cluster Performance Caused by Network Problems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.10 </span><span class="title-name">Poor Cluster Performance Caused by Network Problems</span> <a title="Permalink" class="permalink" href="#storage-bp-performance-net-issues">#</a></h2></div></div></div><p>
   There are more reasons why the cluster performance may become weak. One of
   them can be network problems. In such case, you may notice the cluster
   reaching quorum, OSD and monitor nodes going offline, data transfers taking
   a long time, or a lot of reconnect attempts.
  </p><p>
   To check whether cluster performance is degraded by network problems,
   inspect the Ceph log files under the <code class="filename">/var/log/ceph</code>
   directory.
  </p><p>
   To fix network issues on the cluster, focus on the following points:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Basic network diagnostics. Try DeepSea diagnostics tools runner
     <code class="literal">net.ping</code> to ping between cluster nodes to see if
     individual interface can reach to specific interface and the average
     response time. Any specific response time much slower then average will
     also be reported. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.ping
  Succeeded: 8 addresses from 7 minions average rtt 0.15 ms</pre></div><p>
     Try validating all interface with JumboFrame enable:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.jumbo_ping
  Succeeded: 8 addresses from 7 minions average rtt 0.26 ms</pre></div></li><li class="listitem"><p>
     Network performance benchmark. Try DeepSea's network performance runner
     <code class="literal">net.iperf</code> to test intern-node network bandwidth. On a
     given cluster node, a number of <code class="command">iperf</code> processes
     (according to the number of CPU cores) are started as servers. The
     remaining cluster nodes will be used as clients to generate network
     traffic. The accumulated bandwidth of all per-node
     <code class="command">iperf</code> processes is reported. This should reflect the
     maximum achievable network throughput on all cluster nodes. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.iperf cluster=ceph output=full
192.168.128.1:
    8644.0 Mbits/sec
192.168.128.2:
    10360.0 Mbits/sec
192.168.128.3:
    9336.0 Mbits/sec
192.168.128.4:
    9588.56 Mbits/sec
192.168.128.5:
    10187.0 Mbits/sec
192.168.128.6:
    10465.0 Mbits/sec</pre></div></li><li class="listitem"><p>
     Check firewall settings on cluster nodes. Make sure they do not block
     ports/protocols required by Ceph operation. See
     <a class="xref" href="#storage-bp-net-firewall" title="20.10. Firewall Settings for Ceph">Section 20.10, “Firewall Settings for Ceph”</a> for more information on firewall
     settings.
    </p></li><li class="listitem"><p>
     Check the networking hardware, such as network cards, cables, or switches,
     for proper operation.
    </p></li></ul></div><div id="id-1.3.8.4.13.6" data-id-title="Separate Network" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Separate Network</h6><p>
    To ensure fast and safe network communication between cluster nodes, set up
    a separate network used exclusively by the cluster OSD and monitor nodes.
   </p></div></section><section class="sect1" id="trouble-jobcache" data-id-title="/var Running Out of Space"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.11 </span><span class="title-name"><code class="filename">/var</code> Running Out of Space</span> <a title="Permalink" class="permalink" href="#trouble-jobcache">#</a></h2></div></div></div><p>
   By default, the Salt master saves every minion's return for every job in its
   <span class="emphasis"><em>job cache</em></span>. The cache can then be used later to lookup
   results for previous jobs. The cache directory defaults to
   <code class="filename">/var/cache/salt/master/jobs/</code>.
  </p><p>
   Each job return from every minion is saved in a single file. Over time this
   directory can grow very large, depending on the number of published jobs and
   the value of the <code class="option">keep_jobs</code> option in the
   <code class="filename">/etc/salt/master</code> file. <code class="option">keep_jobs</code> sets
   the number of hours (24 by default) to keep information about past minion
   jobs.
  </p><div class="verbatim-wrap"><pre class="screen">keep_jobs: 24</pre></div><div id="id-1.3.8.4.14.5" data-id-title="Do Not Set keep_jobs: 0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Set <code class="option">keep_jobs: 0</code></h6><p>
    Setting <code class="option">keep_jobs</code> to '0' will cause the job cache cleaner
    to <span class="emphasis"><em>never</em></span> run, possibly resulting in a full partition.
   </p></div><p>
   If you want to disable the job cache, set <code class="option">job_cache</code> to
   'False':
  </p><div class="verbatim-wrap"><pre class="screen">job_cache: False</pre></div><div id="id-1.3.8.4.14.8" data-id-title="Restoring Partition Full because of Job Cache" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Restoring Partition Full because of Job Cache</h6><p>
    When the partition with job cache files gets full because of wrong
    <code class="option">keep_jobs</code> setting, follow these steps to free disk space
    and improve the job cache settings:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop the Salt master service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-master</pre></div></li><li class="step"><p>
      Change the Salt master configuration related to job cache by editing
      <code class="filename">/etc/salt/master</code>:
     </p><div class="verbatim-wrap"><pre class="screen">job_cache: False
keep_jobs: 1</pre></div></li><li class="step"><p>
      Clear the Salt master job cache:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rm -rfv /var/cache/salt/master/jobs/*</pre></div></li><li class="step"><p>
      Start the Salt master service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl start salt-master</pre></div></li></ol></div></div></div></section><section class="sect1" id="ceph-fix-too-many-pgs" data-id-title="Too Many PGs Per OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.12 </span><span class="title-name">Too Many PGs Per OSD</span> <a title="Permalink" class="permalink" href="#ceph-fix-too-many-pgs">#</a></h2></div></div></div><p>
     The <code class="option">TOO_MANY_PGS</code> flag is raised when the number of PGs in
     use is above the configurable threshold of <code class="option">mon_pg_warn_max_per_osd</code>
     PGs per OSD. If this threshold is exceeded, the cluster does not allow new pools
     to be created, pool <code class="option">pg_num</code> to be increased, or pool
     replication to be increased.
   </p><p>
     SUSE Enterprise Storage 4 and 5.5 have two ways to
     solve this issue.
   </p><div class="procedure" id="id-1.3.8.4.15.4" data-id-title="Solution 1"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 22.1: </span><span class="title-name">Solution 1 </span><a title="Permalink" class="permalink" href="#id-1.3.8.4.15.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Set the following in your <code class="filename">ceph.conf</code>:
       </p><div class="verbatim-wrap"><pre class="screen">[global]

mon_max_pg_per_osd = 800  #  depends on your amount of PGs
osd max pg per osd hard ratio = 10 #  the default is 2. We recommend to set at least 5.
mon allow pool delete = true # without it you can't remove a pool</pre></div></li><li class="step"><p>
         Restart all MONs and OSDs one by one.
       </p></li><li class="step"><p>
         Check the value of your MON and OSD ID:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --admin-daemon /var/run/ceph/ceph-mon.<em class="replaceable">ID</em>.asok config get  mon_max_pg_per_osd
<code class="prompt user">cephadm &gt; </code>ceph --admin-daemon /var/run/ceph/ceph-osd.<em class="replaceable">ID</em>.asok config get osd_max_pg_per_osd_hard_ratio</pre></div></li><li class="step"><p>
         Execute the following to determine your default <code class="option">pg_num</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados lspools
<code class="prompt user">cephadm &gt; </code>ceph osd pool get <em class="replaceable">USER-EMAIL</em> pg_num</pre></div></li><li class="step"><p>
          <span class="emphasis"><em>With caution</em></span>, execute the following commands to remove pools:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create <em class="replaceable">USER-EMAIL</em>new 8
<code class="prompt user">cephadm &gt; </code>rados cppool <em class="replaceable">USER-EMAIL</em> default.rgw.lc.new
<code class="prompt user">cephadm &gt; </code>ceph osd pool delete <em class="replaceable">USER-EMAIL</em> <em class="replaceable">USER-EMAIL</em> --yes-i-really-really-mean-it
<code class="prompt user">cephadm &gt; </code>ceph osd pool rename <em class="replaceable">USER-EMAIL</em>.new <em class="replaceable">USER-EMAIL</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool application enable <em class="replaceable">USER-EMAIL</em> rgw</pre></div><p>If this does not remove enough PGs per OSD and you are still
       receiving blocking requests, you may need to find another pool
       to remove.
     </p></li></ol></div></div><div class="procedure" id="id-1.3.8.4.15.5" data-id-title="Solution 2"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 22.2: </span><span class="title-name">Solution 2 </span><a title="Permalink" class="permalink" href="#id-1.3.8.4.15.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Create a new pool with the correct PG count:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create <em class="replaceable">NEW-POOL</em> <em class="replaceable">PG-COUNT</em></pre></div></li><li class="step"><p>
         Copy the contents of the old pool the new pool:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados cppool <em class="replaceable">OLD-POOL</em> <em class="replaceable">NEW-POOL</em></pre></div></li><li class="step"><p>
         Remove the old pool:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool delete <em class="replaceable">OLD-POOL</em> <em class="replaceable">OLD-POOL</em> --yes-i-really-really-mean-it</pre></div></li><li class="step"><p>
         Rename the new pool:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool rename <em class="replaceable">NEW-POOL</em> <em class="replaceable">OLD-POOL</em></pre></div></li><li class="step"><p>
         Restart the Object Gateway.
       </p></li></ol></div></div></section></section></div><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#gloss-storage-glossary">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="gloss-storage-general" data-id-title="General"><h3 class="title">General</h3><dl><dt id="gloss-storage-adminode"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#gloss-storage-adminode">#</a></span></dt><dd class="glossdef"><p>
     The node from which you run the <code class="command">ceph-deploy</code> utility to
     deploy Ceph on OSD nodes.
    </p></dd><dt id="gloss-storage-bucket"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#gloss-storage-bucket">#</a></span></dt><dd class="glossdef"><p>
     A point which aggregates other nodes into a hierarchy of physical
     locations.
    </p></dd><dt id="gloss-storage-crush"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#gloss-storage-crush">#</a></span></dt><dd class="glossdef"><p>
     An algorithm that determines how to store and retrieve data by computing
     data storage locations. CRUSH requires a map of the cluster to
     pseudo-randomly store and retrieve data in OSDs with a uniform
     distribution of data across the cluster.
    </p></dd><dt id="gloss-storage-mon"><span><span class="glossterm">Monitor node, MON</span> <a title="Permalink" class="permalink" href="#gloss-storage-mon">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </p></dd><dt id="gloss-storage-node"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#gloss-storage-node">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="gloss-storage-osd"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#gloss-storage-osd">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.3.9.3.8"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.9.3.8">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.3.9.3.9"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.3.9.3.9">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd></dl></div><div class="glossdiv" id="gloss-storage-ceph" data-id-title="Ceph Specific Terms"><h3 class="title">Ceph Specific Terms</h3><dl><dt id="gloss-storage-ceph-storage-cluster"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#gloss-storage-ceph-storage-cluster">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user’s data. Such a
     set consists of Ceph monitors and OSDs.
    </p><p>
     AKA <span class="quote">“<span class="quote">Ceph Object Store</span>”</span>.
    </p></dd><dt id="gloss-storage-rgw"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#gloss-storage-rgw">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store.
    </p></dd></dl></div></section><section class="appendix" id="app-stage1-custom" data-id-title="DeepSea Stage 1 Custom Example"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span> <a title="Permalink" class="permalink" href="#app-stage1-custom">#</a></h1></div></div></div><div class="verbatim-wrap"><pre class="screen">{% set master = salt['master.minion']() %}

include:
  - ..validate

ready:
  salt.runner:
    - name: minions.ready
    - timeout: {{ salt['pillar.get']('ready_timeout', 300) }}

refresh_pillar0:
  salt.state:
    - tgt: {{ master }}
    - sls: ceph.refresh

discover roles:
  salt.runner:
    - name: populate.proposals
    - require:
        - salt: refresh_pillar0


discover storage profiles:
  salt.runner:
    - name: proposal.populate
    - kwargs:
        'name': 'prod'
        'db-size': '59G'
        'wal-size': '1G'
        'nvme-spinner': True
        'ratio': 12
    - require:
        - salt: refresh_pillar0</pre></div></section><section class="appendix" id="app-alerting-default" data-id-title="Default Alerts for SUSE Enterprise Storage"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">B </span><span class="title-name">Default Alerts for SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#app-alerting-default">#</a></h1></div></div></div><div class="verbatim-wrap"><pre class="screen">groups:
 - name: cluster health
  rules:
   - alert: health error
    expr: ceph_health_status == 2
    for: 5m
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: Ceph in error for &gt; 5m
   - alert: unhealthy
    expr: ceph_health_status != 0
    for: 15m
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: Ceph not healthy for &gt; 5m
 - name: mon
  rules:
   - alert: low monitor quorum count
    expr: ceph_monitor_quorum_count &lt; 3
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: Monitor count in quorum is low
 - name: osd
  rules:
   - alert: 10% OSDs down
    expr: sum(ceph_osd_down) / count(ceph_osd_in) &gt;= 0.1
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: More then 10% of OSDS are down
   - alert: OSD down
    expr: sum(ceph_osd_down) &gt; 1
    for: 15m
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: One or more OSDS down for more then 15 minutes
   - alert: OSDs near full
    expr: (ceph_osd_utilization unless on(osd) ceph_osd_down) &gt; 80
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: OSD {{ $labels.osd }} is dangerously full, over 80%
   # alert on single OSDs flapping
   - alert: flap osd
    expr: rate(ceph_osd_up[5m])*60 &gt; 1
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
       OSD {{ $label.osd }} was marked down at back up at least once a
       minute for 5 minutes.
   # alert on high deviation from average PG count
   - alert: high pg count deviation
    expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
    for: 5m
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
       OSD {{ $labels.osd }} deviates by more then 30% from
       average PG count
   # alert on high commit latency...but how high is too high
 - name: mds
  rules:
  # no mds metrics are exported yet
 - name: mgr
  rules:
  # no mgr metrics are exported yet
 - name: pgs
  rules:
   - alert: pgs inactive
    expr: ceph_total_pgs - ceph_active_pgs &gt; 0
    for: 5m
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: One or more PGs are inactive for more then 5 minutes.
   - alert: pgs unclean
    expr: ceph_total_pgs - ceph_clean_pgs &gt; 0
    for: 15m
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: One or more PGs are not clean for more then 15 minutes.
 - name: nodes
  rules:
   - alert: root volume full
    expr: node_filesystem_avail{mountpoint="/"} / node_filesystem_size{mountpoint="/"} &lt; 0.1
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: Root volume (OSD and MON store) is dangerously full (&lt; 10% free)
   # alert on nic packet errors and drops rates &gt; 1 packet/s
   - alert: network packets dropped
    expr: irate(node_network_receive_drop{device!="lo"}[5m]) + irate(node_network_transmit_drop{device!="lo"}[5m]) &gt; 1
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
      Node {{ $labels.instance }} experiences packet drop &gt; 1
      packet/s on interface {{ $lables.device }}
   - alert: network packet errors
    expr: irate(node_network_receive_errs{device!="lo"}[5m]) + irate(node_network_transmit_errs{device!="lo"}[5m]) &gt; 1
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
      Node {{ $labels.instance }} experiences packet errors &gt; 1
      packet/s on interface {{ $lables.device }}
   # predict fs fillup times
   - alert: storage filling
    expr: ((node_filesystem_free - node_filesystem_size) / deriv(node_filesystem_free[2d]) &lt;= 5) &gt; 0
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
      Mountpoint {{ $lables.mountpoint }} will be full in less then 5 days
      assuming the average fillup rate of the past 48 hours.
 - name: pools
  rules:
   - alert: pool full
    expr: ceph_pool_used_bytes / ceph_pool_available_bytes &gt; 0.9
    labels:
     severity: critical
     type: ses_default
    annotations:
     description: Pool {{ $labels.pool }} at 90% capacity or over
   - alert: pool filling up
    expr: (-ceph_pool_used_bytes / deriv(ceph_pool_available_bytes[2d]) &lt;= 5 ) &gt; 0
    labels:
     severity: warning
     type: ses_default
    annotations:
     description: &gt;
      Pool {{ $labels.pool }} will be full in less then 5 days
      assuming the average fillup rate of the past 48 hours.</pre></div></section><section class="appendix" id="app-storage-manual-inst" data-id-title="Example Procedure of Manual Ceph Installation"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">C </span><span class="title-name">Example Procedure of Manual Ceph Installation</span> <a title="Permalink" class="permalink" href="#app-storage-manual-inst">#</a></h1></div></div></div><p>
  The following procedure shows the commands that you need to install Ceph
  storage cluster manually.
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Generate the key secrets for the Ceph services you plan to run. You can
    use the following command to generate it:
   </p><div class="verbatim-wrap"><pre class="screen">python -c "import os ; import struct ; import time; import base64 ; \
 key = os.urandom(16) ; header = struct.pack('&lt;hiih',1,int(time.time()),0,len(key)) ; \
 print base64.b64encode(header + key)"</pre></div></li><li class="step"><p>
    Add the keys to the related keyrings. First for
    <code class="systemitem">client.admin</code>, then for monitors,
    and then other related services, such as OSD, Object Gateway, or MDS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool -n client.admin \
 --create-keyring /etc/ceph/ceph.client.admin.keyring \
 --cap mds 'allow *' --cap mon 'allow *' --cap osd 'allow *'
ceph-authtool -n mon. \
 --create-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --set-uid=0 --cap mon 'allow *'
ceph-authtool -n client.bootstrap-osd \
 --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring \
 --cap mon 'allow profile bootstrap-osd'
ceph-authtool -n client.bootstrap-rgw \
 --create-keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring \
 --cap mon 'allow profile bootstrap-rgw'
ceph-authtool -n client.bootstrap-mds \
 --create-keyring /var/lib/ceph/bootstrap-mds/ceph.keyring \
 --cap mon 'allow profile bootstrap-mds'</pre></div></li><li class="step"><p>
    Create a monmap—a database of all monitors in a cluster:
   </p><div class="verbatim-wrap"><pre class="screen">monmaptool --create --fsid eaac9695-4265-4ca8-ac2a-f3a479c559b1 \
 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-02 192.168.43.60 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-03 192.168.43.96 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-04 192.168.43.80 /tmp/tmpuuhxm3/monmap</pre></div></li><li class="step"><p>
    Create a new keyring and import keys from the admin and monitors' keyrings
    there. Then use them to start the monitors:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-authtool --create-keyring /tmp/tmpuuhxm3/keyring \
 --import-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring
ceph-authtool /tmp/tmpuuhxm3/keyring \
 --import-keyring /etc/ceph/ceph.client.admin.keyring
sudo -u ceph ceph-mon --mkfs -i osceph-03 \
 --monmap /tmp/tmpuuhxm3/monmap --keyring /tmp/tmpuuhxm3/keyring
systemctl restart ceph-mon@osceph-03</pre></div></li><li class="step"><p>
    Check the monitors state in <code class="systemitem">systemd</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl show --property ActiveState ceph-mon@osceph-03</pre></div></li><li class="step"><p>
    Check if Ceph is running and reports the monitor status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status</pre></div></li><li class="step"><p>
    Check the specific services' status using the existing keys:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status
[...]
ceph --connect-timeout 5 \
 --keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --name mon. -f json-pretty status</pre></div></li><li class="step"><p>
    Import keyring from existing Ceph services and check the status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth import -i /var/lib/ceph/bootstrap-osd/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-rgw/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-mds/ceph.keyring
ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status</pre></div></li><li class="step"><p>
    Prepare disks/partitions for OSDs, using the XFS file system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdb
ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdc
[...]</pre></div></li><li class="step"><p>
    Activate the partitions:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph-disk -v activate --mark-init systemd --mount /dev/vdb1
ceph-disk -v activate --mark-init systemd --mount /dev/vdc1</pre></div></li><li class="step"><p>
    For SUSE Enterprise Storage version 2.1 and earlier, create the default pools:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.swift 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .intent-log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.gc 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.uid 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.control 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .usage 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw 16 16</pre></div></li><li class="step"><p>
    Create the Object Gateway instance key from the bootstrap key:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-rgw \
 --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create \
 client.rgw.0dc1e13033d2467eace46270f0048b39 osd 'allow rwx' mon 'allow rw' \
 -o /var/lib/ceph/radosgw/ceph-rgw.<em class="replaceable">rgw_name</em>/keyring</pre></div></li><li class="step"><p>
    Enable and start Object Gateway:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable ceph-radosgw@rgw.<em class="replaceable">rgw_name</em>
systemctl start ceph-radosgw@rgw.<em class="replaceable">rgw_name</em></pre></div></li><li class="step"><p>
    Optionally, create the MDS instance key from the bootstrap key, then enable
    and start it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-mds \
 --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create \
 mds.mds.<em class="replaceable">rgw_name</em> osd 'allow rwx' mds allow mon \
 'allow profile mds' \
 -o /var/lib/ceph/mds/ceph-mds.<em class="replaceable">rgw_name</em>/keyring
systemctl enable ceph-mds@mds.<em class="replaceable">rgw_name</em>
systemctl start ceph-mds@mds.<em class="replaceable">rgw_name</em></pre></div></li></ol></div></div></section><section class="appendix" id="ap-adm-docupdate" data-id-title="Documentation Updates"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">D </span><span class="title-name">Documentation Updates</span> <a title="Permalink" class="permalink" href="#ap-adm-docupdate">#</a></h1></div></div></div><p>
  This chapter lists content changes for this document since the initial
  release of SUSE Enterprise Storage 4. You can find changes related to the cluster
  deployment that apply to previous versions in
  <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ap-deploy-docupdate" target="_blank">https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ap-deploy-docupdate</a>.
 </p><p>
  The document was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-5main3" title="D.1. The Latest Documentation Update">Section D.1, “The Latest Documentation Update”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-5-5" title="D.2. October, 2018 (Documentation Maintenance Update)">Section D.2, “October, 2018 (Documentation Maintenance Update)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-5maint1" title="D.3. November 2017 (Documentation Maintenance Update)">Section D.3, “November 2017 (Documentation Maintenance Update)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-5" title="D.4. October, 2017 (Release of SUSE Enterprise Storage 5.5)">Section D.4, “October, 2017 (Release of SUSE Enterprise Storage 5.5)”</a>
   </p></li></ul></div><section class="sect1" id="sec-adm-docupdates-5main3" data-id-title="The Latest Documentation Update"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">D.1 </span><span class="title-name">The Latest Documentation Update</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-5main3">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.3.13.6.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added hint on verifying profile proposal in
     <a class="xref" href="#salt-node-add-disk" title="1.5. Adding an OSD Disk to a Node">Section 1.5, “Adding an OSD Disk to a Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1134736" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1134736</a>).
    </p></li><li class="listitem"><p>
     Remove minion's key from the Salt master temporarily when problematic in
     <a class="xref" href="#salt-node-removing" title="1.3. Removing and Reinstalling Cluster Nodes">Section 1.3, “Removing and Reinstalling Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1120662" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1120662</a>).
    </p></li><li class="listitem"><p>
     Added procedure on making <code class="filename">ceph.conf</code> parameters unique
     in <a class="xref" href="#ds-custom-cephconf" title="1.12. Adjusting ceph.conf with Custom Settings">Section 1.12, “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1116349" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1116349</a>).
    </p></li><li class="listitem"><p>
     Extended a tip on preventing rebalancing after adding a node in
     <a class="xref" href="#salt-adding-nodes" title="1.1. Adding New Cluster Nodes">Section 1.1, “Adding New Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1131044" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1131044</a>).
    </p></li><li class="listitem"><p>
     Added performance notes to <span class="intraxref">Book “Deployment Guide”, Chapter 13 “Exporting Ceph Data via Samba”</span> and
     <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 16. NFS Ganesha: Export Ceph Data via NFS">Chapter 16, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1124674" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1124674</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#device-classes-reclassify" title="7.1.1.5. Migrating from a Legacy SSD Rule to Device Classes">Section 7.1.1.5, “Migrating from a Legacy SSD Rule to Device Classes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1112883" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1112883</a>).
    </p></li><li class="listitem"><p>
     Added a list of corresponding software repositories in
     <a class="xref" href="#deepsea-rolling-updates" title="1.10. Updating the Cluster Nodes">Section 1.10, “Updating the Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1117474" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1117474</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#rbd-features" title="9.5. Advanced Features">Section 9.5, “Advanced Features”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1120706" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1120706</a>).
    </p></li><li class="listitem"><p>
     Extended examples of Ceph services identification in
     <a class="xref" href="#ceph-operating-services-finding-names" title="3.1.3. Identifying Individual Services">Section 3.1.3, “Identifying Individual Services”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1120682" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1120682</a>).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#ses-tiered-storage" title="11.6. Setting Up an Example Tiered Storage">Section 11.6, “Setting Up an Example Tiered Storage”</a> to use device classes
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1114827" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1114827</a>).
    </p></li><li class="listitem"><p>
     Added example to <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.1.5 “Updates and Reboots during Stage 0”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1119451" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1119451</a>).
    </p></li><li class="listitem"><p>
     Use multiple internal time sources in
     <a class="xref" href="#Cluster-Time-Setting" title="20.4. Time Synchronization of Nodes">Section 20.4, “Time Synchronization of Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1119571" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1119571</a>).
    </p></li><li class="listitem"><p>
     Added listing of pool snapshots in
     <a class="xref" href="#cha-ceph-snapshots-pool" title="8.4. Pool Snapshots">Section 8.4, “Pool Snapshots”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1113911" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1113911</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#crush-devclasses" title="7.1.1. Device Classes">Section 7.1.1, “Device Classes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1113292" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1113292</a>).
    </p></li><li class="listitem"><p>
     Added <code class="option">tcp_nodelay</code> explanation in
     <a class="xref" href="#sec-ceph-rgw-configuration" title="13.4. Configuration Parameters">Section 13.4, “Configuration Parameters”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1106274" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1106274</a>).
    </p></li><li class="listitem"><p>
     Added more details on pool compression in
     <a class="xref" href="#sec-ceph-pool-compression" title="8.5. Data Compression">Section 8.5, “Data Compression”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1113934" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1113934</a>).
    </p></li><li class="listitem"><p>
     Improved <a class="xref" href="#pool-migrate-cache-tier" title="8.3.2. Migrate Using Cache Tier">Section 8.3.2, “Migrate Using Cache Tier”</a> and added
     <a class="xref" href="#migrate-rbd-image" title="8.3.3. Migrating RBD Images">Section 8.3.3, “Migrating RBD Images”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1113900" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1113900</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#imp-removed-osd-in-grains" title="Important: Removed OSD ID Still Present in grains">Important: Removed OSD ID Still Present in grains</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107464" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107464</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-adm-docupdates-5-5" data-id-title="October, 2018 (Documentation Maintenance Update)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">D.2 </span><span class="title-name">October, 2018 (Documentation Maintenance Update)</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-5-5">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.3.13.7.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Moved <a class="xref" href="#tips-orphaned-partitions" title="20.1. Identifying Orphaned Partitions">Section 20.1, “Identifying Orphaned Partitions”</a> into
     <a class="xref" href="#storage-tips" title="Chapter 20. Hints and Tips">Chapter 20, <em>Hints and Tips</em></a>.
    </p></li><li class="listitem"><p>
     Extended <a class="xref" href="#oa-rbds" title="17.4.3. Managing RADOS Block Devices (RBDs)">Section 17.4.3, “Managing RADOS Block Devices (RBDs)”</a>, mainly added a section about snapshots
     (Fate #325642).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#pools-migration" title="8.3. Pool Migration">Section 8.3, “Pool Migration”</a> (Fate#322006).
    </p></li><li class="listitem"><p>
     Inserted <a class="xref" href="#Deepsea-restart" title="3.2. Restarting Ceph Services using DeepSea">Section 3.2, “Restarting Ceph Services using DeepSea”</a> into
     <a class="xref" href="#ceph-operating-services" title="Chapter 3. Operating Ceph Services">Chapter 3, <em>Operating Ceph Services</em></a>.
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-osd-replace" title="1.7. Replacing an OSD Disk">Section 1.7, “Replacing an OSD Disk”</a>.
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.3.13.7.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Extended <a class="xref" href="#osd-replace-auto" title="1.7.2. Automated Configuration">Section 1.7.2, “Automated Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1111442" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1111442</a>).
    </p></li><li class="listitem"><p>
     Added <code class="filename">/etc/ceph</code> to the list of backup content in
     <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Backing Up the Cluster Configuration”, Section 6.1 “Back Up Ceph COnfiguration”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1153342" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1153342</a>).
    </p></li><li class="listitem"><p>
     Removed 'default-update-no-reboot' as it is 'default' now, in
     <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.2 “Modifying Discovered Configuration”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1111318" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1111318</a>).
    </p></li><li class="listitem"><p>
     Fixed path to <code class="filename">*-replace</code> file in
     <a class="xref" href="#ds-osd-replace" title="1.7. Replacing an OSD Disk">Section 1.7, “Replacing an OSD Disk”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1111470" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1111470</a>).
     Removed AppArmor snippets from <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Upgrading from Previous Releases”</span> and added
     <a class="xref" href="#admin-apparmor" title="1.13. Enabling AppArmor Profiles">Section 1.13, “Enabling AppArmor Profiles”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1110861" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1110861</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#monitoring-alerting" title="Chapter 5. Monitoring and Alerting">Chapter 5, <em>Monitoring and Alerting</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107833" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107833</a>).
    </p></li><li class="listitem"><p>
     Disable kernel NFS on <code class="literal">role-ganesha</code> in
     <span class="intraxref">Book “Deployment Guide”, Chapter 12 “Installation of NFS Ganesha”, Section 12.1.2 “Summary of Requirements”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107625" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107625</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ganesha-rgw-supported-operations" title="16.3.3. Supported Operations">Section 16.3.3, “Supported Operations”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107624" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107624</a>).
    </p></li><li class="listitem"><p>
     Improved auto-replacement OSDs procedure in
     <a class="xref" href="#ds-osd-replace" title="1.7. Replacing an OSD Disk">Section 1.7, “Replacing an OSD Disk”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1107090" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1107090</a>).
    </p></li><li class="listitem"><p>
     Fixed libvirt keyring creation in <a class="xref" href="#ceph-libvirt-cfg-ceph" title="18.1. Configuring Ceph">Section 18.1, “Configuring Ceph”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1106495" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1106495</a>).
    </p></li><li class="listitem"><p>
     Simplified a user creation command in
     <a class="xref" href="#ceph-libvirt-cfg-ceph" title="18.1. Configuring Ceph">Section 18.1, “Configuring Ceph”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1102467" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1102467</a>).
    </p></li><li class="listitem"><p>
     Added 'tier' to the command in <a class="xref" href="#pool-migrate-cache-tier" title="8.3.2. Migrate Using Cache Tier">Section 8.3.2, “Migrate Using Cache Tier”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1102212" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1102212</a>).
    </p></li><li class="listitem"><p>
     Added a missing <code class="command">salt</code> command in
     <a class="xref" href="#osd-forced-removal" title="1.6.2. Removing Broken OSDs Forcefully">Section 1.6.2, “Removing Broken OSDs Forcefully”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1100701" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1100701</a>).
    </p></li><li class="listitem"><p>
     Added the <code class="literal">state.apply</code> part to the
     <code class="command">salt</code> in <a class="xref" href="#ds-osd-recover" title="1.8. Recovering a Reinstalled OSD Node">Section 1.8, “Recovering a Reinstalled OSD Node”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1095937" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1095937</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#oa-ssl" title="17.1.1. Enabling Secure Access to openATTIC using SSL">Section 17.1.1, “Enabling Secure Access to openATTIC using SSL”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1083216" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1083216</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-haproxy" title="13.12. Load Balancing the Object Gateway Servers with HAProxy">Section 13.12, “Load Balancing the Object Gateway Servers with HAProxy”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1093513" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1093513</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#scrubbing" title="7.5. Scrubbing">Section 7.5, “Scrubbing”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1079256" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1079256</a>).
    </p></li><li class="listitem"><p>
     Enhanced the port list for Firewall setting in
     <a class="xref" href="#storage-bp-net-firewall" title="20.10. Firewall Settings for Ceph">Section 20.10, “Firewall Settings for Ceph”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1070087" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1070087</a>).
    </p></li><li class="listitem"><p>
     Differed roles restarting as per DeepSea version in
     <a class="xref" href="#deepsea-restart-specific" title="3.2.2. Restarting Specific Services">Section 3.2.2, “Restarting Specific Services”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1091075" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1091075</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-mon" title="1.4. Redeploying Monitor Nodes">Section 1.4, “Redeploying Monitor Nodes”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1038731" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1038731</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-bucket-sharding-dyn" title="13.9.1.1. Dynamic Resharding">Section 13.9.1.1, “Dynamic Resharding”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1076001" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1076001</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-bucket-sharding" title="13.9. Bucket Index Sharding">Section 13.9, “Bucket Index Sharding”</a>
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1076000" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1076000</a>).
    </p></li><li class="listitem"><p>
     Updated the Object Gateway SSL the DeepSea way in
     <a class="xref" href="#ceph-rgw-https" title="13.6. Enabling HTTPS/SSL for Object Gateways">Section 13.6, “Enabling HTTPS/SSL for Object Gateways”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1083756" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1083756</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077809" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077809</a>).
    </p></li><li class="listitem"><p>
     Changes in DeepSea require a modified deployment of NFS Ganesha with
     Object Gateway. See <a class="xref" href="#ceph-nfsganesha-customrole-rgw-multiusers" title="16.3.1. Different Object Gateway Users for NFS Ganesha">Section 16.3.1, “Different Object Gateway Users for NFS Ganesha”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1058821" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1058821</a>).
     <code class="command">ceph osd pool create</code> fails if placement group limit per
     OSD is exceeded. See <a class="xref" href="#ceph-pools-operate-add-pool" title="8.2.2. Create a Pool">Section 8.2.2, “Create a Pool”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1076509" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1076509</a>)
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-osd-recover" title="1.8. Recovering a Reinstalled OSD Node">Section 1.8, “Recovering a Reinstalled OSD Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1057764" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1057764</a>).
    </p></li><li class="listitem"><p>
     Added a reliability warning in <a class="xref" href="#ceph-config-runtime" title="12.1. Runtime Configuration">Section 12.1, “Runtime Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=989349" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=989349</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-deploy" title="13.2. Deploying the Object Gateway">Section 13.2, “Deploying the Object Gateway”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1088895" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1088895</a>).
    </p></li><li class="listitem"><p>
     Removed <code class="literal">lz4</code> from the list of compression algorithms in
     <a class="xref" href="#sec-ceph-pool-bluestore-compression-options" title="8.5.3. Global Compression Options">Section 8.5.3, “Global Compression Options”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1088450" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1088450</a>).
    </p></li><li class="listitem"><p>
     Added a tip on removing multiple OSDs in
     <a class="xref" href="#salt-removing-osd" title="1.6. Removing an OSD">Section 1.6, “Removing an OSD”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1070791" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1070791</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#tips-stopping-osd-without-rebalancing" title="20.3. Stopping OSDs without Rebalancing">Section 20.3, “Stopping OSDs without Rebalancing”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051039" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051039</a>).
    </p></li><li class="listitem"><p>
     Added MDS cache size configurables in
     <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”, Section 11.2.2 “Configuring a Metadata Server”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1062692" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1062692</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-keystone" title="13.10. Integrating OpenStack Keystone">Section 13.10, “Integrating OpenStack Keystone”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077941" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077941</a>).
    </p></li><li class="listitem"><p>
     Added a tip about synchronizing iSCSI Gateway configuration in
     <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”, Section 10.4.3 “Export RBD Images via iSCSI”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073327" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073327</a>).
    </p></li><li class="listitem"><p>
     Changes in DeepSea require a modified deployment of NFS Ganesha with
     Object Gateway. See <a class="xref" href="#ceph-nfsganesha-customrole-rgw-multiusers" title="16.3.1. Different Object Gateway Users for NFS Ganesha">Section 16.3.1, “Different Object Gateway Users for NFS Ganesha”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1058821" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1058821</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-adm-docupdates-5maint1" data-id-title="November 2017 (Documentation Maintenance Update)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">D.3 </span><span class="title-name">November 2017 (Documentation Maintenance Update)</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-5maint1">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.3.13.8.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#storage-bd-replacing-disk" title="20.12. Replacing Storage Disk">Section 20.12, “Replacing Storage Disk”</a> (Fate #321032).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.3.13.8.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#ec-rbd" title="10.4. Erasure Coded Pools with RADOS Block Device">Section 10.4, “Erasure Coded Pools with RADOS Block Device”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1075158" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1075158</a>).
    </p></li><li class="listitem"><p>
     Added a section about adding a disk to OSD nodes. See
     <a class="xref" href="#salt-node-add-disk" title="1.5. Adding an OSD Disk to a Node">Section 1.5, “Adding an OSD Disk to a Node”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1066005" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1066005</a>)
    </p></li><li class="listitem"><p>
     <code class="command">salt-run remove.osd</code> requires OSD_ID digit without
     leading <code class="literal">osd.</code>. See <a class="xref" href="#salt-removing-osd" title="1.6. Removing an OSD">Section 1.6, “Removing an OSD”</a>.
    </p></li><li class="listitem"><p>
     <code class="command">ceph tell</code> requires OSD_ID digit and leading
     <code class="literal">osd.</code>. See
     <a class="xref" href="#storage-bp-performance-slowosd" title="22.8. Finding Slow OSDs">Section 22.8, “Finding Slow OSDs”</a>.
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#trouble-jobcache" title="22.11. /var Running Out of Space">Section 22.11, “<code class="filename">/var</code> Running Out of Space”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1069255" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1069255</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#sec-ceph-rgw-limits" title="13.1. Object Gateway Restrictions and Naming Limitations">Section 13.1, “Object Gateway Restrictions and Naming Limitations”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067613" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067613</a>).
    </p></li><li class="listitem"><p>
     Fixed rbd-mirror starting and stopping commands in
     <a class="xref" href="#rbd-mirror-daemon" title="9.4.1. rbd-mirror Daemon">Section 9.4.1, “rbd-mirror Daemon”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1068061" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1068061</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-adm-docupdates-5" data-id-title="October, 2017 (Release of SUSE Enterprise Storage 5.5)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">D.4 </span><span class="title-name">October, 2017 (Release of SUSE Enterprise Storage 5.5)</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-5">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.3.13.9.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Removed Calamari in favor of openATTIC.
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#oa-webui-nfs" title="17.4.6. Managing NFS Ganesha">Section 17.4.6, “Managing NFS Ganesha”</a> (Fate #321620).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-rbd-rbdmap" title="9.2.1. rbdmap: Map RBD Devices at Boot Time">Section 9.2.1, “rbdmap: Map RBD Devices at Boot Time”</a>
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-rbd" title="Chapter 9. RADOS Block Device">Chapter 9, <em>RADOS Block Device</em></a> (Fate #321061).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#oa-webui-rgw-users" title="17.4.9. Managing Object Gateway Users and Buckets">Section 17.4.9, “Managing Object Gateway Users and Buckets”</a> (Fate #320318).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-rgw-ldap" title="13.8. LDAP Authentication">Section 13.8, “LDAP Authentication”</a> (Fate #321631).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-cephfs-activeactive" title="15.4. Multiple Active MDS Daemons (Active-Active MDS)">Section 15.4, “Multiple Active MDS Daemons (Active-Active MDS)”</a> (Fate #322976).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#oa-webui-iscsi" title="17.4.7. Managing iSCSI Gateways">Section 17.4.7, “Managing iSCSI Gateways”</a> (Fate #321370).
    </p></li><li class="listitem"><p>
     Added iSCSI Gateway and Object Gateway configuration for openATTIC, see <a class="xref" href="#oa-rgw" title="17.1.5. Object Gateway Management">Section 17.1.5, “Object Gateway Management”</a>
     and <a class="xref" href="#oa-igw" title="17.1.6. iSCSI Gateway Management">Section 17.1.6, “iSCSI Gateway Management”</a> (Fate #320318 and #321370).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 16. NFS Ganesha: Export Ceph Data via NFS">Chapter 16, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036495" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036495</a>,
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1031444" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1031444</a>).
    </p></li><li class="listitem"><p>
     RBD images can now be stored in EC pools, see
     <a class="xref" href="#ceph-rbd-cmds-create-ec" title="9.1.2. Creating a Block Device Image in an Erasure Coded Pool">Section 9.1.2, “Creating a Block Device Image in an Erasure Coded Pool”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040752" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040752</a>).
    </p></li><li class="listitem"><p>
     Added section about backing up DeepSea configuration, see
     <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Backing Up the Cluster Configuration”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1046497" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1046497</a>).
    </p></li><li class="listitem"><p>
     Object Gateway failover and disaster recovery, see
     <a class="xref" href="#ceph-rgw-fed-failover" title="13.11.12. Failover and Disaster Recovery">Section 13.11.12, “Failover and Disaster Recovery”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036084" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036084</a>).
    </p></li><li class="listitem"><p>
     BlueStore enables data compression for pools, see
     <a class="xref" href="#sec-ceph-pool-compression" title="8.5. Data Compression">Section 8.5, “Data Compression”</a> (FATE#318582).
    </p></li><li class="listitem"><p>
     CIFS export of CephFS is possible, see <span class="intraxref">Book “Deployment Guide”, Chapter 13 “Exporting Ceph Data via Samba”</span>
     (FATE#321622).
    </p></li><li class="listitem"><p>
     Added procedure for cluster reboot, see
     <a class="xref" href="#sec-salt-cluster-reboot" title="1.11. Halting or Rebooting Cluster">Section 1.11, “Halting or Rebooting Cluster”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047638" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047638</a>).
    </p></li><li class="listitem"><p>
     DeepSea stage 0 can update without rebooting, see
     <a class="xref" href="#deepsea-rolling-updates" title="1.10. Updating the Cluster Nodes">Section 1.10, “Updating the Cluster Nodes”</a>.
    </p></li><li class="listitem"><p>
     <code class="command">ceph fs</code> replaced, see
     <a class="xref" href="#cephfs-activeactive-decrease" title="15.4.3. Decreasing the Number of Ranks">Section 15.4.3, “Decreasing the Number of Ranks”</a> and
     <a class="xref" href="#cephfs-activeactive-increase" title="15.4.2. Increasing the MDS Active Cluster Size">Section 15.4.2, “Increasing the MDS Active Cluster Size”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047638" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047638</a>).
    </p></li><li class="listitem"><p>
     Added section <a class="xref" href="#storage-bp-network-test" title="20.11. Testing Network Performance">Section 20.11, “Testing Network Performance”</a> (FATE#321031).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.3.13.9.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     The swift client package is now part of the 'Public Cloud' module in
     <a class="xref" href="#accessing-ragos-gateway" title="13.5.1. Accessing Object Gateway">Section 13.5.1, “Accessing Object Gateway”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1057591" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1057591</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-config-runtime" title="12.1. Runtime Configuration">Section 12.1, “Runtime Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1061435" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1061435</a>).
    </p></li><li class="listitem"><p>
     CivetWeb binds to multiple ports in
     <a class="xref" href="#rgw-civetweb-multiport" title="Tip: Binding to Multiple Ports">Tip: Binding to Multiple Ports</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1055181" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1055181</a>).
    </p></li><li class="listitem"><p>
     Included 3 Object Gateway options affecting performance in
     <a class="xref" href="#sec-ceph-rgw-configuration" title="13.4. Configuration Parameters">Section 13.4, “Configuration Parameters”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052983" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052983</a>).
    </p></li><li class="listitem"><p>
     Imported <a class="xref" href="#ds-custom-cephconf" title="1.12. Adjusting ceph.conf with Custom Settings">Section 1.12, “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</a> and added the need for Stage
     3
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1057273" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1057273</a>).
    </p></li><li class="listitem"><p>
     Added libvirt keyring creation in <a class="xref" href="#ceph-libvirt-cfg-ceph" title="18.1. Configuring Ceph">Section 18.1, “Configuring Ceph”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1055610" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1055610</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ex-ds-rmnode" title="Removing a Salt minion from the Cluster">Example 1.1, “Removing a Salt minion from the Cluster”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054516" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054516</a>).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#monitor-watch" title="4.3. Watching a Cluster">Section 4.3, “Watching a Cluster”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1053638" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1053638</a>).
    </p></li><li class="listitem"><p>
     Made Salt REST API variables optional in
     <a class="xref" href="#ceph-oa-installation" title="17.1. openATTIC Deployment and Configuration">Section 17.1, “openATTIC Deployment and Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054748" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054748</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054749" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054749</a>).
    </p></li><li class="listitem"><p>
     Removed <code class="command">oaconfig install</code> in
     <a class="xref" href="#ceph-oa-install-oa" title="17.1.3. openATTIC Initial Setup">Section 17.1.3, “openATTIC Initial Setup”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054747" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054747</a>).
    </p></li><li class="listitem"><p>
     Added a section about displaying a pool metadata in
     <a class="xref" href="#ceph-pools-associate" title="8.1. Associate Pools with an Application">Section 8.1, “Associate Pools with an Application”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1053327" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1053327</a>).
    </p></li><li class="listitem"><p>
     A list of health codes imported in <a class="xref" href="#monitor-health" title="4.2. Checking Cluster Health">Section 4.2, “Checking Cluster Health”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052939" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052939</a>).
    </p></li><li class="listitem"><p>
     Updated screenshots and related text in <a class="xref" href="#oa-rgw-user-add" title="17.4.9.1. Adding a New Object Gateway User">Section 17.4.9.1, “Adding a New Object Gateway User”</a>
     and <a class="xref" href="#oa-user-edit" title="17.4.9.3. Editing Object Gateway Users">Section 17.4.9.3, “Editing Object Gateway Users”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051814" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051814</a>
     and
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051816" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051816</a>).
    </p></li><li class="listitem"><p>
     Added Object Gateway buckets in <a class="xref" href="#oa-webui-rgw-users" title="17.4.9. Managing Object Gateway Users and Buckets">Section 17.4.9, “Managing Object Gateway Users and Buckets”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051800" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051800</a>).
    </p></li><li class="listitem"><p>
     Included cephx in mount examples in <a class="xref" href="#ceph-cephfs-krnldrv" title="15.1.3. Mount CephFS">Section 15.1.3, “Mount CephFS”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1053022" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1053022</a>).
    </p></li><li class="listitem"><p>
     Updated and improved the description of pool deletion in
     <a class="xref" href="#ceph-pools-operate-del-pool" title="8.2.4. Delete a Pool">Section 8.2.4, “Delete a Pool”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052981" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052981</a>).
    </p></li><li class="listitem"><p>
     Added compression algorithms description in
     <a class="xref" href="#sec-ceph-pool-compression-options" title="8.5.2. Pool Compression Options">Section 8.5.2, “Pool Compression Options”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051457" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051457</a>).
    </p></li><li class="listitem"><p>
     Replaced network diagnostics and benchmark in
     <a class="xref" href="#storage-bp-performance-net-issues" title="22.10. Poor Cluster Performance Caused by Network Problems">Section 22.10, “Poor Cluster Performance Caused by Network Problems”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050190" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050190</a>).
    </p></li><li class="listitem"><p>
     Extended <a class="xref" href="#storage-bp-recover-stuckinactive" title="22.5. 'nn pg stuck inactive' Status Message">Section 22.5, “'<span class="emphasis"><em>nn</em></span> pg stuck inactive' Status Message”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050183" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050183</a>).
    </p></li><li class="listitem"><p>
     Mentioned pool re-creation in
     <a class="xref" href="#storage-bp-recover-toomanypgs" title="22.4. 'Too Many PGs per OSD' Status Message">Section 22.4, “'Too Many PGs per OSD' Status Message”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050178" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050178</a>).
    </p></li><li class="listitem"><p>
     Fixed RGW section name in <code class="filename">ceph.conf</code> in
     <span class="intraxref">Book “Deployment Guide”, Chapter 9 “Ceph Object Gateway”, Section 9.1 “Object Gateway Manual Installation”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050170" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050170</a>).
    </p></li><li class="listitem"><p>
     Updated commands output in <a class="xref" href="#monitor-stats" title="4.4. Checking a Cluster's Usage Stats">Section 4.4, “Checking a Cluster's Usage Stats”</a> and
     <a class="xref" href="#monitor-watch" title="4.3. Watching a Cluster">Section 4.3, “Watching a Cluster”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050175" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050175</a>).
    </p></li><li class="listitem"><p>
     Removed a preventive HEALTCH_WARN section in
     <a class="xref" href="#monitor-health" title="4.2. Checking Cluster Health">Section 4.2, “Checking Cluster Health”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050174" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050174</a>).
    </p></li><li class="listitem"><p>
     Fixed sudo in <a class="xref" href="#adding-s3-swift-users" title="13.5.2.1. Adding S3 and Swift Users">Section 13.5.2.1, “Adding S3 and Swift Users”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050177" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050177</a>).
    </p></li><li class="listitem"><p>
     Removed a reference to a RADOS striper in
     <a class="xref" href="#storage-bp-cluster-mntc-rados-striping" title="22.2. Sending Large Objects with rados Fails with Full OSD">Section 22.2, “Sending Large Objects with <code class="command">rados</code> Fails with Full OSD”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050171" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050171</a>).
    </p></li><li class="listitem"><p>
     Improved section about OSD failure because of journal failure in
     <a class="xref" href="#storage-bp-monitoring-journalfails" title="21.5. What Happens When a Journal Disk Fails?">Section 21.5, “What Happens When a Journal Disk Fails?”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050169" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050169</a>).
    </p></li><li class="listitem"><p>
     Added a tip on <code class="command">zypper patch</code> during Stage 0 in
     <a class="xref" href="#deepsea-rolling-updates" title="1.10. Updating the Cluster Nodes">Section 1.10, “Updating the Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050165" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050165</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-pools-associate" title="8.1. Associate Pools with an Application">Section 8.1, “Associate Pools with an Application”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1049940" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1049940</a>).
    </p></li><li class="listitem"><p>
     Improved time synchronization information in
     <a class="xref" href="#storage-bp-recover-clockskew" title="22.9. Fixing Clock Skew Warnings">Section 22.9, “Fixing Clock Skew Warnings”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050186" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050186</a>).
    </p></li><li class="listitem"><p>
     Replaced 'erasure pool' with the correct 'erasure coded pool'
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050093" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050093</a>).
    </p></li><li class="listitem"><p>
     Replaced <code class="command">rcceph</code> with <code class="command">systemctl</code>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050111" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050111</a>).
    </p></li><li class="listitem"><p>
     Updated CephFS mounting preparation in
     <a class="xref" href="#cephfs-client-preparation" title="15.1.1. Client Preparation">Section 15.1.1, “Client Preparation”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1049451" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1049451</a>).
    </p></li><li class="listitem"><p>
     Fixed <code class="command">qemu-img</code> command in
     <a class="xref" href="#ceph-libvirt-cfg-ceph" title="18.1. Configuring Ceph">Section 18.1, “Configuring Ceph”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047190" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047190</a>).
    </p></li><li class="listitem"><p>
     Specified which DeepSea stages to run when removing roles in
     <a class="xref" href="#salt-node-removing" title="1.3. Removing and Reinstalling Cluster Nodes">Section 1.3, “Removing and Reinstalling Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047430" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047430</a>).
    </p></li><li class="listitem"><p>
     Added a new DeepSea role Ceph Manager
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047472" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047472</a>).
    </p></li><li class="listitem"><p>
     Adjusted intro for 12 SP3 in <a class="xref" href="#cephfs-client-preparation" title="15.1.1. Client Preparation">Section 15.1.1, “Client Preparation”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1043739" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1043739</a>).
    </p></li><li class="listitem"><p>
     Fixed typo in XML entity in <a class="xref" href="#ceph-libvirt-cfg-vm" title="18.4. Configuring the VM">Section 18.4, “Configuring the VM”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1042917" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1042917</a>).
    </p></li><li class="listitem"><p>
     Added information to re-run DeepSea Stages 2-5 for a role removal in
     <a class="xref" href="#salt-node-removing" title="1.3. Removing and Reinstalling Cluster Nodes">Section 1.3, “Removing and Reinstalling Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1041899" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1041899</a>).
    </p></li><li class="listitem"><p>
     Added Object Gateway, iSCSI Gateway, and NFS Ganesha ports numbers that need to be open in
     SUSE Firewall in <a class="xref" href="#storage-bp-net-firewall" title="20.10. Firewall Settings for Ceph">Section 20.10, “Firewall Settings for Ceph”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1034081" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1034081</a>).
    </p></li><li class="listitem"><p>
     Added description of CRUSH map tree iteration, see
     <a class="xref" href="#datamgm-rules-step-iterate" title="7.3.1. Iterating Through the Node Tree">Section 7.3.1, “Iterating Through the Node Tree”</a>.
    </p></li><li class="listitem"><p>
     Added <span class="emphasis"><em>indep</em></span> parameter to CRUSH rule, see
     <a class="xref" href="#datamgm-rules-step-mode" title="7.3.2. firstn and indep">Section 7.3.2, “firstn and indep”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1025189" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1025189</a>)
    </p></li><li class="listitem"><p>
     Mounting CephFS via <code class="filename">/etc/fstab</code> requires
     <code class="literal">_netdev</code> parameter. See
     <a class="xref" href="#ceph-cephfs-cephfs-fstab" title="15.3. CephFS in /etc/fstab">Section 15.3, “CephFS in <code class="filename">/etc/fstab</code>”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=989349" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=989349</a>)
    </p></li><li class="listitem"><p>
     Added tip on an existing <code class="command">rbdmap</code> <code class="systemitem">systemd</code> service file
     in <a class="xref" href="#storage-bp-integration-mount-rbd" title="9.2. Mounting and Unmounting">Section 9.2, “Mounting and Unmounting”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1015748" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1015748</a>).
    </p></li><li class="listitem"><p>
     Added an explanation to <code class="option">use_gmt_hitset</code> option in
     <a class="xref" href="#ceph-tier-gmt-hitset" title="11.7.1.1. Use GMT for Hit Set">Section 11.7.1.1, “Use GMT for Hit Set”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1024522" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1024522</a>).
    </p></li><li class="listitem"><p>
     Moved mounting CephFS back into the admin guide and added a client
     preparation section in <a class="xref" href="#cephfs-client-preparation" title="15.1.1. Client Preparation">Section 15.1.1, “Client Preparation”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1025447" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1025447</a>).
    </p></li></ul></div></section></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/book_storage_admin.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>