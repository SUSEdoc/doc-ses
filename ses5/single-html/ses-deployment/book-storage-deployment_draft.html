<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Deployment Guide | SUSE Enterprise Storage 5.5 (SES 5 &amp; SES 5.5)</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deployment Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tbazant@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Enterprise Storage 5"/>
<meta property="og:title" content="Deployment Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta property="og:description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deployment Guide | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="twitter:description" content="SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the capabilities of the Ceph (http://ceph.com/) storage project…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-deployment">Deployment Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-deployment" data-id-title="Deployment Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">5.5 (SES 5 &amp; SES 5.5)</span></div><div><h1 class="title">Deployment Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Jana</span> <span class="surname">Haláčková</span>, and <span class="firstname">Sven</span> <span class="surname">Seeberg</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>05/11/2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#id-1.4.2"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.2.7"><span class="title-name">Available Documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.8"><span class="title-name">Feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.9"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.10"><span class="title-name">About the Making of This Manual</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.11"><span class="title-name">Ceph Contributors</span></a></span></li></ul></li><li><span class="part"><a href="#part-ses"><span class="title-number">I </span><span class="title-name">SUSE Enterprise Storage</span></a></span><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Ceph</span></a></span><ul><li><span class="sect1"><a href="#storage-intro-features"><span class="title-number">1.1 </span><span class="title-name">Ceph Features</span></a></span></li><li><span class="sect1"><a href="#storage-intro-core"><span class="title-number">1.2 </span><span class="title-name">Core Components</span></a></span></li><li><span class="sect1"><a href="#sec-privileges-and-prompts"><span class="title-number">1.3 </span><span class="title-name">User Privileges and Command Prompts</span></a></span></li><li><span class="sect1"><a href="#storage-intro-structure"><span class="title-number">1.4 </span><span class="title-name">Storage Structure</span></a></span></li><li><span class="sect1"><a href="#about-bluestore"><span class="title-number">1.5 </span><span class="title-name">BlueStore</span></a></span></li><li><span class="sect1"><a href="#storage-moreinfo"><span class="title-number">1.6 </span><span class="title-name">Additional Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></span><ul><li><span class="sect1"><a href="#deployment-osd-recommendation"><span class="title-number">2.1 </span><span class="title-name">Object Storage Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mon"><span class="title-number">2.2 </span><span class="title-name">Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-rgw"><span class="title-number">2.3 </span><span class="title-name">Object Gateway Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mds"><span class="title-number">2.4 </span><span class="title-name">Metadata Server Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-smaster"><span class="title-number">2.5 </span><span class="title-name">Salt Master</span></a></span></li><li><span class="sect1"><a href="#sysreq-iscsi"><span class="title-number">2.6 </span><span class="title-name">iSCSI Nodes</span></a></span></li><li><span class="sect1"><a href="#ceph-install-ceph-deploy-network"><span class="title-number">2.7 </span><span class="title-name">Network Recommendations</span></a></span></li><li><span class="sect1"><a href="#sysreq-naming"><span class="title-number">2.8 </span><span class="title-name">Naming Limitations</span></a></span></li><li><span class="sect1"><a href="#ses-bp-diskshare"><span class="title-number">2.9 </span><span class="title-name">OSD and Monitor Sharing One Server</span></a></span></li><li><span class="sect1"><a href="#ses-bp-minimum-cluster"><span class="title-number">2.10 </span><span class="title-name">Minimum Cluster Configuration</span></a></span></li><li><span class="sect1"><a href="#ses-bp-production-cluster"><span class="title-number">2.11 </span><span class="title-name">Recommended Production Cluster Configuration</span></a></span></li><li><span class="sect1"><a href="#req-ses-other"><span class="title-number">2.12 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Other SUSE Products</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Ceph Admin Node HA Setup</span></a></span><ul><li><span class="sect1"><a href="#admin-ha-architecture"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA Cluster for Ceph Admin Node</span></a></span></li><li><span class="sect1"><a href="#admin-ha-cluster"><span class="title-number">3.2 </span><span class="title-name">Building HA Cluster with Ceph Admin Node</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#ses-deployment"><span class="title-number">II </span><span class="title-name">Cluster Deployment and Upgrade</span></a></span><ul><li><span class="chapter"><a href="#ceph-install-saltstack"><span class="title-number">4 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-install-relnotes"><span class="title-number">4.1 </span><span class="title-name">Read the Release Notes</span></a></span></li><li><span class="sect1"><a href="#deepsea-description"><span class="title-number">4.2 </span><span class="title-name">Introduction to DeepSea</span></a></span></li><li><span class="sect1"><a href="#ceph-install-stack"><span class="title-number">4.3 </span><span class="title-name">Cluster Deployment</span></a></span></li><li><span class="sect1"><a href="#deepsea-cli"><span class="title-number">4.4 </span><span class="title-name">DeepSea CLI</span></a></span></li><li><span class="sect1"><a href="#deepsea-pillar-salt-configuration"><span class="title-number">4.5 </span><span class="title-name">Configuration and Customization</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">5 </span><span class="title-name">Upgrading from Previous Releases</span></a></span><ul><li><span class="sect1"><a href="#ceph-upgrade-relnotes"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span></a></span></li><li><span class="sect1"><a href="#ceph-upgrade-general"><span class="title-number">5.2 </span><span class="title-name">General Upgrade Procedure</span></a></span></li><li><span class="sect1"><a href="#ds-migrate-osd-encrypted"><span class="title-number">5.3 </span><span class="title-name">Encrypting OSDs during Upgrade</span></a></span></li><li><span class="sect1"><a href="#ceph-upgrade-4to5"><span class="title-number">5.4 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5</span></a></span></li><li><span class="sect1"><a href="#ceph-upgrade-4to5cephdeloy"><span class="title-number">5.5 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5</span></a></span></li><li><span class="sect1"><a href="#ceph-upgrade-4to5crowbar"><span class="title-number">5.6 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5</span></a></span></li><li><span class="sect1"><a href="#ceph-upgrade-3to5"><span class="title-number">5.7 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 3 to 5</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">6 </span><span class="title-name">Backing Up the Cluster Configuration</span></a></span><ul><li><span class="sect1"><a href="#backup-ceph"><span class="title-number">6.1 </span><span class="title-name">Back Up Ceph COnfiguration</span></a></span></li><li><span class="sect1"><a href="#sec-deployment-backup-salt"><span class="title-number">6.2 </span><span class="title-name">Back Up Salt Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-deployment-backup-deepsea"><span class="title-number">6.3 </span><span class="title-name">Back Up DeepSea Configuration</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-deploy-ds-custom"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></span><ul><li><span class="sect1"><a href="#using-customized-files"><span class="title-number">7.1 </span><span class="title-name">Using Customized Configuration Files</span></a></span></li><li><span class="sect1"><a href="#discovered-configuration-modification"><span class="title-number">7.2 </span><span class="title-name">Modifying Discovered Configuration</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#additional-software"><span class="title-number">III </span><span class="title-name">Installation of Additional Services</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-as-intro"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></span></li><li><span class="chapter"><a href="#cha-ceph-additional-software-installation"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#rgw-installation"><span class="title-number">9.1 </span><span class="title-name">Object Gateway Manual Installation</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-as-iscsi"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></span><ul><li><span class="sect1"><a href="#ceph-iscsi-iscsi"><span class="title-number">10.1 </span><span class="title-name">iSCSI Block Storage</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-lrbd"><span class="title-number">10.2 </span><span class="title-name">General Information about lrbd</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-deploy"><span class="title-number">10.3 </span><span class="title-name">Deployment Considerations</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-install"><span class="title-number">10.4 </span><span class="title-name">Installation and Configuration</span></a></span></li><li><span class="sect1"><a href="#iscsi-tcmu"><span class="title-number">10.5 </span><span class="title-name">Exporting RADOS Block Device Images using <code class="systemitem">tcmu-runner</code></span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-as-cephfs"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></span><ul><li><span class="sect1"><a href="#ceph-cephfs-limitations"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-mds"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs"><span class="title-number">11.3 </span><span class="title-name">CephFS</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-as-ganesha"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></span><ul><li><span class="sect1"><a href="#sec-as-ganesha-preparation"><span class="title-number">12.1 </span><span class="title-name">Preparation</span></a></span></li><li><span class="sect1"><a href="#sec-as-ganesha-basic-example"><span class="title-number">12.2 </span><span class="title-name">Example Installation</span></a></span></li><li><span class="sect1"><a href="#sec-as-ganesha-ha-ap"><span class="title-number">12.3 </span><span class="title-name">High Availability Active-Passive Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-as-ganesha-info"><span class="title-number">12.4 </span><span class="title-name">More Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">13 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></span><ul><li><span class="sect1"><a href="#cephfs-samba"><span class="title-number">13.1 </span><span class="title-name">Export CephFS via Samba Share</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#ap-deploy-docupdate"><span class="title-number">A </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-docupdates-5maint3"><span class="title-number">A.1 </span><span class="title-name">The Latest Documentation Update</span></a></span></li><li><span class="sect1"><a href="#sec-depl-docupdates-5-5"><span class="title-number">A.2 </span><span class="title-name">October, 2018 (Documentation Maintenance Update)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-docupdates-5maint1"><span class="title-number">A.3 </span><span class="title-name">November, 2017 (Maintenance Update)</span></a></span></li><li><span class="sect1"><a href="#sec-depl-docupdates-5"><span class="title-number">A.4 </span><span class="title-name">October, 2017 (Release of SUSE Enterprise Storage 5)</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#storage-intro-core-rados-figure"><span class="number">1.1 </span><span class="name">Interfaces to the Ceph Object Store</span></a></span></li><li><span class="figure"><a href="#storage-intro-structure-example-figure"><span class="number">1.2 </span><span class="name">Small Scale Ceph Example</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.6.6"><span class="number">3.1 </span><span class="name">2-Node HA Cluster for Ceph Admin Node</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.2.9.7.11"><span class="number">4.1 </span><span class="name">DeepSea CLI stage execution progress output</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.6.3"><span class="number">10.1 </span><span class="name">Ceph Cluster with a Single iSCSI Gateway</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.6.5"><span class="number">10.2 </span><span class="name">Ceph Cluster with Multiple iSCSI Gateways</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.4.1.6"><p>
  Copyright ©
2022

  SUSE LLC
 </p><p>
  Copyright © 2016, RedHat, Inc, and contributors.

 </p><p>
  The text of and illustrations in this document are licensed
  under a Creative Commons Attribution-Share Alike 4.0 International
  ("CC-BY-SA"). An explanation of CC-BY-SA is available at
  <a class="link" href="http://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">http://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
  In accordance with CC-BY-SA, if you distribute this document or an adaptation
  of it, you must provide the URL for the original version.
 </p><p>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International Corp.
  or its subsidiaries in the United States and/or other countries. All other
  trademarks are the property of their respective owners.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="id-1.4.2" data-id-title="About This Guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About This Guide</span> <a title="Permalink" class="permalink" href="#id-1.4.2">#</a></h1></div></div></div><p>
  SUSE Enterprise Storage 5.5 is an extension to SUSE Linux Enterprise Server 12 SP3. It combines the
  capabilities of the Ceph (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>) storage
  project with the enterprise engineering and support of SUSE. SUSE Enterprise Storage
  5.5 provides IT organizations with the ability to deploy a
  distributed storage architecture that can support a number of use cases using
  commodity hardware platforms.
 </p><p>
  This guide helps you understand the concept of the SUSE Enterprise Storage
  5.5 with the main focus on managing and administrating the Ceph
  infrastructure. It also demonstrates how to use Ceph with other related
  solutions, such as OpenStack or KVM.
 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system as well as documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><section class="sect1" id="id-1.4.2.7" data-id-title="Available Documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available Documentation</span> <a title="Permalink" class="permalink" href="#id-1.4.2.7">#</a></h2></div></div></div><p>

  The following manuals are available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.7.4.1"><span class="term"><span class="intraxref">Book “Administration Guide”</span>
   </span></dt><dd><p>
     The guide describes various administration tasks that are typically
     performed after the installation. The guide also introduces steps to
     integrate Ceph with virtualization solutions such as <code class="systemitem">libvirt</code>, Xen,
     or KVM, and ways to access objects stored in the cluster via iSCSI and
     RADOS gateways.
    </p></dd><dt id="id-1.4.2.7.4.2"><span class="term"><span class="intraxref">Book “Deployment Guide”</span>
   </span></dt><dd><p>
     Guides you through the installation steps of the Ceph cluster and all
     services related to Ceph. The guide also illustrates a basic Ceph
     cluster structure and provides you with related terminology.
    </p></dd></dl></div><p>
  HTML versions of the product manuals can be found in the installed system
  under <code class="filename">/usr/share/doc/manual</code>. Find the latest
  documentation updates at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a> where you can download the
  manuals for your product in multiple formats.
 </p></section><section class="sect1" id="id-1.4.2.8" data-id-title="Feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Feedback</span> <a title="Permalink" class="permalink" href="#id-1.4.2.8">#</a></h2></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.8.4.1"><span class="term">Bugs and Enhancement Requests</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To report bugs for a product component, log in to the Novell Customer Center from
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a> and select <span class="guimenu">My Support</span> / <span class="guimenu">Service Request</span>.
    </p></dd><dt id="id-1.4.2.8.4.2"><span class="term">User Comments</span></dt><dd><p>
     We want to hear your comments and suggestions for this manual and
     the other documentation included with this product. If you have questions,
     suggestions, or corrections, contact doc-team@suse.com, or you can also
     click the <code class="literal">Report Documentation Bug</code> link beside each
     chapter or section heading.
    </p></dd><dt id="id-1.4.2.8.4.3"><span class="term">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.de</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a concise
     description of the problem and refer to the respective section number and
     page (or URL).
    </p></dd></dl></div></section><section class="sect1" id="id-1.4.2.9" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#id-1.4.2.9">#</a></h2></div></div></div><p>
  The following typographical conventions are used in this manual:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">placeholder</em>: replace
    <em class="replaceable">placeholder</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination; keys
    are shown in uppercase as on a keyboard
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> / <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a reference
    to a chapter in another manual.
   </p></li></ul></div></section><section class="sect1" id="id-1.4.2.10" data-id-title="About the Making of This Manual"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">About the Making of This Manual</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10">#</a></h2></div></div></div><p>
  This book is written in GeekoDoc, a subset of DocBook (see
  <a class="link" href="http://www.docbook.org" target="_blank">http://www.docbook.org</a>). The XML source files were
  validated by <code class="command">xmllint</code>, processed by
  <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
  version of Norman Walsh's stylesheets. The final PDF can be formatted through
  FOP from Apache or through XEP from RenderX. The authoring and publishing
  tools used to produce this manual are available in the package
  <code class="systemitem">daps</code>. The DocBook Authoring and
  Publishing Suite (DAPS) is developed as open source software. For more
  information, see <a class="link" href="http://daps.sf.net/" target="_blank">http://daps.sf.net/</a>.
 </p></section><section class="sect1" id="id-1.4.2.11" data-id-title="Ceph Contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph Contributors</span> <a title="Permalink" class="permalink" href="#id-1.4.2.11">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of hundreds of
  contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section></section><div class="part" id="part-ses" data-id-title="SUSE Enterprise Storage"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">SUSE Enterprise Storage </span><a title="Permalink" class="permalink" href="#part-ses">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Ceph</span></a></span></li><dd class="toc-abstract"><p>SUSE Enterprise Storage 5.5 is a distributed storage system designed for scalability, reliability and performance which is based on the Ceph technology. A Ceph cluster can be run on commodity servers in a common network like Ethernet. The cluster scales up well to thousands of servers (later on refe…</p></dd><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></span></li><dd class="toc-abstract"><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p></dd><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Ceph Admin Node HA Setup</span></a></span></li><dd class="toc-abstract"><p>Ceph admin node is a Ceph cluster node where the Salt master service is running. The admin node is a central point of the Ceph cluster because it manages the rest of the cluster nodes by querying and instructing their Salt minion services. It usually includes other services as well, for example the …</p></dd></ul></div><section class="chapter" id="cha-storage-about" data-id-title="SUSE Enterprise Storage 5.5 and Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Ceph</span> <a title="Permalink" class="permalink" href="#cha-storage-about">#</a></h2></div></div></div><p>
  SUSE Enterprise Storage 5.5 is a distributed storage system designed for
  scalability, reliability and performance which is based on the Ceph
  technology. A Ceph cluster can be run on commodity servers in a common
  network like Ethernet. The cluster scales up well to thousands of servers
  (later on referred to as nodes) and into the petabyte range. As opposed to
  conventional systems which have allocation tables to store and fetch data,
  Ceph uses a deterministic algorithm to allocate storage for data and has no
  centralized information structure. Ceph assumes that in storage clusters
  the addition or removal of hardware is the rule, not the exception. The
  Ceph cluster automates management tasks such as data distribution and
  redistribution, data replication, failure detection and recovery. Ceph is
  both self-healing and self-managing which results in a reduction of
  administrative and budget overhead.
 </p><p>
  This chapter provides a high level overview of SUSE Enterprise Storage 5.5
  and briefly describes the most important components.
 </p><div id="id-1.4.3.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
   Since SUSE Enterprise Storage 5.5, the only cluster deployment method is
   DeepSea. Refer to <a class="xref" href="#ceph-install-saltstack" title="Chapter 4. Deploying with DeepSea/Salt">Chapter 4, <em>Deploying with DeepSea/Salt</em></a> for details
   about the deployment process.
  </p></div><section class="sect1" id="storage-intro-features" data-id-title="Ceph Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Ceph Features</span> <a title="Permalink" class="permalink" href="#storage-intro-features">#</a></h2></div></div></div><p>
   The Ceph environment has the following features:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.6.3.1"><span class="term">Scalability</span></dt><dd><p>
      Ceph can scale to thousands of nodes and manage storage in the range of
      petabytes.
     </p></dd><dt id="id-1.4.3.2.6.3.2"><span class="term">Commodity Hardware</span></dt><dd><p>
      No special hardware is required to run a Ceph cluster. For details, see
      <a class="xref" href="#storage-bp-hwreq" title="Chapter 2. Hardware Requirements and Recommendations">Chapter 2, <em>Hardware Requirements and Recommendations</em></a>
     </p></dd><dt id="id-1.4.3.2.6.3.3"><span class="term">Self-managing</span></dt><dd><p>
      The Ceph cluster is self-managing. When nodes are added, removed or
      fail, the cluster automatically redistributes the data. It is also aware
      of overloaded disks.
     </p></dd><dt id="id-1.4.3.2.6.3.4"><span class="term">No Single Point of Failure</span></dt><dd><p>
      No node in a cluster stores important information alone. The number of
      redundancies can be configured.
     </p></dd><dt id="id-1.4.3.2.6.3.5"><span class="term">Open Source Software</span></dt><dd><p>
      Ceph is an open source software solution and independent of specific
      hardware or vendors.
     </p></dd></dl></div></section><section class="sect1" id="storage-intro-core" data-id-title="Core Components"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Core Components</span> <a title="Permalink" class="permalink" href="#storage-intro-core">#</a></h2></div></div></div><p>
   To make full use of Ceph's power, it is necessary to understand some of
   the basic components and concepts. This section introduces some parts of
   Ceph that are often referenced in other chapters.
  </p><section class="sect2" id="storage-intro-core-rados" data-id-title="RADOS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.1 </span><span class="title-name">RADOS</span> <a title="Permalink" class="permalink" href="#storage-intro-core-rados">#</a></h3></div></div></div><p>
    The basic component of Ceph is called <span class="emphasis"><em>RADOS</em></span>
    <span class="emphasis"><em>(Reliable Autonomic Distributed Object Store)</em></span>. It is
    responsible for managing the data stored in the cluster. Data in Ceph is
    usually stored as objects. Each object consists of an identifier and the
    data.
   </p><p>
    RADOS provides the following access methods to the stored objects that
    cover many use cases:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.3.4.1"><span class="term">Object Gateway</span></dt><dd><p>
       Object Gateway is an HTTP REST gateway for the RADOS object store. It enables
       direct access to objects stored in the Ceph cluster.
      </p></dd><dt id="id-1.4.3.2.7.3.4.2"><span class="term">RADOS Block Device</span></dt><dd><p>
       RADOS Block Devices (RBD) can be accessed like any other block device.
       These can be used for example in combination with <code class="systemitem">libvirt</code> for
       virtualization purposes.
      </p></dd><dt id="id-1.4.3.2.7.3.4.3"><span class="term">CephFS</span></dt><dd><p>
       The Ceph File System is a POSIX-compliant file system.
      </p></dd><dt id="id-1.4.3.2.7.3.4.4"><span class="term"><code class="systemitem">librados</code></span></dt><dd><p>
       <code class="systemitem">librados</code> is a library that can
       be used with many programming languages to create an application capable
       of directly interacting with the storage cluster.
      </p></dd></dl></div><p>
    <code class="systemitem">librados</code> is used by Object Gateway and RBD
    while CephFS directly interfaces with RADOS
    <a class="xref" href="#storage-intro-core-rados-figure" title="Interfaces to the Ceph Object Store">Figure 1.1, “Interfaces to the Ceph Object Store”</a>.
   </p><div class="figure" id="storage-intro-core-rados-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/rados-structure.png" target="_blank"><img src="images/rados-structure.png" width="" alt="Interfaces to the Ceph Object Store"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.1: </span><span class="title-name">Interfaces to the Ceph Object Store </span><a title="Permalink" class="permalink" href="#storage-intro-core-rados-figure">#</a></h6></div></div></section><section class="sect2" id="storage-intro-core-crush" data-id-title="CRUSH"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.2 </span><span class="title-name">CRUSH</span> <a title="Permalink" class="permalink" href="#storage-intro-core-crush">#</a></h3></div></div></div><p>
    At the core of a Ceph cluster is the <span class="emphasis"><em>CRUSH</em></span>
    algorithm. CRUSH is the acronym for <span class="emphasis"><em>Controlled Replication Under
    Scalable Hashing</em></span>. CRUSH is a function that handles the storage
    allocation and needs comparably few parameters. That means only a small
    amount of information is necessary to calculate the storage position of an
    object. The parameters are a current map of the cluster including the
    health state, some administrator-defined placement rules and the name of
    the object that needs to be stored or retrieved. With this information, all
    nodes in the Ceph cluster are able to calculate where an object and its
    replicas are stored. This makes writing or reading data very efficient.
    CRUSH tries to evenly distribute data over all nodes in the cluster.
   </p><p>
    The <span class="emphasis"><em>CRUSH map</em></span> contains all storage nodes and
    administrator-defined placement rules for storing objects in the cluster.
    It defines a hierarchical structure that usually corresponds to the
    physical structure of the cluster. For example, the data-containing disks
    are in hosts, hosts are in racks, racks in rows and rows in data centers.
    This structure can be used to define <span class="emphasis"><em>failure domains</em></span>.
    Ceph then ensures that replications are stored on different branches of a
    specific failure domain.
   </p><p>
    If the failure domain is set to rack, replications of objects are
    distributed over different racks. This can mitigate outages caused by a
    failed switch in a rack. If one power distribution unit supplies a row of
    racks, the failure domain can be set to row. When the power distribution
    unit fails, the replicated data is still available on other rows.
   </p></section><section class="sect2" id="storage-intro-core-nodes" data-id-title="Ceph Nodes and Daemons"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.3 </span><span class="title-name">Ceph Nodes and Daemons</span> <a title="Permalink" class="permalink" href="#storage-intro-core-nodes">#</a></h3></div></div></div><p>
    In Ceph, nodes are servers working for the cluster. They can run several
    different types of daemons. It is recommended to run only one type of
    daemon on each node, except for MGR daemons which can be collocated with
    MONs. Each cluster requires at least MON, MGR and OSD daemons:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.5.3.1"><span class="term">Ceph Monitor</span></dt><dd><p>
       <span class="emphasis"><em>Ceph Monitor</em></span> (often abbreviated as
       <span class="emphasis"><em>MON</em></span>) nodes maintain information about the cluster
       health state, a map of all nodes and data distribution rules (see
       <a class="xref" href="#storage-intro-core-crush" title="1.2.2. CRUSH">Section 1.2.2, “CRUSH”</a>).
      </p><p>
       If failures or conflicts occur, the Ceph Monitor nodes in the cluster decide by
       majority which information is correct. To form a qualified majority, it
       is recommended to have an odd number of Ceph Monitor nodes, and at least three
       of them.
      </p><p>
       If more than one site is used, the Ceph Monitor nodes should be distributed
       over an odd number of sites. The number of Ceph Monitor nodes per site should
       be such that more than 50% of the Ceph Monitor nodes remain functional if one
       site fails.
      </p></dd><dt id="id-1.4.3.2.7.5.3.2"><span class="term">Ceph Manager</span></dt><dd><p>
       The Ceph manager (MGR) collects the state information from the whole
       cluster. The Ceph manager daemon runs alongside the monitor daemons.
       It provides additional monitoring, and interfaces the external
       monitoring and management systems.
      </p><p>
       The Ceph manager requires no additional configuration, beyond ensuring
       it is running. You can deploy it as a separate role using DeepSea.
      </p></dd><dt id="id-1.4.3.2.7.5.3.3"><span class="term">Ceph OSD</span></dt><dd><p>
       A <span class="emphasis"><em>Ceph OSD</em></span> is a daemon handling <span class="emphasis"><em>Object
       Storage Devices</em></span> which are a physical or logical storage units
       (hard disks or partitions). Object Storage Devices can be physical
       disks/partitions or logical volumes. The daemon additionally takes care
       of data replication and rebalancing in case of added or removed nodes.
      </p><p>
       Ceph OSD daemons communicate with monitor daemons and provide them
       with the state of the other OSD daemons.
      </p></dd></dl></div><p>
    To use CephFS, Object Gateway, NFS Ganesha, or iSCSI Gateway, additional nodes are required:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.5.5.1"><span class="term">Metadata Server (MDS)</span></dt><dd><p>
       The metadata servers store metadata for the CephFS. By using an MDS
       you can execute basic file system commands such as <code class="command">ls</code>
       without overloading the cluster.
      </p></dd><dt id="id-1.4.3.2.7.5.5.2"><span class="term">Object Gateway</span></dt><dd><p>
       The Ceph Object Gateway provided by Object Gateway is an HTTP REST gateway for
       the RADOS object store. It is compatible with OpenStack Swift and Amazon
       S3 and has its own user management.
      </p></dd><dt id="id-1.4.3.2.7.5.5.3"><span class="term">NFS Ganesha</span></dt><dd><p>
       NFS Ganesha provides an NFS access to either the Object Gateway or the CephFS. It
       runs in the user instead of the kernel space and directly interacts with
       the Object Gateway or CephFS.
      </p></dd><dt id="id-1.4.3.2.7.5.5.4"><span class="term">iSCSI Gateway</span></dt><dd><p>
       iSCSI is a storage network protocol that allows clients to send SCSI
       commands to SCSI storage devices (targets) on remote servers.
      </p></dd></dl></div></section></section><section class="sect1" id="sec-privileges-and-prompts" data-id-title="User Privileges and Command Prompts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">User Privileges and Command Prompts</span> <a title="Permalink" class="permalink" href="#sec-privileges-and-prompts">#</a></h2></div></div></div><p>
   As a Ceph cluster administrator, you will be configuring and adjusting the
   cluster behavior by running specific commands. There are several types of
   commands you will need:
  </p><section class="sect2" id="sec-salt-commands" data-id-title="Salt/DeepSea Related Commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.1 </span><span class="title-name">Salt/DeepSea Related Commands</span> <a title="Permalink" class="permalink" href="#sec-salt-commands">#</a></h3></div></div></div><p>
    These commands help you to deploy or upgrade the Ceph cluster, run
    commands on several (or all) cluster nodes at the same time, or assist you
    when adding or removing cluster nodes. The most frequently used are
    <code class="command">salt</code>, <code class="command">salt-run</code>, and
    <code class="command">deepsea</code>. You need to run Salt commands on the
    Salt master node (refer to <a class="xref" href="#deepsea-description" title="4.2. Introduction to DeepSea">Section 4.2, “Introduction to DeepSea”</a> for
    details) as <code class="systemitem">root</code>. These commands are introduced with the following
    prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div></section><section class="sect2" id="sec-ceph-commands" data-id-title="Ceph Related Commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.2 </span><span class="title-name">Ceph Related Commands</span> <a title="Permalink" class="permalink" href="#sec-ceph-commands">#</a></h3></div></div></div><p>
    These are lower level commands to configure and fine tune all aspects of
    the cluster and its gateways on the command line. <code class="command">ceph</code>,
    <code class="command">rbd</code>, <code class="command">radosgw-admin</code>, or
    <code class="command">crushtool</code> to name some of them.
   </p><p>
    To run Ceph related commands, you need to have read access to a Ceph
    key. The key's capabilities then define your privileges within the Ceph
    environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
    <code class="command">sudo</code>) and use the unrestricted default keyring
    'ceph.client.admin.key'.
   </p><p>
    Safer and recommended option is to create a more restrictive individual key
    for each administrator user and put it in a directory where the users can
    read it, for example:
   </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.4.3.2.8.4.6" data-id-title="Path to Ceph Keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph Keys</h6><p>
     To use a custom admin user and keyring, you need to specify the user name
     and path to the key each time you run the <code class="command">ceph</code> command
     using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
     and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
     options.
    </p><p>
     To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
     variable in the individual users' <code class="filename">~/.bashrc</code> files.
    </p></div><p>
    Although you can run Ceph related commands on any cluster node, we
    recommend running them on the node with the 'admin' role (see
    <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a> for details). This documentation
    uses the <code class="systemitem">cephadm</code> user to run the commands, therefore they are introduced
    with the following prompt:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth list</pre></div></section><section class="sect2" id="sec-general-prompts" data-id-title="General Linux Commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.3 </span><span class="title-name">General Linux Commands</span> <a title="Permalink" class="permalink" href="#sec-general-prompts">#</a></h3></div></div></div><p>
    Linux commands not related to Ceph or DeepSea, such as
    <code class="command">mount</code>, <code class="command">cat</code>, or
    <code class="command">openssl</code>, are introduced either with the
    <code class="prompt user">cephadm &gt; </code> or <code class="prompt user">root # </code> prompts, depending on which privileges
    the related command requires.
   </p></section><section class="sect2" id="sec-prompt-moreinfo" data-id-title="Additional Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.4 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#sec-prompt-moreinfo">#</a></h3></div></div></div><p>
    For more information on Ceph key management, refer to
    <span class="intraxref">Book “Administration Guide”, Chapter 6 “Authentication with <code class="systemitem">cephx</code>”, Section 6.2 “Key Management”</span>.
   </p></section></section><section class="sect1" id="storage-intro-structure" data-id-title="Storage Structure"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Storage Structure</span> <a title="Permalink" class="permalink" href="#storage-intro-structure">#</a></h2></div></div></div><section class="sect2" id="storage-intro-structure-pool" data-id-title="Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.4.1 </span><span class="title-name">Pool</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pool">#</a></h3></div></div></div><p>
    Objects that are stored in a Ceph cluster are put into
    <span class="emphasis"><em>pools</em></span>. Pools represent logical partitions of the
    cluster to the outside world. For each pool a set of rules can be defined,
    for example, how many replications of each object must exist. The standard
    configuration of pools is called <span class="emphasis"><em>replicated pool</em></span>.
   </p><p>
    Pools usually contain objects but can also be configured to act similar to
    a RAID 5. In this configuration, objects are stored in chunks along with
    additional coding chunks. The coding chunks contain the redundant
    information. The number of data and coding chunks can be defined by the
    administrator. In this configuration, pools are referred to as
    <span class="emphasis"><em>erasure coded pools</em></span>.
   </p></section><section class="sect2" id="storage-intro-structure-pg" data-id-title="Placement Group"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.4.2 </span><span class="title-name">Placement Group</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pg">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Placement Groups</em></span> (PGs) are used for the distribution
    of data within a pool. When creating a pool, a certain number of placement
    groups is set. The placement groups are used internally to group objects
    and are an important factor for the performance of a Ceph cluster. The PG
    for an object is determined by the object's name.
   </p></section><section class="sect2" id="storage-intro-structure-example" data-id-title="Example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.4.3 </span><span class="title-name">Example</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-example">#</a></h3></div></div></div><p>
    This section provides a simplified example of how Ceph manages data (see
    <a class="xref" href="#storage-intro-structure-example-figure" title="Small Scale Ceph Example">Figure 1.2, “Small Scale Ceph Example”</a>). This example
    does not represent a recommended configuration for a Ceph cluster. The
    hardware setup consists of three storage nodes or Ceph OSDs
    (<code class="literal">Host 1</code>, <code class="literal">Host 2</code>, <code class="literal">Host
    3</code>). Each node has three hard disks which are used as OSDs
    (<code class="literal">osd.1</code> to <code class="literal">osd.9</code>). The Ceph Monitor nodes are
    neglected in this example.
   </p><div id="id-1.4.3.2.9.4.3" data-id-title="Difference between Ceph OSD and OSD" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Difference between Ceph OSD and OSD</h6><p>
     While <span class="emphasis"><em>Ceph OSD</em></span> or <span class="emphasis"><em>Ceph OSD
     daemon</em></span> refers to a daemon that is run on a node, the word
     <span class="emphasis"><em>OSD</em></span> refers to the logical disk that the daemon
     interacts with.
    </p></div><p>
    The cluster has two pools, <code class="literal">Pool A</code> and <code class="literal">Pool
    B</code>. While Pool A replicates objects only two times, resilience for
    Pool B is more important and it has three replications for each object.
   </p><p>
    When an application puts an object into a pool, for example via the REST
    API, a Placement Group (<code class="literal">PG1</code> to <code class="literal">PG4</code>)
    is selected based on the pool and the object name. The CRUSH algorithm then
    calculates on which OSDs the object is stored, based on the Placement Group
    that contains the object.
   </p><p>
    In this example the failure domain is set to host. This ensures that
    replications of objects are stored on different hosts. Depending on the
    replication level set for a pool, the object is stored on two or three OSDs
    that are used by the Placement Group.
   </p><p>
    An application that writes an object only interacts with one Ceph OSD,
    the primary Ceph OSD. The primary Ceph OSD takes care of replication
    and confirms the completion of the write process after all other OSDs have
    stored the object.
   </p><p>
    If <code class="literal">osd.5</code> fails, all object in <code class="literal">PG1</code> are
    still available on <code class="literal">osd.1</code>. As soon as the cluster
    recognizes that an OSD has failed, another OSD takes over. In this example
    <code class="literal">osd.4</code> is used as a replacement for
    <code class="literal">osd.5</code>. The objects stored on <code class="literal">osd.1</code>
    are then replicated to <code class="literal">osd.4</code> to restore the replication
    level.
   </p><div class="figure" id="storage-intro-structure-example-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/data-structure-example.png" target="_blank"><img src="images/data-structure-example.png" width="" alt="Small Scale Ceph Example"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.2: </span><span class="title-name">Small Scale Ceph Example </span><a title="Permalink" class="permalink" href="#storage-intro-structure-example-figure">#</a></h6></div></div><p>
    If a new node with new OSDs is added to the cluster, the cluster map is
    going to change. The CRUSH function then returns different locations for
    objects. Objects that receive new locations will be relocated. This process
    results in a balanced usage of all OSDs.
   </p></section></section><section class="sect1" id="about-bluestore" data-id-title="BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">BlueStore</span> <a title="Permalink" class="permalink" href="#about-bluestore">#</a></h2></div></div></div><p>
   BlueStore is a new default storage back end for Ceph since SUSE Enterprise Storage
   5. It has better performance than FileStore, full data check-summing, and
   built-in compression.
  </p><div id="id-1.4.3.2.10.3" data-id-title="bluestore_cache_autotune" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: <code class="option">bluestore_cache_autotune</code></h6><p>
    Since <span class="package">ceph</span> version 12.2.10, a new setting
    <code class="option">bluestore_cache_autotune</code> was introduced that disables all
    <code class="option">bluestore_cache</code> options for manual cache sizing. To keep
    the old behavior, you need to set
    <code class="option">bluestore_cache_autotune=false</code>. Refer to
    <span class="intraxref">Book “Administration Guide”, Chapter 12 “Ceph Cluster Configuration”, Section 12.2 “Ceph OSD and BlueStore”</span> for more details.
   </p></div><p>
   BlueStore manages either one, two, or three storage devices. In the
   simplest case, BlueStore consumes a single primary storage device. The
   storage device is normally partitioned into two parts:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     A small partition named BlueFS that implements file system-like
     functionalities required by RocksDB.
    </p></li><li class="listitem"><p>
     The rest of the device is normally a large partition occupying the rest of
     the device. It is managed directly by BlueStore and contains all of the
     actual data. This primary device is normally identified by a block
     symbolic link in the data directory.
    </p></li></ol></div><p>
   It is also possible to deploy BlueStore across two additional devices:
  </p><p>
   A <span class="emphasis"><em>WAL device</em></span> can be used for BlueStore’s internal
   journal or write-ahead log. It is identified by the
   <code class="literal">block.wal</code> symbolic link in the data directory. It is only
   useful to use a separate WAL device if the device is faster than the primary
   device or the DB device, for example when:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The WAL device is an NVMe, and the DB device is an SSD, and the data
     device is either SSD or HDD.
    </p></li><li class="listitem"><p>
     Both the WAL and DB devices are separate SSDs, and the data device is an
     SSD or HDD.
    </p></li></ul></div><p>
   A <span class="emphasis"><em>DB device</em></span> can be used for storing BlueStore’s
   internal metadata. BlueStore (or rather, the embedded RocksDB) will put as
   much metadata as it can on the DB device to improve performance. Again, it
   is only helpful to provision a shared DB device if it is faster than the
   primary device.
  </p><div id="id-1.4.3.2.10.10" data-id-title="Plan for the DB Size" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Plan for the DB Size</h6><p>
    Plan thoroughly for the sufficient size of the DB device. If the DB device
    fills up, metadata will be spilling over to the primary device which badly
    degrades the OSD's performance.
   </p><p>
    You can check if a WAL/DB partition is getting full and spilling over with
    the <code class="command">ceph daemon osd<em class="replaceable">.ID</em> perf
    dump</code> command. The <code class="option">slow_used_bytes</code> value shows
    the amount of data being spilled out:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph daemon osd<em class="replaceable">.ID</em> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,</pre></div></div></section><section class="sect1" id="storage-moreinfo" data-id-title="Additional Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.6 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#storage-moreinfo">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ceph as a community project has its own extensive online documentation.
     For topics not found in this manual, refer to
     <a class="link" href="http://docs.ceph.com/docs/master/" target="_blank">http://docs.ceph.com/docs/master/</a>.
    </p></li><li class="listitem"><p>
     The original publication <span class="emphasis"><em>CRUSH: Controlled, Scalable,
     Decentralized Placement of Replicated Data</em></span> by <span class="emphasis"><em>S.A.
     Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</em></span> provides helpful
     insight into the inner workings of Ceph. Especially when deploying large
     scale clusters it is a recommended reading. The publication can be found
     at <a class="link" href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf" target="_blank">http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf</a>.
    </p></li><li class="listitem"><p>
       SUSE Enterprise Storage can be used with non-SUSE OpenStack distributions.
       The Ceph clients need to be at a level that is compatible with SUSE Enterprise Storage.
     </p><div id="id-1.4.3.2.11.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
         SUSE supports the server component of the Ceph deployment
         and the client is supported by the OpenStack distribution vendor.
       </p></div></li></ul></div></section></section><section class="chapter" id="storage-bp-hwreq" data-id-title="Hardware Requirements and Recommendations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span> <a title="Permalink" class="permalink" href="#storage-bp-hwreq">#</a></h2></div></div></div><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p><p>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </p><section class="sect1" id="deployment-osd-recommendation" data-id-title="Object Storage Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Object Storage Nodes</span> <a title="Permalink" class="permalink" href="#deployment-osd-recommendation">#</a></h2></div></div></div><section class="sect2" id="sysreq-osd" data-id-title="Minimum Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.1 </span><span class="title-name">Minimum Requirements</span> <a title="Permalink" class="permalink" href="#sysreq-osd">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least 4 OSD nodes, with 8 OSD disks each, are required.
     </p></li><li class="listitem"><p>
      For OSDs that do <span class="emphasis"><em>not</em></span> use BlueStore, 1 GB of RAM
      per terabyte of raw OSD capacity is minimally required for each OSD
      storage node. 1.5 GB of RAM per terabyte of raw OSD capacity is
      recommended. During recovery, 2 GB of RAM per terabyte of raw OSD
      capacity may be optimal.
     </p><p>
      For OSDs that <span class="emphasis"><em>use</em></span> BlueStore, first calculate the
      size of RAM that is recommended for OSDs that do not use BlueStore,
      then calculate 2 GB plus the size of the BlueStore cache of RAM is
      recommended for each OSD process, and choose the bigger value of RAM of
      the two results. Note that the default BlueStore cache is 1 GB for HDD
      and 3 GB for SSD drives by default. In summary, pick the greater of:
     </p><div class="verbatim-wrap"><pre class="screen">[1GB * OSD count * OSD size]</pre></div><p>
      or
     </p><div class="verbatim-wrap"><pre class="screen">[(2 + BS cache) * OSD count]</pre></div></li><li class="listitem"><p>
      1.5 GHz of a logical CPU core per OSD is minimally required for each OSD
      daemon process. 2 GHz per OSD daemon process is recommended. Note that
      Ceph runs one OSD daemon process per storage disk; do not count disks
      reserved solely for use as OSD journals, WAL journals, omap metadata, or
      any combination of these three cases.
     </p></li><li class="listitem"><p>
      10 Gb Ethernet (two network interfaces bonded to multiple switches).
     </p></li><li class="listitem"><p>
      OSD disks in JBOD configurations.
     </p></li><li class="listitem"><p>
      OSD disks should be exclusively used by SUSE Enterprise Storage 5.5.
     </p></li><li class="listitem"><p>
      Dedicated disk/SSD for the operating system, preferably in a RAID 1
      configuration.
     </p></li><li class="listitem"><p>
      If this OSD host will host part of a cache pool used for cache tiering,
      allocate at least an additional 4 GB of RAM.
     </p></li><li class="listitem"><p>
      For disk performance reasons, we recommend using bare metal for OSD nodes
      and not virtual machines.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-mindisk" data-id-title="Minimum Disk Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.2 </span><span class="title-name">Minimum Disk Size</span> <a title="Permalink" class="permalink" href="#ses-bp-mindisk">#</a></h3></div></div></div><p>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal (for FileStore) or WAL/DB device (for BlueStore), and the
    primary space for the stored data. The minimum (and default) value for the
    journal/WAL/DB is 6 GB. The minimum space for data is 5 GB, as partitions
    smaller than 5 GB are automatically assigned the weight of 0.
   </p><p>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </p></section><section class="sect2" id="rec-waldb-size" data-id-title="Recommended Size for the BlueStores WAL and DB Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.3 </span><span class="title-name">Recommended Size for the BlueStore's WAL and DB Device</span> <a title="Permalink" class="permalink" href="#rec-waldb-size">#</a></h3></div></div></div><div id="id-1.4.3.3.5.4.2" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
     Refer to <a class="xref" href="#about-bluestore" title="1.5. BlueStore">Section 1.5, “BlueStore”</a> for more information on
     BlueStore.
    </p></div><p>
    Following are several rules for WAL/DB device sizing. When using DeepSea
    to deploy OSDs with BlueStore, it applies the recommended rules
    automatically and notifies the administrator about the fact.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      10GB of the DB device for each Terabyte of the OSD capacity (1/100th of
      the OSD).
     </p></li><li class="listitem"><p>
      Between 500MB and 2GB for the WAL device. The WAL size depends on the
      data traffic and workload, not on the OSD size. If you know that an OSD
      is physically able to handle small writes and overwrites at a very high
      throughput, more WAL is preferred rather than less WAL. 1GB WAL device is
      a good compromise that fulfills most deployments.
     </p></li><li class="listitem"><p>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows Ceph to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as Ceph uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not entirely used then
      its space is not wasted but used for DB operation.
     </p><p>
      To share the DB device with the WAL, do <span class="emphasis"><em>not</em></span> specify
      the WAL device, and specify only the DB device:
     </p><div class="verbatim-wrap"><pre class="screen">bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0</pre></div></li><li class="listitem"><p>
      Alternatively, you can put the WAL on its own separate device. In such
      case, we recommend the fastest device for the WAL operation.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-share-ssd-journal" data-id-title="Using SSD for OSD Journals"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.4 </span><span class="title-name">Using SSD for OSD Journals</span> <a title="Permalink" class="permalink" href="#ses-bp-share-ssd-journal">#</a></h3></div></div></div><p>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </p><p>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk.
   </p><div id="id-1.4.3.3.5.5.4" data-id-title="Sharing an SSD for Multiple Journals" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing an SSD for Multiple Journals</h6><p>
     As journal data occupies relatively little space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than six journals on the same SSD disk and 12 on
     NVMe disks.
    </p></div></section><section class="sect2" id="maximum-count-of-disks-osd" data-id-title="Maximum Recommended Number of Disks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.5 </span><span class="title-name">Maximum Recommended Number of Disks</span> <a title="Permalink" class="permalink" href="#maximum-count-of-disks-osd">#</a></h3></div></div></div><p>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Network bandwidth.</em></span> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Memory.</em></span> For optimum performance, reserve at least 2
      GB of RAM per terabyte of disk space installed.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Fault tolerance.</em></span> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </p></li></ul></div></section></section><section class="sect1" id="sysreq-mon" data-id-title="Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Monitor Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mon">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     At least three Ceph Monitor nodes are required. The number of monitors should
     always be odd (1+2n).
    </p></li><li class="listitem"><p>
     4 GB of RAM.
    </p></li><li class="listitem"><p>
     Processor with four logical cores.
    </p></li><li class="listitem"><p>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <code class="filename">/var/lib/ceph</code> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </p></li><li class="listitem"><p>
     There must only be one monitor process per node.
    </p></li><li class="listitem"><p>
     Mixing OSD, monitor, or Object Gateway nodes is only supported if sufficient
     hardware resources are available. That means that the requirements for all
     services need to be added up.
    </p></li><li class="listitem"><p>
     Two network interfaces bonded to multiple switches.
    </p></li></ul></div></section><section class="sect1" id="sysreq-rgw" data-id-title="Object Gateway Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Object Gateway Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-rgw">#</a></h2></div></div></div><p>
   Object Gateway nodes should have six to eight CPU cores and 32 GB of RAM (64 GB
   recommended). When other processes are co-located on the same machine, their
   requirements need to be added up.
  </p></section><section class="sect1" id="sysreq-mds" data-id-title="Metadata Server Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Metadata Server Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mds">#</a></h2></div></div></div><p>
   Proper sizing of the Metadata Server nodes depends on the specific use case.
   Generally, the more open files the Metadata Server is to handle, the more CPU and RAM
   it needs. Following are the minimal requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     3G of RAM per one Metadata Server daemon.
    </p></li><li class="listitem"><p>
     Bonded network interface.
    </p></li><li class="listitem"><p>
     2.5 GHz CPU with at least 2 cores.
    </p></li></ul></div></section><section class="sect1" id="sysreq-smaster" data-id-title="Salt Master"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Salt Master</span> <a title="Permalink" class="permalink" href="#sysreq-smaster">#</a></h2></div></div></div><p>
   At least 4 GB of RAM and a quad-core CPU are required. This is includes
   running openATTIC on the Salt master. For large clusters with hundreds of nodes, 6
   GB of RAM is suggested.
  </p></section><section class="sect1" id="sysreq-iscsi" data-id-title="iSCSI Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">iSCSI Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-iscsi">#</a></h2></div></div></div><p>
   iSCSI nodes should have six to eight CPU cores and 16 GB of RAM.
  </p></section><section class="sect1" id="ceph-install-ceph-deploy-network" data-id-title="Network Recommendations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Network Recommendations</span> <a title="Permalink" class="permalink" href="#ceph-install-ceph-deploy-network">#</a></h2></div></div></div><p>
   The network environment where you intend to run Ceph should ideally be a
   bonded set of at least two network interfaces that is logically split into a
   public part and a trusted internal part using VLANs. The bonding mode is
   recommended to be 802.3ad if possible to provide maximum bandwidth and
   resiliency.
  </p><p>
   The public VLAN serves to provide the service to the customers, while the
   internal part provides for the authenticated Ceph network communication.
   The main reason for this is that although Ceph provides authentication and
   protection against attacks once secret keys are in place, the messages used
   to configure these keys may be transferred openly and are vulnerable.
  </p><div id="id-1.4.3.3.11.4" data-id-title="Administration Network not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Administration Network not Supported</h6><p>
    Additional administration network setup—that enables for example
    separating SSH, Salt, or DNS networking—is neither tested nor
    supported.
   </p></div><div id="id-1.4.3.3.11.5" data-id-title="Nodes Configured via DHCP" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Nodes Configured via DHCP</h6><p>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    Ceph daemons start. If this happens, the Ceph MONs and OSDs will not
    start correctly (running <code class="command">systemctl status ceph\*</code> will
    result in "unable to bind" errors) To avoid this issue, we recommend
    increasing the DHCP client timeout to at least 30 seconds on each node in
    your storage cluster. This can be done by changing the following settings
    on each node:
   </p><p>
    In <code class="filename">/etc/sysconfig/network/dhcp</code>, set
   </p><div class="verbatim-wrap"><pre class="screen">DHCLIENT_WAIT_AT_BOOT="30"</pre></div><p>
    In <code class="filename">/etc/sysconfig/network/config</code>, set
   </p><div class="verbatim-wrap"><pre class="screen">WAIT_FOR_INTERFACES="60"</pre></div></div><section class="sect2" id="storage-bp-net-private" data-id-title="Adding a Private Network to a Running Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.1 </span><span class="title-name">Adding a Private Network to a Running Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-net-private">#</a></h3></div></div></div><p>
    If you do not specify a cluster network during Ceph deployment, it
    assumes a single public network environment. While Ceph operates fine
    with a public network, its performance and security improves when you set a
    second private cluster network. To support two networks, each Ceph node
    needs to have at least two network cards.
   </p><p>
    You need to apply the following changes to each Ceph node. It is
    relatively quick to do for a small cluster, but can be very time consuming
    if you have a cluster consisting of hundreds or thousands of nodes.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop Ceph related services on each cluster node.
     </p><p>
      Add a line to <code class="filename">/etc/ceph/ceph.conf</code> to define the
      cluster network, for example:
     </p><div class="verbatim-wrap"><pre class="screen">cluster network = 10.0.0.0/24</pre></div><p>
      If you need to specifically assign static IP addresses or override
      <code class="option">cluster network</code> settings, you can do so with the
      optional <code class="option">cluster addr</code>.
     </p></li><li class="step"><p>
      Check that the private cluster network works as expected on the OS level.
     </p></li><li class="step"><p>
      Start Ceph related services on each cluster node.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph.target</pre></div></li></ol></div></div></section><section class="sect2" id="storage-bp-net-subnets" data-id-title="Monitor Nodes on Different Subnets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.2 </span><span class="title-name">Monitor Nodes on Different Subnets</span> <a title="Permalink" class="permalink" href="#storage-bp-net-subnets">#</a></h3></div></div></div><p>
    If the monitor nodes are on multiple subnets, for example they are located
    in different rooms and served by different switches, you need to adjust the
    <code class="filename">ceph.conf</code> file accordingly. For example if the nodes
    have IP addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12, add the
    following lines to its global section:
   </p><div class="verbatim-wrap"><pre class="screen">[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</pre></div><p>
    Additionally, if you need to specify a per-monitor public address or
    network, you need to add a
    <code class="literal">[mon.<em class="replaceable">X</em>]</code> section per each
    monitor:
   </p><div class="verbatim-wrap"><pre class="screen">[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</pre></div></section></section><section class="sect1" id="sysreq-naming" data-id-title="Naming Limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Naming Limitations</span> <a title="Permalink" class="permalink" href="#sysreq-naming">#</a></h2></div></div></div><p>
   Ceph does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a Ceph
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all Ceph
   object/configuration names.
  </p></section><section class="sect1" id="ses-bp-diskshare" data-id-title="OSD and Monitor Sharing One Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">OSD and Monitor Sharing One Server</span> <a title="Permalink" class="permalink" href="#ses-bp-diskshare">#</a></h2></div></div></div><p>
   Although it is technically possible to run Ceph OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance—the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </p><p>
   Another consideration is whether to share disks between an OSD, a monitor
   node, and the operating system on the server. The answer is simple: if
   possible, dedicate a separate disk to OSD, and a separate server to a
   monitor node.
  </p><p>
   Although Ceph supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </p><div id="id-1.4.3.3.13.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If it is <span class="emphasis"><em>really</em></span> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <code class="filename">/var/lib/ceph/mon</code> directory for slightly better
    performance.
   </p></div></section><section class="sect1" id="ses-bp-minimum-cluster" data-id-title="Minimum Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">Minimum Cluster Configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-minimum-cluster">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Four Object Storage Nodes
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       10 Gb Ethernet (two networks bonded to multiple switches)
      </p></li><li class="listitem"><p>
       32 OSDs per storage cluster
      </p></li><li class="listitem"><p>
       OSD journal can reside on OSD disk
      </p></li><li class="listitem"><p>
       Dedicated OS disk for each Object Storage Node
      </p></li><li class="listitem"><p>
       1 GB of RAM per TB of raw OSD capacity for each Object Storage Node
      </p></li><li class="listitem"><p>
       1.5 GHz per OSD for each Object Storage Node
      </p></li><li class="listitem"><p>
       Ceph Monitors, gateway and Metadata Servers can reside on Object Storage Nodes
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Three Ceph Monitor nodes (requires SSD for dedicated OS drive)
        </p></li><li class="listitem"><p>
         Ceph Monitors, Object Gateways and Metadata Servers nodes require redundant deployment
        </p></li><li class="listitem"><p>
         iSCSI Gateways, Object Gateways and Metadata Servers require incremental 4 GB RAM and four cores
        </p></li></ul></div></li></ul></div></li><li class="listitem"><p>
     Separate management node with 4 GB RAM, four cores, 1 TB capacity
    </p></li></ul></div></section><section class="sect1" id="ses-bp-production-cluster" data-id-title="Recommended Production Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Recommended Production Cluster Configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-production-cluster">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Seven Object Storage Nodes
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       No single node exceeds ~15% of total storage
      </p></li><li class="listitem"><p>
       10 Gb Ethernet (four physical networks bonded to multiple switches)
      </p></li><li class="listitem"><p>
       56+ OSDs per storage cluster
      </p></li><li class="listitem"><p>
       RAID 1 OS disks for each OSD storage node
      </p></li><li class="listitem"><p>
       SSDs for Journal with 6:1 ratio SSD journal to OSD
      </p></li><li class="listitem"><p>
       1.5 GB of RAM per TB of raw OSD capacity for each Object Storage Node
      </p></li><li class="listitem"><p>
       2 GHz per OSD for each Object Storage Node
      </p></li></ul></div></li><li class="listitem"><p>
     Dedicated physical infrastructure nodes
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Three Ceph Monitor nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk
      </p></li><li class="listitem"><p>
       One SES management node: 4 GB RAM, 4 core processor, RAID 1 SSDs for
       disk
      </p></li><li class="listitem"><p>
       Redundant physical deployment of gateway or Metadata Server nodes:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Object Gateway nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk
        </p></li><li class="listitem"><p>
         iSCSI Gateway nodes: 16 GB RAM, 4 core processor, RAID 1 SSDs for disk
        </p></li><li class="listitem"><p>
         Metadata Server nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
         RAID 1 SSDs for disk
        </p></li></ul></div></li></ul></div></li></ul></div></section><section class="sect1" id="req-ses-other" data-id-title="SUSE Enterprise Storage 5.5 and Other SUSE Products"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Other SUSE Products</span> <a title="Permalink" class="permalink" href="#req-ses-other">#</a></h2></div></div></div><p>
   This section contains important information about integrating SUSE Enterprise Storage
   5.5 with other SUSE products.
  </p><section class="sect2" id="req-ses-suma" data-id-title="SUSE Manager"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.1 </span><span class="title-name">SUSE Manager</span> <a title="Permalink" class="permalink" href="#req-ses-suma">#</a></h3></div></div></div><p>
    SUSE Manager and SUSE Enterprise Storage are not integrated, therefore SUSE Manager cannot
    currently manage a SUSE Enterprise Storage 5.5 cluster.
   </p></section></section></section><section class="chapter" id="cha-admin-ha" data-id-title="Ceph Admin Node HA Setup"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Ceph Admin Node HA Setup</span> <a title="Permalink" class="permalink" href="#cha-admin-ha">#</a></h2></div></div></div><p>
  <span class="emphasis"><em>Ceph admin node</em></span> is a Ceph cluster node where the
  Salt master service is running. The admin node is a central point of the Ceph
  cluster because it manages the rest of the cluster nodes by querying and
  instructing their Salt minion services. It usually includes other services as
  well, for example the openATTIC Web UI with the <span class="emphasis"><em>Grafana</em></span>
  dashboard backed by the <span class="emphasis"><em>Prometheus</em></span> monitoring toolkit.
 </p><p>
  In case of the Ceph admin node failure, you usually need to provide a new
  working hardware for the node and restore the complete cluster configuration
  stack from a recent backup. Such method is time consuming and causes cluster
  outage.
 </p><p>
  To prevent the Ceph cluster performance downtime caused by the admin node
  failure, we recommend to make use of the High Availability (HA) cluster for the Ceph
  admin node.
 </p><section class="sect1" id="admin-ha-architecture" data-id-title="Outline of the HA Cluster for Ceph Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA Cluster for Ceph Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-architecture">#</a></h2></div></div></div><p>
   The idea of an HA cluster is that in case of one cluster node failure, the
   other node automatically takes over its role including the virtualized
   Ceph admin node. This way other Ceph cluster nodes do not notice that
   the Ceph admin node failed.
  </p><p>
   The minimal HA solution for the Ceph admin node requires the following
   hardware:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Two bare metal servers able to run SUSE Linux Enterprise with the High Availability extension and
     virtualize the Ceph admin node.
    </p></li><li class="listitem"><p>
     Two or more redundant network communication paths, for example via Network
     Device Bonding.
    </p></li><li class="listitem"><p>
     Shared storage to host the disk image(s) of the Ceph admin node virtual
     machine. The shared storage needs to be accessible form both servers. It
     can be for example an NFS export, a Samba share, or iSCSI target.
    </p></li></ul></div><p>
   Find more details on the cluster requirements at
   <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#sec-ha-inst-quick-req" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#sec-ha-inst-quick-req</a>.
  </p><div class="figure" id="id-1.4.3.4.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_admin_ha1.png" target="_blank"><img src="images/ceph_admin_ha1.png" width="" alt="2-Node HA Cluster for Ceph Admin Node"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.1: </span><span class="title-name">2-Node HA Cluster for Ceph Admin Node </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.6.6">#</a></h6></div></div></section><section class="sect1" id="admin-ha-cluster" data-id-title="Building HA Cluster with Ceph Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Building HA Cluster with Ceph Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-cluster">#</a></h2></div></div></div><p>
   The following procedure summarizes the most important steps of building the
   HA cluster for virtualizing the Ceph admin node. For details, refer to the
   indicated links.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up a basic 2-node HA cluster with shared storage as described in
     <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#art-sleha-install-quick" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#art-sleha-install-quick</a>.
    </p></li><li class="step"><p>
     On both cluster nodes, install all packages required for running the KVM
     hypervisor and the <code class="systemitem">libvirt</code> toolkit as described in
     <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-vt-installation-kvm" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-vt-installation-kvm</a>.
    </p></li><li class="step"><p>
     On the first cluster node, create a new KVM virtual machine (VM) making
     use of <code class="systemitem">libvirt</code> as described in
     <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-libvirt-inst-virt-install" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-libvirt-inst-virt-install</a>.
     Use the preconfigured shared storage to store the disk images of the VM.
    </p></li><li class="step"><p>
     After the VM setup is complete, export its configuration to an XML file on
     the shared storage. Use the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh dumpxml <em class="replaceable">VM_NAME</em> &gt; /path/to/shared/vm_name.xml</pre></div></li><li class="step"><p>
     Create a resource for the Admin Node VM. Refer to
     <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2</a>
     for general info on creating HA resources. Detailed info on creating
     resource for a KVM virtual machine is described in
     <a class="link" href="http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29" target="_blank">http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29</a>.
    </p></li><li class="step"><p>
     On the newly created VM guest, deploy the Ceph admin node including the
     additional services you need there. Follow relevant steps in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>. At the same time, deploy the
     remaining Ceph cluster nodes on the non-HA cluster servers.
    </p></li></ol></div></div></section></section></div><div class="part" id="ses-deployment" data-id-title="Cluster Deployment and Upgrade"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Cluster Deployment and Upgrade </span><a title="Permalink" class="permalink" href="#ses-deployment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#ceph-install-saltstack"><span class="title-number">4 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></span></li><dd class="toc-abstract"><p>
   The <code class="command">ceph-deploy</code> cluster deployment tool was deprecated in
   SUSE Enterprise Storage 4 and is completely removed in favor of DeepSea as of
   SUSE Enterprise Storage 5.
  </p></dd><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">5 </span><span class="title-name">Upgrading from Previous Releases</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage from the previous
  release(s) to version 5.5.
 </p></dd><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">6 </span><span class="title-name">Backing Up the Cluster Configuration</span></a></span></li><dd class="toc-abstract"><p>
  This chapter explains which files on the admin node should be backed up. As
  soon as you are finished with your cluster deployment or migration, create a
  backup of these directories.
 </p></dd><li><span class="chapter"><a href="#ceph-deploy-ds-custom"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></span></li><dd class="toc-abstract"><p>You can change the default cluster configuration generated in Stage 2 (refer to DeepSea Stages Description). For example, you may need to change network settings, or software that is installed on the Salt master by default installed. You can perform the former by modifying the pillar updated after S…</p></dd></ul></div><section class="chapter" id="ceph-install-saltstack" data-id-title="Deploying with DeepSea/Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Deploying with DeepSea/Salt</span> <a title="Permalink" class="permalink" href="#ceph-install-saltstack">#</a></h2></div></div></div><div id="id-1.4.4.2.3" data-id-title="ceph-deploy Removed in SUSE Enterprise Storage 5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="command">ceph-deploy</code> Removed in SUSE Enterprise Storage 5.5</h6><p>
   The <code class="command">ceph-deploy</code> cluster deployment tool was deprecated in
   SUSE Enterprise Storage 4 and is completely removed in favor of DeepSea as of
   SUSE Enterprise Storage 5.
  </p></div><p>
  Salt along with DeepSea is a <span class="emphasis"><em>stack</em></span> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with Salt:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Salt minions</em></span> are the nodes controlled by a dedicated
    node called Salt master. Salt minions have roles, for example Ceph OSD, Ceph Monitor,
    Ceph Manager, Object Gateway, iSCSI Gateway, or NFS Ganesha.
   </p></li><li class="listitem"><p>
    A Salt master runs its own Salt minion. It is required for running privileged
    tasks—for example creating, authorizing, and copying keys to
    minions—so that remote minions never need to run privileged tasks.
   </p><div id="id-1.4.4.2.5.2.2" data-id-title="Sharing Multiple Roles per Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing Multiple Roles per Server</h6><p>
     You will get the best performance from your Ceph cluster when each role
     is deployed on a separate node. But real deployments sometimes require
     sharing one node for multiple roles. To avoid troubles with performance
     and upgrade procedure, do not deploy the Ceph OSD, Metadata Server, or Ceph Monitor role to
     the Salt master.
    </p></div></li><li class="listitem"><p>
    Salt minions need to correctly resolve the Salt master's host name over the
    network. By default, they look for the <code class="systemitem">salt</code> host
    name, but you can specify any other network-reachable host name in the
    <code class="filename">/etc/salt/minion</code> file, see
    <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>.
   </p></li></ul></div><section class="sect1" id="cha-ceph-install-relnotes" data-id-title="Read the Release Notes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Read the Release Notes</span> <a title="Permalink" class="permalink" href="#cha-ceph-install-relnotes">#</a></h2></div></div></div><p>
   In the release notes you can find additional information on changes since
   the previous release of SUSE Enterprise Storage. Check the release notes to see
   whether:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     your hardware needs special considerations.
    </p></li><li class="listitem"><p>
     any used software packages have changed significantly.
    </p></li><li class="listitem"><p>
     special precautions are necessary for your installation.
    </p></li></ul></div><p>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </p><p>
   After having installed the package <span class="package">release-notes-ses</span>,
   find the release notes locally in the directory
   <code class="filename">/usr/share/doc/release-notes</code> or online at
   <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section><section class="sect1" id="deepsea-description" data-id-title="Introduction to DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Introduction to DeepSea</span> <a title="Permalink" class="permalink" href="#deepsea-description">#</a></h2></div></div></div><p>
   The goal of DeepSea is to save the administrator time and confidently
   perform complex operations on a Ceph cluster.
  </p><p>
   Ceph is a very configurable software solution. It increases both the
   freedom and responsibility of system administrators.
  </p><p>
   The minimal Ceph setup is good for demonstration purposes, but does not
   show interesting features of Ceph that you can see with a big number of
   nodes.
  </p><p>
   DeepSea collects and stores data about individual servers, such as
   addresses and device names. For a distributed storage system such as Ceph,
   there can be hundreds of such items to collect and store. Collecting the
   information and entering the data manually into a configuration management
   tool is exhausting and error prone.
  </p><p>
   The steps necessary to prepare the servers, collect the configuration, and
   configure and deploy Ceph are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </p><p>
   DeepSea addresses these observations with the following strategy:
   DeepSea consolidates the administrator's decisions in a single file. The
   decisions include cluster assignment, role assignment and profile
   assignment. And DeepSea collects each set of tasks into a simple goal.
   Each goal is a <span class="emphasis"><em>stage</em></span>:
  </p><div class="itemizedlist" id="deepsea-stage-description" data-id-title="DeepSea Stages Description"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea Stages Description </span><a title="Permalink" class="permalink" href="#deepsea-stage-description">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Stage 0</strong></span>—the
     <span class="bold"><strong>preparation</strong></span>— during this stage, all
     required updates are applied and your system may be rebooted.
    </p><div id="id-1.4.4.2.7.8.2.2" data-id-title="Re-run Stage 0 after Salt master Reboot" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Re-run Stage 0 after Salt master Reboot</h6><p>
      If, during Stage 0, the Salt master reboots to load the new kernel version,
      you need to run Stage 0 again, otherwise minions will not be targeted.
     </p></div></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 1</strong></span>—the
     <span class="bold"><strong>discovery</strong></span>—here you detect all
     hardware in your cluster and collect necessary information for the Ceph
     configuration. For details about configuration, refer to
     <a class="xref" href="#deepsea-pillar-salt-configuration" title="4.5. Configuration and Customization">Section 4.5, “Configuration and Customization”</a>.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 2</strong></span>—the
     <span class="bold"><strong>configuration</strong></span>—you need to prepare
     configuration data in a particular format.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 3</strong></span>—the
     <span class="bold"><strong>deployment</strong></span>—creates a basic Ceph
     cluster with mandatory Ceph services. See
     <a class="xref" href="#storage-intro-core-nodes" title="1.2.3. Ceph Nodes and Daemons">Section 1.2.3, “Ceph Nodes and Daemons”</a> for their list.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 4</strong></span>—the
     <span class="bold"><strong>services</strong></span>—additional features of
     Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each
     is optional.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 5</strong></span>—the removal stage. This
     stage is not mandatory and during the initial setup it is usually not
     needed. In this stage the roles of minions and also the cluster
     configuration are removed. You need to run this stage when you need to
     remove a storage node from your cluster. For details refer to
     <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.3 “Removing and Reinstalling Cluster Nodes”</span>.
    </p></li></ul></div><p>
   You can find a more detailed introduction into DeepSea at
   <a class="link" href="https://github.com/suse/deepsea/wiki" target="_blank">https://github.com/suse/deepsea/wiki</a>.
  </p><section class="sect2" id="deepsea-organisation-locations" data-id-title="Organization and Important Locations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.1 </span><span class="title-name">Organization and Important Locations</span> <a title="Permalink" class="permalink" href="#deepsea-organisation-locations">#</a></h3></div></div></div><p>
    Salt has several standard locations and several naming conventions used
    on your master node:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.7.10.3.1"><span class="term"><code class="filename">/srv/pillar</code></span></dt><dd><p>
       The directory stores configuration data for your cluster minions.
       <span class="emphasis"><em>Pillar</em></span> is an interface for providing global
       configuration values to all your cluster minions.
      </p></dd><dt id="id-1.4.4.2.7.10.3.2"><span class="term"><code class="filename">/srv/salt/</code></span></dt><dd><p>
       The directory stores Salt state files (also called
       <span class="emphasis"><em>sls</em></span> files). State files are formatted descriptions
       of states in which the cluster should be. For more information, refer to
       the
       <a class="link" href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html" target="_blank">Salt
       documentation</a>.
      </p></dd><dt id="id-1.4.4.2.7.10.3.3"><span class="term"><code class="filename">/srv/module/runners</code></span></dt><dd><p>
       The directory stores Python scripts known as runners. Runners are
       executed on the master node.
      </p></dd><dt id="id-1.4.4.2.7.10.3.4"><span class="term"><code class="filename">/srv/salt/_modules</code></span></dt><dd><p>
       The directory stores Python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </p></dd><dt id="id-1.4.4.2.7.10.3.5"><span class="term"><code class="filename">/srv/pillar/ceph</code></span></dt><dd><p>
       The directory is used by DeepSea. Collected configuration data are
       stored here.
      </p></dd><dt id="id-1.4.4.2.7.10.3.6"><span class="term"><code class="filename">/srv/salt/ceph</code></span></dt><dd><p>
       A directory used by DeepSea. It stores sls files that can be in
       different formats, but each subdirectory contains sls files. Each
       subdirectory contains only one type of sls file. For example,
       <code class="filename">/srv/salt/ceph/stage</code> contains orchestration files
       that are executed by <code class="command">salt-run state.orchestrate</code>.
      </p></dd></dl></div></section><section class="sect2" id="ds-minion-targeting" data-id-title="Targeting the Minions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.2 </span><span class="title-name">Targeting the Minions</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting">#</a></h3></div></div></div><p>
    DeepSea commands are executed via the Salt infrastructure. When using
    the <code class="command">salt</code> command, you need to specify a set of
    Salt minions that the command will affect. We describe the set of the minions
    as a <span class="emphasis"><em>target</em></span> for the <code class="command">salt</code> command.
    The following sections describe possible methods to target the minions.
   </p><section class="sect3" id="ds-minion-targeting-name" data-id-title="Matching the Minion Name"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.2.2.1 </span><span class="title-name">Matching the Minion Name</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-name">#</a></h4></div></div></div><p>
     You can target a minion or a group of minions by matching their names. A
     minion's name is usually the short host name of the node where the minion
     runs. This is a general Salt targeting method, not related to DeepSea.
     You can use globbing, regular expressions, or lists to limit the range of
     minion names. The general syntax follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> example.module</pre></div><div id="id-1.4.4.2.7.11.3.4" data-id-title="Ceph-only Cluster" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Ceph-only Cluster</h6><p>
      If all Salt minions in your environment belong to your Ceph cluster, you
      can safely substitute <em class="replaceable">target</em> with
      <code class="literal">'*'</code> to include <span class="emphasis"><em>all</em></span> registered
      minions.
     </p></div><p>
     Match all minions in the example.net domain (assuming the minion names are
     identical to their "full" host names):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div><p>
     Match the 'web1' to 'web5' minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'web[1-5]' test.ping</pre></div><p>
     Match both 'web1-prod' and 'web1-devel' minions using a regular
     expression:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -E 'web1-(prod|devel)' test.ping</pre></div><p>
     Match a simple list of minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -L 'web1,web2,web3' test.ping</pre></div><p>
     Match all minions in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-grain" data-id-title="Targeting with a deepsea Grain"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.2.2.2 </span><span class="title-name">Targeting with a 'deepsea' Grain</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-grain">#</a></h4></div></div></div><p>
     In a heterogeneous Salt-managed environment where SUSE Enterprise Storage
     5.5 is deployed on a subset of nodes alongside other cluster
     solution(s), it is a good idea to 'mark' the relevant minions by applying
     a 'deepsea' grain to them. This way you can easily target DeepSea
     minions in environments where matching by the minion name is problematic.
    </p><p>
     To apply the 'deepsea' grain to a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.append deepsea default</pre></div><p>
     To remove the 'deepsea' grain from a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.delval deepsea destructive=True</pre></div><p>
     After applying the 'deepsea' grain to the relevant minions, you can target
     them as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -G 'deepsea:*' test.ping</pre></div><p>
     The following command is an equivalent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'G@deepsea:*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-dsminions" data-id-title="Set the deepsea_minions Option"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.2.2.3 </span><span class="title-name">Set the <code class="option">deepsea_minions</code> Option</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-dsminions">#</a></h4></div></div></div><p>
     Setting the <code class="option">deepsea_minions</code> option's target is a
     requirement for DeepSea deployments. DeepSea uses it to instruct
     minions during stages execution (refer to
     <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> for details.
    </p><p>
     To set or change the <code class="option">deepsea_minions</code> option, edit the
     <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code> file on the
     Salt master and add or replace the following line:
    </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: <em class="replaceable">target</em></pre></div><div id="id-1.4.4.2.7.11.5.5" data-id-title="deepsea_minions Target" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="option">deepsea_minions</code> Target</h6><p>
      As the <em class="replaceable">target</em> for the
      <code class="option">deepsea_minions</code> option, you can use any targeting
      method: both
      <a class="xref" href="#ds-minion-targeting-name" title="4.2.2.1. Matching the Minion Name">Matching the Minion Name</a> and
      <a class="xref" href="#ds-minion-targeting-grain" title="4.2.2.2. Targeting with a 'deepsea' Grain">Targeting with a 'deepsea' Grain</a>.
     </p><p>
      Match all Salt minions in the cluster:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: '*'</pre></div><p>
      Match all minions with the 'deepsea' grain:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: 'G@deepsea:*'</pre></div></div></section><section class="sect3" id="id-1.4.4.2.7.11.6" data-id-title="For More Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.2.2.4 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.4.4.2.7.11.6">#</a></h4></div></div></div><p>
     You can use more advanced ways to target minions using the Salt
     infrastructure. Refer to
     <a class="link" href="https://docs.saltstack.com/en/latest/topics/targeting/" target="_blank">https://docs.saltstack.com/en/latest/topics/targeting/</a>
     for a description of all targeting techniques.
    </p><p>
     Also, the 'deepsea-minions' manual page gives you more detail about
     DeepSea targeting (<code class="command">man 7 deepsea_minions</code>).
    </p></section></section></section><section class="sect1" id="ceph-install-stack" data-id-title="Cluster Deployment"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">Cluster Deployment</span> <a title="Permalink" class="permalink" href="#ceph-install-stack">#</a></h2></div></div></div><p>
   The cluster deployment process has several phases. First, you need to
   prepare all nodes of the cluster by configuring Salt and then deploy and
   configure Ceph.
  </p><div id="id-1.4.4.2.8.3" data-id-title="Deploying Monitor Nodes without Defining OSD Profiles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Deploying Monitor Nodes without Defining OSD Profiles</h6><p>
    If you need to skip defining OSD profiles and deploy the monitor nodes
    first, you can do so by setting the <code class="option">DEV_ENV</code> variable. It
    allows deploying monitors without the presence of the
    <code class="filename">profile/</code> directory, as well as deploying a cluster
    with at least <span class="emphasis"><em>one</em></span> storage, monitor, and manager node.
   </p><p>
    To set the environment variable, either enable it globally by setting it in
    the <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file, or set it
    for the current shell session only:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>export DEV_ENV=true</pre></div></div><p>
   The following procedure describes the cluster preparation in detail.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install and register SUSE Linux Enterprise Server 12 SP3 together with SUSE Enterprise Storage 5.5
     extension on each node of the cluster.
    </p><div id="id-1.4.4.2.8.5.1.2" data-id-title="SUSE Linux Enterprise Server 12 SP4 Not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: SUSE Linux Enterprise Server 12 SP4 Not Supported</h6><p>
      SUSE Linux Enterprise Server 12 SP4 is <span class="bold"><strong>not</strong></span> a supported base
      operating system for SUSE Enterprise Storage 5.5.
     </p></div></li><li class="step"><p>
     Verify that proper products are installed and registered by listing
     existing software repositories. The list will be similar to this output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper lr -E
#  | Alias   | Name                              | Enabled | GPG Check | Refresh
---+---------+-----------------------------------+---------+-----------+--------
 4 | [...]   | SUSE-Enterprise-Storage-5-Pool    | Yes     | (r ) Yes  | No
 6 | [...]   | SUSE-Enterprise-Storage-5-Updates | Yes     | (r ) Yes  | Yes
 9 | [...]   | SLES12-SP3-Pool                   | Yes     | (r ) Yes  | No
11 | [...]   | SLES12-SP3-Updates                | Yes     | (r ) Yes  | Yes</pre></div><div id="id-1.4.4.2.8.5.2.3" data-id-title="LTSS Repositories Are Not Required" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: LTSS Repositories Are Not Required</h6><p>
      LTSS updates for SUSE Linux Enterprise Server are delivered as part of the SUSE Enterprise Storage
      5.5 repositories. Therefore, no LTSS repositories need to be
      added.
     </p></div></li><li class="step"><p>
     Configure network settings including proper DNS name resolution on each
     node. The Salt master and all the Salt minions need to resolve each other by
     their host names. For more information on configuring a network, see
     <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-basicnet-yast" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-basicnet-yast</a>
     For more information on configuring a DNS server, see
     <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-dns" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-dns</a>.
    </p></li><li class="step"><p>
     Select one or more time servers/pools, and synchronize the local time
     against them. Verify that the time synchronization service is enabled on
     each system start-up. You can use the <code class="command">yast ntp-client</code>
     command found in a <span class="package">yast2-ntp-client</span> package to
     configure time synchronization.
    </p><div id="id-1.4.4.2.8.5.4.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      Virtual machines are not reliable NTP sources.
     </p></div><p>
     Find more information on setting up NTP in
     <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-ntp-yast" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-ntp-yast</a>.
    </p></li><li class="step"><p>
     Install the <code class="literal">salt-master</code> and
     <code class="literal">salt-minion</code> packages on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in salt-master salt-minion</pre></div><p>
     Check that the <code class="systemitem">salt-master</code> service is enabled and
     started, and enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl enable salt-master.service
<code class="prompt user">root@master # </code>systemctl start salt-master.service</pre></div></li><li class="step"><p>
     If you intend to use firewall, verify that the Salt master node has ports
     4505 and 4506 open to all Salt minion nodes. If the ports are closed, you
     can open them using the <code class="command">yast2 firewall</code> command by
     allowing the <span class="guimenu">SaltStack</span> service.
    </p><div id="id-1.4.4.2.8.5.6.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
      DeepSea deployment stages fail when firewall is active (and even
      configured). To pass the stages correctly, you need to either turn the
      firewall off by running
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop SuSEfirewall2.service</pre></div><p>
      or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div></li><li class="step"><p>
     Install the package <code class="literal">salt-minion</code> on all minion nodes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in salt-minion</pre></div><p>
     Make sure that the <span class="emphasis"><em>fully qualified domain name</em></span> of
     each node can be resolved to the public network IP address by all other
     nodes.
    </p></li><li class="step"><p>
     Configure all minions (including the master minion) to connect to the
     master. If your Salt master is not reachable by the host name
     <code class="literal">salt</code>, edit the file
     <code class="filename">/etc/salt/minion</code> or create a new file
     <code class="filename">/etc/salt/minion.d/master.conf</code> with the following
     content:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">host_name_of_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the Salt service on all Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     Check that the <code class="systemitem">salt-minion</code> service is enabled and
     started on all nodes. Enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable salt-minion.service
<code class="prompt user">root # </code>systemctl start salt-minion.service</pre></div></li><li class="step"><p>
     Verify each Salt minion's fingerprint and accept all salt keys on the
     Salt master if the fingerprints match.
    </p><p>
     View each minion's fingerprint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     After gathering fingerprints of all the Salt minions, list fingerprints of
     all unaccepted minion keys on the Salt master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     If the minions' fingerprint match, accept them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept-all</pre></div></li><li class="step"><p>
     Verify that the keys have been accepted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --list-all</pre></div></li><li class="step" id="deploy-wiping-disk"><p>
     Prior to deploying SUSE Enterprise Storage 5.5, manually zap all the
     disks. Remember to replace 'X' with the correct disk letter:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Stop all processes that are using the specific disk.
      </p></li><li class="step"><p>
       Verify whether any partition on the disk is mounted, and unmount if
       needed.
      </p></li><li class="step"><p>
       If the disk is managed by LVM, deactivate and delete the whole LVM
       infrastructure. Refer to
       <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#cha-lvm" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#cha-lvm</a>
       for more details.
      </p></li><li class="step"><p>
       If the disk is part of MD RAID, deactivate the RAID. Refer to
       <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#part-software-raid" target="_blank">https://documentation.suse.com/sles/12-SP5/single-html/SLES-storage/#part-software-raid</a>
       for more details.
      </p></li><li class="step"><div id="id-1.4.4.2.8.5.12.2.5.1" data-id-title="Rebooting the Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Rebooting the Server</h6><p>
        If you get error messages such as 'partition in use' or 'kernel can not
        be updated with the new partition table' during the following steps,
        reboot the server.
       </p></div><p>
       Wipe the beginning of each partition (as <code class="systemitem">root</code>):
      </p><div class="verbatim-wrap"><pre class="screen">for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</pre></div></li><li class="step"><p>
       Wipe the beginning of the drive:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct</pre></div></li><li class="step"><p>
       Wipe the end of the drive:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct</pre></div></li><li class="step"><p>
       Create a new GPT partition table:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>sgdisk -Z --clear -g /dev/sdX</pre></div></li><li class="step"><p>
       Verify the result with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>parted -s /dev/sdX print free</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C</pre></div></li></ol></li><li class="step"><p>
     Optionally, if you need to preconfigure the cluster's network settings
     before the <span class="package">deepsea</span> package is installed, create
     <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> manually and
     set the <code class="option">cluster_network:</code> and
     <code class="option">public_network:</code> options. Note that the file will not be
     overwritten after you install <span class="package">deepsea</span>.
    </p></li><li class="step"><p>
     Install DeepSea on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in deepsea</pre></div></li><li class="step"><p>
     Check that the file
     <code class="filename">/srv/pillar/ceph/master_minion.sls</code> on the Salt master
     points to your Salt master. If your Salt master is reachable via more host
     names, use the one suitable for the storage cluster. If you used the
     default host name for your
     Salt master—<span class="emphasis"><em>salt</em></span>—in the
     <span class="emphasis"><em>ses</em></span> domain, then the file looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">master_minion: salt.ses</pre></div></li></ol></div></div><p>
   Now you deploy and configure Ceph. Unless specified otherwise, all steps
   are mandatory.
  </p><div id="id-1.4.4.2.8.7" data-id-title="Salt Command Conventions" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Salt Command Conventions</h6><p>
    There are two possible ways how to run <code class="command">salt-run
    state.orch</code>—one is with <code class="literal">stage.&lt;stage
    number&gt;</code>, the other is with the name of the stage. Both
    notations have the same impact and it is fully your preference which
    command you use.
   </p></div><div class="procedure" id="ds-depl-stages" data-id-title="Running Deployment Stages"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 4.1: </span><span class="title-name">Running Deployment Stages </span><a title="Permalink" class="permalink" href="#ds-depl-stages">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Include the Salt minions belonging to the Ceph cluster that you are
     currently deploying. Refer to <a class="xref" href="#ds-minion-targeting-name" title="4.2.2.1. Matching the Minion Name">Section 4.2.2.1, “Matching the Minion Name”</a>
     for more information on targeting the minions.
    </p></li><li class="step"><p>
     Prepare your cluster. Refer to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>
     for more details.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.prep</pre></div><div id="id-1.4.4.2.8.8.3.5" data-id-title="Run or Monitor Stages using DeepSea CLI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Run or Monitor Stages using DeepSea CLI</h6><p>
      Using the DeepSea CLI, you can follow the stage execution progress in
      real-time, either by running the DeepSea CLI in the monitoring mode, or
      by running the stage directly through DeepSea CLI. For details refer to
      <a class="xref" href="#deepsea-cli" title="4.4. DeepSea CLI">Section 4.4, “DeepSea CLI”</a>.
     </p></div></li><li class="step"><p>
     <span class="emphasis"><em>Optional</em></span>: create Btrfs sub-volumes for
     <code class="filename">/var/lib/ceph/</code>. This step should only be executed
     before the next stages of DeepSea have been executed. To migrate
     existing directories or for more details, see
     <span class="intraxref">Book “Administration Guide”, Chapter 20 “Hints and Tips”, Section 20.6 “Btrfs Sub-volume for /var/lib/ceph”</span>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.migrate.subvolume</pre></div></li><li class="step"><p>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <code class="filename">/srv/pillar/ceph/proposals</code>. The data are stored in
     the YAML format in *.sls or *.yml files.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.discovery</pre></div></li><li class="step"><p>
     After the previous command finishes successfully, create a
     <code class="filename">policy.cfg</code> file in
     <code class="filename">/srv/pillar/ceph/proposals</code>. For details refer to
     <a class="xref" href="#policy-configuration" title="4.5.1. The policy.cfg File">Section 4.5.1, “The <code class="filename">policy.cfg</code> File”</a>.
    </p><div id="id-1.4.4.2.8.8.6.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you need to change the cluster's network setting, edit
      <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> and adjust
      the lines starting with <code class="literal">cluster_network:</code> and
      <code class="literal">public_network:</code>.
     </p></div></li><li class="step"><p>
     The configuration stage parses the <code class="filename">policy.cfg</code> file
     and merges the included files into their final form. Cluster and role
     related content are placed in
     <code class="filename">/srv/pillar/ceph/cluster</code>, while Ceph specific
     content is placed in <code class="filename">/srv/pillar/ceph/stack/default</code>.
    </p><p>
     Run the following command to trigger the configuration stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.configure</pre></div><p>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for the specified minions (for
     example, named <code class="literal">ceph_minion1</code>,
     <code class="literal">ceph_minion2</code>, etc.) by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'ceph_minion*' pillar.items</pre></div><div id="id-1.4.4.2.8.8.7.8" data-id-title="Overwriting Defaults" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Overwriting Defaults</h6><p>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <a class="xref" href="#ceph-deploy-ds-custom" title="Chapter 7. Customizing the Default Configuration">Chapter 7, <em>Customizing the Default Configuration</em></a>.
     </p></div></li><li class="step"><p>
     Now you run the deployment stage. In this stage, the pillar is validated,
     and monitors and ODS daemons are started on the storage nodes. Run the
     following to start the stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.deploy</pre></div><p>
     The command may take several minutes. If it fails, you need to fix the
     issue and run the previous stages again. After the command succeeds, run
     the following to check the status:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph -s</pre></div></li><li class="step"><p>
     The last step of the Ceph cluster deployment is the
     <span class="emphasis"><em>services</em></span> stage. Here you instantiate any of the
     currently supported services: iSCSI Gateway, CephFS, Object Gateway, openATTIC, and NFS Ganesha.
     In this stage, the necessary pools, authorizing keyrings, and starting
     services are created. To start the stage, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.services</pre></div><p>
     Depending on the setup, the command may run for several minutes.
    </p></li></ol></div></div></section><section class="sect1" id="deepsea-cli" data-id-title="DeepSea CLI"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.4 </span><span class="title-name">DeepSea CLI</span> <a title="Permalink" class="permalink" href="#deepsea-cli">#</a></h2></div></div></div><p>
   DeepSea also provides a CLI tool that allows the user to monitor or run
   stages while visualizing the execution progress in real-time.
  </p><p>
   Two modes are supported for visualizing a stage's execution progress:
  </p><div class="itemizedlist" id="deepsea-cli-modes" data-id-title="DeepSea CLI Modes"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea CLI Modes </span><a title="Permalink" class="permalink" href="#deepsea-cli-modes">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Monitoring mode</strong></span>: visualizes the execution
     progress of a DeepSea stage triggered by the <code class="command">salt-run</code>
     command issued in another terminal session.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stand-alone mode</strong></span>: runs a DeepSea stage
     while providing real-time visualization of its component steps as they are
     executed.
    </p></li></ul></div><div id="id-1.4.4.2.9.5" data-id-title="DeepSea CLI Commands" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: DeepSea CLI Commands</h6><p>
    The DeepSea CLI commands can only be run on the Salt master node with the
    <code class="systemitem">root</code> privileges.
   </p></div><section class="sect2" id="deepsea-cli-monitor" data-id-title="DeepSea CLI: Monitor Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.4.1 </span><span class="title-name">DeepSea CLI: Monitor Mode</span> <a title="Permalink" class="permalink" href="#deepsea-cli-monitor">#</a></h3></div></div></div><p>
    The progress monitor provides a detailed, real-time visualization of what
    is happening during execution of stages using <code class="command">salt-run
    state.orch</code> commands in other terminal sessions.
   </p><div id="id-1.4.4.2.9.6.3" data-id-title="Start Monitor in a New Terminal Session" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Start Monitor in a New Terminal Session</h6><p>
     You need to start the monitor in a new terminal window
     <span class="emphasis"><em>before</em></span> running any <code class="command">salt-run
     state.orch</code> so that the monitor can detect the start of the
     stage's execution.
    </p></div><p>
    If you start the monitor after issuing the <code class="command">salt-run
    state.orch</code> command, then no execution progress will be shown.
   </p><p>
    You can start the monitor mode by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea monitor</pre></div><p>
    For more information about the available command line options of the
    <code class="command">deepsea monitor</code> command check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>man deepsea-monitor</pre></div></section><section class="sect2" id="deepsea-cli-standalone" data-id-title="DeepSea CLI: Stand-alone Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.4.2 </span><span class="title-name">DeepSea CLI: Stand-alone Mode</span> <a title="Permalink" class="permalink" href="#deepsea-cli-standalone">#</a></h3></div></div></div><p>
    In the stand-alone mode, DeepSea CLI can be used to run a DeepSea
    stage, showing its execution in real-time.
   </p><p>
    The command to run a DeepSea stage from the DeepSea CLI has the
    following form:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea stage run <em class="replaceable">stage-name</em></pre></div><p>
    where <em class="replaceable">stage-name</em> corresponds to the way Salt
    orchestration state files are referenced. For example, stage
    <span class="bold"><strong>deploy</strong></span>, which corresponds to the directory
    located in <code class="filename">/srv/salt/ceph/stage/deploy</code>, is referenced
    as <span class="bold"><strong>ceph.stage.deploy</strong></span>.
   </p><p>
    This command is an alternative to the Salt-based commands for running
    DeepSea stages (or any DeepSea orchestration state file).
   </p><p>
    The command <code class="command">deepsea stage run ceph.stage.0</code> is equivalent
    to <code class="command">salt-run state.orch ceph.stage.0</code>.
   </p><p>
    For more information about the available command line options accepted by
    the <code class="command">deepsea stage run</code> command check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>man deepsea-stage run</pre></div><p>
    In the following figure shows an example of the output of the DeepSea CLI
    when running <span class="underline">Stage 2</span>:
   </p><div class="figure" id="id-1.4.4.2.9.7.11"><div class="figure-contents"><div class="mediaobject"><a href="images/deepsea-cli-stage2-screenshot.png" target="_blank"><img src="images/deepsea-cli-stage2-screenshot.png" width="" alt="DeepSea CLI stage execution progress output"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.1: </span><span class="title-name">DeepSea CLI stage execution progress output </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.7.11">#</a></h6></div></div><section class="sect3" id="deepsea-cli-run-alias" data-id-title="DeepSea CLI stage run Alias"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.4.2.1 </span><span class="title-name">DeepSea CLI <code class="command">stage run</code> Alias</span> <a title="Permalink" class="permalink" href="#deepsea-cli-run-alias">#</a></h4></div></div></div><p>
     For advanced users of Salt, we also support an alias for running a
     DeepSea stage that takes the Salt command used to run a stage, for
     example, <code class="command">salt-run state.orch
     <em class="replaceable">stage-name</em></code>, as a command of the
     DeepSea CLI.
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea salt-run state.orch <em class="replaceable">stage-name</em></pre></div></section></section></section><section class="sect1" id="deepsea-pillar-salt-configuration" data-id-title="Configuration and Customization"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.5 </span><span class="title-name">Configuration and Customization</span> <a title="Permalink" class="permalink" href="#deepsea-pillar-salt-configuration">#</a></h2></div></div></div><section class="sect2" id="policy-configuration" data-id-title="The policy.cfg File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.5.1 </span><span class="title-name">The <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="#policy-configuration">#</a></h3></div></div></div><p>
    The <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>
    configuration file is used to determine roles of individual cluster nodes.
    For example, which node acts as an OSD or which as a monitor node. Edit
    <code class="filename">policy.cfg</code> in order to reflect your desired cluster
    setup. The order of the sections is arbitrary, but the content of included
    lines overwrites matching keys from the content of previous lines.
   </p><div id="id-1.4.4.2.10.2.3" data-id-title="Examples of policy.cfg" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Examples of <code class="filename">policy.cfg</code></h6><p>
     You can find several examples of complete policy files in the
     <code class="filename">/usr/share/doc/packages/deepsea/examples/</code> directory.
    </p></div><section class="sect3" id="policy-cluster-assignment" data-id-title="Cluster Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.1 </span><span class="title-name">Cluster Assignment</span> <a title="Permalink" class="permalink" href="#policy-cluster-assignment">#</a></h4></div></div></div><p>
     In the <span class="bold"><strong>cluster</strong></span> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <span class="bold"><strong>ceph</strong></span> follow.
    </p><p>
     To include <span class="bold"><strong>all</strong></span> minions, add the following
     lines:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/*.sls</pre></div><p>
     To <span class="bold"><strong>whitelist</strong></span> a particular minion:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/abc.domain.sls</pre></div><p>
     or a group of minions—you can shell glob matching:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/mon*.sls</pre></div><p>
     To <span class="bold"><strong>blacklist</strong></span> minions, set the them to
     <code class="literal">unassigned</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/client*.sls</pre></div></section><section class="sect3" id="policy-role-assignment" data-id-title="Role Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.2 </span><span class="title-name">Role Assignment</span> <a title="Permalink" class="permalink" href="#policy-role-assignment">#</a></h4></div></div></div><p>
     This section provides you with details on assigning 'roles' to your
     cluster nodes. A 'role' in this context means the service you need to run
     on the node, such as Ceph Monitor, Object Gateway, iSCSI Gateway, or openATTIC. No role is assigned
     automatically, only roles added to <code class="command">policy.cfg</code> will be
     deployed.
    </p><p>
     The assignment follows this pattern:
    </p><div class="verbatim-wrap"><pre class="screen">role-<em class="replaceable">ROLE_NAME</em>/<em class="replaceable">PATH</em>/<em class="replaceable">FILES_TO_INCLUDE</em></pre></div><p>
     Where the items have the following meaning and values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <em class="replaceable">ROLE_NAME</em> is any of the following: 'master',
       'admin', 'mon', 'mgr', 'mds', 'igw', 'rgw', 'ganesha', or 'openattic'.
      </p></li><li class="listitem"><p>
       <em class="replaceable">PATH</em> is a relative directory path to .sls or
       .yml files. In case of .sls files, it usually is
       <code class="filename">cluster</code>, while .yml files are located at
       <code class="filename">stack/default/ceph/minions</code>.
      </p></li><li class="listitem"><p>
       <em class="replaceable">FILES_TO_INCLUDE</em> are the Salt state files
       or YAML configuration files. They normally consist of Salt minions host
       names, for example <code class="filename">ses5min2.yml</code>. Shell globbing can
       be used for more specific matching.
      </p></li></ul></div><p>
     An example for each role follows:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>master</em></span> - the node has admin keyrings to all Ceph
       clusters. Currently, only a single Ceph cluster is supported. As the
       <span class="emphasis"><em>master</em></span> role is mandatory, always add a similar line
       to the following:
      </p><div class="verbatim-wrap"><pre class="screen">role-master/cluster/master*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>admin</em></span> - the minion will have an admin keyring. You
       define the role as follows:
      </p><div class="verbatim-wrap"><pre class="screen">role-admin/cluster/abc*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mon</em></span> - the minion will provide the monitoring
       service to the Ceph cluster. This role requires addresses of the
       assigned minions. As of SUSE Enterprise Storage 5.5, the public
       address are calculated dynamically and are no longer needed in the
       Salt pillar.
      </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls</pre></div><p>
       The example assigns the monitoring role to a group of minions.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>mgr</em></span> - the Ceph manager daemon which collects all
       the state information from the whole cluster. Deploy it on all minions
       where you plan to deploy the Ceph monitor role.
      </p><div class="verbatim-wrap"><pre class="screen">role-mgr/cluster/mgr*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mds</em></span> - the minion will provide the metadata service
       to support CephFS.
      </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mds*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>igw</em></span> - the minion will act as an iSCSI Gateway. This role
       requires addresses of the assigned minions, thus you need to also
       include the files from the <code class="filename">stack</code> directory:
      </p><div class="verbatim-wrap"><pre class="screen">role-igw/cluster/*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>rgw</em></span> - the minion will act as an Object Gateway:
      </p><div class="verbatim-wrap"><pre class="screen">role-rgw/cluster/rgw*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>openattic</em></span> - the minion will act as an openATTIC server:
      </p><div class="verbatim-wrap"><pre class="screen">role-openattic/cluster/openattic*.sls</pre></div><p>
       For more information, see <span class="intraxref">Book “Administration Guide”, Chapter 17 “openATTIC”</span>.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>ganesha</em></span> - the minion will act as an NFS Ganesha
       server. The 'ganesha' role requires either an 'rgw' or 'mds' role in
       cluster, otherwise the validation will fail in Stage 3.
      </p><p>
       To successfully install NFS Ganesha, additional configuration is required.
       If you want to use NFS Ganesha, read <a class="xref" href="#cha-as-ganesha" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a>
       before executing stages 2 and 4. However, it is possible to install
       NFS Ganesha later.
      </p><p>
       In some cases it can be useful to define custom roles for NFS Ganesha
       nodes. For details, see <span class="intraxref">Book “Administration Guide”, Chapter 16 “NFS Ganesha: Export Ceph Data via NFS”, Section 16.3 “Custom NFS Ganesha Roles”</span>.
      </p></li></ul></div><div id="id-1.4.4.2.10.2.5.9" data-id-title="Multiple Roles of Cluster Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Multiple Roles of Cluster Nodes</h6><p>
      You can assign several roles to a single node. For example, you can
      assign the mds roles to the monitor nodes:
     </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mon[1,2]*.sls</pre></div></div></section><section class="sect3" id="policy-common-configuration" data-id-title="Common Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.3 </span><span class="title-name">Common Configuration</span> <a title="Permalink" class="permalink" href="#policy-common-configuration">#</a></h4></div></div></div><p>
     The common configuration section includes configuration files generated
     during the <span class="emphasis"><em>discovery (Stage 1)</em></span>. These configuration
     files store parameters like <code class="literal">fsid</code> or
     <code class="literal">public_network</code>. To include the required Ceph common
     configuration, add the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</pre></div></section><section class="sect3" id="policy-profile-assignment" data-id-title="Profile Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.4 </span><span class="title-name">Profile Assignment</span> <a title="Permalink" class="permalink" href="#policy-profile-assignment">#</a></h4></div></div></div><p>
     In Ceph, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. DeepSea stage
     1 will generate a default storage profile proposal. By default this
     proposal will be a <code class="literal">bluestore</code> profile and will try to
     propose the highest performing configuration for the given hardware setup.
     For example, external journals will be preferred over a single disk
     containing objects and metadata. Solid state storage will be prioritized
     over spinning disks. Profiles are assigned in the
     <code class="filename">policy.cfg</code> similar to roles.
    </p><p>
     The default proposal can be found in the profile-default directory tree.
     To include this add the following two lines to your
     <code class="filename">policy.cfg</code>.
    </p><div class="verbatim-wrap"><pre class="screen">profile-default/cluster/*.sls
profile-default/stack/default/ceph/minions/*.yml</pre></div><p>
     You can also create a customized storage profile to your liking by using
     the proposal runner. This runner offers three methods: help, peek, and
     populate.
    </p><p>
     <code class="command">salt-run proposal.help</code> prints the runner help text
     about the various arguments it accepts.
    </p><p>
     <code class="command">salt-run proposal.peek</code> shows the generated proposal
     according to the arguments passed.
    </p><p>
     <code class="command">salt-run proposal.populate</code> writes the proposal to the
     <code class="filename">/srv/pillar/ceph/proposals</code> subdirectory. Pass
     <code class="option">name=myprofile</code> to name the storage profile. This will
     result in a profile-myprofile subdirectory.
    </p><p>
     For all other arguments, consult the output of <code class="command">salt-run
     proposal.help</code>.
    </p></section><section class="sect3" id="stage1-override-devices" data-id-title="Overriding Default Search for Disk Devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.5 </span><span class="title-name">Overriding Default Search for Disk Devices</span> <a title="Permalink" class="permalink" href="#stage1-override-devices">#</a></h4></div></div></div><p>
     If you have a Salt minion with multiple disk devices assigned and the device
     names do not seem to be consistent or persistent, you can override the
     default search behavior by editing
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Edit <code class="filename">global.yml</code> and make the necessary changes:
      </p><p>
       To override the default match expression of <code class="option">-name ata* -o -name
       scsi* -o -name nvme*</code> for the <code class="command">find</code> command
       with for example <code class="option">-name wwn*</code>, add the following:
      </p><div class="verbatim-wrap"><pre class="screen">ceph:
  modules:
    cephdisks:
      device:
        match: '-name wwn*'</pre></div><p>
       To override the default pathname of <code class="filename">/dev/disk/by-id</code>
       with for example <code class="filename">/dev/disk/by-label</code>, add the
       following:
      </p><div class="verbatim-wrap"><pre class="screen">ceph:
  modules:
    cephdisks:
      device:
        pathname: '/dev/disk/by-label'</pre></div></li><li class="step"><p>
       Refresh the Pillar:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">root@master # </code>salt '<em class="replaceable">DEEPSEA_MINIONS</em>' saltutil.pillar_refresh</pre></div></li><li class="step"><p>
       Try a query for a device that was previously wrongly assigned:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">root@master # </code>salt 'SPECIFIC_MINION' cephdisks.device <em class="replaceable">PATH_TO_DEVICE</em></pre></div><p>
       If the command returns 'module not found', be sure to synchronize:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.sync_all</pre></div></li></ol></div></div></section><section class="sect3" id="ds-profile-osd-encrypted" data-id-title="Deploying Encrypted OSDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.6 </span><span class="title-name">Deploying Encrypted OSDs</span> <a title="Permalink" class="permalink" href="#ds-profile-osd-encrypted">#</a></h4></div></div></div><p>
     Since SUSE Enterprise Storage 5.5, OSDs are by default deployed using
     BlueStore instead of FileStore. Although BlueStore supports
     encryption, Ceph OSDs are deployed unencrypted by default. The following
     procedure describes steps to encrypt OSDs during the upgrade process. Let
     us assume that both data and WAL/DB disks to be used for OSD deployment
     are clean with no partitions. If the disk were previously used, wipe them
     following the procedure described in <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>.
    </p><p>
     To use encrypted OSDs for your new deployment, first wipe the disks
     following the procedure described in <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>,
     then use the <code class="literal">proposal.populate</code> runner with the
     <code class="option">encryption=dmcrypt</code> argument:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run proposal.populate encryption=dmcrypt</pre></div><div id="id-1.4.4.2.10.2.9.5" data-id-title="Slow Boots" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Slow Boots</h6><p>
      Encrypted OSDs require longer boot and activation times compared to the
      default unencrypted ones.
     </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Determine the <code class="option">bluestore block db size</code> and
       <code class="option">bluestore block wal size</code> values for your deployment and
       add them to the
       <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>
       file on the Salt master. The values need to be specified in bytes.
      </p><div class="verbatim-wrap"><pre class="screen">[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648</pre></div><p>
       For more information on customizing the <code class="filename">ceph.conf</code>
       file, refer to <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>.
      </p></li><li class="step"><p>
       Run DeepSea Stage 3 to distribute the changes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li><li class="step"><p>
       Verify that the <code class="filename">ceph.conf</code> file is updated on the
       relevant OSD nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>cat /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
       Edit the *.yml files in the
       <code class="filename">/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</code>
       directory that are relevant to the OSDs you are encrypting. Double check
       their path with the one defined in the
       <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> file to
       ensure that you modify the correct *.yml files.
      </p><div id="id-1.4.4.2.10.2.9.6.4.2" data-id-title="Long Disk Identifiers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Long Disk Identifiers</h6><p>
        When identifying OSD disks in the
        <code class="filename">/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</code>
        files, use long disk identifiers.
       </p></div><p>
       An example of an OSD configuration follows. Note that because we need
       encryption, the <code class="option">db_size</code> and <code class="option">wal_size</code>
       options are removed:
      </p><div class="verbatim-wrap"><pre class="screen">ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN</pre></div></li><li class="step"><p>
       Deploy the new Block Storage OSDs with encryption by running DeepSea
       Stages 2 and 3:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
       You can watch the progress with <code class="command">ceph -s</code> or
       <code class="command">ceph osd tree</code>. It is critical that you let the
       cluster rebalance before repeating the process on the next OSD node.
      </p></li></ol></div></div></section><section class="sect3" id="deepsea-policy-filtering" data-id-title="Item Filtering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.7 </span><span class="title-name">Item Filtering</span> <a title="Permalink" class="permalink" href="#deepsea-policy-filtering">#</a></h4></div></div></div><p>
     Sometimes it is not practical to include all files from a given directory
     with *.sls globbing. The <code class="filename">policy.cfg</code> file parser
     understands the following filters:
    </p><div id="id-1.4.4.2.10.2.10.3" data-id-title="Advanced Techniques" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Advanced Techniques</h6><p>
      This section describes filtering techniques for advanced users. When not
      used correctly, filtering can cause problems for example in case your
      node numbering changes.
     </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.10.2.10.4.1"><span class="term">slice=[start:end]</span></dt><dd><p>
        Use the slice filter to include only items <span class="emphasis"><em>start</em></span>
        through <span class="emphasis"><em>end-1</em></span>. Note that items in the given
        directory are sorted alphanumerically. The following line includes the
        third to fifth files from the <code class="filename">role-mon/cluster/</code>
        subdirectory:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/*.sls slice[3:6]</pre></div></dd><dt id="id-1.4.4.2.10.2.10.4.2"><span class="term">re=regexp</span></dt><dd><p>
        Use the regular expression filter to include only items matching the
        given expressions. For example:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</pre></div></dd></dl></div></section><section class="sect3" id="deepsea-example-policy-cfg" data-id-title="Example policy.cfg File"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.5.1.8 </span><span class="title-name">Example <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="#deepsea-example-policy-cfg">#</a></h4></div></div></div><p>
     Following is an example of a basic <code class="filename">policy.cfg</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">## Cluster Assignment
cluster-ceph/cluster/*.sls <span class="callout" id="co-policy-1">1</span>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <span class="callout" id="co-policy-2">2</span>
role-admin/cluster/sesclient*.sls <span class="callout" id="co-policy-3">3</span>

# MON
role-mon/cluster/ses-example-[123].sls <span class="callout" id="co-policy-5">4</span>

# MGR
role-mgr/cluster/ses-example-[123].sls <span class="callout" id="co-policy-mgr">5</span>

# MDS
role-mds/cluster/ses-example-4.sls <span class="callout" id="co-policy-6">6</span>

# IGW
role-igw/cluster/ses-example-4.sls <span class="callout" id="co-policy-10">7</span>

# RGW
role-rgw/cluster/ses-example-4.sls <span class="callout" id="co-policy-11">8</span>

# openATTIC
role-openattic/cluster/openattic*.sls <span class="callout" id="co-policy-oa">9</span>

# COMMON
config/stack/default/global.yml <span class="callout" id="co-policy-8">10</span>
config/stack/default/ceph/cluster.yml <span class="callout" id="co-policy-13">11</span>

## Profiles
profile-default/cluster/*.sls <span class="callout" id="co-policy-9">12</span>
profile-default/stack/default/ceph/minions/*.yml <span class="callout" id="co-policy-12">13</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Indicates that all minions are included in the Ceph cluster. If you
       have minions you do not want to include in the Ceph cluster, use:
      </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</pre></div><p>
       The first line marks all minions as unassigned. The second line
       overrides minions matching 'ses-example-*.sls', and assigns them to the
       Ceph cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-2"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minion called 'examplesesadmin' has the 'master' role. This, by the
       way, means it will get admin keys to the cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-3"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'sesclient*' will get admin keys as well.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-5"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (presumably three minions:
       ses-example-1, ses-example-2, and ses-example-3) will be set up as MON
       nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-mgr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (all MON nodes in the example)
       will be set up as MGR nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-6"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the MDS role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-10"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the IGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-11"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the RGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-oa"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Specifies to deploy the openATTIC user interface to administer the Ceph
       cluster. See <span class="intraxref">Book “Administration Guide”, Chapter 17 “openATTIC”</span> for more details.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-8"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-13"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-9"><span class="callout">12</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       We are telling DeepSea to use the default hardware profile for each
       minion. Choosing the default hardware profile means that we want all
       additional disks (other than the root disk) as OSDs.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-12"><span class="callout">13</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       We are telling DeepSea to use the default hardware profile for each
       minion. Choosing the default hardware profile means that we want all
       additional disks (other than the root disk) as OSDs.
      </p></td></tr></table></div></section></section><section class="sect2" id="id-1.4.4.2.10.3" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.5.2 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="#id-1.4.4.2.10.3">#</a></h3></div></div></div><p>
    If you need to put custom settings into the <code class="filename">ceph.conf</code>
    configuration file, see <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span> for more
    details.
   </p></section></section></section><section class="chapter" id="cha-ceph-upgrade" data-id-title="Upgrading from Previous Releases"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Upgrading from Previous Releases</span> <a title="Permalink" class="permalink" href="#cha-ceph-upgrade">#</a></h2></div></div></div><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage from the previous
  release(s) to version 5.5.
 </p><section class="sect1" id="ceph-upgrade-relnotes" data-id-title="Read the Release Notes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-relnotes">#</a></h2></div></div></div><p>
   In the release notes you can find additional information on changes since
   the previous release of SUSE Enterprise Storage. Check the release notes to see
   whether:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     your hardware needs special considerations.
    </p></li><li class="listitem"><p>
     any used software packages have changed significantly.
    </p></li><li class="listitem"><p>
     special precautions are necessary for your installation.
    </p></li></ul></div><p>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </p><p>
   After having installed the package <span class="package">release-notes-ses</span> ,
   find the release notes locally in the directory
   <code class="filename">/usr/share/doc/release-notes</code> or online at
   <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section><section class="sect1" id="ceph-upgrade-general" data-id-title="General Upgrade Procedure"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">General Upgrade Procedure</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-general">#</a></h2></div></div></div><p>
   Consider the following items before starting the upgrade procedure:
  </p><div class="variablelist"><dl class="variablelist"><dt id="upgrade-order"><span class="term">Upgrade Order</span></dt><dd><p>
      Before upgrading the Ceph cluster, you need to have both the underlying
      SUSE Linux Enterprise Server and SUSE Enterprise Storage correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example Ceph
      Object Gateways depend upon Ceph monitors and Ceph OSD daemons. We recommend
      upgrading in this order:
     </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
        Ceph Monitors
       </p></li><li class="listitem"><p>
        Ceph Managers
       </p></li><li class="listitem"><p>
        Ceph OSDs
       </p></li><li class="listitem"><p>
        Metadata Servers
       </p></li><li class="listitem"><p>
        Object Gateways
       </p></li><li class="listitem"><p>
        iSCSI Gateways
       </p></li><li class="listitem"><p>
        NFS Ganesha
       </p></li></ol></div></dd><dt id="id-1.4.4.3.5.3.2"><span class="term">Delete Unnecessary Operating System Snapshots</span></dt><dd><p>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </p></dd><dt id="id-1.4.4.3.5.3.3"><span class="term">Check Cluster Health</span></dt><dd><p>
      We recommend to check the cluster health before starting the upgrade
      procedure.
     </p></dd><dt id="id-1.4.4.3.5.3.4"><span class="term">Upgrade One by One</span></dt><dd><p>
      We recommend upgrading all the daemons of a specific type—for
      example all monitor daemons or all OSD daemons—one by one to ensure
      that they are all on the same release. We also recommend that you upgrade
      all the daemons in your cluster before you try to exercise new
      functionality in a release.
     </p><p>
      After all the daemons of a specific type are upgraded, check their
      status.
     </p><p>
      Ensure each monitor has rejoined the quorum after all monitors are
      upgraded:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mon stat</pre></div><p>
      Ensure each Ceph OSD daemon has rejoined the cluster after all OSDs are
      upgraded:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd stat</pre></div></dd><dt id="id-1.4.4.3.5.3.5"><span class="term">Set <code class="option">require-osd-release luminous</code> Flag</span></dt><dd><p>
      When the last OSD is upgraded to SUSE Enterprise Storage 5.5, the
      monitor nodes will detect that all OSDs are running the 'luminous'
      version of Ceph and they may complain that the
      <code class="option">require-osd-release luminous</code> osdmap flag is not set. In
      that case, you need to set this flag manually to acknowledge
      that—now that the cluster has been upgraded to 'luminous'—it
      cannot be downgraded back to Ceph 'jewel'. Set the flag by running the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd require-osd-release luminous</pre></div><p>
      After the command completes, the warning disappears.
     </p><p>
      On fresh installs of SUSE Enterprise Storage 5.5, this flag is set
      automatically when the Ceph monitors create the initial osdmap, so no
      end user action is needed.
     </p></dd></dl></div></section><section class="sect1" id="ds-migrate-osd-encrypted" data-id-title="Encrypting OSDs during Upgrade"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.3 </span><span class="title-name">Encrypting OSDs during Upgrade</span> <a title="Permalink" class="permalink" href="#ds-migrate-osd-encrypted">#</a></h2></div></div></div><p>
   Since SUSE Enterprise Storage 5.5, OSDs are by default deployed using
   BlueStore instead of FileStore. Although BlueStore supports
   encryption, Ceph OSDs are deployed unencrypted by default. The following
   procedure describes steps to encrypt OSDs during the upgrade process. Let us
   assume that both data and WAL/DB disks to be used for OSD deployment are
   clean with no partitions. If the disk were previously used, wipe them
   following the procedure described in <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>.
  </p><div id="id-1.4.4.3.6.3" data-id-title="One OSD at a Time" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: One OSD at a Time</h6><p>
    You need to deploy encrypted OSDs one by one, not simultaneously. The
    reason is that OSD's data is drained, and the cluster goes through several
    iterations of rebalancing.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Determine the <code class="option">bluestore block db size</code> and
     <code class="option">bluestore block wal size</code> values for your deployment and
     add them to the
     <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>
     file on the Salt master. The values need to be specified in bytes.
    </p><div class="verbatim-wrap"><pre class="screen">[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648</pre></div><p>
     For more information on customizing the <code class="filename">ceph.conf</code>
     file, refer to <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>.
    </p></li><li class="step"><p>
     Run DeepSea Stage 3 to distribute the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li><li class="step"><p>
     Verify that the <code class="filename">ceph.conf</code> file is updated on the
     relevant OSD nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>cat /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
     Edit the *.yml files in the
     <code class="filename">/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</code>
     directory that are relevant to the OSDs you are encrypting. Double check
     their path with the one defined in the
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> file to ensure
     that you modify the correct *.yml files.
    </p><div id="id-1.4.4.3.6.4.4.2" data-id-title="Long Disk Identifiers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Long Disk Identifiers</h6><p>
      When identifying OSD disks in the
      <code class="filename">/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</code>
      files, use long disk identifiers.
     </p></div><p>
     An example of an OSD configuration follows. Note that because we need
     encryption, the <code class="option">db_size</code> and <code class="option">wal_size</code>
     options are removed:
    </p><div class="verbatim-wrap"><pre class="screen">ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN</pre></div></li><li class="step"><p>
     Deploy the new Block Storage OSDs with encryption by running DeepSea
     Stages 2 and 3:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
     You can watch the progress with <code class="command">ceph -s</code> or
     <code class="command">ceph osd tree</code>. It is critical that you let the cluster
     rebalance before repeating the process on the next OSD node.
    </p></li></ol></div></div></section><section class="sect1" id="ceph-upgrade-4to5" data-id-title="Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.4 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-4to5">#</a></h2></div></div></div><div id="u4to5-softreq" data-id-title="Software Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Software Requirements</h6><p>
    You need to have the following software installed and updated to the latest
    package versions on all the Ceph nodes you want to upgrade before you can
    start with the upgrade procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      SUSE Linux Enterprise Server 12 SP2
     </p></li><li class="listitem"><p>
      SUSE Enterprise Storage 4
     </p></li></ul></div></div><div id="id-1.4.4.3.7.3" data-id-title="Points to Consider before the Upgrade" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Points to Consider before the Upgrade</h6><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Although the cluster is fully functional during the upgrade, DeepSea
      sets the 'noout' flag which prevents Ceph from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </p></li><li class="listitem"><p>
      To optimize the upgrade process, DeepSea upgrades your nodes in the
      order, based on their assigned role as recommended by Ceph upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </p><p>
      Note that DeepSea cannot prevent the prescribed order from being
      violated if a node runs multiple services.
     </p></li><li class="listitem"><p>
      Although the Ceph cluster is operational during the upgrade, nodes may
      get rebooted in order to apply, for example, new kernel versions. To
      reduce waiting I/O operations, we recommend declining incoming requests
      for the duration of the upgrade process.
     </p></li><li class="listitem"><p>
      The cluster upgrade may take a very long time—approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </p></li><li class="listitem"><p>
      Since Ceph Luminous, the <code class="option">osd crush location</code>
      configuration option is no longer supported. Update your DeepSea
      configuration files to use <code class="command">crush location</code> before
      upgrading.
     </p></li><li class="listitem"><p>
      There are two ways to obtain SUSE Linux Enterprise Server and SUSE Enterprise Storage 5.5
      update repositories:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        If your cluster nodes are registered with SUSEConnect and use SCC/SMT,
        you will use the <code class="command">zypper migration</code> method and the
        update repositories will be assigned automatically.
       </p></li><li class="listitem"><p>
        If you are <span class="bold"><strong>not</strong></span> using SCC/SMT but a
        Media-ISO or other package source, you will use the <code class="command">zypper
        dup</code> method. In this case, you need to add the following
        repositories to all cluster nodes manually: SLE12-SP3 Base, SLE12-SP3
        Update, SES5 Base, and SES5 Update. You can do so using the
        <code class="command">zypper</code> command. First remove all existing software
        repositories, then add the required new ones, and finally refresh the
        repositories sources:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper sd {0..99}
<code class="prompt user">root # </code>zypper ar \
 http://<em class="replaceable">REPO_SERVER</em>/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<code class="prompt user">root # </code>zypper ar \
 http://<em class="replaceable">REPO_SERVER</em>/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<code class="prompt user">root # </code>zypper ar \
 http://<em class="replaceable">REPO_SERVER</em>/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<code class="prompt user">root # </code>zypper ar \
 http://<em class="replaceable">REPO_SERVER</em>/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<code class="prompt user">root # </code>zypper ref</pre></div></li></ul></div></li></ul></div></div><p>
   To upgrade the SUSE Enterprise Storage 4 cluster to version 5, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Upgrade the Salt master node to SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage
     5.5. Depending on your upgrade method, use either
     <code class="command">zypper migration</code> or <code class="command">zypper dup</code>.
    </p><p>
     Using <code class="command">rpm -q deepsea</code>, verify that the version of the
     DeepSea package on the Salt master node starts with at least
     <code class="literal">0.7</code>. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</pre></div><p>
     If the DeepSea package version number starts with 0.6, double check
     whether you successfully migrated the Salt master node to SUSE Linux Enterprise Server 12 SP3 and
     SUSE Enterprise Storage 5.5.
    </p></li><li class="step"><p>
     Set the new internal object sort order, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set sortbitwise</pre></div><div id="id-1.4.4.3.7.5.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      To verify that the command was successful, we recommend running
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</pre></div></div></li><li class="step"><p>
     If your cluster nodes are <span class="bold"><strong>not</strong></span> registered
     with SUSEConnect and do not use SCC/SMT, you will use the <code class="command">zypper
     dup</code> method. Change your Pillar data in order to use the
     different strategy. Edit
    </p><div class="verbatim-wrap"><pre class="screen">/srv/pillar/ceph/stack/<em class="replaceable">name_of_cluster</em>/cluster.yml</pre></div><p>
     and add the following line:
    </p><div class="verbatim-wrap"><pre class="screen">upgrade_init: zypper-dup</pre></div></li><li class="step" id="step-updatepillar"><p>
     Update your Pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.sync_all</pre></div><p>
     See <a class="xref" href="#ds-minion-targeting" title="4.2.2. Targeting the Minions">Section 4.2.2, “Targeting the Minions”</a> for details about Salt minions
     targeting.
    </p></li><li class="step"><p>
     Verify that you successfully wrote to the Pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.get upgrade_init</pre></div><p>
     The command's output should mirror the entry you added.
    </p></li><li class="step"><p>
     Upgrade Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> state.apply ceph.updates.salt</pre></div></li><li class="step"><p>
     Verify that all Salt minions are upgraded:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> test.version</pre></div></li><li class="step"><p>
     Include the cluster's Salt minions. Refer to
     <a class="xref" href="#ds-minion-targeting" title="4.2.2. Targeting the Minions">Section 4.2.2, “Targeting the Minions”</a> of <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a>
     for more details.
    </p></li><li class="step"><p>
     Start the upgrade of SUSE Linux Enterprise Server and Ceph:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.maintenance.upgrade</pre></div><p>
     Refer to <a class="xref" href="#ceph-maintenance-upgrade-details" title="5.4.2. Details on the salt target ceph.maintenance.upgrade Command">Section 5.4.2, “Details on the <code class="command">salt <em class="replaceable">target</em> ceph.maintenance.upgrade</code> Command”</a> for more
     information.
    </p><div id="id-1.4.4.3.7.5.9.4" data-id-title="Re-run on Reboot" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Re-run on Reboot</h6><p>
      If the process results in a reboot of the Salt master, re-run the command
      to start the upgrade process for the Salt minions again.
     </p></div></li><li class="step"><p>
     After the upgrade, the Ceph Managers are not installed yet. To reach a healthy
     cluster state, do the following:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Run Stage 0 to enable the Salt REST API:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
       Run Stage 1 to create the <code class="filename">role-mgr/</code> subdirectory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li><li class="step"><p>
       Edit <span class="guimenu">policy.cfg</span> as described in
       <a class="xref" href="#policy-configuration" title="4.5.1. The policy.cfg File">Section 4.5.1, “The <code class="filename">policy.cfg</code> File”</a> and add a Ceph Manager role to the nodes
       where Ceph Monitors are deployed, or uncomment the 'role-mgr' lines if you
       followed the steps of <a class="xref" href="#ceph-upgrade-4to5cephdeloy" title="5.5. Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5">Section 5.5, “Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5”</a> until
       here. Also, add the openATTIC role to one of the cluster nodes. Refer to
       <span class="intraxref">Book “Administration Guide”, Chapter 17 “openATTIC”</span> for more details.
      </p></li><li class="step"><p>
       Run Stage 2 to update the Pillar:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div></li><li class="step"><p>
       DeepSea uses a different approach to generate the
       <code class="filename">ceph.conf</code> configuration file now, refer to
       <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span> for more details.
      </p></li><li class="step"><p>
       Set any of the three AppArmor states to all DeepSea minions. For example
       to disable them, run
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">root@master # </code>salt '<em class="replaceable">TARGET</em>' state.apply ceph.apparmor.default-disable</pre></div><p>
       For more information, refer to <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.13 “Enabling AppArmor Profiles”</span>.
      </p></li><li class="step"><p>
       Run Stage 3 to deploy Ceph Managers:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li><li class="step"><p>
       Run Stage 4 to configure openATTIC properly:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol><div id="id-1.4.4.3.7.5.10.3" data-id-title="Ceph Key Caps Mismatch" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Ceph Key Caps Mismatch</h6><p>
      If <code class="literal">ceph.stage.3</code> fails with "Error EINVAL: entity
      client.bootstrap-osd exists but caps do not match", it means the key
      capabilities (caps) for the existing cluster's
      <code class="literal">client.bootstrap.osd</code> key do not match the caps that
      DeepSea is trying to set. Above the error message, in red text, you can
      see a dump of the <code class="command">ceph auth</code> command that failed. Look
      at this command to check the key ID and file being used. In the case of
      <code class="literal">client.bootstrap-osd</code>, the command will be
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</pre></div><p>
      To fix mismatched key caps, check the content of the keyring file
      DeepSea is trying to deploy, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</pre></div><p>
      Compare this with the output of <code class="command">ceph auth get
      client.bootstrap-osd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</pre></div><p>
      Note how the latter key is missing <code class="literal">caps mgr = "allow
      r"</code>. To fix this, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</pre></div><p>
      Running <code class="literal">ceph.stage.3</code> should now succeed.
     </p><p>
      The same issue can occur with other daemon and gateway keyrings when
      running <code class="command">ceph.stage.3</code> and
      <code class="command">ceph.stage.4</code>. The same procedure as above applies:
      check the command that failed, the keyring file being deployed, and the
      capabilities of the existing key. Then run <code class="command">ceph auth
      caps</code> to update the existing key capabilities to match to what
      is being deployed by DeepSea. The keyring files that DeepSea tries to
      deploy are typically placed under the
      <code class="filename">/srv/salt/ceph/<em class="replaceable">DAEMON_OR_GATEWAY_NAME</em>/cache</code>
      directory.
     </p></div></li></ol></div></div><div id="id-1.4.4.3.7.6" data-id-title="Upgrade Failure" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Upgrade Failure</h6><p>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </p></div><div id="id-1.4.4.3.7.7" data-id-title="Rebooting OSDs" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Rebooting OSDs</h6><p>
    After upgrading to SUSE Enterprise Storage 5.5, FileStore OSDs need
    approximately five minutes longer to start as the OSD will do a one-off
    conversion of its on-disk files.
   </p></div><div id="id-1.4.4.3.7.8" data-id-title="Check for the Version of Cluster Components/Nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Check for the Version of Cluster Components/Nodes</h6><p>
    When you need to find out the versions of individual cluster components and
    nodes—for example to find out if all your nodes are actually on the
    same patch level after the upgrade—you can run
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run status.report</pre></div><p>
    The command goes through the connected Salt minions and scans for the version
    numbers of Ceph, Salt, and SUSE Linux Enterprise Server, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </p></div><section class="sect2" id="filestore2bluestore" data-id-title="OSD Migration to BlueStore"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.1 </span><span class="title-name">OSD Migration to BlueStore</span> <a title="Permalink" class="permalink" href="#filestore2bluestore">#</a></h3></div></div></div><p>
    OSD BlueStore is a new back end for the OSD daemons. It is the default
    option since SUSE Enterprise Storage 5.5. Compared to FileStore, which
    stores objects as files in an XFS file system, BlueStore can deliver
    increased performance because it stores objects directly on the underlying
    block device. BlueStore also enables other features, such as built-in
    compression and EC overwrites, that are unavailable with FileStore.
   </p><p>
    Specifically for BlueStore, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a BlueStore OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on faster/different
    media.
   </p><p>
    In SES5, both FileStore and BlueStore are supported and it is possible
    for FileStore and BlueStore OSDs to co-exist in a single cluster. During
    the SUSE Enterprise Storage upgrade procedure, FileStore OSDs are not
    automatically converted to BlueStore. Be aware that the
    BlueStore-specific features will not be available on OSDs that have not
    been migrated to BlueStore.
   </p><p>
    Before converting to BlueStore, the OSDs need to be running SUSE Enterprise Storage
    5.5. The conversion is a slow process as all data gets
    re-written twice. Though the migration process can take a long time to
    complete, there is no cluster outage and all clients can continue accessing
    the cluster during this period. However, do expect lower performance for
    the duration of the migration. This is caused by rebalancing and
    backfilling of cluster data.
   </p><p>
    Use the following procedure to migrate FileStore OSDs to BlueStore:
   </p><div id="id-1.4.4.3.7.9.7" data-id-title="Turn Off Safety Measures" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Turn Off Safety Measures</h6><p>
     Salt commands needed for running the migration are blocked by safety
     measures. In order to turn these precautions off, run the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety</pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Migrate hardware profiles:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.migrate.policy</pre></div><p>
      This runner migrates any hardware profiles currently in use by the
      <code class="filename">policy.cfg</code> file. It processes
      <code class="filename">policy.cfg</code>, finds any hardware profile using the
      original data structure, and converts it to the new data structure. The
      result is a new hardware profile named
      'migrated-<em class="replaceable">original_name</em>'.
      <code class="filename">policy.cfg</code> is updated as well.
     </p><p>
      If the original configuration had separate journals, the BlueStore
      configuration will use the same device for the 'wal' and 'db' for that
      OSD.
     </p></li><li class="step"><p>
      DeepSea migrates OSDs by setting their weight to 0 which 'vacuums' the
      data until the OSD is empty. You can either migrate OSDs one by one, or
      all OSDs at once. In either case, when the OSD is empty, the
      orchestration removes it and then re-creates it with the new
      configuration.
     </p><div id="id-1.4.4.3.7.9.8.2.2" data-id-title="Recommended Method" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Recommended Method</h6><p>
       Use <code class="command">ceph.migrate.nodes</code> if you have a large number of
       physical storage nodes or almost no data. If one node represents less
       than 10% of your capacity, then the
       <code class="command">ceph.migrate.nodes</code> may be marginally faster moving
       all the data from those OSDs in parallel.
      </p><p>
       If you are not sure about which method to use, or the site has few
       storage nodes (for example each node has more than 10% of the cluster
       data), then select <code class="command">ceph.migrate.osds</code>.
      </p></div><ol type="a" class="substeps"><li class="step"><p>
        To migrate OSDs one at a time, run:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.migrate.osds</pre></div></li><li class="step"><p>
        To migrate all OSDs on each node in parallel, run:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.migrate.nodes</pre></div></li></ol><div id="id-1.4.4.3.7.9.8.2.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
       As the orchestration gives no feedback about the migration progress, use
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd tree</pre></div><p>
       to see which OSDs have a weight of zero periodically.
      </p></div></li></ol></div></div><p>
    After the migration to BlueStore, the object count will remain the same
    and disk usage will be nearly the same.
   </p></section><section class="sect2" id="ceph-maintenance-upgrade-details" data-id-title="Details on the salt target ceph.maintenance.upgrade Command"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.2 </span><span class="title-name">Details on the <code class="command">salt <em class="replaceable">target</em> ceph.maintenance.upgrade</code> Command</span> <a title="Permalink" class="permalink" href="#ceph-maintenance-upgrade-details">#</a></h3></div></div></div><p>
    During an upgrade via <code class="command">salt
    <em class="replaceable">target</em>ceph.maintenance.upgrade</code>,
    DeepSea applies all available updates/patches on all servers in the
    cluster in parallel without rebooting them. After these updates/patches are
    applied, the actual upgrade begins:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      The admin node is upgraded to SUSE Linux Enterprise Server 12 SP3. This also upgrades the
      <span class="package">salt-master</span> and <span class="package">deepsea</span> packages.
     </p></li><li class="step"><p>
      All Salt minions are upgraded to a version that corresponds to the
      Salt master.
     </p></li><li class="step"><p>
      The migration is performed sequentially on all cluster nodes in the
      recommended order (the Ceph Monitors first, see
      <a class="xref" href="#upgrade-order">Upgrade Order</a>) using the preferred method. As a
      consequence, the <span class="package">ceph</span> package is upgraded.
     </p></li><li class="step"><p>
      After updating all Ceph Monitors, their services are restarted but the nodes are
      <span class="bold"><strong>not rebooted</strong></span>. This way we ensure that
      all running Ceph Monitors have identical version.
     </p><div id="id-1.4.4.3.7.10.3.4.2" data-id-title="Do Not Reboot Monitor Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Reboot Monitor Nodes</h6><p>
       If the cluster monitor nodes host OSDs, <span class="bold"><strong>do not
       reboot</strong></span> the nodes during this stage because the shared OSDs
       will not join the cluster after the reboot.
      </p></div></li><li class="step"><p>
      All the remaining cluster nodes are updated and rebooted in the
      recommended order.
     </p></li><li class="step"><p>
      After all nodes are on the same patch-level, the following command is
      run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="command">ceph require osd release <em class="replaceable">RELEASE</em></code></pre></div></li></ol></div></div><p>
    In case this process is interrupted by an accident or intentionally by the
    administrator, <span class="bold"><strong>never reboot</strong></span> the nodes
    manually because after rebooting the first OSD node and OSD daemon, it will
    not be able to join the cluster anymore.
   </p></section></section><section class="sect1" id="ceph-upgrade-4to5cephdeloy" data-id-title="Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.5 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-4to5cephdeloy">#</a></h2></div></div></div><div id="id-1.4.4.3.8.2" data-id-title="Software Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Software Requirements</h6><p>
    You need to have the following software installed and updated to the latest
    package versions on all the Ceph nodes you want to upgrade before you can
    start with the upgrade procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      SUSE Linux Enterprise Server 12 SP2
     </p></li><li class="listitem"><p>
      SUSE Enterprise Storage 4
     </p></li></ul></div><p>
    Choose the Salt master for your cluster. If your cluster has Calamari
    deployed, then the Calamari node already <span class="emphasis"><em>is</em></span> the
    Salt master. Alternatively, the admin node from which you ran the
    <code class="command">ceph-deploy</code> command will become the Salt master.
   </p><p>
    Before starting the procedure below, you need to upgrade the Salt master node
    to SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage 5.5 by running
    <code class="command">zypper migration</code> (or your preferred way of upgrading).
   </p></div><p>
   To upgrade the SUSE Enterprise Storage 4 cluster which was deployed with
   <code class="command">ceph-deploy</code> to version 5, follow these steps:
  </p><div class="procedure" id="upgrade4to5cephdeploy-all" data-id-title="Steps to Apply to All Cluster Nodes (including the Calamari Node)"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 5.1: </span><span class="title-name">Steps to Apply to All Cluster Nodes (including the Calamari Node) </span><a title="Permalink" class="permalink" href="#upgrade4to5cephdeploy-all">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install the <code class="systemitem">salt</code> package from SLE-12-SP2/SES4:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install salt</pre></div></li><li class="step"><p>
     Install the <code class="systemitem">salt-minion</code> package from
     SLE-12-SP2/SES4, then enable and start the related service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install salt-minion
<code class="prompt user">root # </code>systemctl enable salt-minion
<code class="prompt user">root # </code>systemctl start salt-minion</pre></div></li><li class="step"><p>
     Ensure that the host name 'salt' resolves to the IP address of the
     Salt master node. If your Salt master is not reachable by the host name
     <code class="literal">salt</code>, edit the file
     <code class="filename">/etc/salt/minion</code> or create a new file
     <code class="filename">/etc/salt/minion.d/master.conf</code> with the following
     content:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">host_name_of_salt_master</em></pre></div><div id="id-1.4.4.3.8.4.4.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      The existing Salt minions have the <code class="option">master:</code> option already
      set in <code class="filename">/etc/salt/minion.d/calamari.conf</code>. The
      configuration file name does not matter, the
      <code class="filename">/etc/salt/minion.d/</code> directory is important.
     </p></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the Salt service on all Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </p></li><li class="step"><p>
       If you are <span class="bold"><strong>not</strong></span> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <code class="command">zypper</code> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper sd {0..99}
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<code class="prompt user">root # </code>zypper ref</pre></div></li></ol></li></ol></div></div><div class="procedure" id="upgrade4to5cephdeploy-admin" data-id-title="Steps to Apply to the Salt master Node"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 5.2: </span><span class="title-name">Steps to Apply to the Salt master Node </span><a title="Permalink" class="permalink" href="#upgrade4to5cephdeploy-admin">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set the new internal object sort order, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd set sortbitwise</pre></div><div id="id-1.4.4.3.8.5.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      To verify that the command was successful, we recommend running
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</pre></div></div></li><li class="step"><p>
     Upgrade the Salt master node to SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage 5.5.
     For SCC-registered systems, use <code class="command">zypper migration</code>. If
     you provide the required software repositories manually, use
     <code class="command">zypper dup</code>. After the upgrade, ensure that only
     repositories for SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage 5.5 are active
     (and refreshed) on the Salt master node before proceeding.
    </p></li><li class="step"><p>
     If not already present, install the <code class="systemitem">salt-master</code>
     package, then enable and start the related service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install salt-master
<code class="prompt user">root@master # </code>systemctl enable salt-master
<code class="prompt user">root@master # </code>systemctl start salt-master</pre></div></li><li class="step"><p>
     Verify the presence of all Salt minions by listing their keys:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -L</pre></div></li><li class="step"><p>
     Add all Salt minions keys to Salt master including the minion master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -A -y</pre></div></li><li class="step"><p>
     Ensure that all Salt minions' keys were accepted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -L</pre></div></li><li class="step"><p>
     Make sure that the software on your Salt master node is up to date:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper migration</pre></div></li><li class="step"><p>
     Install the <code class="systemitem">deepsea</code> package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install deepsea</pre></div></li><li class="step"><p>
     Include the cluster's Salt minions. Refer to
     <a class="xref" href="#ds-minion-targeting" title="4.2.2. Targeting the Minions">Section 4.2.2, “Targeting the Minions”</a> of <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a>
     for more details.
    </p></li><li class="step"><p>
     Import the existing <code class="command">ceph-deploy</code> installed cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run populate.engulf_existing_cluster</pre></div><p>
     The command will do the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Distribute all the required Salt and DeepSea modules to all the
       Salt minions.
      </p></li><li class="listitem"><p>
       Inspect the running Ceph cluster and populate
       <code class="filename">/srv/pillar/ceph/proposals</code> with a layout of the
       cluster.
      </p><p>
       <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> will be
       created with roles matching all detected running Ceph services. If no
       <code class="systemitem">ceph-mgr</code> daemons are detected a
       'role-mgr' is added for every node with 'role-mon'. View this file to
       verify that each of your existing MON, OSD, RGW and MDS nodes have the
       appropriate roles. OSD nodes will be imported into the
       <code class="filename">profile-import/</code> subdirectory, so you can examine
       the files in
       <code class="filename">/srv/pillar/ceph/proposals/profile-import/cluster/</code>
       and
       <code class="filename">/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</code>
       to confirm that the OSDs were correctly picked up.
      </p><div id="id-1.4.4.3.8.5.11.4.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        The generated <code class="filename">policy.cfg</code> will only apply roles for
        detected Ceph services 'role-mon', 'role-mds', 'role-rgw',
        'role-admin', and 'role-master' for the Salt master node. Any other
        desired roles will need to be added to the file manually (see
        <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>).
       </p></div></li><li class="listitem"><p>
       The existing cluster's <code class="filename">ceph.conf</code> will be saved to
       <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.import</code>.
      </p></li><li class="listitem"><p>
       <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>
       will include the cluster's fsid, cluster and public networks, and also
       specifies the <code class="option">configuration_init: default-import</code>
       option, which makes DeepSea use the
       <code class="filename">ceph.conf.import</code> configuration file mentioned
       previously, rather than using DeepSea's default
       <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
       template.
      </p><div id="id-1.4.4.3.8.5.11.4.4.2" data-id-title="Custom Settings in ceph.conf" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Custom Settings in <code class="filename">ceph.conf</code></h6><p>
        If you need to integrate the <code class="filename">ceph.conf</code> file with
        custom changes, wait until the engulf/upgrade process successfully
        finishes. Then edit the
        <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>
        file and comment the following line:
       </p><div class="verbatim-wrap"><pre class="screen">configuration_init: default-import</pre></div><p>
        Save the file and follow the information in
        <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>.
       </p></div></li><li class="listitem"><p>
       The cluster's various keyrings will be saved to the following
       directories:
      </p><div class="verbatim-wrap"><pre class="screen">/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</pre></div><p>
       Verify that the keyring files exist, and that there is
       <span class="emphasis"><em>no</em></span> keyring file in the following directory (the
       Ceph Manager did not exist before SUSE Enterprise Storage 5.5):
      </p><div class="verbatim-wrap"><pre class="screen">/srv/salt/ceph/mgr/cache/</pre></div></li></ul></div></li><li class="step"><p>
     If the <code class="command">salt-run populate.engulf_existing_cluster</code>
     command cannot detect <code class="systemitem">ceph-mgr</code>
     daemons, the <code class="filename">policy.cfg</code> file will contain a 'mgr'
     role for each node that has the 'role-mon' assigned. This will deploy
     <code class="systemitem">ceph-mgr</code> daemons together with the
     monitor daemons in a later step. Since there are no
     <code class="systemitem">ceph-mgr</code> daemons running at this
     time, please edit
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and comment out
     all lines starting with 'role-mgr' by prepending a '#' character.
    </p></li><li class="step"><p>
     The <code class="command">salt-run populate.engulf_existing_cluster</code> command
     does not handle importing the openATTIC configuration. You need to manually
     edit the <code class="filename">policy.cfg</code> file and add a
     <code class="literal">role-openattic</code> line. Refer to
     <a class="xref" href="#policy-configuration" title="4.5.1. The policy.cfg File">Section 4.5.1, “The <code class="filename">policy.cfg</code> File”</a> for more details.
    </p></li><li class="step"><p>
     The <code class="command">salt-run populate.engulf_existing_cluster</code> command
     does not handle importing the iSCSI Gateways configurations. If your cluster
     includes iSCSI Gateways, import their configurations manually:
    </p><ol type="a" class="substeps"><li class="step"><p>
       On one of iSCSI Gateway nodes, export the current <code class="filename">lrbd.conf</code>
       and copy it to the Salt master node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>lrbd -o &gt;/tmp/lrbd.conf
<code class="prompt user">root@minion &gt; </code>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf</pre></div></li><li class="step"><p>
       On the Salt master node, add the default iSCSI Gateway configuration to the
       DeepSea setup:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>mkdir -p /srv/pillar/ceph/stack/ceph/
<code class="prompt user">root@master # </code>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<code class="prompt user">root@master # </code>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml</pre></div></li><li class="step"><p>
       Add the iSCSI Gateway roles to <code class="filename">policy.cfg</code> and save the
       file:
      </p><div class="verbatim-wrap"><pre class="screen">role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]</pre></div></li></ol></li><li class="step"><p>
     Run Stages 0 and 1 to update packages and create all possible roles:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     Generate required subdirectories under
     <code class="filename">/srv/pillar/ceph/stack</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run push.proposal</pre></div></li><li class="step"><p>
     Verify that there is a working DeepSea-managed cluster with correctly
     assigned roles:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.get roles</pre></div><p>
     Compare the output with the actual layout of the cluster.
    </p></li><li class="step"><p>
     Calamari leaves a scheduled Salt job running to check the cluster
     status. Remove the job:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> schedule.delete ceph.heartbeat</pre></div></li><li class="step"><p>
     From this point on, follow the procedure described in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>.
    </p></li></ol></div></div></section><section class="sect1" id="ceph-upgrade-4to5crowbar" data-id-title="Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.6 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-4to5crowbar">#</a></h2></div></div></div><div id="id-1.4.4.3.9.2" data-id-title="Software Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Software Requirements</h6><p>
    You need to have the following software installed and updated to the latest
    package versions on all the Ceph nodes you want to upgrade before you can
    start with the upgrade procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      SUSE Linux Enterprise Server 12 SP2
     </p></li><li class="listitem"><p>
      SUSE Enterprise Storage 4
     </p></li></ul></div></div><p>
   To upgrade SUSE Enterprise Storage 4 deployed using Crowbar to version 5, follow these
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     For each Ceph node (including the Calamari node), stop and disable all
     Crowbar-related services :
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl stop chef-client
<code class="prompt user">root@minion &gt; </code>systemctl disable chef-client
<code class="prompt user">root@minion &gt; </code>systemctl disable crowbar_join
<code class="prompt user">root@minion &gt; </code>systemctl disable crowbar_notify_shutdown</pre></div></li><li class="step"><p>
     For each Ceph node (including the Calamari node), verify that the
     software repositories point to SUSE Enterprise Storage 5.5 and SUSE Linux Enterprise Server 12
     SP3 products. If repositories pointing to older product versions are still
     present, disable them.
    </p></li><li class="step"><p>
     For each Ceph node (including the Calamari node), verify that the
     <span class="package">salt-minion</span> is installed. If not, install it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt salt-minion</pre></div></li><li class="step"><p>
     For the Ceph nodes that did not have the <span class="package">salt-minion</span>
     package installed, create the file
     <code class="filename">/etc/salt/minion.d/master.conf</code> with the
     <code class="option">master</code> option pointing to the full Calamari node host
     name:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">full_calamari_hostname</em></pre></div><div id="id-1.4.4.3.9.4.4.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      The existing Salt minions have the <code class="option">master:</code> option already
      set in <code class="filename">/etc/salt/minion.d/calamari.conf</code>. The
      configuration file name does not matter, the
      <code class="filename">/etc/salt/minion.d/</code> directory is important.
     </p></div><p>
     Enable and start the <code class="systemitem">salt-minion</code>
     service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl enable salt-minion
<code class="prompt user">root@minion &gt; </code>systemctl start salt-minion</pre></div></li><li class="step"><p>
     On the Calamari node, accept any remaining salt minion keys:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

<code class="prompt user">root@master # </code>salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.</pre></div></li><li class="step"><p>
     If Ceph was deployed on the public network and no VLAN interface is
     present, add a VLAN interface on Crowbar's public network to the Calamari
     node.
    </p></li><li class="step"><p>
     Upgrade the Calamari node to SUSE Linux Enterprise Server 12 SP3 and SUSE Enterprise Storage
     5.5, either by using <code class="command">zypper migration</code> or
     your favorite method. From here onward, the Calamari node becomes the
     <span class="emphasis"><em>Salt master</em></span>. After the upgrade, reboot the Salt master.
    </p></li><li class="step"><p>
     Install DeepSea on the Salt master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in deepsea</pre></div></li><li class="step"><p>
     Specify the <code class="option">deepsea_minions</code> option to include the correct
     group of Salt minions into deployment stages. Refer to
     <a class="xref" href="#ds-minion-targeting-dsminions" title="4.2.2.3. Set the deepsea_minions Option">Section 4.2.2.3, “Set the <code class="option">deepsea_minions</code> Option”</a> for more details.
    </p></li><li class="step"><p>
     DeepSea expects all Ceph nodes to have an identical
     <code class="filename">/etc/ceph/ceph.conf</code>. Crowbar deploys a slightly
     different <code class="filename">ceph.conf</code> to each node, so you need to
     consolidate them:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Remove the <code class="option">osd crush location hook</code> option, it was
       included by Calamari.
      </p></li><li class="listitem"><p>
       Remove the <code class="option">public addr</code> option from the
       <code class="literal">[mon]</code> section.
      </p></li><li class="listitem"><p>
       Remove the port numbers from the <code class="option">mon host</code> option.
      </p></li></ul></div></li><li class="step"><p>
     If you were running the Object Gateway, Crowbar deployed a separate
     <code class="filename">/etc/ceph/ceph.conf.radosgw</code> file to keep the keystone
     secrets separated from the regular <code class="filename">ceph.conf</code> file.
     Crowbar also added a custom
     <code class="filename">/etc/systemd/system/ceph-radosgw@.service</code> file.
     Because DeepSea does not support it, you need to remove it:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Append all <code class="literal">[client.rgw....]</code> sections from the
       <code class="filename">ceph.conf.radosgw</code> file to
       <code class="filename">/etc/ceph/ceph.conf</code> on all nodes.
      </p></li><li class="listitem"><p>
       On the Object Gateway node, run the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<em class="replaceable">hostname</em></pre></div></li></ul></div></li><li class="step"><p>
     Double check that <code class="command">ceph status</code> works when run from the
     Salt master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]</pre></div></li><li class="step"><p>
     Import the existing cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run populate.engulf_existing_cluster
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run push.proposal</pre></div></li><li class="step"><p>
     The <code class="command">salt-run populate.engulf_existing_cluster</code> command
     does not handle importing the iSCSI Gateways configurations. If your cluster
     includes iSCSI Gateways, import their configurations manually:
    </p><ol type="a" class="substeps"><li class="step"><p>
       On one of iSCSI Gateway nodes, export the current <code class="filename">lrbd.conf</code>
       and copy it to the Salt master node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>lrbd -o &gt; /tmp/lrbd.conf
<code class="prompt user">root@minion &gt; </code>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf</pre></div></li><li class="step"><p>
       On the Salt master node, add the default iSCSI Gateway configuration to the
       DeepSea setup:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>mkdir -p /srv/pillar/ceph/stack/ceph/
<code class="prompt user">root@master # </code>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<code class="prompt user">root@master # </code>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml</pre></div></li><li class="step"><p>
       Add the iSCSI Gateway roles to <code class="filename">policy.cfg</code> and save the
       file:
      </p><div class="verbatim-wrap"><pre class="screen">role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]</pre></div></li></ol></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </p></li><li class="step"><p>
       If you are <span class="bold"><strong>not</strong></span> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <code class="command">zypper</code> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper sd {0..99}
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<code class="prompt user">root # </code>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<code class="prompt user">root # </code>zypper ref</pre></div><p>
       Then change your Pillar data in order to use a different strategy. Edit
      </p><div class="verbatim-wrap"><pre class="screen">/srv/pillar/ceph/stack/<em class="replaceable">name_of_cluster</em>/cluster.yml</pre></div><p>
       and add the following line:
      </p><div class="verbatim-wrap"><pre class="screen">upgrade_init: zypper-dup</pre></div><div id="id-1.4.4.3.9.4.15.1.2.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
        The <code class="literal">zypper-dup</code> strategy requires you to manually add
        the latest software repositories, while the default
        <code class="literal">zypper-migration</code> relies on the repositories provided
        by SCC/SMT.
       </p></div></li></ol></li><li class="step"><p>
     Fix host grains to make DeepSea use short host names on the public
     network for the Ceph daemon instance IDs. For each node, you need to run
     <code class="command">grains.set</code> with the new (short) host name. Before
     running <code class="command">grains.set</code>, verify the current monitor
     instances by running <code class="command">ceph status</code>. A before and after
     example follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
<code class="prompt user">root@master # </code>salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
<code class="prompt user">root@master # </code>salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
<code class="prompt user">root@master # </code>salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30</pre></div></li><li class="step"><p>
     Run the upgrade:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> state.apply ceph.updates
<code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> test.version
<code class="prompt user">root@master # </code>salt-run state.orch ceph.maintenance.upgrade</pre></div><p>
     Every node will reboot. The cluster will come back up complaining that
     there is no active Ceph Manager instance. This is normal. Calamari should not be
     installed/running anymore at this point.
    </p></li><li class="step"><p>
     Run all the required deployment stages to get the cluster to a healthy
     state:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li><li class="step"><p>
     To deploy openATTIC (see <span class="intraxref">Book “Administration Guide”, Chapter 17 “openATTIC”</span>), add an appropriate
     <code class="literal">role-openattic</code> (see
     <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>) line to
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>, then run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li><li class="step"><p>
     During the upgrade, you may receive "Error EINVAL: entity [...] exists but
     caps do not match" errors. To fix them, refer to
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>.
    </p></li><li class="step"><p>
     Do the remaining cleanup:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Crowbar creates entries in <code class="filename">/etc/fstab</code> for each OSD.
       They are not necessary, so delete them.
      </p></li><li class="listitem"><p>
       Calamari leaves a scheduled Salt job running to check the cluster
       status. Remove the job:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> schedule.delete ceph.heartbeat</pre></div></li><li class="listitem"><p>
       There are still some unnecessary packages installed, mostly ruby gems,
       and chef related. Their removal is not required but you may want to
       delete them by running <code class="command">zypper rm
       <em class="replaceable">pkg_name</em></code>.
      </p></li></ul></div></li></ol></div></div></section><section class="sect1" id="ceph-upgrade-3to5" data-id-title="Upgrade from SUSE Enterprise Storage 3 to 5"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.7 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 3 to 5</span> <a title="Permalink" class="permalink" href="#ceph-upgrade-3to5">#</a></h2></div></div></div><div id="id-1.4.4.3.10.2" data-id-title="Software Requirements" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Software Requirements</h6><p>
    You need to have the following software installed and updated to the latest
    package versions on all the Ceph nodes you want to upgrade before you can
    start with the upgrade procedure:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      SUSE Linux Enterprise Server 12 SP1
     </p></li><li class="listitem"><p>
      SUSE Enterprise Storage 3
     </p></li></ul></div></div><p>
   To upgrade the SUSE Enterprise Storage 3 cluster to version 5, follow the steps
   described in <a class="xref" href="#upgrade4to5cephdeploy-all" title="Steps to Apply to All Cluster Nodes (including the Calamari Node)">Procedure 5.1, “Steps to Apply to All Cluster Nodes (including the Calamari Node)”</a> and then
   <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>.
  </p></section></section><section class="chapter" id="cha-deployment-backup" data-id-title="Backing Up the Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Backing Up the Cluster Configuration</span> <a title="Permalink" class="permalink" href="#cha-deployment-backup">#</a></h2></div></div></div><p>
  This chapter explains which files on the admin node should be backed up. As
  soon as you are finished with your cluster deployment or migration, create a
  backup of these directories.
 </p><section class="sect1" id="backup-ceph" data-id-title="Back Up Ceph COnfiguration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Back Up Ceph COnfiguration</span> <a title="Permalink" class="permalink" href="#backup-ceph">#</a></h2></div></div></div><p>
   Back up the <code class="filename">/etc/ceph</code> directory. It contains crucial
   cluster configuration. You will need the backup of
   <code class="filename">/etc/ceph</code> for example when you need to replace the
   Admin Node.
  </p></section><section class="sect1" id="sec-deployment-backup-salt" data-id-title="Back Up Salt Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Back Up Salt Configuration</span> <a title="Permalink" class="permalink" href="#sec-deployment-backup-salt">#</a></h2></div></div></div><p>
   You need to back up the <code class="filename">/etc/salt/</code> directory. It
   contains the Salt configuration files, for example the Salt master key and
   accepted client keys.
  </p><p>
   The Salt files are not strictly required for backing up the admin node,
   but make redeploying the Salt cluster easier. If there is no backup of
   these files, the Salt minions need to be registered again at the new admin
   node.
  </p><div id="id-1.4.4.4.5.4" data-id-title="Security of the Salt Master Private Key" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Security of the Salt Master Private Key</h6><p>
    Make sure that the backup of the Salt master private key is stored in a safe
    location. The Salt master key can be used to manipulate all cluster nodes.
   </p></div><p>
   After restoring the <code class="filename">/etc/salt</code> directory from a backup,
   restart the Salt services:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">systemctl</code> restart salt-master
<code class="prompt user">root@master # </code><code class="command">systemctl</code> restart salt-minion</pre></div></section><section class="sect1" id="sec-deployment-backup-deepsea" data-id-title="Back Up DeepSea Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Back Up DeepSea Configuration</span> <a title="Permalink" class="permalink" href="#sec-deployment-backup-deepsea">#</a></h2></div></div></div><p>
   All files required by DeepSea are stored in
   <code class="filename">/srv/pillar/</code>, <code class="filename">/srv/salt/</code> and
   <code class="filename">/etc/salt/master.d</code>.
  </p><p>
   If you need to redeploy the admin node, install the DeepSea package on the
   new node and move the backed up data back into the directories. DeepSea
   can then be used again without any further changes being required. Before
   using DeepSea again, make sure that all Salt minions are correctly
   registered on the admin node.
  </p></section></section><section class="chapter" id="ceph-deploy-ds-custom" data-id-title="Customizing the Default Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span> <a title="Permalink" class="permalink" href="#ceph-deploy-ds-custom">#</a></h2></div></div></div><p>
  You can change the default cluster configuration generated in Stage 2 (refer
  to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>). For example, you may need to
  change network settings, or software that is installed on the Salt master by
  default installed. You can perform the former by modifying the pillar updated
  after Stage 2, while the latter is usually done by creating a custom
  <code class="literal">sls</code> file and adding it to the pillar. Details are
  described in following sections.
 </p><section class="sect1" id="using-customized-files" data-id-title="Using Customized Configuration Files"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Using Customized Configuration Files</span> <a title="Permalink" class="permalink" href="#using-customized-files">#</a></h2></div></div></div><p>
   This section lists several tasks that require adding/changing your own
   <code class="literal">sls</code> files. Such a procedure is typically used when you
   need to change the default deployment process.
  </p><div id="id-1.4.4.5.4.3" data-id-title="Prefix Custom .sls Files" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Prefix Custom .sls Files</h6><p>
    Your custom .sls files belong to the same subdirectory as DeepSea's .sls
    files. To prevent overwriting your .sls files with the possibly newly added
    ones from the DeepSea package, prefix their name with the
    <code class="filename">custom-</code> string.
   </p></div><section class="sect2" id="id-1.4.4.5.4.4" data-id-title="Disabling a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.1 </span><span class="title-name">Disabling a Deployment Step</span> <a title="Permalink" class="permalink" href="#id-1.4.4.5.4.4">#</a></h3></div></div></div><p>
    If you address a specific task outside of the DeepSea deployment process
    and therefore need to skip it, create a 'no-operation' file following this
    example:
   </p><div class="procedure" id="id-1.4.4.5.4.4.3" data-id-title="Disabling Time Synchronization"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.1: </span><span class="title-name">Disabling Time Synchronization </span><a title="Permalink" class="permalink" href="#id-1.4.4.5.4.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/time/disabled.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">disable time setting:
test.nop</pre></div></li><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code>, add the
      following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">time_init: disabled</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.time
admin.ceph:
  Name: disable time setting - Function: test.nop - Result: Clean

Summary for admin.ceph
------------
Succeeded: 1
Failed:    0
------------
Total states run:     1</pre></div><div id="id-1.4.4.5.4.4.3.4.3" data-id-title="Unique ID" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unique ID</h6><p>
       The task ID 'disable time setting' may be any message unique within an
       <code class="literal">sls</code> file. Prevent ID collisions by specifying unique
       descriptions.
      </p></div></li></ol></div></div></section><section class="sect2" id="deepsea-replacing-step" data-id-title="Replacing a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.2 </span><span class="title-name">Replacing a Deployment Step</span> <a title="Permalink" class="permalink" href="#deepsea-replacing-step">#</a></h3></div></div></div><p>
    If you need to replace the default behavior of a specific step with a
    custom one, create a custom <code class="literal">sls</code> file with replacement
    content.
   </p><p>
    By default <code class="filename">/srv/salt/ceph/pool/default.sls</code> creates an
    rbd image called 'demo'. In our example, we do not want this image to be
    created, but we need two images: 'archive1' and 'archive2'.
   </p><div class="procedure" id="id-1.4.4.5.4.5.4" data-id-title="Replacing the demo rbd Image with Two Custom rbd Images"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.2: </span><span class="title-name">Replacing the <span class="emphasis"><em>demo</em></span> rbd Image with Two Custom rbd Images </span><a title="Permalink" class="permalink" href="#id-1.4.4.5.4.5.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/pool/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">wait:
  module.run:
    - name: wait.out
    - kwargs:
        'status': "HEALTH_ERR"<span class="callout" id="co-deepsea-replace-wait">1</span>
    - fire_event: True

archive1:
  cmd.run:
    - name: "rbd -p rbd create archive1 --size=1024"<span class="callout" id="co-deepsea-replace-rbd">2</span>
    - unless: "rbd -p rbd ls | grep -q archive1$"
    - fire_event: True

archive2:
  cmd.run:
    - name: "rbd -p rbd create archive2 --size=768"
    - unless: "rbd -p rbd ls | grep -q archive2$"
    - fire_event: True</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-deepsea-replace-wait"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The <span class="bold"><strong>wait</strong></span> module will pause until the
        Ceph cluster does not have a status of <code class="literal">HEALTH_ERR</code>.
        In fresh installations, a Ceph cluster may have this status until a
        sufficient number of OSDs become available and the creation of pools
        has completed.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-deepsea-replace-rbd"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The <code class="command">rbd</code> command is not idempotent. If the same
        creation command is re-run after the image exists, the Salt state
        will fail. The <span class="bold"><strong>unless</strong></span> statement
        prevents this.
       </p></td></tr></table></div></li><li class="step"><p>
      To call the newly created custom file instead of the default, you need to
      edit <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>, add
      the following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">pool_init: custom</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.pool</pre></div></li></ol></div></div><div id="id-1.4.4.5.4.5.5" data-id-title="Authorization" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Authorization</h6><p>
     The creation of pools or images requires sufficient authorization. The
     <code class="literal">admin.ceph</code> minion has an admin keyring.
    </p></div><div id="id-1.4.4.5.4.5.6" data-id-title="Alternative Way" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Alternative Way</h6><p>
     Another option is to change the variable in
     <code class="filename">/srv/pillar/ceph/stack/ceph/roles/master.yml</code> instead.
     Using this file will reduce the clutter of pillar data for other minions.
    </p></div></section><section class="sect2" id="id-1.4.4.5.4.6" data-id-title="Modifying a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.3 </span><span class="title-name">Modifying a Deployment Step</span> <a title="Permalink" class="permalink" href="#id-1.4.4.5.4.6">#</a></h3></div></div></div><p>
    Sometimes you may need a specific step to do some additional tasks. We do
    not recommend modifying the related state file as it may complicate a
    future upgrade. Instead, create a separate file to carry out the additional
    tasks identical to what was described in
    <a class="xref" href="#deepsea-replacing-step" title="7.1.2. Replacing a Deployment Step">Section 7.1.2, “Replacing a Deployment Step”</a>.
   </p><p>
    Name the new <code class="literal">sls</code> file descriptively. For example, if you
    need to create two rbd images in addition to the demo image, name the file
    <code class="filename">archive.sls</code>.
   </p><div class="procedure" id="id-1.4.4.5.4.6.4" data-id-title="Creating Two Additional rbd Images"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.3: </span><span class="title-name">Creating Two Additional rbd Images </span><a title="Permalink" class="permalink" href="#id-1.4.4.5.4.6.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/pool/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">include:
 - .archive
 - .default</pre></div><div id="id-1.4.4.5.4.6.4.2.3" data-id-title="Include Precedence" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Include Precedence</h6><p>
       In this example, Salt will create the <span class="emphasis"><em>archive</em></span>
       images and then create the <span class="emphasis"><em>demo</em></span> image. The order
       does not matter in this example. To change the order, reverse the lines
       after the <code class="literal">include:</code> directive.
      </p><p>
       You can add the include line directly to
       <code class="filename">archive.sls</code> and all the images will get created as
       well. However, regardless of where the include line is placed, Salt
       processes the steps in the included file first. Although this behavior
       can be overridden with <span class="emphasis"><em>requires</em></span> and
       <span class="emphasis"><em>order</em></span> statements, a separate file that includes the
       others guarantees the order and reduces the chances of confusion.
      </p></div></li><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>, add
      the following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">pool_init: custom</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.pool</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.4.5.4.7" data-id-title="Modifying a Deployment Stage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.4 </span><span class="title-name">Modifying a Deployment Stage</span> <a title="Permalink" class="permalink" href="#id-1.4.4.5.4.7">#</a></h3></div></div></div><p>
    If you need to add a completely separate deployment step, create three new
    files—an <code class="literal">sls</code> file that performs the command, an
    orchestration file, and a custom file which aligns the new step with the
    original deployment steps.
   </p><p>
    For example, if you need to run <code class="command">logrotate</code> on all minions
    as part of the preparation stage:
   </p><p>
    First create an <code class="literal">sls</code> file and include the
    <code class="command">logrotate</code> command.
   </p><div class="procedure" id="id-1.4.4.5.4.7.5" data-id-title="Running logrotate on all Salt minions"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.4: </span><span class="title-name">Running <code class="command">logrotate</code> on all Salt minions </span><a title="Permalink" class="permalink" href="#id-1.4.4.5.4.7.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a directory such as <code class="filename">/srv/salt/ceph/logrotate</code>.
     </p></li><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/logrotate/init.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">rotate logs:
  cmd.run:
    - name: "/usr/sbin/logrotate /etc/logrotate.conf"</pre></div></li><li class="step"><p>
      Verify that the command works on a minion:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.logrotate</pre></div></li></ol></div></div><p>
    Because the orchestration file needs to run before all other preparation
    steps, add it to the <span class="emphasis"><em>Prep</em></span> stage 0:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/stage/prep/logrotate.sls</code> with
      the following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">logrotate:
  salt.state:
    - tgt: '*'
    - sls: ceph.logrotate</pre></div></li><li class="step"><p>
      Verify that the orchestration file works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.prep.logrotate</pre></div></li></ol></div></div><p>
    The last file is the custom one which includes the additional step with the
    original steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/stage/prep/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">include:
  - .logrotate
  - .master
  - .minion</pre></div></li><li class="step"><p>
      Override the default behavior. Edit
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>, add the following
      line, and save the file:
     </p><div class="verbatim-wrap"><pre class="screen">stage_prep: custom</pre></div></li><li class="step"><p>
      Verify that Stage 0 works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li></ol></div></div><div id="id-1.4.4.5.4.7.10" data-id-title="Why global.yml?" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Why <code class="filename">global.yml</code>?</h6><p>
     The <code class="filename">global.yml</code> file is chosen over the
     <code class="filename">cluster.yml</code> because during the
     <span class="emphasis"><em>prep</em></span> stage, no minion belongs to the Ceph cluster
     and has no access to any settings in <code class="filename">cluster.yml</code>.
    </p></div></section><section class="sect2" id="ds-disable-reboots" data-id-title="Updates and Reboots during Stage 0"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.5 </span><span class="title-name">Updates and Reboots during Stage 0</span> <a title="Permalink" class="permalink" href="#ds-disable-reboots">#</a></h3></div></div></div><p>
    During Stage 0 (refer to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> for
    more information on DeepSea stages), the Salt master and Salt minions may
    optionally reboot because newly updated packages, for example
    <span class="package">kernel</span>, require rebooting the system.
   </p><p>
    The default behavior is to install available new updates and
    <span class="emphasis"><em>not</em></span> reboot the nodes even in case of kernel updates.
   </p><p>
    You can change the default update/reboot behavior of DeepSea Stage 0 by
    adding/changing the <code class="option">stage_prep_master</code> and
    <code class="option">stage_prep_minion</code> options in the
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file.
    <code class="option">stage_prep_master</code> sets the behavior of the Salt master, and
    <code class="option">stage_prep_minion</code> sets the behavior of all minions. All
    available parameters are:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.5.4.8.5.1"><span class="term">default</span></dt><dd><p>
       Install updates without rebooting.
      </p></dd><dt id="id-1.4.4.5.4.8.5.2"><span class="term">default-update-reboot</span></dt><dd><p>
       Install updates and reboot after updating.
      </p></dd><dt id="id-1.4.4.5.4.8.5.3"><span class="term">default-no-update-reboot</span></dt><dd><p>
       Reboots without installing updates.
      </p></dd><dt id="id-1.4.4.5.4.8.5.4"><span class="term">default-no-update-no-reboot</span></dt><dd><p>
       Do not install updates or reboot.
      </p></dd></dl></div><p>
    For example, to prevent the cluster nodes from installing updates and
    rebooting, edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
    add the following lines:
   </p><div class="verbatim-wrap"><pre class="screen">stage_prep_master: default-no-update-no-reboot
stage_prep_minion: default-no-update-no-reboot</pre></div><div id="id-1.4.4.5.4.8.8" data-id-title="Values and Corresponding Files" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Values and Corresponding Files</h6><p>
     The values of <code class="option">stage_prep_master</code> correspond to file names
     located in <code class="filename">/srv/salt/ceph/stage/0/master</code>, while
     values of <code class="option">stage_prep_minion</code> correspond to files in
     <code class="filename">/srv/salt/ceph/stage/0/minion</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ls -l /srv/salt/ceph/stage/0/master
default-no-update-no-reboot.sls
default-no-update-reboot.sls
default-update-reboot.sls
[...]

<code class="prompt user">cephadm &gt; </code>ls -l /srv/salt/ceph/stage/0/minion
default-no-update-no-reboot.sls
default-no-update-reboot.sls
default-update-reboot.sls
[...]</pre></div></div></section></section><section class="sect1" id="discovered-configuration-modification" data-id-title="Modifying Discovered Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Modifying Discovered Configuration</span> <a title="Permalink" class="permalink" href="#discovered-configuration-modification">#</a></h2></div></div></div><p>
   After you completed Stage 2, you may want to change the discovered
   configuration. To view the current settings, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div><p>
   The output of the default configuration for a single minion is usually
   similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</pre></div><p>
   The above mentioned settings are distributed across several configuration
   files. The directory structure with these files is defined in the
   <code class="filename">/srv/pillar/ceph/stack/stack.cfg</code> directory. The
   following files usually describe your cluster:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> - the file affects
     all minions in the Salt cluster.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>/cluster.yml</code>
     - the file affects all minions in the Ceph cluster called
     <code class="literal">ceph</code>.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>/roles/<em class="replaceable">role</em>.yml</code>
     - affects all minions that are assigned the specific role in the
     <code class="literal">ceph</code> cluster.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>minions/<em class="replaceable">minion
     ID</em>/yml</code> - affects the individual minion.
    </p></li></ul></div><div id="id-1.4.4.5.5.8" data-id-title="Overwriting Directories with Default Values" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Overwriting Directories with Default Values</h6><p>
    There is a parallel directory tree that stores the default configuration
    setup in <code class="filename">/srv/pillar/ceph/stack/default</code>. Do not change
    values here, as they are overwritten.
   </p></div><p>
   The typical procedure for changing the collected configuration is the
   following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Find the location of the configuration item you need to change. For
     example, if you need to change cluster related setting such as cluster
     network, edit the file
     <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>.
    </p></li><li class="step"><p>
     Save the file.
    </p></li><li class="step"><p>
     Verify the changes by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh</pre></div><p>
     and then
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div></li></ol></div></div></section></section></div><div class="part" id="additional-software" data-id-title="Installation of Additional Services"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Installation of Additional Services </span><a title="Permalink" class="permalink" href="#additional-software">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-as-intro"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></span></li><dd class="toc-abstract"><p>After you deploy your SUSE Enterprise Storage 5.5 cluster you may need to install additional software for accessing your data, such as the Object Gateway or the iSCSI Gateway, or you can deploy a clustered file system on top of the Ceph cluster. This chapter mainly focuses on manual installation. If…</p></dd><li><span class="chapter"><a href="#cha-ceph-additional-software-installation"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
  Ceph Object Gateway is an object storage interface built on top of
  <code class="literal">librgw</code> to provide applications with a RESTful gateway to
  Ceph clusters. It supports two interfaces:
 </p></dd><li><span class="chapter"><a href="#cha-ceph-as-iscsi"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></span></li><dd class="toc-abstract"><p>iSCSI is a storage area network (SAN) protocol that allows clients (called initiators) to send SCSI commands to SCSI storage devices (targets) on remote servers. SUSE Enterprise Storage 5.5 includes a facility that opens Ceph storage management to heterogeneous clients, such as Microsoft Windows* an…</p></dd><li><span class="chapter"><a href="#cha-ceph-as-cephfs"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></span></li><dd class="toc-abstract"><p>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<code class="systemitem">librados</code>).
 </p></dd><li><span class="chapter"><a href="#cha-as-ganesha"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></span></li><dd class="toc-abstract"><p>
  NFS Ganesha provides NFS access to either the Object Gateway or the CephFS. In
  SUSE Enterprise Storage 5.5, NFS versions 3 and 4 are supported. NFS Ganesha runs in the
  user space instead of the kernel space and directly interacts with the Object Gateway
  or CephFS.
 </p></dd><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">13 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></span></li><dd class="toc-abstract"><p>This chapter describes how to export data stored in a Ceph cluster via a Samba/CIFS share so that you can easily access them from Windows* client machines. It also includes information that will help you configure a Ceph Samba gateway to join Active Directory in the Windows* domain to authenticate a…</p></dd></ul></div><section class="chapter" id="cha-ceph-as-intro" data-id-title="Installation of Services to Access your Data"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-intro">#</a></h2></div></div></div><p>
  After you deploy your SUSE Enterprise Storage 5.5 cluster you may need to
  install additional software for accessing your data, such as the Object Gateway or the
  iSCSI Gateway, or you can deploy a clustered file system on top of the Ceph
  cluster. This chapter mainly focuses on manual installation. If you have a
  cluster deployed using Salt, refer to
  <a class="xref" href="#ceph-install-saltstack" title="Chapter 4. Deploying with DeepSea/Salt">Chapter 4, <em>Deploying with DeepSea/Salt</em></a> for a procedure on installing
  particular gateways or the CephFS.
 </p></section><section class="chapter" id="cha-ceph-additional-software-installation" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-additional-software-installation">#</a></h2></div></div></div><p>
  Ceph Object Gateway is an object storage interface built on top of
  <code class="literal">librgw</code> to provide applications with a RESTful gateway to
  Ceph clusters. It supports two interfaces:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>S3-compatible</em></span>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Swift-compatible</em></span>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the OpenStack Swift API.
   </p></li></ul></div><p>
  The Object Gateway daemon uses an embedded HTTP server (CivetWeb) for interacting with
  the Ceph cluster. Since it provides interfaces compatible with OpenStack
  Swift and Amazon S3, the Object Gateway has its own user management. Object Gateway can store
  data in the same cluster that is used to store data from CephFS clients or
  RADOS Block Device clients. The S3 and Swift APIs share a common name space, so you may
  write data with one API and retrieve it with the other.
 </p><div id="id-1.4.5.3.6" data-id-title="Object Gateway Deployed by DeepSea" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Object Gateway Deployed by DeepSea</h6><p>
   Since SUSE Enterprise Storage 5.5, the Object Gateway is installed as a DeepSea role, therefore you
   do not need to install it manually.
  </p><p>
   To install the Object Gateway during the cluster deployment, see
   <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>.
  </p><p>
   To add a new node with Object Gateway to the cluster, see
   <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.2 “Adding New Roles to Nodes”</span>.
  </p></div><section class="sect1" id="rgw-installation" data-id-title="Object Gateway Manual Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Object Gateway Manual Installation</span> <a title="Permalink" class="permalink" href="#rgw-installation">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install Object Gateway on a node that is not using port 80. For example a node
     already running openATTIC is already using port 80. The following command
     installs all required components:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper ref &amp;&amp; zypper in ceph-radosgw</pre></div></li><li class="step"><p>
     If the Apache server from the previous Object Gateway instance is running, stop it
     and disable the relevant service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop disable apache2.service</pre></div></li><li class="step"><p>
     Edit <code class="filename">/etc/ceph/ceph.conf</code> and add the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway_host]
 rgw frontends = "civetweb port=80"</pre></div><div id="id-1.4.5.3.7.2.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you want to configure Object Gateway/CivetWeb for use with SSL encryption,
      modify the line accordingly:
     </p><div class="verbatim-wrap"><pre class="screen">rgw frontends = civetweb port=7480s ssl_certificate=<em class="replaceable">path_to_certificate.pem</em></pre></div></div></li><li class="step"><p>
     Restart the Object Gateway service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart ceph-radosgw@rgw.gateway_host</pre></div></li></ol></div></div><section class="sect2" id="ses-rgw-config" data-id-title="Object Gateway Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.1 </span><span class="title-name">Object Gateway Configuration</span> <a title="Permalink" class="permalink" href="#ses-rgw-config">#</a></h3></div></div></div><p>
    Several steps are required to configure an Object Gateway.
   </p><section class="sect3" id="id-1.4.5.3.7.3.3" data-id-title="Basic Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.1 </span><span class="title-name">Basic Configuration</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.7.3.3">#</a></h4></div></div></div><p>
     Configuring a Ceph Object Gateway requires a running Ceph Storage Cluster. The
     Ceph Object Gateway is a client of the Ceph Storage Cluster. As a Ceph
     Storage Cluster client, it requires:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A host name for the gateway instance, for example
       <code class="systemitem">gateway</code>.
      </p></li><li class="listitem"><p>
       A storage cluster user name with appropriate permissions and a keyring.
      </p></li><li class="listitem"><p>
       Pools to store its data.
      </p></li><li class="listitem"><p>
       A data directory for the gateway instance.
      </p></li><li class="listitem"><p>
       An instance entry in the Ceph configuration file.
      </p></li></ul></div><p>
     Each instance must have a user name and key to communicate with a Ceph
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the Object Gateway instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the Ceph Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a keyring for the gateway:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
<code class="prompt user">root # </code>chmod +r /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Generate a Ceph Object Gateway user name and key for each instance. As an
       example, we will use the name <code class="systemitem">gateway</code> after
       <code class="systemitem">client.radosgw</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</pre></div></li><li class="step"><p>
       Add capabilities to the key:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Once you have created a keyring and key to enable the Ceph Object
       Gateway with access to the Ceph Storage Cluster, add the key to your
       Ceph Storage Cluster. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Distribute the keyring to the node with the gateway instance:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>scp /etc/ceph/ceph.client.rgw.keyring  ceph@<em class="replaceable">hostname</em>:/home/ceph
<code class="prompt user">cephadm &gt; </code>ssh <em class="replaceable">hostname</em>
<code class="prompt user">root # </code>mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</pre></div></li></ol></div></div><div id="id-1.4.5.3.7.3.3.6" data-id-title="Use Bootstrap Keyring" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use Bootstrap Keyring</h6><p>
      An alternative way is to create the Object Gateway bootstrap keyring, and then
      create the Object Gateway keyring from it:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Create an Object Gateway bootstrap keyring on one of the monitor nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<em class="replaceable">node_host</em>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</pre></div></li><li class="step"><p>
        Create the
        <code class="filename">/var/lib/ceph/radosgw/ceph-<em class="replaceable">rgw_name</em></code>
        directory for storing the bootstrap keyring:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir \
/var/lib/ceph/radosgw/ceph-<em class="replaceable">rgw_name</em></pre></div></li><li class="step"><p>
        Create an Object Gateway keyring from the newly created bootstrap keyring:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph \
 auth get-or-create client.rgw.<em class="replaceable">rgw_name</em> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<em class="replaceable">rgw_name</em>/keyring</pre></div></li><li class="step"><p>
        Copy the Object Gateway keyring to the Object Gateway host:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>scp \
/var/lib/ceph/radosgw/ceph-<em class="replaceable">rgw_name</em>/keyring \
<em class="replaceable">rgw_host</em>:/var/lib/ceph/radosgw/ceph-<em class="replaceable">rgw_name</em>/keyring</pre></div></li></ol></div></div></div></section><section class="sect3" id="ogw-pool-create" data-id-title="Create Pools (Optional)"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.2 </span><span class="title-name">Create Pools (Optional)</span> <a title="Permalink" class="permalink" href="#ogw-pool-create">#</a></h4></div></div></div><p>
     Ceph Object Gateways require Ceph Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the Ceph
     configuration file.
    </p><p>
     The pool names follow the
     <code class="literal"><em class="replaceable">ZONE_NAME</em>.<em class="replaceable">POOL_NAME</em></code>
     syntax. When configuring a gateway with the default region and zone, the
     default zone name is 'default' as in our example:
    </p><div class="verbatim-wrap"><pre class="screen">.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
default.rgw.buckets.index
default.rgw.buckets.data</pre></div><p>
     To create the pools manually, see
     <span class="intraxref">Book “Administration Guide”, Chapter 8 “Managing Storage Pools”, Section 8.2.2 “Create a Pool”</span>.
    </p><div id="id-1.4.5.3.7.3.4.6" data-id-title="Object Gateway and Erasure-coded Pools" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Object Gateway and Erasure-coded Pools</h6><p>
      Only the <code class="literal">default.rgw.buckets.data</code> pool can be
      erasure-coded. All other pools need to be replicated, otherwise the
      gateway is not accessible.
     </p></div></section><section class="sect3" id="id-1.4.5.3.7.3.5" data-id-title="Adding Gateway Configuration to Ceph"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.3 </span><span class="title-name">Adding Gateway Configuration to Ceph</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.7.3.5">#</a></h4></div></div></div><p>
     Add the Ceph Object Gateway configuration to the Ceph Configuration file. The
     Ceph Object Gateway configuration requires you to identify the Ceph Object Gateway
     instance. Then, specify the host name where you installed the Ceph Object Gateway
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable">instance-name</em>]
host = <em class="replaceable">hostname</em>
keyring = /etc/ceph/ceph.client.rgw.keyring</pre></div><div id="id-1.4.5.3.7.3.5.4" data-id-title="Object Gateway Log File" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Object Gateway Log File</h6><p>
      To override the default Object Gateway log file, include the following:
     </p><div class="verbatim-wrap"><pre class="screen">log file = /var/log/radosgw/client.rgw.<em class="replaceable">instance-name</em>.log</pre></div></div><p>
     The <code class="literal">[client.rgw.*]</code> portion of the gateway instance
     identifies this portion of the Ceph configuration file as configuring a
     Ceph Storage Cluster client where the client type is a Ceph Object Gateway
     (radosgw). The instance name follows. For example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</pre></div><div id="id-1.4.5.3.7.3.5.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The <em class="replaceable">host</em> must be your machine host name,
      excluding the domain name.
     </p></div><p>
     Then turn off <code class="literal">print continue</code>. If you have it set to
     true, you may encounter problems with PUT operations:
    </p><div class="verbatim-wrap"><pre class="screen">rgw print continue = false</pre></div><p>
     To use a Ceph Object Gateway with subdomain S3 calls (for example
     <code class="literal">http://bucketname.hostname</code>), you must add the Ceph
     Object Gateway DNS name under the <code class="literal">[client.rgw.gateway]</code> section
     of the Ceph configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway]
...
rgw dns name = <em class="replaceable">hostname</em></pre></div><p>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <code class="literal">http://<em class="replaceable">bucketname</em>.<em class="replaceable">hostname</em></code>
     syntax. The <code class="filename">dnsmasq.conf</code> file should include the
     following settings:
    </p><div class="verbatim-wrap"><pre class="screen">address=/<em class="replaceable">hostname</em>/<em class="replaceable">host-ip-address</em>
listen-address=<em class="replaceable">client-loopback-ip</em></pre></div><p>
     Then, add the <em class="replaceable">client-loopback-ip</em> IP address as
     the first DNS server on the client machine(s).
    </p></section><section class="sect3" id="id-1.4.5.3.7.3.6" data-id-title="Create Data Directory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.4 </span><span class="title-name">Create Data Directory</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.7.3.6">#</a></h4></div></div></div><p>
     Deployment scripts may not create the default Ceph Object Gateway data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <code class="literal">host</code> variables in the Ceph
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name,
     and the daemon ID.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /var/lib/ceph/radosgw/<em class="replaceable">cluster</em>-<em class="replaceable">id</em></pre></div><p>
     Using the example <code class="filename">ceph.conf</code> settings above, you would
     execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</pre></div></section><section class="sect3" id="id-1.4.5.3.7.3.7" data-id-title="Restart Services and Start the Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.5 </span><span class="title-name">Restart Services and Start the Gateway</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.7.3.7">#</a></h4></div></div></div><p>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your Ceph Storage Cluster service. Then, start up
     the <code class="systemitem">radosgw</code> service. For more information, see
     <span class="intraxref">Book “Administration Guide”, Chapter 2 “Introduction”</span> and
     <span class="intraxref">Book “Administration Guide”, Chapter 13 “Ceph Object Gateway”, Section 13.3 “Operating the Object Gateway Service”</span>.
    </p><p>
     When the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-as-iscsi" data-id-title="Installation of iSCSI Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-iscsi">#</a></h2></div></div></div><p>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <span class="emphasis"><em>initiators</em></span>) to send SCSI commands to SCSI storage
  devices (<span class="emphasis"><em>targets</em></span>) on remote servers. SUSE Enterprise Storage
  5.5 includes a facility that opens Ceph storage management to
  heterogeneous clients, such as Microsoft Windows* and VMware* vSphere, through the
  iSCSI protocol. Multipath iSCSI access enables availability and scalability
  for these clients, and the standardized iSCSI protocol also provides an
  additional layer of security isolation between clients and the SUSE Enterprise Storage
  5.5 cluster. The configuration facility is named
  <code class="systemitem">lrbd</code>. Using <code class="systemitem">lrbd</code>, Ceph
  storage administrators can define thin-provisioned, replicated,
  highly-available volumes supporting read-only snapshots, read-write clones,
  and automatic resizing with Ceph RADOS Block Device (RBD). Administrators
  can then export volumes either via a single <code class="systemitem">lrbd</code>
  gateway host, or via multiple gateway hosts supporting multipath failover.
  Linux, Microsoft Windows, and VMware hosts can connect to volumes using the iSCSI
  protocol, which makes them available like any other SCSI block device. This
  means SUSE Enterprise Storage 5.5 customers can effectively run a complete
  block-storage infrastructure subsystem on Ceph that provides all the
  features and benefits of a conventional SAN, enabling future growth.
 </p><p>
  This chapter introduces detailed information to set up a Ceph cluster
  infrastructure together with an iSCSI gateway so that the client hosts can
  use remotely stored data as local storage devices using the iSCSI protocol.
 </p><section class="sect1" id="ceph-iscsi-iscsi" data-id-title="iSCSI Block Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">iSCSI Block Storage</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi">#</a></h2></div></div></div><p>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
   is implemented as a service where a client (the initiator) talks to a server
   (the target) via a session on TCP port 3260. An iSCSI target's IP address
   and port are called an iSCSI portal, where a target can be exposed through
   one or more portals. The combination of a target and one or more portals is
   called the target portal group (TPG).
  </p><p>
   The underlying data link layer protocol for iSCSI is commonly Ethernet. More
   specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet or faster
   networks for optimal throughput. 10 Gigabit Ethernet connectivity between
   the iSCSI gateway and the back-end Ceph cluster is strongly recommended.
  </p><section class="sect2" id="ceph-iscsi-iscsi-target" data-id-title="The Linux Kernel iSCSI Target"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.1 </span><span class="title-name">The Linux Kernel iSCSI Target</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-target">#</a></h3></div></div></div><p>
    The Linux kernel iSCSI target was originally named LIO for linux-iscsi.org,
    the project's original domain and Web site. For some time, no fewer than
    four competing iSCSI target implementations were available for the Linux
    platform, but LIO ultimately prevailed as the single iSCSI reference
    target. The mainline kernel code for LIO uses the simple, but somewhat
    ambiguous name "target", distinguishing between "target core" and a variety
    of front-end and back-end target modules.
   </p><p>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol is
    supported by SUSE Enterprise Storage.
   </p><p>
    The most frequently used target back-end module is one that is capable of
    simply re-exporting any available block device on the target host. This
    module is named iblock. However, LIO also has an RBD-specific back-end
    module supporting parallelized multipath I/O access to RBD images.
   </p></section><section class="sect2" id="ceph-iscsi-iscsi-initiators" data-id-title="iSCSI Initiators"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.2 </span><span class="title-name">iSCSI Initiators</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-initiators">#</a></h3></div></div></div><p>
    This section introduces brief information on iSCSI initiators used on
    Linux, Microsoft Windows, and VMware platforms.
   </p><section class="sect3" id="id-1.4.5.4.5.5.3" data-id-title="Linux"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.1 </span><span class="title-name">Linux</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.3">#</a></h4></div></div></div><p>
     The standard initiator for the Linux platform is
     <code class="systemitem">open-iscsi</code>. <code class="systemitem">open-iscsi</code>
     launches a daemon, <code class="systemitem">iscsid</code>, which the user can
     then use to discover iSCSI targets on any given portal, log in to targets,
     and map iSCSI volumes. <code class="systemitem">iscsid</code> communicates with
     the SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <code class="systemitem">open-iscsi</code> initiator can be deployed in
     conjunction with the Device Mapper Multipath
     (<code class="systemitem">dm-multipath</code>) facility to provide a highly
     available iSCSI block device.
    </p></section><section class="sect3" id="id-1.4.5.4.5.5.4" data-id-title="Microsoft Windows and Hyper-V"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.2 </span><span class="title-name">Microsoft Windows and Hyper-V</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.4">#</a></h4></div></div></div><p>
     The default iSCSI initiator for the Microsoft Windows operating system is the
     Microsoft iSCSI initiator. The iSCSI service can be configured via a
     graphical user interface (GUI), and supports multipath I/O for high
     availability.
    </p></section><section class="sect3" id="id-1.4.5.4.5.5.5" data-id-title="VMware"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.3 </span><span class="title-name">VMware</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.5">#</a></h4></div></div></div><p>
     The default iSCSI initiator for VMware vSphere and ESX is the VMware
     ESX software iSCSI initiator, <code class="systemitem">vmkiscsi</code>. When
     enabled, it can be configured either from the vSphere client, or using the
     <code class="command">vmkiscsi-tool</code> command. You can then format storage
     volumes connected through the vSphere iSCSI storage adapter with VMFS, and
     use them like any other VM storage device. The VMware initiator also
     supports multipath I/O for high availability.
    </p></section></section></section><section class="sect1" id="ceph-iscsi-lrbd" data-id-title="General Information about lrbd"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">General Information about lrbd</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-lrbd">#</a></h2></div></div></div><p>
   <code class="systemitem">lrbd</code> combines the benefits of RADOS Block Devices
   with the ubiquitous versatility of iSCSI. By employing
   <code class="systemitem">lrbd</code> on an iSCSI target host (known as the
   <code class="systemitem">lrbd</code> gateway), any application that needs to make
   use of block storage can benefit from Ceph, even if it does not speak any
   Ceph client protocol. Instead, users can use iSCSI or any other target
   front-end protocol to connect to an LIO target, which translates all target
   I/O to RBD storage operations.
  </p><div class="figure" id="id-1.4.5.4.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme1.png" target="_blank"><img src="images/lrbd_scheme1.png" width="" alt="Ceph Cluster with a Single iSCSI Gateway"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.1: </span><span class="title-name">Ceph Cluster with a Single iSCSI Gateway </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.6.3">#</a></h6></div></div><p>
   <code class="systemitem">lrbd</code> is inherently highly-available and supports
   multipath operations. Thus, downstream initiator hosts can use multiple
   iSCSI gateways for both high availability and scalability. When
   communicating with an iSCSI configuration with more than one gateway,
   initiators may load-balance iSCSI requests across multiple gateways. In the
   event of a gateway failing, being temporarily unreachable, or being disabled
   for maintenance, I/O will transparently continue via another gateway.
  </p><div class="figure" id="id-1.4.5.4.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme2.png" target="_blank"><img src="images/lrbd_scheme2.png" width="" alt="Ceph Cluster with Multiple iSCSI Gateways"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.2: </span><span class="title-name">Ceph Cluster with Multiple iSCSI Gateways </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.6.5">#</a></h6></div></div></section><section class="sect1" id="ceph-iscsi-deploy" data-id-title="Deployment Considerations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Deployment Considerations</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-deploy">#</a></h2></div></div></div><p>
   A minimum configuration of SUSE Enterprise Storage 5.5 with
   <code class="systemitem">lrbd</code> consists of the following components:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A Ceph storage cluster. The Ceph cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </p></li><li class="listitem"><p>
     An iSCSI target server running the LIO iSCSI target, configured via
     <code class="systemitem">lrbd</code>.
    </p></li><li class="listitem"><p>
     An iSCSI initiator host, running <code class="systemitem">open-iscsi</code>
     (Linux), the Microsoft iSCSI Initiator (Microsoft Windows), or any other compatible
     iSCSI initiator implementation.
    </p></li></ul></div><p>
   A recommended production configuration of SUSE Enterprise Storage 5.5 with
   <code class="systemitem">lrbd</code> consists of:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A Ceph storage cluster. A production Ceph cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running 10-12
     object storage daemons (OSDs), with no fewer than three dedicated MON
     hosts.
    </p></li><li class="listitem"><p>
     Several iSCSI target servers running the LIO iSCSI target, configured via
     <code class="systemitem">lrbd</code>. For iSCSI fail-over and load-balancing,
     these servers must run a kernel supporting the
     <code class="systemitem">target_core_rbd</code> module. Update packages are
     available from the SUSE Linux Enterprise Server maintenance channel.
    </p></li><li class="listitem"><p>
     Any number of iSCSI initiator hosts, running
     <code class="systemitem">open-iscsi</code> (Linux), the Microsoft iSCSI Initiator
     (Microsoft Windows), or any other compatible iSCSI initiator implementation.
    </p></li></ul></div></section><section class="sect1" id="ceph-iscsi-install" data-id-title="Installation and Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Installation and Configuration</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-install">#</a></h2></div></div></div><p>
   This section describes steps to install and configure an iSCSI Gateway on top of
   SUSE Enterprise Storage.
  </p><section class="sect2" id="id-1.4.5.4.8.3" data-id-title="Deploy the iSCSI Gateway to a Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.1 </span><span class="title-name">Deploy the iSCSI Gateway to a Ceph Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.3">#</a></h3></div></div></div><p>
    You can deploy the iSCSI Gateway either during Ceph cluster deployment process,
    or add it to an existing cluster using DeepSea.
   </p><p>
    To include the iSCSI Gateway during the cluster deployment process, refer to
    <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>.
   </p><p>
    To add the iSCSI Gateway to an existing cluster, refer to
    <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.2 “Adding New Roles to Nodes”</span>.
   </p></section><section class="sect2" id="id-1.4.5.4.8.4" data-id-title="Create RBD Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.2 </span><span class="title-name">Create RBD Images</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.4">#</a></h3></div></div></div><p>
    RBD images are created in the Ceph store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
    You can create a volume from any host that is able to connect to your
    storage cluster using the Ceph <code class="command">rbd</code> command line
    utility. This requires the client to have at least a minimal ceph.conf
    configuration file, and appropriate CephX authentication credentials.
   </p><p>
    To create a new volume for subsequent export via iSCSI, use the
    <code class="command">rbd create</code> command, specifying the volume size in
    megabytes. For example, in order to create a 100 GB volume named
    <code class="literal">testvol</code> in the pool named <code class="literal">iscsi</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd --pool iscsi create --size=102400 testvol</pre></div><p>
    The above command creates an RBD volume in the default format 2.
   </p><div id="id-1.4.5.4.8.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Since SUSE Enterprise Storage 3, the default volume format is 2, and format 1 is
     deprecated. However, you can still create the deprecated format 1 volumes
     with the <code class="option">--image-format 1</code> option.
    </p></div></section><section class="sect2" id="ceph-iscsi-rbd-export" data-id-title="Export RBD Images via iSCSI"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.3 </span><span class="title-name">Export RBD Images via iSCSI</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-export">#</a></h3></div></div></div><p>
    To export RBD images via iSCSI, use the <code class="systemitem">lrbd</code>
    utility. <code class="systemitem">lrbd</code> allows you to create, review, and
    modify the iSCSI target configuration, which uses a JSON format.
   </p><div id="id-1.4.5.4.8.5.3" data-id-title="Import Changes into openATTIC" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Import Changes into openATTIC</h6><p>
     Any changes made to the iSCSI Gateway configuration using the
     <code class="command">lrbd</code> command are not visible to DeepSea and openATTIC. To
     import your manual changes, you need to export the iSCSI Gateway configuration to
     a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>lrbd -o /tmp/lrbd.conf</pre></div><p>
     Then copy it to the Salt master so that DeepSea and openATTIC can see it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>scp /tmp/lrbd.conf ses5master:/srv/salt/ceph/igw/cache/lrbd.conf</pre></div><p>
     Finally, edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
     set:
    </p><div class="verbatim-wrap"><pre class="screen">igw_config: default-ui</pre></div></div><p>
    In order to edit the configuration, use <code class="command">lrbd -e</code> or
    <code class="command">lrbd --edit</code>. This command will invoke the default
    editor, as defined by the <code class="literal">EDITOR</code> environment variable.
    You may override this behavior by setting the <code class="option">-E</code> option in
    addition to <code class="option">-e</code>.
   </p><p>
    Below is an example configuration for
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      two iSCSI gateway hosts named <code class="literal">iscsi1.example.com</code> and
      <code class="literal">iscsi2.example.com</code>,
     </p></li><li class="listitem"><p>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <code class="literal">iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</code>,
     </p></li><li class="listitem"><p>
      with a single iSCSI Logical Unit (LU),
     </p></li><li class="listitem"><p>
      backed by an RBD image named <code class="literal">testvol</code> in the RADOS pool
      <code class="literal">rbd</code>,
     </p></li><li class="listitem"><p>
      and exporting the target via two portals named "east" and "west":
     </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "authentication": "none"
        }
    ],
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "hosts": [
                {
                    "host": "iscsi1.example.com",
                    "portal": "east"
                },
                {
                    "host": "iscsi2.example.com",
                    "portal": "west"
                }
            ]
        }
    ],
    "portals": [
        {
            "name": "east",
            "addresses": [
                "192.168.124.104"
            ]
        },
        {
            "name": "west",
            "addresses": [
                "192.168.124.105"
            ]
        }
    ],
    "pools": [
        {
            "pool": "rbd",
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</pre></div><p>
    Note that whenever you refer to a host name in the configuration, this host
    name must match the iSCSI gateway's <code class="command">uname -n</code> command
    output.
   </p><p>
    The edited JSON is stored in the extended attributes (xattrs) of a single
    RADOS object per pool. This object is available to the gateway hosts where
    the JSON is edited, as well as to all gateway hosts connected to the same
    Ceph cluster. No configuration information is stored locally on the
    <code class="systemitem">lrbd</code> gateway.
   </p><p>
    To activate the configuration, store it in the Ceph cluster, and do one
    of the following things (as <code class="systemitem">root</code>):
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Run the <code class="command">lrbd</code> command (without additional options) from
      the command line,
     </p></li></ul></div><p>
    or
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Restart the <code class="systemitem">lrbd</code> service with <code class="command">service
      lrbd restart</code>.
     </p></li></ul></div><p>
    The <code class="systemitem">lrbd</code> "service" does not operate any background
    daemon. Instead, it simply invokes the <code class="command">lrbd</code> command.
    This type of service is known as a "one-shot" service.
   </p><p>
    You should also enable <code class="systemitem">lrbd</code> to auto-configure on
    system start-up. To do so, run the <code class="command">systemctl enable lrbd</code>
    command.
   </p><p>
    The configuration above reflects a simple, one-gateway setup.
    <code class="systemitem">lrbd</code> configuration can be much more complex and
    powerful. The <code class="systemitem">lrbd</code> RPM package comes with an
    extensive set of configuration examples, which you may refer to by checking
    the content of the
    <code class="filename">/usr/share/doc/packages/lrbd/samples</code> directory after
    installation. The samples are also available from
    <a class="link" href="https://github.com/SUSE/lrbd/tree/master/samples" target="_blank">https://github.com/SUSE/lrbd/tree/master/samples</a>.
   </p></section><section class="sect2" id="iscsi-lrbd-autentication" data-id-title="Authentication and Access Control"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.4 </span><span class="title-name">Authentication and Access Control</span> <a title="Permalink" class="permalink" href="#iscsi-lrbd-autentication">#</a></h3></div></div></div><p>
    iSCSI authentication is flexible and covers many possibilities. The five
    possible top level settings are <code class="option">none</code>,
    <code class="option">tpg</code>, <code class="option">acls</code>,
    <code class="option">tpg+identified</code> and <code class="option">identified</code>.
   </p><section class="sect3" id="id-1.4.5.4.8.6.3" data-id-title="No Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.1 </span><span class="title-name">No Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.3">#</a></h4></div></div></div><p>
     'No authentication' means that no initiator will require a user name and
     password to access any LUNs for a specified host or target. 'No
     authentication' can be set explicitly or implicitly. Specify a value of
     'none' for authentication to be set explicitly:
    </p><div class="verbatim-wrap"><pre class="screen">{
    "host": "igw1",
    "authentication": none
}</pre></div><p>
     Removing the entire <code class="option">auth</code> section from the configuration
     will use no authentication implicitly.
    </p></section><section class="sect3" id="id-1.4.5.4.8.6.4" data-id-title="TPG Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.2 </span><span class="title-name">TPG Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.4">#</a></h4></div></div></div><p>
     For common credentials or a shared user name/password, set authentication
     to <code class="option">tpg</code>. This setting will apply to all initiators for the
     associated host or target. In the following example, the same user name
     and password are used for the redundant target and a target local to
     <code class="literal">igw1</code>:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "target": "iqn.2003-01.org.linux-iscsi.igw.x86:sn.redundant",
  "authentication": tpg,
  "tpg": {
      "userid": "common1",
      "password": "pass1"
  }
},
{
    "host": "igw1",
    "authentication": tpg,
    "tpg": {
        "userid": "common1",
        "password": "pass1"
    }
}</pre></div><p>
     Redundant configurations will have the same credentials across gateways
     but are independent of other configurations. In other words, LUNs
     configured specifically for a host and multiple redundant configurations
     can have a unique user name and password for each.
    </p><p>
     One caveat is that any initiator setting will be ignored when using
     <code class="option">tpg</code> authentication. Using common credentials does not
     restrict which initiators may connect. This configuration may be suitable
     in isolated network environments.
    </p></section><section class="sect3" id="id-1.4.5.4.8.6.5" data-id-title="ACLs Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.3 </span><span class="title-name">ACLs Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.5">#</a></h4></div></div></div><p>
     For unique credentials for each initiator, set authentication to
     <code class="option">acls</code>. Additionally, only defined initiators are allowed
     to connect.
    </p><div class="verbatim-wrap"><pre class="screen">{
    "host": "igw1",
    "authentication": acls,
    "acls": [
        {
            "initiator": "iqn.1996-04.de.suse:01:e6ca28cc9f20",
            "userid": "initiator1",
            "password": "pass1",
        }
    ]
},</pre></div></section><section class="sect3" id="id-1.4.5.4.8.6.6" data-id-title="TPG+identified Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.4 </span><span class="title-name">TPG+identified Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.6">#</a></h4></div></div></div><p>
     The previous two authentication settings pair two independent features:
     TPG pairs common credentials with unidentified initiators, while ACLs pair
     unique credentials with identified initiators.
    </p><p>
     Setting authentication to <code class="option">tpg+identified</code> pairs common
     credentials with identified initiators. Although you can imitate the same
     behavior choosing <code class="option">acls</code> and repeating the same credentials
     with each initiator, the configuration would grow huge and harder to
     maintain.
    </p><p>
     The following configuration uses the <code class="option">tpg</code> configuration
     with only the authentication keyword changing.
    </p><div class="verbatim-wrap"><pre class="screen">{
  "target": "iqn.2003-01.org.linux-iscsi.igw.x86:sn.redundant",
  "authentication": tpg+identified,
  "tpg": {
      "userid": "common1",
      "password": "pass1"
  }
},
{
    "host": "igw1",
    "authentication": tpg+identified,
    "tpg": {
        "userid": "common1",
        "password": "pass1"
    }
}</pre></div><p>
     The list of initiators is gathered from those defined in the pools for the
     given hosts and targets in the authentication section.
    </p></section><section class="sect3" id="id-1.4.5.4.8.6.7" data-id-title="Identified Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.5 </span><span class="title-name">Identified Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.7">#</a></h4></div></div></div><p>
     This type of authentication does not use any credentials. In secure
     environments where only assignment of initiators is needed, set the
     authentication to <code class="option">identified</code>. All initiators will connect
     but only have access to the images listed in the <code class="literal">pools</code>
     section.
    </p><div class="verbatim-wrap"><pre class="screen">{
    "target": "iqn.2003-01.org.linux-iscsi:igw.x86:sn.redundant",
    "authentication": "identified",
},
{
    "host": "igw1",
    "authentication": "identified",
}</pre></div></section><section class="sect3" id="id-1.4.5.4.8.6.8" data-id-title="Discovery and Mutual Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.6 </span><span class="title-name">Discovery and Mutual Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.8">#</a></h4></div></div></div><p>
     <span class="emphasis"><em>Discovery authentication</em></span> is independent of the
     previous authentication methods. It requires credentials for browsing.
    </p><p>
     Authentication of type <code class="option">tpg</code>,
     <code class="option">tpg+identified</code>, <code class="option">acls</code>, and
     <code class="option">discovery</code> support mutual authentication. Specifying the
     mutual settings requires that the target authenticates against the
     initiator.
    </p><p>
     Discovery and mutual authentications are optional. These options can be
     present, but disabled allowing experimentation with a particular
     configuration. After you decide, you can remove the disabled entries
     without breaking the configuration.
    </p><p>
     Refer to the examples in
     <code class="filename">/usr/share/doc/packages/lrbd/samples</code>. You can combine
     excerpts from one file with others to create unique configurations.
    </p></section></section><section class="sect2" id="ceph-iscsi-rbd-optional" data-id-title="Optional Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.5 </span><span class="title-name">Optional Settings</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-optional">#</a></h3></div></div></div><p>
    The following settings may be useful for some environments. For images,
    there are <code class="option">uuid</code>, <code class="option">lun</code>,
    <code class="option">retries</code>, <code class="option">sleep</code>, and
    <code class="option">retry_errors</code> attributes. The first
    two—<code class="option">uuid</code> and <code class="option">lun</code>—allow
    hardcoding of the 'uuid' or 'lun' for a specific image. You can specify
    either of them for an image. The <code class="option">retries</code>,
    <code class="option">sleep</code> and <code class="option">retry_errors</code> affect attempts to
    map an rbd image.
   </p><div id="id-1.4.5.4.8.7.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     If a site needs statically assigned LUNs, then assign numbers to each LUN.
    </p></div><div class="verbatim-wrap"><pre class="screen">"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "uuid": "12345678-abcd-9012-efab-345678901234",
                        "lun": "2",
                        "retries": "3",
                        "sleep": "4",
                        "retry_errors": [ 95 ],
                        [...]
                    }
                ]
            }
        ]
    }
]</pre></div></section><section class="sect2" id="ceph-iscsi-rbd-advanced" data-id-title="Advanced Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.6 </span><span class="title-name">Advanced Settings</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-advanced">#</a></h3></div></div></div><p>
    <code class="systemitem">lrbd</code> can be configured with advanced parameters
    which are subsequently passed on to the LIO I/O target. The parameters are
    divided up into iSCSI and backing store components, which can then be
    specified in the "targets" and "tpg" sections, respectively, of the
    <code class="systemitem">lrbd</code> configuration.
   </p><div id="id-1.4.5.4.8.8.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Unless otherwise noted, changing these parameters from the default setting
     is not recommended.
    </p></div><div class="verbatim-wrap"><pre class="screen">"targets": [
    {
        [...]
        "tpg_default_cmdsn_depth": "64",
        "tpg_default_erl": "0",
        "tpg_login_timeout": "10",
        "tpg_netif_timeout": "2",
        "tpg_prod_mode_write_protect": "0",
    }
]</pre></div><p>
    A description of the options follows:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.8.6.1"><span class="term">tpg_default_cmdsn_depth</span></dt><dd><p>
       Default CmdSN (Command Sequence Number) depth. Limits the amount of
       requests that an iSCSI initiator can have outstanding at any moment.
      </p></dd><dt id="id-1.4.5.4.8.8.6.2"><span class="term">tpg_default_erl</span></dt><dd><p>
       Default error recovery level.
      </p></dd><dt id="id-1.4.5.4.8.8.6.3"><span class="term">tpg_login_timeout</span></dt><dd><p>
       Login timeout value in seconds.
      </p></dd><dt id="id-1.4.5.4.8.8.6.4"><span class="term">tpg_netif_timeout</span></dt><dd><p>
       NIC failure timeout in seconds.
      </p></dd><dt id="id-1.4.5.4.8.8.6.5"><span class="term">tpg_prod_mode_write_protect</span></dt><dd><p>
       If set to 1, prevents writes to LUNs.
      </p></dd></dl></div><div class="verbatim-wrap"><pre class="screen">"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "backstore_block_size": "512",
                        "backstore_emulate_3pc": "1",
                        "backstore_emulate_caw": "1",
                        "backstore_emulate_dpo": "0",
                        "backstore_emulate_fua_read": "0",
                        "backstore_emulate_fua_write": "1",
                        "backstore_emulate_model_alias": "0",
                        "backstore_emulate_pr": "1",
                        "backstore_emulate_rest_reord": "0",
                        "backstore_emulate_tas": "1",
                        "backstore_emulate_tpu": "0",
                        "backstore_emulate_tpws": "0",
                        "backstore_emulate_ua_intlck_ctrl": "0",
                        "backstore_emulate_write_cache": "0",
                        "backstore_enforce_pr_isids": "1",
                        "backstore_fabric_max_sectors": "8192",
                        "backstore_hw_block_size": "512",
                        "backstore_hw_max_sectors": "8192",
                        "backstore_hw_pi_prot_type": "0",
                        "backstore_hw_queue_depth": "128",
                        "backstore_is_nonrot": "1",
                        "backstore_max_unmap_block_desc_count": "1",
                        "backstore_max_unmap_lba_count": "8192",
                        "backstore_max_write_same_len": "65535",
                        "backstore_optimal_sectors": "8192",
                        "backstore_pi_prot_format": "0",
                        "backstore_pi_prot_type": "0",
                        "backstore_queue_depth": "128",
                        "backstore_unmap_granularity": "8192",
                        "backstore_unmap_granularity_alignment": "4194304"
                    }
                ]
            }
        ]
    }
]</pre></div><p>
    A description of the options follows:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.8.9.1"><span class="term">backstore_block_size</span></dt><dd><p>
       Block size of the underlying device.
      </p></dd><dt id="id-1.4.5.4.8.8.9.2"><span class="term">backstore_emulate_3pc</span></dt><dd><p>
       If set to 1, enables Third Party Copy.
      </p></dd><dt id="id-1.4.5.4.8.8.9.3"><span class="term">backstore_emulate_caw</span></dt><dd><p>
       If set to 1, enables Compare and Write.
      </p></dd><dt id="id-1.4.5.4.8.8.9.4"><span class="term">backstore_emulate_dpo</span></dt><dd><p>
       If set to 1, turns on Disable Page Out.
      </p></dd><dt id="id-1.4.5.4.8.8.9.5"><span class="term">backstore_emulate_fua_read</span></dt><dd><p>
       If set to 1, enables Force Unit Access read.
      </p></dd><dt id="id-1.4.5.4.8.8.9.6"><span class="term">backstore_emulate_fua_write</span></dt><dd><p>
       If set to 1, enables Force Unit Access write.
      </p></dd><dt id="id-1.4.5.4.8.8.9.7"><span class="term">backstore_emulate_model_alias</span></dt><dd><p>
       If set to 1, uses the back-end device name for the model alias.
      </p></dd><dt id="id-1.4.5.4.8.8.9.8"><span class="term">backstore_emulate_pr</span></dt><dd><p>
       If set to 0, support for SCSI Reservations, including Persistent Group
       Reservations, is disabled. While disabled, the SES iSCSI Gateway can
       ignore reservation state, resulting in improved request latency.
      </p><div id="id-1.4.5.4.8.8.9.8.2.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
        Setting backstore_emulate_pr to 0 is recommended if iSCSI initiators do
        not require SCSI Reservation support.
       </p></div></dd><dt id="id-1.4.5.4.8.8.9.9"><span class="term">backstore_emulate_rest_reord</span></dt><dd><p>
       If set to 0, the Queue Algorithm Modifier has Restricted Reordering.
      </p></dd><dt id="id-1.4.5.4.8.8.9.10"><span class="term">backstore_emulate_tas</span></dt><dd><p>
       If set to 1, enables Task Aborted Status.
      </p></dd><dt id="id-1.4.5.4.8.8.9.11"><span class="term">backstore_emulate_tpu</span></dt><dd><p>
       If set to 1, enables Thin Provisioning Unmap.
      </p></dd><dt id="id-1.4.5.4.8.8.9.12"><span class="term">backstore_emulate_tpws</span></dt><dd><p>
       If set to 1, enables Thin Provisioning Write Same.
      </p></dd><dt id="id-1.4.5.4.8.8.9.13"><span class="term">backstore_emulate_ua_intlck_ctrl</span></dt><dd><p>
       If set to 1, enables Unit Attention Interlock.
      </p></dd><dt id="id-1.4.5.4.8.8.9.14"><span class="term">backstore_emulate_write_cache</span></dt><dd><p>
       If set to 1, turns on Write Cache Enable.
      </p></dd><dt id="id-1.4.5.4.8.8.9.15"><span class="term">backstore_enforce_pr_isids</span></dt><dd><p>
       If set to 1, enforces persistent reservation ISIDs.
      </p></dd><dt id="id-1.4.5.4.8.8.9.16"><span class="term">backstore_fabric_max_sectors</span></dt><dd><p>
       Maximum number of sectors the fabric can transfer at once.
      </p></dd><dt id="id-1.4.5.4.8.8.9.17"><span class="term">backstore_hw_block_size</span></dt><dd><p>
       Hardware block size in bytes.
      </p></dd><dt id="id-1.4.5.4.8.8.9.18"><span class="term">backstore_hw_max_sectors</span></dt><dd><p>
       Maximum number of sectors the hardware can transfer at once.
      </p></dd><dt id="id-1.4.5.4.8.8.9.19"><span class="term">backstore_hw_pi_prot_type</span></dt><dd><p>
       If non-zero, DIF protection is enabled on the underlying hardware.
      </p></dd><dt id="id-1.4.5.4.8.8.9.20"><span class="term">backstore_hw_queue_depth</span></dt><dd><p>
       Hardware queue depth.
      </p></dd><dt id="id-1.4.5.4.8.8.9.21"><span class="term">backstore_is_nonrot</span></dt><dd><p>
       If set to 1, the backstore is a non-rotational device.
      </p></dd><dt id="id-1.4.5.4.8.8.9.22"><span class="term">backstore_max_unmap_block_desc_count</span></dt><dd><p>
       Maximum number of block descriptors for UNMAP.
      </p></dd><dt id="id-1.4.5.4.8.8.9.23"><span class="term">backstore_max_unmap_lba_count:</span></dt><dd><p>
       Maximum number of LBAs for UNMAP.
      </p></dd><dt id="id-1.4.5.4.8.8.9.24"><span class="term">backstore_max_write_same_len</span></dt><dd><p>
       Maximum length for WRITE_SAME.
      </p></dd><dt id="id-1.4.5.4.8.8.9.25"><span class="term">backstore_optimal_sectors</span></dt><dd><p>
       Optimal request size in sectors.
      </p></dd><dt id="id-1.4.5.4.8.8.9.26"><span class="term">backstore_pi_prot_format</span></dt><dd><p>
       DIF protection format.
      </p></dd><dt id="id-1.4.5.4.8.8.9.27"><span class="term">backstore_pi_prot_type</span></dt><dd><p>
       DIF protection type.
      </p></dd><dt id="id-1.4.5.4.8.8.9.28"><span class="term">backstore_queue_depth</span></dt><dd><p>
       Queue depth.
      </p></dd><dt id="id-1.4.5.4.8.8.9.29"><span class="term">backstore_unmap_granularity</span></dt><dd><p>
       UNMAP granularity.
      </p></dd><dt id="id-1.4.5.4.8.8.9.30"><span class="term">backstore_unmap_granularity_alignment</span></dt><dd><p>
       UNMAP granularity alignment.
      </p></dd></dl></div><p>
    For targets, the <code class="option">tpg</code> attributes allow tuning of kernel
    parameters. Use with caution.
   </p><div class="verbatim-wrap"><pre class="screen">"targets": [
{
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "tpg_default_cmdsn_depth": "64",
    "tpg_default_erl": "0",
    "tpg_login_timeout": "10",
    "tpg_netif_timeout": "2",
    "tpg_prod_mode_write_protect": "0",
    "tpg_t10_pi": "0"
}</pre></div><p>
    For initiators, the <code class="option">attrib</code> and <code class="option">param</code>
    settings allow the tuning of kernel parameters. Use with caution. These are
    set in the authentication section. If the authentication is
    <code class="option">tpg+identified</code> or <code class="option">identified</code>, then the
    subsection is identified.
   </p><div class="verbatim-wrap"><pre class="screen">"auth": [
  {
      "authentication": "tpg+identified",
      "identified": [
        {
          "initiator": "iqn.1996-04.de.suse:01:e6ca28cc9f20",
          "attrib_dataout_timeout": "3",
          "attrib_dataout_timeout_retries": "5",
          "attrib_default_erl": "0",
          "attrib_nopin_response_timeout": "30",
          "attrib_nopin_timeout": "15",
          "attrib_random_datain_pdu_offsets": "0",
          "attrib_random_datain_seq_offsets": "0",
          "attrib_random_r2t_offsets": "0",
          "param_DataPDUInOrder": "1",
          "param_DataSequenceInOrder": "1",
          "param_DefaultTime2Retain": "0",
          "param_DefaultTime2Wait": "2",
          "param_ErrorRecoveryLevel": "0",
          "param_FirstBurstLength": "65536",
          "param_ImmediateData": "1",
          "param_InitialR2T": "1",
          "param_MaxBurstLength": "262144",
          "param_MaxConnections": "1",
          "param_MaxOutstandingR2T": "1"
        }
      ]
  }
]</pre></div><p>
    If the authentication is <code class="option">acls</code>, then the settings are
    included in the <code class="option">acls</code> subsection. One caveat is that
    settings are only applied for active initiators. If an initiator is absent
    from the pools section, the <code class="option">acl</code> entry is not created and
    settings cannot be applied.
   </p></section></section><section class="sect1" id="iscsi-tcmu" data-id-title="Exporting RADOS Block Device Images using tcmu-runner"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.5 </span><span class="title-name">Exporting RADOS Block Device Images using <code class="systemitem">tcmu-runner</code></span> <a title="Permalink" class="permalink" href="#iscsi-tcmu">#</a></h2></div></div></div><p>
   Since version 5, SUSE Enterprise Storage ships a user space RBD back-end for
   <code class="systemitem">tcmu-runner</code> (see <code class="command">man 8
   tcmu-runner</code> for details).
  </p><div id="id-1.4.5.4.9.3" data-id-title="Technology Preview" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Technology Preview</h6><p>
    <code class="systemitem">tcmu-runner</code> based iSCSI Gateway deployments are currently
    a technology preview. See <a class="xref" href="#cha-ceph-as-iscsi" title="Chapter 10. Installation of iSCSI Gateway">Chapter 10, <em>Installation of iSCSI Gateway</em></a> for
    instructions on kernel-based iSCSI Gateway deployment with
    <code class="systemitem">lrbd</code>.
   </p></div><p>
   Unlike kernel-based <code class="systemitem">lrbd</code> iSCSI Gateway deployments,
   <code class="systemitem">tcmu-runner</code> based iSCSI Gateways do not offer support for
   multipath I/O or SCSI Persistent Reservations.
  </p><p>
   As DeepSea and openATTIC do not currently support
   <code class="systemitem">tcmu-runner</code> deployments, you need to manage the
   installation, deployment, and monitoring manually.
  </p><section class="sect2" id="iscsi-tcmu-install" data-id-title="Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.5.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#iscsi-tcmu-install">#</a></h3></div></div></div><p>
    On your iSCSI Gateway node, install the
    <code class="systemitem">tcmu-runner-handler-rbd</code> package from the
    SUSE Enterprise Storage 5 media, together with the <code class="systemitem">libtcmu1</code>
    and <code class="systemitem">tcmu-runner</code> package dependencies. Install the
    <code class="systemitem">targetcli-fb</code> package for configuration purposes.
    Note that the <code class="systemitem">targetcli-fb</code> package is incompatible
    with the 'non-fb' version of the <code class="systemitem">targetcli</code>
    package.
   </p><p>
    Confirm that the <code class="systemitem">tcmu-runner</code> <code class="systemitem">systemd</code> service is
    running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable tcmu-runner
tcmu-gw:~ # systemctl status tcmu-runner
● tcmu-runner.service - LIO Userspace-passthrough daemon
  Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; static; vendor
  preset: disabled)
    Active: active (running) since ...</pre></div></section><section class="sect2" id="iscsi-tcmu-depl" data-id-title="Configuration and Deployment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.5.2 </span><span class="title-name">Configuration and Deployment</span> <a title="Permalink" class="permalink" href="#iscsi-tcmu-depl">#</a></h3></div></div></div><p>
    Create a RADOS Block Device image on your existing Ceph cluster. In the following
    example, we will use a 10G image called 'tcmu-lu' located in the 'rbd'
    pool.
   </p><p>
    Following RADOS Block Device image creation, run <code class="command">targetcli</code>, and
    ensure that the tcmu-runner RBD handler (plug-in) is available:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>targetcli
targetcli shell version 2.1.fb46
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/&gt; ls
o- / ................................... [...]
  o- backstores ........................ [...]
...
  | o- user:rbd ......... [Storage Objects: 0]</pre></div><p>
    Create a backstore configuration entry for the RBD image:
   </p><div class="verbatim-wrap"><pre class="screen">/&gt; cd backstores/user:rbd
/backstores/user:rbd&gt; create tcmu-lu 10G /rbd/tcmu-lu
Created user-backed storage object tcmu-lu size 10737418240.</pre></div><p>
    Create an iSCSI transport configuration entry. In the following example,
    the target IQN "iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a" is
    automatically generated by <code class="command">targetcli</code> for use as a unique
    iSCSI target identifier:
   </p><div class="verbatim-wrap"><pre class="screen">/backstores/user:rbd&gt; cd /iscsi
/iscsi&gt; create
Created target iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.</pre></div><p>
    Create an ACL entry for the iSCSI initiator(s) that you want to connect
    to the target. In the following example, an initiator IQN of
    "iqn.1998-01.com.vmware:esxi-872c4888" is used:
   </p><div class="verbatim-wrap"><pre class="screen">/iscsi&gt; cd
iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a/tpg1/acls/
/iscsi/iqn.20...a3a/tpg1/acls&gt; create iqn.1998-01.com.vmware:esxi-872c4888</pre></div><p>
    Finally, link the previously created RBD backstore configuration to the
    iSCSI target:
   </p><div class="verbatim-wrap"><pre class="screen">/iscsi/iqn.20...a3a/tpg1/acls&gt; cd ../luns
/iscsi/iqn.20...a3a/tpg1/luns&gt; create /backstores/user:rbd/tcmu-lu
Created LUN 0.
Created LUN 0-&gt;0 mapping in node ACL iqn.1998-01.com.vmware:esxi-872c4888</pre></div><p>
    Exit the shell to save the existing configuration:
   </p><div class="verbatim-wrap"><pre class="screen">/iscsi/iqn.20...a3a/tpg1/luns&gt; exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup.
Configuration saved to /etc/target/saveconfig.json</pre></div></section><section class="sect2" id="iscsi-tcmu-use" data-id-title="Usage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.5.3 </span><span class="title-name">Usage</span> <a title="Permalink" class="permalink" href="#iscsi-tcmu-use">#</a></h3></div></div></div><p>
    From your iSCSI initiator (client) node, connect to your newly
    provisioned iSCSI target using the IQN and host name configured above.
   </p><div class="verbatim-wrap"><pre class="screen">"auth": [
    {
        "host": "igw1",
        "authentication": "acls",
        "acls": [
            {
                "initiator": "iqn.1996-04.de.suse:01:e6ca28cc9f20",
                "userid": "initiator1",
                "password": "pass1",
                "attrib_dataout_timeout": "3",
                "attrib_dataout_timeout_retries": "5",
                "attrib_default_erl": "0",
                "attrib_nopin_response_timeout": "30",
                "attrib_nopin_timeout": "15",
                "attrib_random_datain_pdu_offsets": "0",
                "attrib_random_datain_seq_offsets": "0",
                "attrib_random_r2t_offsets": "0",
                "param_DataPDUInOrder": "1",
                "param_DataSequenceInOrder": "1",
                "param_DefaultTime2Retain": "0",
                "param_DefaultTime2Wait": "2",
                "param_ErrorRecoveryLevel": "0",
                "param_FirstBurstLength": "65536",
                "param_ImmediateData": "1",
                "param_InitialR2T": "1",
                "param_MaxBurstLength": "262144",
                "param_MaxConnections": "1",
                "param_MaxOutstandingR2T": "1"
            }
        ]
    },
]</pre></div></section></section></section><section class="chapter" id="cha-ceph-as-cephfs" data-id-title="Installation of CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-cephfs">#</a></h2></div></div></div><p>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<code class="systemitem">librados</code>).
 </p><p>
  To use CephFS, you need to have a running Ceph storage cluster, and at
  least one running <span class="emphasis"><em>Ceph metadata server</em></span>.
 </p><section class="sect1" id="ceph-cephfs-limitations" data-id-title="Supported CephFS Scenarios and Guidance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-limitations">#</a></h2></div></div></div><p>
   With SUSE Enterprise Storage, SUSE introduces official support for many scenarios in
   which the scale-out and distributed component CephFS is used. This entry
   describes hard limits and provides guidance for the suggested use cases.
  </p><p>
   A supported CephFS deployment must meet these requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A minimum of one Metadata Server. SUSE recommends to deploy several nodes with the
     MDS role. Only one will be 'active' and the rest will be 'passive'.
     Remember to mention all the MON nodes in the <code class="command">mount</code>
     command when mounting the CephFS from a client.
    </p></li><li class="listitem"><p>
     CephFS snapshots are disabled (default) and not supported in this
     version.
    </p></li><li class="listitem"><p>
     Clients are SUSE Linux Enterprise Server 12 SP2 or SP3 based, using the
     <code class="literal">cephfs</code> kernel module driver. The FUSE module is not
     supported.
    </p></li><li class="listitem"><p>
     CephFS quotas are not supported in SUSE Enterprise Storage, as support for quotas
     is implemented in the FUSE client only.
    </p></li><li class="listitem"><p>
     CephFS supports file layout changes as documented in
     <a class="link" href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/" target="_blank">http://docs.ceph.com/docs/jewel/cephfs/file-layouts/</a>.
     However, while the file system is mounted by any client, new data pools
     may not be added to an existing CephFS file system (<code class="literal">ceph mds
     add_data_pool</code>). They may only be added while the file system is
     unmounted.
    </p></li></ul></div></section><section class="sect1" id="ceph-cephfs-mds" data-id-title="Ceph Metadata Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mds">#</a></h2></div></div></div><p>
   Ceph metadata server (MDS) stores metadata for the CephFS. Ceph block
   devices and Ceph object storage <span class="emphasis"><em>do not</em></span> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands—such as <code class="command">ls</code> or
   <code class="command">find</code>—without placing an enormous burden on the
   Ceph storage cluster.
  </p><section class="sect2" id="ceph-cephfs-mdf-add" data-id-title="Adding a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.1 </span><span class="title-name">Adding a Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mdf-add">#</a></h3></div></div></div><p>
    You can deploy MDS either during the initial cluster deployment process as
    described in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>, or add it to an already
    deployed cluster as described in <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.1 “Adding New Cluster Nodes”</span>.
   </p><p>
    After you deploy your MDS, allow the <code class="literal">Ceph OSD/MDS</code>
    service in the firewall setting of the server where MDS is deployed: Start
    <code class="literal">yast</code>, navigate to <span class="guimenu">Security and
    Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
    Services</span> and in the <span class="guimenu">Service to
    Allow</span> drop–down menu select <span class="guimenu">Ceph
    OSD/MDS</span>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </p></section><section class="sect2" id="ceph-cephfs-mds-config" data-id-title="Configuring a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.2 </span><span class="title-name">Configuring a Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mds-config">#</a></h3></div></div></div><p>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <code class="filename">ceph.conf</code> configuration file.
   </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MDS Cache Size </span><a title="Permalink" class="permalink" href="#id-1.4.5.5.6.4.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.5.6.4.3.2"><span class="term"><code class="option">mds cache memory limit</code>
     </span></dt><dd><p>
       The soft memory limit (in bytes) that the MDS will enforce for its
       cache. Administrators should use this instead of the old <code class="option">mds
       cache size</code> setting. Defaults to 1GB.
      </p></dd><dt id="id-1.4.5.5.6.4.3.3"><span class="term"><code class="option">mds cache reservation</code>
     </span></dt><dd><p>
       The cache reservation (memory or inodes) for the MDS cache to maintain.
       When the MDS begins touching its reservation, it will recall client
       state until its cache size shrinks to restore the reservation. Defaults
       to 0.05.
      </p></dd></dl></div><p>
    For a detailed list of MDS related configuration options, see
    <a class="link" href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/" target="_blank">http://docs.ceph.com/docs/master/cephfs/mds-config-ref/</a>.
   </p><p>
    For a detailed list of MDS journaler configuration options, see
    <a class="link" href="http://docs.ceph.com/docs/master/cephfs/journaler/" target="_blank">http://docs.ceph.com/docs/master/cephfs/journaler/</a>.
   </p></section></section><section class="sect1" id="ceph-cephfs-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.3 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs">#</a></h2></div></div></div><p>
   When you have a healthy Ceph storage cluster with at least one Ceph
   metadata server, you can create and mount your Ceph file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </p><section class="sect2" id="ceph-cephfs-cephfs-create" data-id-title="Creating CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.1 </span><span class="title-name">Creating CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-create">#</a></h3></div></div></div><p>
    A CephFS requires at least two RADOS pools: one for
    <span class="emphasis"><em>data</em></span> and one for <span class="emphasis"><em>metadata</em></span>. When
    configuring these pools, you might consider:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </p></li><li class="listitem"><p>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </p></li></ul></div><p>
    When assigning a <code class="literal">role-mds</code> in the
    <code class="filename">policy.cfg</code>, the required pools are automatically
    created. You can manually create the pools <code class="literal">cephfs_data</code>
    and <code class="literal">cephfs_metadata</code> for manual performance tuning before
    setting up the Metadata Server. DeepSea will not create these pools if they already
    exist.
   </p><p>
    For more information on managing pools, see <span class="intraxref">Book “Administration Guide”, Chapter 8 “Managing Storage Pools”</span>.
   </p><p>
    To create the two required pools—for example, 'cephfs_data' and
    'cephfs_metadata'—with default settings for use with CephFS, run
    the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create cephfs_data <em class="replaceable">pg_num</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool create cephfs_metadata <em class="replaceable">pg_num</em></pre></div><p>
    It is possible to use EC pools instead of replicated pools. We recommend to
    only use EC pools for low performance requirements and infrequent random
    access, for example cold storage, backups, archiving. CephFS on EC pools
    requires BlueStore to be enabled and the pool must have the
    <code class="literal">allow_ec_overwrite</code> option set. This option can be set by
    running <code class="command">ceph osd pool set ec_pool allow_ec_overwrites
    true</code>.
   </p><p>
    Erasure coding adds significant overhead to file system operations,
    especially small updates. This overhead is inherent to using erasure coding
    as a fault tolerance mechanism. This penalty is the trade off for
    significantly reduced storage space overhead.
   </p><p>
    When the pools are created, you may enable the file system with the
    <code class="command">ceph fs new</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs new <em class="replaceable">fs_name</em> <em class="replaceable">metadata_pool_name</em> <em class="replaceable">data_pool_name</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs new cephfs cephfs_metadata cephfs_data</pre></div><p>
    You can check that the file system was created by listing all available
    CephFSs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> <code class="option">fs ls</code>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</pre></div><p>
    When the file system has been created, your MDS will be able to enter an
    <span class="emphasis"><em>active</em></span> state. For example, in a single MDS system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> <code class="option">mds stat</code>
e5: 1/1/1 up</pre></div><div id="id-1.4.5.5.7.3.18" data-id-title="More Topics" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Topics</h6><p>
     You can find more information of specific tasks—for example
     mounting, unmounting, and advanced CephFS setup—in
     <span class="intraxref">Book “Administration Guide”, Chapter 15 “Clustered File System”</span>.
    </p></div></section><section class="sect2" id="ceph-cephfs-multimds" data-id-title="MDS Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.2 </span><span class="title-name">MDS Cluster Size</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-multimds">#</a></h3></div></div></div><p>
    A CephFS instance can be served by multiple active MDS daemons. All
    active MDS daemons that are assigned to a CephFS instance will distribute
    the file system's directory tree between themselves, and thus spread the
    load of concurrent clients. In order to add an active MDS daemon to a
    CephFS instance, a spare standby is needed. Either start an additional
    daemon or use an existing standby instance.
   </p><p>
    The following command will display the current number of active and passive
    MDS daemons.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mds stat</pre></div><p>
    The following command sets the number of active MDS's to two in a file
    system instance.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 2</pre></div><p>
    In order to shrink the MDS cluster prior to an update, two steps are
    necessary. First set <code class="option">max_mds</code> so that only one instance
    remains:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 1</pre></div><p>
    and after that explicitly deactivate the other active MDS daemons:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mds deactivate <em class="replaceable">fs_name</em>:<em class="replaceable">rank</em></pre></div><p>
    where <em class="replaceable">rank</em> is the number of an active MDS daemon
    of a file system instance, ranging from 0 to <code class="option">max_mds</code>-1.
    See
    <a class="link" href="http://docs.ceph.com/docs/luminous/cephfs/multimds/" target="_blank">http://docs.ceph.com/docs/luminous/cephfs/multimds/</a>
    for additional information.
   </p></section><section class="sect2" id="ceph-cephfs-multimds-updates" data-id-title="MDS Cluster and Updates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.3 </span><span class="title-name">MDS Cluster and Updates</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-multimds-updates">#</a></h3></div></div></div><p>
    During Ceph updates, the feature flags on a file system instance may
    change (usually by adding new features). Incompatible daemons (such as the
    older versions) are not able to function with an incompatible feature set
    and will refuse to start. This means that updating and restarting one
    daemon can cause all other not yet updated daemons to stop and refuse to
    start. For this reason we, recommend shrinking the active MDS cluster to
    size one and stopping all standby daemons before updating Ceph. The
    manual steps for this update procedure are as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the Ceph related packages using <code class="command">zypper</code>.
     </p></li><li class="step"><p>
      Shrink the active MDS cluster as described above to 1 instance and stop
      all standby MDS daemons using their <code class="systemitem">systemd</code> units on all other nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Only then restart the single remaining MDS daemon, causing it to restart
      using the updated binary.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Restart all other MDS daemons and re-set the desired
      <code class="option">max_mds</code> setting.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-mds.target</pre></div></li></ol></div></div><p>
    If you use DeepSea, it will follow this procedure in case the
    <span class="package">ceph</span> package was updated during Stages 0 and 4. It is
    possible to perform this procedure while clients have the CephFS instance
    mounted and I/O is ongoing. Note however that there will be a very brief
    I/O pause while the active MDS restarts. Clients will recover
    automatically.
   </p><p>
    It is good practice to reduce the I/O load as much as possible before
    updating an MDS cluster. An idle MDS cluster will go through this update
    procedure quicker. Conversely, on a heavily loaded cluster with multiple
    MDS daemons it is essential to reduce the load in advance to prevent a
    single MDS daemon from being overwhelmed by ongoing I/O.
   </p></section></section></section><section class="chapter" id="cha-as-ganesha" data-id-title="Installation of NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span> <a title="Permalink" class="permalink" href="#cha-as-ganesha">#</a></h2></div></div></div><p>
  NFS Ganesha provides NFS access to either the Object Gateway or the CephFS. In
  SUSE Enterprise Storage 5.5, NFS versions 3 and 4 are supported. NFS Ganesha runs in the
  user space instead of the kernel space and directly interacts with the Object Gateway
  or CephFS.
 </p><div id="id-1.4.5.6.4" data-id-title="Cross Protocol Access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Cross Protocol Access</h6><p>
   Native CephFS and NFS clients are not restricted by file locks obtained
   via Samba, and vice-versa. Applications that rely on cross protocol file
   locking may experience data corruption if CephFS backed Samba share paths
   are accessed via other means.
  </p></div><section class="sect1" id="sec-as-ganesha-preparation" data-id-title="Preparation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Preparation</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation">#</a></h2></div></div></div><section class="sect2" id="sec-as-ganesha-preparation-general" data-id-title="General Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.1 </span><span class="title-name">General Information</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation-general">#</a></h3></div></div></div><p>
    To successfully deploy NFS Ganesha, you need to add a
    <code class="literal">role-ganesha</code> to your
    <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>. For details,
    see <a class="xref" href="#policy-configuration" title="4.5.1. The policy.cfg File">Section 4.5.1, “The <code class="filename">policy.cfg</code> File”</a>. NFS Ganesha also needs either a
    <code class="literal">role-rgw</code> or a <code class="literal">role-mds</code> present in the
    <code class="filename">policy.cfg</code>.
   </p><p>
    Although it is possible to install and run the NFS Ganesha server on an
    already existing Ceph node, we recommend running it on a dedicated host
    with access to the Ceph cluster. The client hosts are typically not part
    of the cluster, but they need to have network access to the NFS Ganesha
    server.
   </p><p>
    To enable the NFS Ganesha server at any point after the initial installation,
    add the <code class="literal">role-ganesha</code> to the
    <code class="filename">policy.cfg</code> and re-run at least DeepSea stages 2 and
    4. For details, see <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>.
   </p><p>
    NFS Ganesha is configured via the file
    <code class="filename">/etc/ganesha/ganesha.conf</code> that exists on the NFS Ganesha
    node. However, this file is overwritten each time DeepSea stage 4 is
    executed. Therefore we recommend to edit the template used by Salt, which
    is the file
    <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> on the
    Salt master. For details about the configuration file, see
    <span class="intraxref">Book “Administration Guide”, Chapter 16 “NFS Ganesha: Export Ceph Data via NFS”, Section 16.2 “Configuration”</span>.
   </p></section><section class="sect2" id="sec-as-ganesha-preparation-requirements" data-id-title="Summary of Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.2 </span><span class="title-name">Summary of Requirements</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation-requirements">#</a></h3></div></div></div><p>
    The following requirements need to be met before DeepSea stages 2 and 4
    can be executed to install NFS Ganesha:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least one node needs to be assigned the
      <code class="literal">role-ganesha</code>.
     </p></li><li class="listitem"><p>
      You can define only one <code class="literal">role-ganesha</code> per minion.
     </p></li><li class="listitem"><p>
      NFS Ganesha needs either an Object Gateway or CephFS to work.
     </p></li><li class="listitem"><p>
      If NFS Ganesha is supposed to use the Object Gateway to interface with the cluster,
      the <code class="filename">/srv/pillar/ceph/rgw.sls</code> on the Salt master needs
      to be populated.
     </p></li><li class="listitem"><p>
      The kernel based NFS needs to be disabled on minions with the
      <code class="literal">role-ganesha</code> role.
     </p></li></ul></div></section></section><section class="sect1" id="sec-as-ganesha-basic-example" data-id-title="Example Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Example Installation</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-basic-example">#</a></h2></div></div></div><p>
   This procedure provides an example installation that uses both the Object Gateway and
   CephFS File System Abstraction Layers (FSAL) of NFS Ganesha.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If you have not done so, execute DeepSea stages 0 and 1 before
     continuing with this procedure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.0
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     After having executed stage 1 of DeepSea, edit the
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and add the
     line
    </p><div class="verbatim-wrap"><pre class="screen">role-ganesha/cluster/<em class="replaceable">NODENAME</em></pre></div><p>
     Replace <em class="replaceable">NODENAME</em> with the name of a node in
     your cluster.
    </p><p>
     Also make sure that a <code class="literal">role-mds</code> and a
     <code class="literal">role-rgw</code> are assigned.
    </p></li><li class="step"><p>
     Create a file with '.yml' extension in the
     <code class="filename">/srv/salt/ceph/rgw/users/users.d</code> directory and insert
     the following content:
    </p><div class="verbatim-wrap"><pre class="screen">- { uid: "demo", name: "Demo", email: "demo@demo.nil" }
- { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</pre></div><p>
     These users are later created as Object Gateway users, and API keys are generated.
     On the Object Gateway node, you can later run <code class="command">radosgw-admin user
     list</code> to list all created users and <code class="command">radosgw-admin user
     info --uid=demo</code> to obtain details about single users.
    </p><p>
     DeepSea makes sure that Object Gateway and NFS Ganesha both receive the credentials
     of all users listed in the <code class="literal">rgw</code> section of the
     <code class="filename">rgw.sls</code>.
    </p><p>
     The exported NFS uses these user names on the first level of the file
     system, in this example the paths <code class="filename">/demo</code> and
     <code class="filename">/demo1</code> would be exported.
    </p></li><li class="step"><p>
     Execute at least stages 2 and 4 of DeepSea. Running stage 3 in between
     is recommended.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.3 # optional but recommended
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.4</pre></div></li><li class="step"><p>
     Verify that NFS Ganesha is working by mounting the NFS share from a client
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -o sync -t nfs <em class="replaceable">GANESHA_NODE</em>:/ /mnt
<code class="prompt user">root # </code><code class="command">ls</code> /mnt
cephfs  demo  demo1</pre></div><p>
     <code class="filename">/mnt</code> should contain all exported paths. Directories
     for CephFS and both Object Gateway users should exist. For each bucket a user
     owns, a path
     <code class="filename">/mnt/<em class="replaceable">USERNAME</em>/<em class="replaceable">BUCKETNAME</em></code>
     would be exported.
    </p></li></ol></div></div></section><section class="sect1" id="sec-as-ganesha-ha-ap" data-id-title="High Availability Active-Passive Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.3 </span><span class="title-name">High Availability Active-Passive Configuration</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-ha-ap">#</a></h2></div></div></div><p>
   This section provides an example of how to set up a two-node active-passive
   configuration of NFS Ganesha servers. The setup requires the SUSE Linux Enterprise High Availability Extension. The
   two nodes are called <code class="systemitem">earth</code> and <code class="systemitem">mars</code>.
  </p><p>
   For details about SUSE Linux Enterprise High Availability Extension, see
   <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
  </p><section class="sect2" id="sec-as-ganesha-ha-ap-basic" data-id-title="Basic Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.1 </span><span class="title-name">Basic Installation</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-ha-ap-basic">#</a></h3></div></div></div><p>
    In this setup <code class="systemitem">earth</code> has the IP address
    <code class="systemitem">192.168.1.1</code> and <code class="systemitem">mars</code> has
    the address <code class="systemitem">192.168.1.2</code>.
   </p><p>
    Additionally, two floating virtual IP addresses are used, allowing clients
    to connect to the service independent of which physical node it is running
    on. <code class="systemitem">192.168.1.10</code> is used for
    cluster administration with Hawk2 and
    <code class="systemitem">192.168.2.1</code> is used exclusively
    for the NFS exports. This makes it easier to apply security restrictions
    later.
   </p><p>
    The following procedure describes the example installation. More details
    can be found at
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/</a>.
   </p><div class="procedure" id="proc-as-ganesha-ha-ap"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Prepare the NFS Ganesha nodes on the Salt master:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Run DeepSea stages 0 and 1.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.0
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.1</pre></div></li><li class="step"><p>
        Assign the nodes <code class="systemitem">earth</code> and <code class="systemitem">mars</code> the
        <code class="literal">role-ganesha</code> in the
        <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
       </p><div class="verbatim-wrap"><pre class="screen">role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</pre></div></li><li class="step"><p>
        Run DeepSea stages 2 to 4.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.3
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.4</pre></div></li></ol></li><li class="step"><p>
      Register the SUSE Linux Enterprise High Availability Extension on <code class="systemitem">earth</code> and <code class="systemitem">mars</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div></li><li class="step"><p>
      Install <span class="package">ha-cluster-bootstrap</span> on both nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
        Initialize the cluster on <code class="systemitem">earth</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">ha-cluster-init</code></pre></div></li><li class="step"><p>
        Let <code class="systemitem">mars</code> join the cluster:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">ha-cluster-join</code> -c earth</pre></div></li></ol></li><li class="step"><p>
      Check the status of the cluster. You should see two nodes added to the
      cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status</pre></div></li><li class="step"><p>
      On both nodes, disable the automatic start of the NFS Ganesha service at
      boot time:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> disable nfs-ganesha</pre></div></li><li class="step"><p>
      Start the <code class="command">crm</code> shell on <code class="systemitem">earth</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> configure</pre></div><p>
      The next commands are executed in the crm shell.
     </p></li><li class="step"><p>
      On <code class="systemitem">earth</code>, run the crm shell to execute the following commands to
      configure the resource for NFS Ganesha daemons as clone of systemd resource
      type:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<code class="prompt user">crm(live)configure# </code>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<code class="prompt user">crm(live)configure# </code>commit
<code class="prompt user">crm(live)configure# </code>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</pre></div></li><li class="step"><p>
      Create a primitive IPAddr2 with the crm shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<code class="prompt user">crm(live)# </code>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</pre></div></li><li class="step"><p>
      To set up a relationship between the NFS Ganesha server and the floating
      Virtual IP, we use collocation and ordering.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<code class="prompt user">crm(live)configure# </code>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip</pre></div></li><li class="step"><p>
      Use the <code class="command">mount</code> command from the client to ensure that
      cluster setup is complete:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</pre></div></li></ol></div></div></section><section class="sect2" id="sec-as-ganesha-ha-ap-cleanup" data-id-title="Clean Up Resources"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.2 </span><span class="title-name">Clean Up Resources</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-ha-ap-cleanup">#</a></h3></div></div></div><p>
    In the event of an NFS Ganesha failure at one of the node, for example
    <code class="systemitem">earth</code>, fix the issue and clean up the resource. Only after the
    resource is cleaned up can the resource fail back to <code class="systemitem">earth</code> in case
    NFS Ganesha fails at <code class="systemitem">mars</code>.
   </p><p>
    To clean up the resource:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> resource cleanup nfs-ganesha-clone earth
<code class="prompt user">root@earth # </code><code class="command">crm</code> resource cleanup ganesha-ip earth</pre></div></section><section class="sect2" id="sec-as-ganesha-ha-ap-ping-resource" data-id-title="Setting Up Ping Resource"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.3 </span><span class="title-name">Setting Up Ping Resource</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-ha-ap-ping-resource">#</a></h3></div></div></div><p>
    It may happen that the server is unable to reach the client because of a
    network issue. A ping resource can detect and mitigate this problem.
    Configuring this resource is optional.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Define the ping resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<em class="replaceable">CLIENT1</em> <em class="replaceable">CLIENT2</em>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</pre></div><p>
      <code class="literal">host_list</code> is a list of IP addresses separated by space
      characters. The IP addresses will be pinged regularly to check for
      network outages. If a client must always have access to the NFS server,
      add it to <code class="literal">host_list</code>.
     </p></li><li class="step"><p>
      Create a clone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</pre></div></li><li class="step"><p>
      The following command creates a constraint for the NFS Ganesha service. It
      forces the service to move to another node when
      <code class="literal">host_list</code> is unreachable.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</pre></div></li></ol></div></div></section><section class="sect2" id="setup-portblock-resource" data-id-title="Setting Up PortBlock Resource"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.4 </span><span class="title-name">Setting Up PortBlock Resource</span> <a title="Permalink" class="permalink" href="#setup-portblock-resource">#</a></h3></div></div></div><p>When a service goes down, the TCP connection that is in use by
      NFS Ganesha is required to be closed otherwise it
      continues to run until a system-specific timeout occurs. This timeout
      can take upwards of 3 minutes.</p><p>To shorten the timeout time, the TCP connection needs to be reset.
      We recommend configuring <code class="literal">portblock</code> to reset stale TCP
      connections.</p><p>You can choose to use portblock with or without the
      <code class="literal">tickle_dir</code> parameters that could unblock and
      reconnect clients to the new service faster. We recommend to
      have <code class="literal">tickle_dir</code> as the shared CephFS mount
      between two HA nodes (where NFS Ganesha services are running).</p><div class="procedure"><div class="procedure-contents"><div id="id-1.4.5.6.7.7.5.1" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>Configuring the following resource is optional.</p></div><ol class="procedure" type="1"><li class="step"><p>On <code class="systemitem">earth</code>, run the crm shell to execute the following
            commands to configure the resource for NFS Ganesha daemons:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> configure</pre></div></li><li class="step"><p>Configure the <code class="literal">block</code> action for
          <code class="literal">portblock</code> and omit the <code class="literal">tickle_dir</code>
          option if you have not configured a shared directory:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code> primitive nfs-ganesha-block ocf:portblock \
protocol=tcp portno=2049 action=block ip=192.168.2.1 op monitor depth="0" timeout="10" interval="10" tickle_dir="/tmp/ganesha/tickle/"</pre></div></li><li class="step"><p>Configure the <code class="literal">unblock</code> action for
          <code class="literal">portblock</code> and omit the <code class="literal">reset_local_on_unblock_stop</code>
          option if you have not configured a shared directory:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code> primitive nfs-ganesha-unblock ocf:portblock \
protocol=tcp portno=2049 action=unblock ip=192.168.2.1 op monitor depth="0" timeout="10" interval="10" reset_local_on_unblock_stop=true tickle_dir="/tmp/ganesha/tickle/"</pre></div></li><li class="step"><p>Configure the <code class="literal">IPAddr2</code> resource with <code class="literal">portblock</code>:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code> colocation ganesha-portblock inf: ganesha-ip nfs-ganesha-block nfs-ganesha-unblock
<code class="prompt user">crm(live)configure# </code> edit ganesha-ip-after-nfs-ganesha-server
order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-block nfs-ganesha-clone ganesha-ip nfs-ganesha-unblock</pre></div></li><li class="step"><p>Save your changes:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code> commit</pre></div></li><li class="step"><p>Your configuration should look like this:</p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code> show</pre></div><div class="verbatim-wrap"><pre class="screen">"
node 1084782956: nfs1
node 1084783048: nfs2
primitive ganesha-ip IPaddr2 \
        params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
        op monitor interval=10 timeout=20
primitive nfs-ganesha-block portblock \
        params protocol=tcp portno=2049 action=block ip=192.168.2.1 \
        tickle_dir="/tmp/ganesha/tickle/" op monitor timeout=10 interval=10 depth=0
primitive nfs-ganesha-server systemd:nfs-ganesha \
        op monitor interval=30s
primitive nfs-ganesha-unblock portblock \
        params protocol=tcp portno=2049 action=unblock ip=192.168.2.1 \
        reset_local_on_unblock_stop=true tickle_dir="/tmp/ganesha/tickle/" \
        op monitor timeout=10 interval=10 depth=0
clone nfs-ganesha-clone nfs-ganesha-server \
        meta interleave=true
location cli-prefer-ganesha-ip ganesha-ip role=Started inf: nfs1
order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-block nfs-ganesha-clone ganesha-ip nfs-ganesha-unblock
colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
colocation ganesha-portblock inf: ganesha-ip nfs-ganesha-block nfs-ganesha-unblock
property cib-bootstrap-options: \
        have-watchdog=false \
        dc-version=1.1.16-6.5.1-77ea74d \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=false \
        placement-strategy=balanced \
        last-lrm-refresh=1544793779
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true
"</pre></div><p>In this example <code class="filename">/tmp/ganesha/</code> is the CephFS
        mount on both nodes (nfs1 and nfs2):</p><div class="verbatim-wrap"><pre class="screen">172.16.1.11:6789:/ganesha on /tmp/ganesha type ceph (rw,relatime,name=admin,secret=...hidden...,acl,wsize=16777216)</pre></div><p>The <code class="literal">tickle</code> directory has been initially
        created.</p></li></ol></div></div></section><section class="sect2" id="ganesha-ha-deepsea" data-id-title="NFS Ganesha HA and DeepSea"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.5 </span><span class="title-name">NFS Ganesha HA and DeepSea</span> <a title="Permalink" class="permalink" href="#ganesha-ha-deepsea">#</a></h3></div></div></div><p>
    DeepSea does not support configuring NFS Ganesha HA. To prevent DeepSea
    from failing after NFS Ganesha HA was configured, exclude starting and
    stopping the NFS Ganesha service from DeepSea Stage 4:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Copy <code class="filename">/srv/salt/ceph/ganesha/default.sls</code> to
      <code class="filename">/srv/salt/ceph/ganesha/ha.sls</code>.
     </p></li><li class="step"><p>
      Remove the <code class="literal">.service</code> entry from
      <code class="filename">/srv/salt/ceph/ganesha/ha.sls</code> so that it looks as
      follows:
     </p><div class="verbatim-wrap"><pre class="screen">include:
- .keyring
- .install
- .configure</pre></div></li><li class="step"><p>
      Add the following line to
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">ganesha_init: ha</pre></div></li></ol></div></div><p>
     To prevent DeepSea from restarting NFS Ganesha service on stage 4:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
         Copy <code class="filename">/srv/salt/ceph/stage/ganesha/default.sls</code> to
         <code class="filename">/srv/salt/ceph/stage/ganesha/ha.sls</code>.
       </p></li><li class="step"><p>
         Remove the line <code class="literal">  - ...restart.ganesha.lax</code> from the
         <code class="filename">/srv/salt/ceph/stage/ganesha/ha.sls</code> so it looks as follows:
       </p><div class="verbatim-wrap"><pre class="screen">include:
  - .migrate
  - .core</pre></div></li><li class="step"><p>
         Add the following line to <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
       </p><div class="verbatim-wrap"><pre class="screen">stage_ganesha: ha</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-as-ganesha-info" data-id-title="More Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.4 </span><span class="title-name">More Information</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-info">#</a></h2></div></div></div><p>
   More information can be found in <span class="intraxref">Book “Administration Guide”, Chapter 16 “NFS Ganesha: Export Ceph Data via NFS”</span>.
  </p></section></section><section class="chapter" id="cha-ses-cifs" data-id-title="Exporting Ceph Data via Samba"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13 </span><span class="title-name">Exporting Ceph Data via Samba</span> <a title="Permalink" class="permalink" href="#cha-ses-cifs">#</a></h2></div></div></div><p>
  This chapter describes how to export data stored in a Ceph cluster via a
  Samba/CIFS share so that you can easily access them from Windows* client
  machines. It also includes information that will help you configure a Ceph
  Samba gateway to join Active Directory in the Windows* domain to authenticate and
  authorize users.
 </p><div id="id-1.4.5.7.4" data-id-title="Samba Gateway Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Samba Gateway Performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing CephFS
   via a Samba Gateway may significantly reduce application performance when
   compared to native Ceph clients.
  </p></div><section class="sect1" id="cephfs-samba" data-id-title="Export CephFS via Samba Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">Export CephFS via Samba Share</span> <a title="Permalink" class="permalink" href="#cephfs-samba">#</a></h2></div></div></div><div id="id-1.4.5.7.5.2" data-id-title="Cross Protocol Access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Cross Protocol Access</h6><p>
    Native CephFS and NFS clients are not restricted by file locks obtained
    via Samba, and vice versa. Applications that rely on cross protocol file
    locking may experience data corruption if CephFS backed Samba share paths
    are accessed via other means.
   </p></div><section class="sect2" id="cephfs-samba-packages" data-id-title="Samba Related Packages Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.1 </span><span class="title-name">Samba Related Packages Installation</span> <a title="Permalink" class="permalink" href="#cephfs-samba-packages">#</a></h3></div></div></div><p>
    To configure and export a Samba share, the following packages need to be
    installed: <span class="package">samba-ceph</span> and
    <span class="package">samba-winbind</span>. If these packages are not installed,
    install them:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>zypper install samba-ceph samba-winbind</pre></div></section><section class="sect2" id="sec-ses-cifs-example" data-id-title="Single Gateway Example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.2 </span><span class="title-name">Single Gateway Example</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-example">#</a></h3></div></div></div><p>
    In preparation for exporting a Samba share, choose an appropriate node to
    act as a Samba Gateway. The node needs to have access to the Ceph client network,
    as well as sufficient CPU, memory, and networking resources.
   </p><p>
    Failover functionality can be provided with CTDB and the SUSE Linux Enterprise High Availability Extension.
    Refer to <a class="xref" href="#sec-ses-cifs-ha" title="13.1.3. High Availability Configuration">Section 13.1.3, “High Availability Configuration”</a> for more information on HA
    setup.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make sure that a working CephFS already exists in your cluster. For
      details, see <a class="xref" href="#cha-ceph-as-cephfs" title="Chapter 11. Installation of CephFS">Chapter 11, <em>Installation of CephFS</em></a>.
     </p></li><li class="step"><p>
      Create a Samba Gateway specific keyring on the Ceph admin node and copy it to
      both Samba Gateway nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephadm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <em class="replaceable">SAMBA_NODE</em>:/etc/ceph/</pre></div><p>
      Replace <em class="replaceable">SAMBA_NODE</em> with the name of the Samba
      gateway node.
     </p></li><li class="step"><p>
      The following steps are executed on the Samba Gateway node. Install Samba
      together with the Ceph integration package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>sudo zypper in samba samba-ceph</pre></div></li><li class="step"><p>
      Replace the default contents of the
      <code class="filename">/etc/samba/smb.conf</code> file with the following:
     </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<em class="replaceable">SHARE_NAME</em>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><div id="id-1.4.5.7.5.4.4.4.3" data-id-title="Oplocks and Share Modes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Oplocks and Share Modes</h6><p>
       <code class="option">oplocks</code> (also known as SMB2+ leases) allow for improved
       performance through aggressive client caching, but are currently unsafe
       when Samba is deployed together with other CephFS clients, such as
       kernel <code class="literal">mount.ceph</code>, FUSE, or NFS Ganesha.
      </p><p>
       Currently <code class="option">kernel share modes</code> needs to be disabled in a
       share running with the CephFS vfs module for file serving to work
       properly.
      </p></div><div id="id-1.4.5.7.5.4.4.4.4" data-id-title="Permitting Access" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Permitting Access</h6><p>
       Since <code class="systemitem">vfs_ceph</code> does not require a file system
       mount, the share path is interpreted as an absolute path within the
       Ceph file system on the attached Ceph cluster. For successful share
       I/O, the path's access control list (ACL) needs to permit access from
       the mapped user for the given Samba client. You can modify the ACL by
       temporarily mounting via the CephFS kernel client and using the
       <code class="command">chmod</code>, <code class="command">chown</code> or
       <code class="command">setfacl</code> utilities against the share path. For
       example, to permit access for all users, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod 777 <em class="replaceable">MOUNTED_SHARE_PATH</em></pre></div></div></li><li class="step"><p>
      Start and enable the Samba daemon:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>sudo systemctl start smb.service
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl enable smb.service
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl start nmb.service
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl enable nmb.service</pre></div></li></ol></div></div></section><section class="sect2" id="sec-ses-cifs-ha" data-id-title="High Availability Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.3 </span><span class="title-name">High Availability Configuration</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-ha">#</a></h3></div></div></div><div id="id-1.4.5.7.5.5.2" data-id-title="Transparent Failover Not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Transparent Failover Not Supported</h6><p>
     Although a multi-node Samba + CTDB deployment is more highly available
     compared to the single node (see <a class="xref" href="#cha-ses-cifs" title="Chapter 13. Exporting Ceph Data via Samba">Chapter 13, <em>Exporting Ceph Data via Samba</em></a>),
     client-side transparent failover is not supported. Applications will
     likely experience a short outage on Samba Gateway node failure.
    </p></div><p>
    This section provides an example of how to set up a two-node high
    availability configuration of Samba servers. The setup requires the SUSE Linux Enterprise
    High Availability Extension. The two nodes are called <code class="systemitem">earth</code>
    (<code class="systemitem">192.168.1.1</code>) and <code class="systemitem">mars</code>
    (<code class="systemitem">192.168.1.2</code>).
   </p><p>
    For details about SUSE Linux Enterprise High Availability Extension, see
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/</a>.
   </p><p>
    Additionally, two floating virtual IP addresses allow clients to connect to
    the service no matter which physical node it is running on.
    <code class="systemitem">192.168.1.10</code> is used for cluster
    administration with Hawk2 and
    <code class="systemitem">192.168.2.1</code> is used exclusively
    for the CIFS exports. This makes it easier to apply security restrictions
    later.
   </p><p>
    The following procedure describes the example installation. More details
    can be found at
    <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/</a>.
   </p><div class="procedure" id="proc-sec-ses-cifs-ha"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a Samba Gateway specific keyring on the Admin Node and copy it to both nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephadm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">earth</code>:/etc/ceph/
<code class="prompt user">cephadm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">mars</code>:/etc/ceph/</pre></div></li><li class="step"><p>
      SLE-HA setup requires a fencing device to avoid a <span class="emphasis"><em>split
      brain</em></span> situation when active cluster nodes become
      unsynchronized. For this purpose, you can use a Ceph RBD image with
      Stonith Block Device (SBD). Refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-storage-protect-fencing-setup" target="_blank">https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-storage-protect-fencing-setup</a>
      for more details.
     </p><p>
      If it does not yet exist, create an RBD pool called
      <code class="literal">rbd</code> (see
      <span class="intraxref">Book “Administration Guide”, Chapter 8 “Managing Storage Pools”, Section 8.2.2 “Create a Pool”</span>) and associate it
      with <code class="literal">rbd</code> (see
      <span class="intraxref">Book “Administration Guide”, Chapter 8 “Managing Storage Pools”, Section 8.1 “Associate Pools with an Application”</span>). Then create a related
      RBD image called <code class="literal">sbd01</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create rbd <em class="replaceable">PG_NUM</em> <em class="replaceable">PGP_NUM</em> replicated
<code class="prompt user">cephadm &gt; </code>ceph osd pool application enable rbd rbd
<code class="prompt user">cephadm &gt; </code>rbd -p rbd create sbd01 --size 64M --image-shared</pre></div></li><li class="step"><p>
      Prepare <code class="systemitem">earth</code> and <code class="systemitem">mars</code> to host the Samba service:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Make sure the following packages are installed before you proceed:
        <span class="package">ctdb</span>, <span class="package">tdb-tools</span>, and
        <span class="package">samba</span> (needed for
        <code class="systemitem">smb.service</code> and
        <code class="systemitem">nmb.service</code>).
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code><code class="command">zypper</code> in ctdb tdb-tools samba samba-ceph</pre></div></li><li class="step"><p>
        Make sure the services <code class="literal">ctdb</code>, <code class="literal">smb</code>,
        and <code class="literal">nmb</code> are stopped and disabled:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>sudo systemctl disable ctdb
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl disable smb
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl disable nmb
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl stop smb
<code class="prompt user">cephadm@smb &gt; </code>sudo systemctl stop nmb</pre></div></li><li class="step"><p>
        Open port <code class="literal">4379</code> of your firewall on all nodes. This
        is needed for CTDB to communicate with other cluster nodes.
       </p></li></ol></li><li class="step"><p>
      On <code class="systemitem">earth</code>, create the configuration files for Samba. They will later
      automatically synchronize to <code class="systemitem">mars</code>.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Insert a list of private IP addresses of Samba Gateway nodes in the
        <code class="filename">/etc/ctdb/nodes</code> file. Find more details in the
        ctdb manual page (<code class="command">man 7 ctdb</code>).
       </p><div class="verbatim-wrap"><pre class="screen">192.168.1.1
192.168.1.2</pre></div></li><li class="step"><p>
        Configure Samba. Add the following lines in the
        <code class="literal">[global]</code> section of
        <code class="filename">/etc/samba/smb.conf</code>. Use the host name of your
        choice in place of <em class="replaceable">CTDB-SERVER</em> (all nodes in
        the cluster will appear as one big node with this name). Add a share
        definition as well, consider <em class="replaceable">SHARE_NAME</em> as
        an example:
       </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><p>
        Note that the <code class="filename">/etc/ctdb/nodes</code> and
        <code class="filename">/etc/samba/smb.conf</code> files need to match on all
        Samba Gateway nodes.
       </p></li></ol></li><li class="step"><p>
      Install and bootstrap the SUSE Linux Enterprise High Availability cluster.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Register the SUSE Linux Enterprise High Availability Extension on <code class="systemitem">earth</code> and <code class="systemitem">mars</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div></li><li class="step"><p>
        Install <span class="package">ha-cluster-bootstrap</span> on both nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div></li><li class="step"><p>
        Map the RBD image <code class="literal">sbd01</code> on both Samba Gateways via
        <code class="systemitem">rbdmap.service</code>.
       </p><p>
        Edit <code class="filename">/etc/ceph/rbdmap</code> and add an entry for the SBD
        image:
       </p><div class="verbatim-wrap"><pre class="screen">rbd/sbd01 id=samba.gw,keyring=/etc/ceph/ceph.client.samba.gw.keyring</pre></div><p>
        Enable and start
        <code class="systemitem">rbdmap.service</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
<code class="prompt user">root@mars # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service</pre></div><p>
        The <code class="filename">/dev/rbd/rbd/sbd01</code> device should be available
        on both Samba Gateways.
       </p></li><li class="step"><p>
        Initialize the cluster on <code class="systemitem">earth</code> and let <code class="systemitem">mars</code> join it.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">ha-cluster-init</code></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">ha-cluster-join</code> -c earth</pre></div><div id="id-1.4.5.7.5.5.7.5.2.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
         During the process of initialization and joining the cluster, you will
         be interactively asked whether to use SBD. Confirm with
         <code class="option">y</code> and then specify
         <code class="filename">/dev/rbd/rbd/sbd01</code> as a path to the storage
         device.
        </p></div></li></ol></li><li class="step"><p>
      Check the status of the cluster. You should see two nodes added in the
      cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</pre></div></li><li class="step"><p>
      Execute the following commands on <code class="systemitem">earth</code> to configure the CTDB
      resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> configure
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">group</code> g-ctdb ctdb nmb smb
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ctdb g-ctdb meta interleave="true"
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
      The binary
      <code class="command">/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</code> in the
      configuration option <code class="literal">ctdb_recovery_lock</code> has the
      parameters <em class="replaceable">CLUSTER_NAME</em>,
      <em class="replaceable">CEPHX_USER</em>,
      <em class="replaceable">RADOS_POOL</em>, and
      <em class="replaceable">RADOS_OBJECT</em>, in this order.
     </p><p>
      An extra lock-timeout parameter can be appended to override the default
      value used (10 seconds). A higher value will increase the CTDB recovery
      master failover time, whereas a lower value may result in the recovery
      master being incorrectly detected as down, triggering flapping failovers.
     </p></li><li class="step"><p>
      Add a clustered IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<code class="prompt user">crm(live)configure# </code><code class="command">colocation</code> col-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">order</code> o-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
      If <code class="literal">unique_clone_address</code> is set to
      <code class="literal">true</code>, the IPaddr2 resource agent adds a clone ID to
      the specified address, leading to three different IP addresses. These are
      usually not needed, but help with load balancing. For further information
      about this topic, see
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-lb" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-lb</a>.
     </p></li><li class="step"><p>
      Check the result:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</pre></div></li><li class="step"><p>
      Test from a client machine. On a Linux client, run the following command
      to see if you can copy files from and to the system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">smbclient</code> <code class="option">//192.168.2.1/myshare</code></pre></div></li></ol></div></div></section></section></section></div><section class="appendix" id="ap-deploy-docupdate" data-id-title="Documentation Updates"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Documentation Updates</span> <a title="Permalink" class="permalink" href="#ap-deploy-docupdate">#</a></h1></div></div></div><p>
  This chapter lists content changes for this document since the initial
  release of SUSE Enterprise Storage 4. You can find changes related to the cluster
  deployment that apply to previous versions in
  <a class="link" href="https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ap-deploy-docupdate" target="_blank">https://documentation.suse.com/ses/5.5/single-html/ses-deployment/#ap-deploy-docupdate</a>.
 </p><p>
  The document was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-5maint3" title="A.1. The Latest Documentation Update">Section A.1, “The Latest Documentation Update”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-5-5" title="A.2. October, 2018 (Documentation Maintenance Update)">Section A.2, “October, 2018 (Documentation Maintenance Update)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-5maint1" title="A.3. November, 2017 (Maintenance Update)">Section A.3, “November, 2017 (Maintenance Update)”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-5" title="A.4. October, 2017 (Release of SUSE Enterprise Storage 5)">Section A.4, “October, 2017 (Release of SUSE Enterprise Storage 5)”</a>
   </p></li></ul></div><section class="sect1" id="sec-depl-docupdates-5maint3" data-id-title="The Latest Documentation Update"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">A.1 </span><span class="title-name">The Latest Documentation Update</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-5maint3">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.4.6.6.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added an important box stating that encrypted OSDs boot longer than
     unencrypted in <a class="xref" href="#ds-profile-osd-encrypted" title="4.5.1.6. Deploying Encrypted OSDs">Section 4.5.1.6, “Deploying Encrypted OSDs”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1124813" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1124813</a>).
    </p></li><li class="listitem"><p>
     Added a step to disable AppArmor profiles in <a class="xref" href="#u4to5-softreq" title="Important: Software Requirements">Important: Software Requirements</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1127297" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1127297</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#iscsi-lrbd-autentication" title="10.4.4. Authentication and Access Control">Section 10.4.4, “Authentication and Access Control”</a> and extended
     <a class="xref" href="#ceph-iscsi-rbd-advanced" title="10.4.6. Advanced Settings">Section 10.4.6, “Advanced Settings”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1114705" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1114705</a>).
    </p></li><li class="listitem"><p>
     Added DeepSea stage 2 to the deployment process in
     <a class="xref" href="#proc-as-ganesha-ha-ap">Procedure 12.0, “”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1119167" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1119167</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#sec-privileges-and-prompts" title="1.3. User Privileges and Command Prompts">Section 1.3, “User Privileges and Command Prompts”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1116537" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1116537</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-maintenance-upgrade-details" title="5.4.2. Details on the salt target ceph.maintenance.upgrade Command">Section 5.4.2, “Details on the <code class="command">salt <em class="replaceable">target</em> ceph.maintenance.upgrade</code> Command”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1104794" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1104794</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-depl-docupdates-5-5" data-id-title="October, 2018 (Documentation Maintenance Update)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">A.2 </span><span class="title-name">October, 2018 (Documentation Maintenance Update)</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-5-5">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.4.6.7.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#cha-admin-ha" title="Chapter 3. Ceph Admin Node HA Setup">Chapter 3, <em>Ceph Admin Node HA Setup</em></a> (Fate#325622).
    </p></li><li class="listitem"><p>
     Encrypted OSDs during deployment and upgrade in
     <a class="xref" href="#ds-profile-osd-encrypted" title="4.5.1.6. Deploying Encrypted OSDs">Section 4.5.1.6, “Deploying Encrypted OSDs”</a> and
     <a class="xref" href="#ds-migrate-osd-encrypted" title="5.3. Encrypting OSDs during Upgrade">Section 5.3, “Encrypting OSDs during Upgrade”</a> (Fate#321665).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.4.6.7.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Cleaned the update repository sections in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1109377" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1109377</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#stage1-override-devices" title="4.5.1.5. Overriding Default Search for Disk Devices">Section 4.5.1.5, “Overriding Default Search for Disk Devices”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1105967" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1105967</a>).
    </p></li><li class="listitem"><p>
     Prepended DeepSea Stage 0 execution in
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1110440" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1110440</a>).
    </p></li><li class="listitem"><p>
     Fixed creation of demo users in
     <a class="xref" href="#sec-as-ganesha-basic-example" title="12.2. Example Installation">Section 12.2, “Example Installation”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1105739" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1105739</a>).
    </p></li><li class="listitem"><p>
     Removed the default <code class="option">stage_prep_master</code> and
     <code class="option">stage_prep_minion</code> values in
     <a class="xref" href="#ds-disable-reboots" title="7.1.5. Updates and Reboots during Stage 0">Section 7.1.5, “Updates and Reboots during Stage 0”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1103242" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1103242</a>).
    </p></li><li class="listitem"><p>
     Updated various parts of <a class="xref" href="#cha-ses-cifs" title="Chapter 13. Exporting Ceph Data via Samba">Chapter 13, <em>Exporting Ceph Data via Samba</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1101478" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1101478</a>).
    </p></li><li class="listitem"><p>
     Added information on preconfiguring network settings by a custom
     <code class="filename">cluster.yml</code> in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1099448" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1099448</a>).
    </p></li><li class="listitem"><p>
     Removed spare <code class="literal">role-igw</code> definitions
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1099687" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1099687</a>).
    </p></li><li class="listitem"><p>
     Added a tip on running a second terminal session for the monitor mode in
     <a class="xref" href="#deepsea-cli-monitor" title="4.4.1. DeepSea CLI: Monitor Mode">Section 4.4.1, “DeepSea CLI: Monitor Mode”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1099453" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1099453</a>).
    </p></li><li class="listitem"><p>
     Non-data Object Gateway pools need to be replicated in
     <a class="xref" href="#ogw-pool-create" title="9.1.1.2. Create Pools (Optional)">Section 9.1.1.2, “Create Pools (Optional)”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1095743" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1095743</a>).
    </p></li><li class="listitem"><p>
     FQDN of all nodes must be resolvable to the public network IP. See
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067113" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067113</a>).
    </p></li><li class="listitem"><p>
     Added a tip on sharing multiple roles in
     <a class="xref" href="#ceph-install-saltstack" title="Chapter 4. Deploying with DeepSea/Salt">Chapter 4, <em>Deploying with DeepSea/Salt</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1093824" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1093824</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#sysreq-mds" title="2.4. Metadata Server Nodes">Section 2.4, “Metadata Server Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047230" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047230</a>).
    </p></li><li class="listitem"><p>
     Manually edit <code class="filename">policy.cfg</code> for openATTIC
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073331" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073331</a>).
    </p></li><li class="listitem"><p>
     Recommended <span class="guimenu">/var/lib/ceph</span> on SSD in
     <a class="xref" href="#sysreq-mon" title="2.2. Monitor Nodes">Section 2.2, “Monitor Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1056322" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1056322</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#about-bluestore" title="1.5. BlueStore">Section 1.5, “BlueStore”</a> and
     <a class="xref" href="#rec-waldb-size" title="2.1.3. Recommended Size for the BlueStore's WAL and DB Device">Section 2.1.3, “Recommended Size for the BlueStore's WAL and DB Device”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1072502" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1072502</a>).
    </p></li><li class="listitem"><p>
     Extended the deployment of encrypted OSDs in
     <a class="xref" href="#ds-profile-osd-encrypted" title="4.5.1.6. Deploying Encrypted OSDs">Section 4.5.1.6, “Deploying Encrypted OSDs”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1093003" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1093003</a>).
    </p></li><li class="listitem"><p>
     Increased the number of bytes to erase to 4M in
     <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1093331" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1093331</a>).
    </p></li><li class="listitem"><p>
     Firewall breaks DeepSea stages, in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1090683" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1090683</a>).
    </p></li><li class="listitem"><p>
     Added the list of repositories in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1088170" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1088170</a>).
    </p></li><li class="listitem"><p>
     Added instructions to manually add repositories using
     <code class="command">zypper</code> <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>,
     <a class="xref" href="#ceph-upgrade-4to5cephdeloy" title="5.5. Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5">Section 5.5, “Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5”</a>, and
     <a class="xref" href="#ceph-upgrade-4to5crowbar" title="5.6. Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5">Section 5.6, “Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073308" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073308</a>).
    </p></li><li class="listitem"><p>
     Added a list of upgrade repositories + brief explanation of the
     DeepSea's <code class="option">upgrade_init</code> option in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073372" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073372</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-disable-reboots" title="7.1.5. Updates and Reboots during Stage 0">Section 7.1.5, “Updates and Reboots during Stage 0”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1081524" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1081524</a>).
    </p></li><li class="listitem"><p>
     Fixed prompts in <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1084307" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1084307</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#req-ses-other" title="2.12. SUSE Enterprise Storage 5.5 and Other SUSE Products">Section 2.12, “SUSE Enterprise Storage 5.5 and Other SUSE Products”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1089717" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1089717</a>).
    </p></li><li class="listitem"><p>
     Added a note on the Object Gateway configuration sections in
     <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1089300" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1089300</a>).
    </p></li><li class="listitem"><p>
     Added WAL/DB snippets to <a class="xref" href="#ses-bp-mindisk" title="2.1.2. Minimum Disk Size">Section 2.1.2, “Minimum Disk Size”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1057797" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1057797</a>).
    </p></li><li class="listitem"><p>
     MONs' public addresses are calculated dynamically
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1089151" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1089151</a>).
    </p></li><li class="listitem"><p>
     Fixed keyrings location in <a class="xref" href="#ceph-upgrade-4to5cephdeloy" title="5.5. Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5">Section 5.5, “Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073368" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073368</a>).
    </p></li><li class="listitem"><p>
     Provided several helper snippets in
     <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1061629" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1061629</a>).
    </p></li><li class="listitem"><p>
     Engulfing custom <code class="filename">ceph.conf</code> in
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1085443" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1085443</a>).
    </p></li><li class="listitem"><p>
     Updated RAM value recommended for BlueStore deployment in
     <a class="xref" href="#sysreq-osd" title="2.1.1. Minimum Requirements">Section 2.1.1, “Minimum Requirements”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1076385)" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1076385)</a>).
    </p></li><li class="listitem"><p>
     Added manual steps to upgrade the iSCSI Gateways after the engulf command in
     <a class="xref" href="#ceph-upgrade-4to5cephdeloy" title="5.5. Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5">Section 5.5, “Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5”</a> and
     <a class="xref" href="#ceph-upgrade-4to5crowbar" title="5.6. Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5">Section 5.6, “Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073327)" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073327)</a>).
    </p></li><li class="listitem"><p>
     The iSCSI Gateway deployment updated to the DeepSea way in
     <a class="xref" href="#ceph-iscsi-install" title="10.4. Installation and Configuration">Section 10.4, “Installation and Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073327" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073327</a>).
    </p></li><li class="listitem"><p>
     CephFS quotas are not supported. See
     <a class="xref" href="#ceph-cephfs-limitations" title="11.1. Supported CephFS Scenarios and Guidance">Section 11.1, “Supported CephFS Scenarios and Guidance”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077269" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077269</a>).
     CephFS quotas are not supported. See
     <a class="xref" href="#ceph-cephfs-limitations" title="11.1. Supported CephFS Scenarios and Guidance">Section 11.1, “Supported CephFS Scenarios and Guidance”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1077269)" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1077269)</a>)
    </p></li><li class="listitem"><p>
     Include partitions with higher numbers than 9 in zeroing step, see
     <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050230" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050230</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-depl-docupdates-5maint1" data-id-title="November, 2017 (Maintenance Update)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">A.3 </span><span class="title-name">November, 2017 (Maintenance Update)</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-5maint1">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.4.6.8.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#ceph-cephfs-multimds" title="11.3.2. MDS Cluster Size">Section 11.3.2, “MDS Cluster Size”</a> and
     <a class="xref" href="#ceph-cephfs-multimds-updates" title="11.3.3. MDS Cluster and Updates">Section 11.3.3, “MDS Cluster and Updates”</a>.
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.4.6.8.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Enhanced the disk wiping strategy in <a class="xref" href="#deploy-wiping-disk" title="Step 12">Step 12</a>
     of <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073897" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073897</a>).
    </p></li><li class="listitem"><p>
     Added tip on disengaging safety measures in
     <a class="xref" href="#filestore2bluestore" title="5.4.1. OSD Migration to BlueStore">Section 5.4.1, “OSD Migration to BlueStore”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073720" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073720</a>).
    </p></li><li class="listitem"><p>
     Referred to <a class="xref" href="#ds-minion-targeting-name" title="4.2.2.1. Matching the Minion Name">Section 4.2.2.1, “Matching the Minion Name”</a> in
     <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a> and <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.1 “Adding New Cluster Nodes”</span>
     to unify the information source
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073374" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073374</a>).
    </p></li><li class="listitem"><p>
     In <a class="xref" href="#ceph-upgrade-general" title="5.2. General Upgrade Procedure">Section 5.2, “General Upgrade Procedure”</a> only update Salt master and
     minion and not all packages. Therefore replace <code class="command">salt target
     state.apply ceph.updates</code> with <code class="command">salt target state.apply
     ceph.updates.salt</code>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073373" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073373</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-upgrade-4to5crowbar" title="5.6. Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5">Section 5.6, “Upgrade from SUSE Enterprise Storage 4 (Crowbar Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073317" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073317</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1073701" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1073701</a>).
    </p></li><li class="listitem"><p>
     Replaced '*' with <em class="replaceable">target</em> and extended the
     targeting introduction in <a class="xref" href="#ds-minion-targeting" title="4.2.2. Targeting the Minions">Section 4.2.2, “Targeting the Minions”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1068956" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1068956</a>).
    </p></li><li class="listitem"><p>
     Added verification of Salt minions' fingerprints in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064045" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064045</a>).
    </p></li><li class="listitem"><p>
     Removed advice to copy the example refactor.conf file in
     <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.9 “Automated Installation via Salt”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1065926" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1065926</a>).
    </p></li><li class="listitem"><p>
     Fixed path to network configuration YAML file in
     <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067730" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067730</a>).
    </p></li><li class="listitem"><p>
     Verify the cluster layout in <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067189" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067189</a>).
    </p></li><li class="listitem"><p>
     Added <code class="command">ceph osd set sortbitwise</code> to
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a> and
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067146" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067146</a>).
    </p></li><li class="listitem"><p>
     <code class="option">osd crush location</code> is gone,
     <code class="filename">ceph.conf</code> is customized differently in
     <a class="xref" href="#u4to5-softreq" title="Important: Software Requirements">Important: Software Requirements</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1067381" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1067381</a>).
    </p></li><li class="listitem"><p>
     Fixed 'role-admin' from 'role-master' in
     <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064056" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064056</a>).
    </p></li><li class="listitem"><p>
     Fixed path to <code class="filename">cluster.yml</code> in
     <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1066711" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1066711</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ganesha-ha-deepsea" title="12.3.5. NFS Ganesha HA and DeepSea">Section 12.3.5, “NFS Ganesha HA and DeepSea”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1058313" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1058313</a>).
    </p></li><li class="listitem"><p>
     Re-added <a class="xref" href="#storage-bp-net-subnets" title="2.7.2. Monitor Nodes on Different Subnets">Section 2.7.2, “Monitor Nodes on Different Subnets”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050115" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050115</a>).
    </p></li><li class="listitem"><p>
     The file <code class="filename">deepsea_minions.sls</code> must contain only one
     <code class="literal">deepsea_minions</code> entry. See
     <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1065403" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1065403</a>)
    </p></li><li class="listitem"><p>
     Changed order of steps in first procedure of
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064770" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064770</a>)
    </p></li><li class="listitem"><p>
     Clarified <a class="xref" href="#filestore2bluestore" title="5.4.1. OSD Migration to BlueStore">Section 5.4.1, “OSD Migration to BlueStore”</a>.
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1063250" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1063250</a>)
    </p></li></ul></div></section><section class="sect1" id="sec-depl-docupdates-5" data-id-title="October, 2017 (Release of SUSE Enterprise Storage 5)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">A.4 </span><span class="title-name">October, 2017 (Release of SUSE Enterprise Storage 5)</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-5">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.4.6.9.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#ceph-upgrade-3to5" title="5.7. Upgrade from SUSE Enterprise Storage 3 to 5">Section 5.7, “Upgrade from SUSE Enterprise Storage 3 to 5”</a> (Fate #323072).
    </p></li><li class="listitem"><p>
     Removed the obsolete <code class="command">Crowbar</code> installation tool in favor
     of DeepSea.
    </p></li><li class="listitem"><p>
     Removed the obsolete <code class="command">ceph-deploy</code> tool in favor of
     DeepSea.
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#storage-bp-hwreq" title="Chapter 2. Hardware Requirements and Recommendations">Chapter 2, <em>Hardware Requirements and Recommendations</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1029544" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1029544</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1042283" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1042283</a>).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#cha-as-ganesha" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036495" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036495</a>,
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1031444" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1031444</a>,
     FATE#322464).
    </p></li><li class="listitem"><p>
     DeepSea naming schema of profiles changed. See
     <a class="xref" href="#policy-profile-assignment" title="4.5.1.4. Profile Assignment">Section 4.5.1.4, “Profile Assignment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1046108" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1046108</a>).
    </p></li><li class="listitem"><p>
     CephFS can be used on EC pools, see
     <a class="xref" href="#ceph-cephfs-cephfs-create" title="11.3.1. Creating CephFS">Section 11.3.1, “Creating CephFS”</a> (FATE#321617).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.4.6.9.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#iscsi-tcmu" title="10.5. Exporting RADOS Block Device Images using tcmu-runner">Section 10.5, “Exporting RADOS Block Device Images using <code class="systemitem">tcmu-runner</code>”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064467" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064467</a>).
    </p></li><li class="listitem"><p>
     Improved the upgrade procedure to include the openATTIC role in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064621" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064621</a>).
    </p></li><li class="listitem"><p>
     Added a reference to <a class="xref" href="#ds-depl-stages" title="Running Deployment Stages">Procedure 4.1, “Running Deployment Stages”</a> in
     <a class="xref" href="#step-updatepillar" title="Step 4">Step 4</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1064276" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1064276</a>).
    </p></li><li class="listitem"><p>
     Modified the upgrade procedure in
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1061608" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1061608</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048959" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048959</a>).
    </p></li><li class="listitem"><p>
     Added <code class="filename">rgw.conf</code> in
     <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1062109" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1062109</a>).
    </p></li><li class="listitem"><p>
     Moved DeepSea installation to the very end in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1056292" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1056292</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-profile-osd-encrypted" title="4.5.1.6. Deploying Encrypted OSDs">Section 4.5.1.6, “Deploying Encrypted OSDs”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1061751" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1061751</a>).
    </p></li><li class="listitem"><p>
     Updated and simplified the upgrade procedure in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1059362" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1059362</a>).
    </p></li><li class="listitem"><p>
     Check for DeepSea version before upgrade in
     <a class="xref" href="#u4to5-softreq" title="Important: Software Requirements">Important: Software Requirements</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1059331" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1059331</a>).
    </p></li><li class="listitem"><p>
     Prefixing custom .sls files with <code class="filename">custom-</code> in
     <a class="xref" href="#using-customized-files" title="7.1. Using Customized Configuration Files">Section 7.1, “Using Customized Configuration Files”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048568" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048568</a>).
    </p></li><li class="listitem"><p>
     Added a note about key caps mismatch in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054186" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054186</a>).
    </p></li><li class="listitem"><p>
     Merged redundant list items in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1055140" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1055140</a>).
    </p></li><li class="listitem"><p>
     Added a note about the long time the cluster upgrade may take in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054079" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054079</a>).
    </p></li><li class="listitem"><p>
     Salt minions targeting with <code class="literal">deepsea_minions:</code> is mandatory
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054229" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054229</a>).
    </p></li><li class="listitem"><p>
     Inserted Stage 1 after engulfing in
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1054155" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1054155</a>).
    </p></li><li class="listitem"><p>
     Added <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.12 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052806" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052806</a>).
    </p></li><li class="listitem"><p>
     Added missing steps in <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052597" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052597</a>).
    </p></li><li class="listitem"><p>
     Fixed <code class="command">radosgw-admin</code> command syntax in
     <a class="xref" href="#sec-as-ganesha-basic-example" title="12.2. Example Installation">Section 12.2, “Example Installation”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052698" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052698</a>).
    </p></li><li class="listitem"><p>
     'salt' is not the required host name of the Salt master during the upgrade
     in <a class="xref" href="#upgrade4to5cephdeploy-all" title="Steps to Apply to All Cluster Nodes (including the Calamari Node)">Procedure 5.1, “Steps to Apply to All Cluster Nodes (including the Calamari Node)”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052907" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052907</a>).
    </p></li><li class="listitem"><p>
     Better wording and text flow in the 'important' section of
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052147" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052147</a>).
    </p></li><li class="listitem"><p>
     Added a note about manual role assignment during engulfment in
     <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050554" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050554</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#filestore2bluestore" title="5.4.1. OSD Migration to BlueStore">Section 5.4.1, “OSD Migration to BlueStore”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052210" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052210</a>).
    </p></li><li class="listitem"><p>
     Explained <code class="command">salt-run populate.engulf_existing_cluster</code> in
     detail in <a class="xref" href="#upgrade4to5cephdeploy-admin" title="Steps to Apply to the Salt master Node">Procedure 5.2, “Steps to Apply to the Salt master Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051258" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051258</a>).
    </p></li><li class="listitem"><p>
     Added openATTIC role in <a class="xref" href="#deepsea-example-policy-cfg" title="4.5.1.8. Example policy.cfg File">Section 4.5.1.8, “Example <code class="filename">policy.cfg</code> File”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1052076" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1052076</a>).
    </p></li><li class="listitem"><p>
     Fixed <code class="filename">profile-default</code> paths in
     <a class="xref" href="#deepsea-example-policy-cfg" title="4.5.1.8. Example policy.cfg File">Section 4.5.1.8, “Example <code class="filename">policy.cfg</code> File”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1051760" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1051760</a>).
    </p></li><li class="listitem"><p>
     Detached previous section into a new chapter
     <a class="xref" href="#ceph-deploy-ds-custom" title="Chapter 7. Customizing the Default Configuration">Chapter 7, <em>Customizing the Default Configuration</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050238" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050238</a>).
    </p></li><li class="listitem"><p>
     Referencing to <a class="xref" href="#storage-intro-core-nodes" title="1.2.3. Ceph Nodes and Daemons">Section 1.2.3, “Ceph Nodes and Daemons”</a> from
     <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> to keep the list of Ceph
     services up-to-date
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050221" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050221</a>).
    </p></li><li class="listitem"><p>
     Improved Salt master description and wording in
     <a class="xref" href="#ceph-install-saltstack" title="Chapter 4. Deploying with DeepSea/Salt">Chapter 4, <em>Deploying with DeepSea/Salt</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050214" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050214</a>).
    </p></li><li class="listitem"><p>
     Added optional node roles description in
     <a class="xref" href="#storage-intro-core-nodes" title="1.2.3. Ceph Nodes and Daemons">Section 1.2.3, “Ceph Nodes and Daemons”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1050085" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1050085</a>).
    </p></li><li class="listitem"><p>
     Updated the upgrade procedure in general
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048436" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048436</a>,
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048959" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048959</a>,
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=104i7085" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=104i7085</a>).
    </p></li><li class="listitem"><p>
     Added a new DeepSea role Ceph Manager
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1047472" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1047472</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-upgrade-4to5cephdeloy" title="5.5. Upgrade from SUSE Enterprise Storage 4 (ceph-deploy Deployment) to 5">Section 5.5, “Upgrade from SUSE Enterprise Storage 4 (<code class="command">ceph-deploy</code> Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1048436" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1048436</a>).
    </p></li><li class="listitem"><p>
     Made Stage 0 fully optional in <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1045845" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1045845</a>).
    </p></li><li class="listitem"><p>
     Updated the list of default pools in <a class="xref" href="#ses-rgw-config" title="9.1.1. Object Gateway Configuration">Section 9.1.1, “Object Gateway Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1034039" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1034039</a>).
    </p></li><li class="listitem"><p>
     Added an 'important' snippet about Object Gateway being deployed by DeepSea now
     in <a class="xref" href="#cha-ceph-additional-software-installation" title="Chapter 9. Ceph Object Gateway">Chapter 9, <em>Ceph Object Gateway</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1044928" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1044928</a>).
    </p></li><li class="listitem"><p>
     Fixed shell script in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1044684" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1044684</a>).
    </p></li><li class="listitem"><p>
     Added "Set <code class="option">require-osd-release luminous</code> Flag" to
     <a class="xref" href="#ceph-upgrade-general" title="5.2. General Upgrade Procedure">Section 5.2, “General Upgrade Procedure”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1040750" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1040750</a>).
    </p></li><li class="listitem"><p>
     Added annotation to the example <code class="filename">policy.cfg</code> in
     <a class="xref" href="#deepsea-example-policy-cfg" title="4.5.1.8. Example policy.cfg File">Section 4.5.1.8, “Example <code class="filename">policy.cfg</code> File”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1042691" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1042691</a>).
    </p></li><li class="listitem"><p>
     Improved commands for OSD disk zapping in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1042074" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1042074</a>).
    </p></li><li class="listitem"><p>
     Removed advice to install <code class="literal">salt-minion</code> on Salt master in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1041590" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1041590</a>).
    </p></li><li class="listitem"><p>
     Added firewall recommendation to <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1039344" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1039344</a>).
    </p></li><li class="listitem"><p>
     Removed XML-RPC references from openATTIC <code class="systemitem">systemd</code> command lines in
     <a class="xref" href="#using-customized-files" title="7.1. Using Customized Configuration Files">Section 7.1, “Using Customized Configuration Files”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1037371" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1037371</a>).
    </p></li><li class="listitem"><p>
     Fixed YAML syntax in <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1035498" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1035498</a>).
    </p></li><li class="listitem"><p>
     Added the 'ganesha' role explanation in
     <a class="xref" href="#policy-role-assignment" title="4.5.1.2. Role Assignment">Section 4.5.1.2, “Role Assignment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1037365" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1037365</a>).
    </p></li><li class="listitem"><p>
     Clarified and improved text flow in <a class="xref" href="#policy-configuration" title="4.5.1. The policy.cfg File">Section 4.5.1, “The <code class="filename">policy.cfg</code> File”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1037360" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1037360</a>).
    </p></li><li class="listitem"><p>
     Added the SUSE Enterprise Storage 4 to 5 upgrade procedure in
     <a class="xref" href="#ceph-upgrade-4to5" title="5.4. Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5">Section 5.4, “Upgrade from SUSE Enterprise Storage 4 (DeepSea Deployment) to 5”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036266" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036266</a>).
    </p></li><li class="listitem"><p>
     Replaced the term 'provisioning' with 'preparation' in
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036400" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036400</a>
     and
     <a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036492" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036492</a>).
    </p></li><li class="listitem"><p>
     Added warning about advanced techniques in
     <a class="xref" href="#deepsea-policy-filtering" title="4.5.1.7. Item Filtering">Section 4.5.1.7, “Item Filtering”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036278" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036278</a>).
    </p></li><li class="listitem"><p>
     Replaced redundant <code class="literal">role-admin</code> assignment in
     <a class="xref" href="#deepsea-example-policy-cfg" title="4.5.1.8. Example policy.cfg File">Section 4.5.1.8, “Example <code class="filename">policy.cfg</code> File”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036506" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036506</a>).
    </p></li><li class="listitem"><p>
     Improved <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> and
     <a class="xref" href="#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1036278" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1036278</a>).
    </p></li><li class="listitem"><p>
     Added deployment steps modifications in
     <a class="xref" href="#using-customized-files" title="7.1. Using Customized Configuration Files">Section 7.1, “Using Customized Configuration Files”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1026782" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1026782</a>).
    </p></li><li class="listitem"><p>
     Clarified and enhanced <a class="xref" href="#ceph-install-saltstack" title="Chapter 4. Deploying with DeepSea/Salt">Chapter 4, <em>Deploying with DeepSea/Salt</em></a> as
     suggested by
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1020920" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1020920</a>).
    </p></li><li class="listitem"><p>
     Recommended enabling custom openATTIC services in
     <a class="xref" href="#using-customized-files" title="7.1. Using Customized Configuration Files">Section 7.1, “Using Customized Configuration Files”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=989349" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=989349</a>).
    </p></li><li class="listitem"><p>
     Moved network recommendations to <a class="xref" href="#storage-bp-hwreq" title="Chapter 2. Hardware Requirements and Recommendations">Chapter 2, <em>Hardware Requirements and Recommendations</em></a> and
     included <a class="xref" href="#storage-bp-net-private" title="2.7.1. Adding a Private Network to a Running Cluster">Section 2.7.1, “Adding a Private Network to a Running Cluster”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1026569" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1026569</a>).
    </p></li></ul></div></section></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/book_storage_deployment.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>