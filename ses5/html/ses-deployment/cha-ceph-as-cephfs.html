<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Installation of CephFS | Deployment Guide | SUSE Enterprise Storage 5.5 (SES 5 &amp; SES 5.5)</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Installation of CephFS | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="description" content="The Ceph file system (CephFS) is a POSIX-compliant file system that uses a Ceph storage cluster to store its data. CephFS uses the same cluster system as Ceph …"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="chapter-title" content="Chapter 11. Installation of CephFS"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tbazant@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Enterprise Storage 5"/>
<meta property="og:title" content="Installation of CephFS | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta property="og:description" content="The Ceph file system (CephFS) is a POSIX-compliant file system that uses a Ceph storage cluster to store its data. CephFS uses the same cluster system as Ceph …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Installation of CephFS | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="twitter:description" content="The Ceph file system (CephFS) is a POSIX-compliant file system that uses a Ceph storage cluster to store its data. CephFS uses the same cluster system as Ceph …"/>
<link rel="prev" href="cha-ceph-as-iscsi.html" title="Chapter 10. Installation of iSCSI Gateway"/><link rel="next" href="cha-as-ganesha.html" title="Chapter 12. Installation of NFS Ganesha"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deployment Guide</a><span> / </span><a class="crumb" href="additional-software.html">Installation of Additional Services</a><span> / </span><a class="crumb" href="cha-ceph-as-cephfs.html">Installation of CephFS</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Deployment Guide</div><ol><li><a href="bk02pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-ses.html" class="has-children "><span class="title-number">I </span><span class="title-name">SUSE Enterprise Storage</span></a><ol><li><a href="cha-storage-about.html" class=" "><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 5.5 and Ceph</span></a></li><li><a href="storage-bp-hwreq.html" class=" "><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></li><li><a href="cha-admin-ha.html" class=" "><span class="title-number">3 </span><span class="title-name">Ceph Admin Node HA Setup</span></a></li></ol></li><li><a href="ses-deployment.html" class="has-children "><span class="title-number">II </span><span class="title-name">Cluster Deployment and Upgrade</span></a><ol><li><a href="ceph-install-saltstack.html" class=" "><span class="title-number">4 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></li><li><a href="cha-ceph-upgrade.html" class=" "><span class="title-number">5 </span><span class="title-name">Upgrading from Previous Releases</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">6 </span><span class="title-name">Backing Up the Cluster Configuration</span></a></li><li><a href="ceph-deploy-ds-custom.html" class=" "><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></li></ol></li><li class="active"><a href="additional-software.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Installation of Additional Services</span></a><ol><li><a href="cha-ceph-as-intro.html" class=" "><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></li><li><a href="cha-ceph-additional-software-installation.html" class=" "><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-as-iscsi.html" class=" "><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></li><li><a href="cha-ceph-as-cephfs.html" class=" you-are-here"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></li><li><a href="cha-as-ganesha.html" class=" "><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">13 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li></ol></li><li><a href="ap-deploy-docupdate.html" class=" "><span class="title-number">A </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-as-cephfs" data-id-title="Installation of CephFS"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">5.5 (SES 5 &amp; SES 5.5)</span></div><div><h2 class="title"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#">#</a></h2></div></div></div><p>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<code class="systemitem">librados</code>).
 </p><p>
  To use CephFS, you need to have a running Ceph storage cluster, and at
  least one running <span class="emphasis"><em>Ceph metadata server</em></span>.
 </p><section class="sect1" id="ceph-cephfs-limitations" data-id-title="Supported CephFS Scenarios and Guidance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-limitations">#</a></h2></div></div></div><p>
   With SUSE Enterprise Storage, SUSE introduces official support for many scenarios in
   which the scale-out and distributed component CephFS is used. This entry
   describes hard limits and provides guidance for the suggested use cases.
  </p><p>
   A supported CephFS deployment must meet these requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A minimum of one Metadata Server. SUSE recommends to deploy several nodes with the
     MDS role. Only one will be 'active' and the rest will be 'passive'.
     Remember to mention all the MON nodes in the <code class="command">mount</code>
     command when mounting the CephFS from a client.
    </p></li><li class="listitem"><p>
     CephFS snapshots are disabled (default) and not supported in this
     version.
    </p></li><li class="listitem"><p>
     Clients are SUSE Linux Enterprise Server 12 SP2 or SP3 based, using the
     <code class="literal">cephfs</code> kernel module driver. The FUSE module is not
     supported.
    </p></li><li class="listitem"><p>
     CephFS quotas are not supported in SUSE Enterprise Storage, as support for quotas
     is implemented in the FUSE client only.
    </p></li><li class="listitem"><p>
     CephFS supports file layout changes as documented in
     <a class="link" href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/" target="_blank">http://docs.ceph.com/docs/jewel/cephfs/file-layouts/</a>.
     However, while the file system is mounted by any client, new data pools
     may not be added to an existing CephFS file system (<code class="literal">ceph mds
     add_data_pool</code>). They may only be added while the file system is
     unmounted.
    </p></li></ul></div></section><section class="sect1" id="ceph-cephfs-mds" data-id-title="Ceph Metadata Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-mds">#</a></h2></div></div></div><p>
   Ceph metadata server (MDS) stores metadata for the CephFS. Ceph block
   devices and Ceph object storage <span class="emphasis"><em>do not</em></span> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands—such as <code class="command">ls</code> or
   <code class="command">find</code>—without placing an enormous burden on the
   Ceph storage cluster.
  </p><section class="sect2" id="ceph-cephfs-mdf-add" data-id-title="Adding a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.1 </span><span class="title-name">Adding a Metadata Server</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-mdf-add">#</a></h3></div></div></div><p>
    You can deploy MDS either during the initial cluster deployment process as
    described in <a class="xref" href="ceph-install-saltstack.html#ceph-install-stack" title="4.3. Cluster Deployment">Section 4.3, “Cluster Deployment”</a>, or add it to an already
    deployed cluster as described in <span class="intraxref">Book “Administration Guide”, Chapter 1 “Salt Cluster Administration”, Section 1.1 “Adding New Cluster Nodes”</span>.
   </p><p>
    After you deploy your MDS, allow the <code class="literal">Ceph OSD/MDS</code>
    service in the firewall setting of the server where MDS is deployed: Start
    <code class="literal">yast</code>, navigate to <span class="guimenu">Security and
    Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
    Services</span> and in the <span class="guimenu">Service to
    Allow</span> drop–down menu select <span class="guimenu">Ceph
    OSD/MDS</span>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </p></section><section class="sect2" id="ceph-cephfs-mds-config" data-id-title="Configuring a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.2 </span><span class="title-name">Configuring a Metadata Server</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-mds-config">#</a></h3></div></div></div><p>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <code class="filename">ceph.conf</code> configuration file.
   </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MDS Cache Size </span><a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#id-1.4.5.5.6.4.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.5.6.4.3.2"><span class="term"><code class="option">mds cache memory limit</code>
     </span></dt><dd><p>
       The soft memory limit (in bytes) that the MDS will enforce for its
       cache. Administrators should use this instead of the old <code class="option">mds
       cache size</code> setting. Defaults to 1GB.
      </p></dd><dt id="id-1.4.5.5.6.4.3.3"><span class="term"><code class="option">mds cache reservation</code>
     </span></dt><dd><p>
       The cache reservation (memory or inodes) for the MDS cache to maintain.
       When the MDS begins touching its reservation, it will recall client
       state until its cache size shrinks to restore the reservation. Defaults
       to 0.05.
      </p></dd></dl></div><p>
    For a detailed list of MDS related configuration options, see
    <a class="link" href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/" target="_blank">http://docs.ceph.com/docs/master/cephfs/mds-config-ref/</a>.
   </p><p>
    For a detailed list of MDS journaler configuration options, see
    <a class="link" href="http://docs.ceph.com/docs/master/cephfs/journaler/" target="_blank">http://docs.ceph.com/docs/master/cephfs/journaler/</a>.
   </p></section></section><section class="sect1" id="ceph-cephfs-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.3 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-cephfs">#</a></h2></div></div></div><p>
   When you have a healthy Ceph storage cluster with at least one Ceph
   metadata server, you can create and mount your Ceph file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </p><section class="sect2" id="ceph-cephfs-cephfs-create" data-id-title="Creating CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.1 </span><span class="title-name">Creating CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-cephfs-create">#</a></h3></div></div></div><p>
    A CephFS requires at least two RADOS pools: one for
    <span class="emphasis"><em>data</em></span> and one for <span class="emphasis"><em>metadata</em></span>. When
    configuring these pools, you might consider:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </p></li><li class="listitem"><p>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </p></li></ul></div><p>
    When assigning a <code class="literal">role-mds</code> in the
    <code class="filename">policy.cfg</code>, the required pools are automatically
    created. You can manually create the pools <code class="literal">cephfs_data</code>
    and <code class="literal">cephfs_metadata</code> for manual performance tuning before
    setting up the Metadata Server. DeepSea will not create these pools if they already
    exist.
   </p><p>
    For more information on managing pools, see <span class="intraxref">Book “Administration Guide”, Chapter 8 “Managing Storage Pools”</span>.
   </p><p>
    To create the two required pools—for example, 'cephfs_data' and
    'cephfs_metadata'—with default settings for use with CephFS, run
    the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create cephfs_data <em class="replaceable">pg_num</em>
<code class="prompt user">cephadm &gt; </code>ceph osd pool create cephfs_metadata <em class="replaceable">pg_num</em></pre></div><p>
    It is possible to use EC pools instead of replicated pools. We recommend to
    only use EC pools for low performance requirements and infrequent random
    access, for example cold storage, backups, archiving. CephFS on EC pools
    requires BlueStore to be enabled and the pool must have the
    <code class="literal">allow_ec_overwrite</code> option set. This option can be set by
    running <code class="command">ceph osd pool set ec_pool allow_ec_overwrites
    true</code>.
   </p><p>
    Erasure coding adds significant overhead to file system operations,
    especially small updates. This overhead is inherent to using erasure coding
    as a fault tolerance mechanism. This penalty is the trade off for
    significantly reduced storage space overhead.
   </p><p>
    When the pools are created, you may enable the file system with the
    <code class="command">ceph fs new</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs new <em class="replaceable">fs_name</em> <em class="replaceable">metadata_pool_name</em> <em class="replaceable">data_pool_name</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs new cephfs cephfs_metadata cephfs_data</pre></div><p>
    You can check that the file system was created by listing all available
    CephFSs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> <code class="option">fs ls</code>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</pre></div><p>
    When the file system has been created, your MDS will be able to enter an
    <span class="emphasis"><em>active</em></span> state. For example, in a single MDS system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code><code class="command">ceph</code> <code class="option">mds stat</code>
e5: 1/1/1 up</pre></div><div id="id-1.4.5.5.7.3.18" data-id-title="More Topics" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Topics</h6><p>
     You can find more information of specific tasks—for example
     mounting, unmounting, and advanced CephFS setup—in
     <span class="intraxref">Book “Administration Guide”, Chapter 15 “Clustered File System”</span>.
    </p></div></section><section class="sect2" id="ceph-cephfs-multimds" data-id-title="MDS Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.2 </span><span class="title-name">MDS Cluster Size</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-multimds">#</a></h3></div></div></div><p>
    A CephFS instance can be served by multiple active MDS daemons. All
    active MDS daemons that are assigned to a CephFS instance will distribute
    the file system's directory tree between themselves, and thus spread the
    load of concurrent clients. In order to add an active MDS daemon to a
    CephFS instance, a spare standby is needed. Either start an additional
    daemon or use an existing standby instance.
   </p><p>
    The following command will display the current number of active and passive
    MDS daemons.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mds stat</pre></div><p>
    The following command sets the number of active MDS's to two in a file
    system instance.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 2</pre></div><p>
    In order to shrink the MDS cluster prior to an update, two steps are
    necessary. First set <code class="option">max_mds</code> so that only one instance
    remains:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 1</pre></div><p>
    and after that explicitly deactivate the other active MDS daemons:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph mds deactivate <em class="replaceable">fs_name</em>:<em class="replaceable">rank</em></pre></div><p>
    where <em class="replaceable">rank</em> is the number of an active MDS daemon
    of a file system instance, ranging from 0 to <code class="option">max_mds</code>-1.
    See
    <a class="link" href="http://docs.ceph.com/docs/luminous/cephfs/multimds/" target="_blank">http://docs.ceph.com/docs/luminous/cephfs/multimds/</a>
    for additional information.
   </p></section><section class="sect2" id="ceph-cephfs-multimds-updates" data-id-title="MDS Cluster and Updates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.3 </span><span class="title-name">MDS Cluster and Updates</span> <a title="Permalink" class="permalink" href="cha-ceph-as-cephfs.html#ceph-cephfs-multimds-updates">#</a></h3></div></div></div><p>
    During Ceph updates, the feature flags on a file system instance may
    change (usually by adding new features). Incompatible daemons (such as the
    older versions) are not able to function with an incompatible feature set
    and will refuse to start. This means that updating and restarting one
    daemon can cause all other not yet updated daemons to stop and refuse to
    start. For this reason we, recommend shrinking the active MDS cluster to
    size one and stopping all standby daemons before updating Ceph. The
    manual steps for this update procedure are as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the Ceph related packages using <code class="command">zypper</code>.
     </p></li><li class="step"><p>
      Shrink the active MDS cluster as described above to 1 instance and stop
      all standby MDS daemons using their <code class="systemitem">systemd</code> units on all other nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Only then restart the single remaining MDS daemon, causing it to restart
      using the updated binary.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Restart all other MDS daemons and re-set the desired
      <code class="option">max_mds</code> setting.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-mds.target</pre></div></li></ol></div></div><p>
    If you use DeepSea, it will follow this procedure in case the
    <span class="package">ceph</span> package was updated during Stages 0 and 4. It is
    possible to perform this procedure while clients have the CephFS instance
    mounted and I/O is ongoing. Note however that there will be a very brief
    I/O pause while the active MDS restarts. Clients will recover
    automatically.
   </p><p>
    It is good practice to reduce the I/O load as much as possible before
    updating an MDS cluster. An idle MDS cluster will go through this update
    procedure quicker. Conversely, on a heavily loaded cluster with multiple
    MDS daemons it is essential to reduce the load in advance to prevent a
    single MDS daemon from being overwhelmed by ongoing I/O.
   </p></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ceph-as-iscsi.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 10 </span>Installation of iSCSI Gateway</span></a> </div><div><a class="pagination-link next" href="cha-as-ganesha.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 12 </span>Installation of NFS Ganesha</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-as-cephfs.html#ceph-cephfs-limitations"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span></a></span></li><li><span class="sect1"><a href="cha-ceph-as-cephfs.html#ceph-cephfs-mds"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span></a></span></li><li><span class="sect1"><a href="cha-ceph-as-cephfs.html#ceph-cephfs-cephfs"><span class="title-number">11.3 </span><span class="title-name">CephFS</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/deployment_cephfs.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>