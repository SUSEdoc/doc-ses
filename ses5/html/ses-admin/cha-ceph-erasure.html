<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Erasure Coded Pools | Administration Guide | SUSE Enterprise Storage 5.5 (SES 5 &amp; SES 5.5)</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Erasure Coded Pools | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="description" content="Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality o…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 10. Erasure Coded Pools"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tbazant@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Enterprise Storage 5"/>
<meta property="og:title" content="Erasure Coded Pools | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta property="og:description" content="Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality o…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Erasure Coded Pools | SES 5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="twitter:description" content="Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality o…"/>
<link rel="prev" href="ceph-rbd.html" title="Chapter 9. RADOS Block Device"/><link rel="next" href="cha-ceph-tiered.html" title="Chapter 11. Cache Tiering"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-operate.html">Operating a Cluster</a><span> / </span><a class="crumb" href="cha-ceph-erasure.html">Erasure Coded Pools</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">1 </span><span class="title-name">Salt Cluster Administration</span></a></li></ol></li><li class="active"><a href="part-operate.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">2 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">3 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">4 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">5 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">6 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">7 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">9 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" you-are-here"><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-tiered.html" class=" "><span class="title-number">11 </span><span class="title-name">Cache Tiering</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">12 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li><a href="part-dataccess.html" class="has-children "><span class="title-number">III </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">13 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">14 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">15 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-gui.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Managing Cluster with GUI Tools</span></a><ol><li><a href="ceph-oa.html" class=" "><span class="title-number">17 </span><span class="title-name">openATTIC</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">18 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">19 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">20 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">21 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">22 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="gloss-storage-glossary.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="app-alerting-default.html" class=" "><span class="title-number">B </span><span class="title-name">Default Alerts for SUSE Enterprise Storage</span></a></li><li><a href="app-storage-manual-inst.html" class=" "><span class="title-number">C </span><span class="title-name">Example Procedure of Manual Ceph Installation</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">D </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-erasure" data-id-title="Erasure Coded Pools"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">5.5 (SES 5 &amp; SES 5.5)</span></div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="cha-ceph-erasure.html#">#</a></h2></div></div></div><p>
  Ceph provides an alternative to the normal replication of data in pools,
  called <span class="emphasis"><em>erasure</em></span> or <span class="emphasis"><em>erasure coded</em></span>
  pool. Erasure pools do not provide all functionality of
  <span class="emphasis"><em>replicated</em></span> pools (for example it cannot store metadata
  for RBD pools), but require less raw storage. A default erasure pool capable
  of storing 1 TB of data requires 1,5 TB of raw storage, allowing a single
  disk failure. This compares favorably to a replicated pool which needs 2 TB
  of raw storage for the same purpose.
 </p><p>
  For background information on Erasure Code, see
  <a class="link" href="https://en.wikipedia.org/wiki/Erasure_code" target="_blank">https://en.wikipedia.org/wiki/Erasure_code</a>.
 </p><div id="id-1.3.4.10.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   When using FileStore, you cannot access erasure coded pools with the RBD
   interface unless you have a cache tier configured. Refer to
   <a class="xref" href="cha-ceph-tiered.html#ceph-tier-erasure" title="11.5. Erasure Coded Pool and Cache Tiering">Section 11.5, “Erasure Coded Pool and Cache Tiering”</a> for more details, or use BlueStore.
  </p></div><section class="sect1" id="ec-prerequisite" data-id-title="Prerequisite for Erasure Coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="cha-ceph-erasure.html#ec-prerequisite">#</a></h2></div></div></div><p>
   To make use of erasure coding, you need to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Define an erasure rule in the CRUSH Map.
    </p></li><li class="listitem"><p>
     Define an erasure code profile that specifies the coding algorithm to be
     used.
    </p></li><li class="listitem"><p>
     Create a pool using the previously mentioned rule and profile.
    </p></li></ul></div><p>
   Keep in mind that changing the profile and the details in the profile will
   not be possible after the pool was created and has data.
  </p><p>
   Ensure that the CRUSH rules for <span class="emphasis"><em>erasure pools</em></span> use
   <code class="literal">indep</code> for <code class="literal">step</code>. For details see
   <a class="xref" href="cha-storage-datamgm.html#datamgm-rules-step-mode" title="7.3.2. firstn and indep">Section 7.3.2, “firstn and indep”</a>.
  </p></section><section class="sect1" id="cha-ceph-erasure-default-profile" data-id-title="Creating a Sample Erasure Coded Pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="cha-ceph-erasure.html#cha-ceph-erasure-default-profile">#</a></h2></div></div></div><p>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     The command <code class="command">ceph osd pool create</code> is used to create a
     pool with type <span class="emphasis"><em>erasure</em></span>. The <code class="literal">12</code>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</pre></div></li><li class="step"><p>
     The string <code class="literal">ABCDEFGHI</code> is written into an object called
     <code class="literal">NYAN</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -</pre></div></li><li class="step"><p>
     For testing purposes OSDs can now be disabled, for example by
     disconnecting them from the network.
    </p></li><li class="step"><p>
     To test whether the pool can handle the failure of devices, the content of
     the file can be accessed with the <code class="command">rados</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div></li></ol></div></div></section><section class="sect1" id="cha-ceph-erasure-erasure-profiles" data-id-title="Erasure Code Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Erasure Code Profiles</span> <a title="Permalink" class="permalink" href="cha-ceph-erasure.html#cha-ceph-erasure-erasure-profiles">#</a></h2></div></div></div><p>
   When the <code class="command">ceph osd pool create</code> command is invoked to
   create an <span class="emphasis"><em>erasure pool</em></span>, the default profile is used,
   unless another profile is specified. Profiles define the redundancy of data.
   This is done by setting two parameters, arbitrarily named
   <code class="literal">k</code> and <code class="literal">m</code>. k and m define in how many
   <code class="literal">chunks</code> a piece of data is split and how many coding
   chunks are created. Redundant chunks are then stored on different OSDs.
  </p><p>
   Definitions required for erasure pool profiles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.10.8.4.1"><span class="term">chunk</span></dt><dd><p>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </p></dd><dt id="id-1.3.4.10.8.4.2"><span class="term">k</span></dt><dd><p>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <code class="literal">k = 2</code> a
      10KB object will be divided into <code class="literal">k</code> objects of 5KB
      each.
     </p></dd><dt id="id-1.3.4.10.8.4.3"><span class="term">m</span></dt><dd><p>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </p></dd><dt id="id-1.3.4.10.8.4.4"><span class="term">crush-failure-domain</span></dt><dd><p>
      defines to which devices the chunks are distributed. A bucket type needs
      to be set as value. For all bucket types, see
      <a class="xref" href="cha-storage-datamgm.html#datamgm-buckets" title="7.2. Buckets">Section 7.2, “Buckets”</a>. If the failure domain is
      <code class="literal">rack</code>, the chunks will be stored on different racks to
      increase the resilience in case of rack failures. Keep in mind that this
      requires k+m racks.
     </p></dd></dl></div><p>
   With the default erasure code profile used in
   <a class="xref" href="cha-ceph-erasure.html#cha-ceph-erasure-default-profile" title="10.2. Creating a Sample Erasure Coded Pool">Section 10.2, “Creating a Sample Erasure Coded Pool”</a>, you will not lose
   cluster data if a single OSD or host fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage are
   required for 1 TB of data (due to k=2,m=1). This is equivalent to a common
   RAID 5 configuration. For comparison: a replicated pool needs 2 TB of raw
   storage to store 1 TB of data.
  </p><p>
   The settings of the default profile can be displayed with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</pre></div><p>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new one (see
   <a class="xref" href="ceph-pools.html#pools-migration" title="8.3. Pool Migration">Section 8.3, “Pool Migration”</a>).
  </p><p>
   The most important parameters of the profile are <code class="literal">k</code>,
   <code class="literal">m</code> and <code class="literal">crush-failure-domain</code> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66% overhead, the following profile can be defined. Note that
   this is only valid with a CRUSH Map that has buckets of type 'rack':
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd erasure-code-profile set <em class="replaceable">myprofile</em> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</pre></div><p>
   The example <a class="xref" href="cha-ceph-erasure.html#cha-ceph-erasure-default-profile" title="10.2. Creating a Sample Erasure Coded Pool">Section 10.2, “Creating a Sample Erasure Coded Pool”</a> can be
   repeated with this new profile:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool create ecpool 12 12 erasure <em class="replaceable">myprofile</em>
<code class="prompt user">cephadm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<code class="prompt user">cephadm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div><p>
   The NYAN object will be divided in three (<code class="literal">k=3</code>) and two
   additional chunks will be created (<code class="literal">m=2</code>). The value of
   <code class="literal">m</code> defines how many OSDs can be lost simultaneously
   without losing any data. The <code class="literal">crush-failure-domain=rack</code>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/ceph_erasure_obj.png" target="_blank"><img src="images/ceph_erasure_obj.png" width=""/></a></div></div><p>
   For more information about the erasure code profiles, see
   <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile" target="_blank">http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile</a>.
  </p></section><section class="sect1" id="ec-rbd" data-id-title="Erasure Coded Pools with RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span> <a title="Permalink" class="permalink" href="cha-ceph-erasure.html#ec-rbd">#</a></h2></div></div></div><p>
   To mark an EC pool as a RBD pool, tag it accordingly:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>ceph osd pool application enable rbd <em class="replaceable">ec_pool_name</em></pre></div><p>
   RBD can store image <span class="emphasis"><em>data</em></span> in EC pools. However, the
   image header and metadata still needs to be stored in a replicated pool.
   Assuming you have the pool named 'rbd' for this purpose:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm &gt; </code>rbd create rbd/<em class="replaceable">image_name</em> --size 1T --data-pool <em class="replaceable">ec_pool_name</em></pre></div><p>
   You can use the image normally like any other image, except that all of the
   data will be stored in the <em class="replaceable">ec_pool_name</em> pool
   instead of 'rbd' pool.
  </p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="ceph-rbd.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 9 </span>RADOS Block Device</span></a> </div><div><a class="pagination-link next" href="cha-ceph-tiered.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 11 </span>Cache Tiering</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-erasure.html#ec-prerequisite"><span class="title-number">10.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span></a></span></li><li><span class="sect1"><a href="cha-ceph-erasure.html#cha-ceph-erasure-default-profile"><span class="title-number">10.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span></a></span></li><li><span class="sect1"><a href="cha-ceph-erasure.html#cha-ceph-erasure-erasure-profiles"><span class="title-number">10.3 </span><span class="title-name">Erasure Code Profiles</span></a></span></li><li><span class="sect1"><a href="cha-ceph-erasure.html#ec-rbd"><span class="title-number">10.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/admin_ceph_erasure.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>