<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>NFS Ganesha: Export Ceph Data via NFS | Administration Guide | SUSE Enterprise Storage 5.5 (SES 5 &amp; SES 5.5)</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="NFS Ganesha: Export Ceph Data via NFS | SES 5.5 (SES 5…"/>
<meta name="description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="5.5 (SES 5 &amp; SES 5.5)"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 16. NFS Ganesha: Export Ceph Data via NFS"/>
<meta name="tracker-url" content="https://bugzilla.suse.com/enter_bug.cgi"/>
<meta name="tracker-type" content="bsc"/>
<meta name="tracker-bsc-assignee" content="tbazant@suse.com"/>
<meta name="tracker-bsc-component" content="Documentation"/>
<meta name="tracker-bsc-product" content="SUSE Enterprise Storage 5"/>
<meta property="og:title" content="NFS Ganesha: Export Ceph Data via NFS | SES 5.5 (SES 5…"/>
<meta property="og:description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NFS Ganesha: Export Ceph Data via NFS | SES 5.5 (SES 5…"/>
<meta name="twitter:description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<link rel="prev" href="cha-ceph-cephfs.html" title="Chapter 15. Clustered File System"/><link rel="next" href="part-gui.html" title="Part IV. Managing Cluster with GUI Tools"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-dataccess.html">Accessing Cluster Data</a><span> / </span><a class="crumb" href="cha-ceph-nfsganesha.html">NFS Ganesha: Export Ceph Data via NFS</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">1 </span><span class="title-name">Salt Cluster Administration</span></a></li></ol></li><li><a href="part-operate.html" class="has-children "><span class="title-number">II </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">2 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">3 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">4 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">5 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">6 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">7 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">9 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">10 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-tiered.html" class=" "><span class="title-number">11 </span><span class="title-name">Cache Tiering</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">12 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li class="active"><a href="part-dataccess.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">13 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">14 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">15 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" you-are-here"><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-gui.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Managing Cluster with GUI Tools</span></a><ol><li><a href="ceph-oa.html" class=" "><span class="title-number">17 </span><span class="title-name">openATTIC</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">18 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">19 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">20 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">21 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">22 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="gloss-storage-glossary.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="app-alerting-default.html" class=" "><span class="title-number">B </span><span class="title-name">Default Alerts for SUSE Enterprise Storage</span></a></li><li><a href="app-storage-manual-inst.html" class=" "><span class="title-number">C </span><span class="title-name">Example Procedure of Manual Ceph Installation</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">D </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-nfsganesha" data-id-title="NFS Ganesha: Export Ceph Data via NFS"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">5.5 (SES 5 &amp; SES 5.5)</span></div><div><h2 class="title"><span class="title-number">16 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#">#</a></h2></div></div></div><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#cha-nfs" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p><p>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/<em class="replaceable">USERNAME</em>/<em class="replaceable">BUCKETNAME</em></code>.
 </p><p>
  A CephFS is exported by default via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/cephfs</code>.
 </p><div id="id-1.3.5.5.6" data-id-title="NFS Ganesha Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha Performance</h6><p>
   Due to increased protocol overhead and additional latency caused by extra
   network hops between the client and the storage, accessing Ceph via an NFS
   Gateway may significantly reduce application performance when compared to
   native CephFS or Object Gateway clients.
  </p></div><section class="sect1" id="ceph-nfsganesha-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-install">#</a></h2></div></div></div><p>
   For installation instructions, see <span class="intraxref">Book “Deployment Guide”, Chapter 12 “Installation of NFS Ganesha”</span>.
  </p></section><section class="sect1" id="ceph-nfsganesha-config" data-id-title="Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config">#</a></h2></div></div></div><p>
   For a list of all parameters available within the configuration file, see:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">man ganesha-config</code>
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-ceph-config</code> for CephFS File System
     Abstraction Layer (FSAL) options.
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-rgw-config</code> for Object Gateway FSAL options.
    </p></li></ul></div><p>
   This section includes information to help you configure the NFS Ganesha server
   to export the cluster data accessible via Object Gateway and CephFS.
  </p><p>
   NFS Ganesha configuration is controlled by
   <code class="filename">/etc/ganesha/ganesha.conf</code>. Note that changes to this
   file are overwritten when DeepSea Stage 4 is executed. To persistently
   change the settings, edit the file
   <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> located on
   the Salt master.
  </p><section class="sect2" id="ceph-nfsganesha-config-general" data-id-title="Export Section"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.1 </span><span class="title-name">Export Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general">#</a></h3></div></div></div><p>
    This section describes how to configure the <code class="literal">EXPORT</code>
    sections in the <code class="filename">ganesha.conf</code>.
   </p><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><section class="sect3" id="ceph-nfsganesha-config-general-export" data-id-title="Export Main Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">16.2.1.1 </span><span class="title-name">Export Main Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-export">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.6.4.2.1"><span class="term">Export_Id</span></dt><dd><p>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.2"><span class="term">Path</span></dt><dd><p>
        Export path in the related CephFS pool (mandatory). This allows
        subdirectories to be exported from the CephFS.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.3"><span class="term">Pseudo</span></dt><dd><p>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </p><p>
        Example: with the value <code class="literal">/cephfs/</code> and after executing
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount <em class="replaceable">GANESHA_IP</em>:/cephfs/ /mnt/</pre></div><p>
        The CephFS data is available in the directory
        <code class="filename">/mnt/cephfs/</code> on the client.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.4"><span class="term">Access_Type</span></dt><dd><p>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </p><div id="id-1.3.5.5.8.6.4.2.4.2.2" data-id-title="Limit Access to Clients" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Limit Access to Clients</h6><p>
         If you leave <code class="literal">Access_Type = RW</code> in the main
         <code class="literal">EXPORT</code> section and limit access to a specific
         client in the <code class="literal">CLIENT</code> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <code class="literal">Access_Type =
         None</code> in the <code class="literal">EXPORT</code> section and then
         specify less restrictive access mode for one or more clients in the
         <code class="literal">CLIENT</code> section:
        </p><div class="verbatim-wrap"><pre class="screen">EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}</pre></div></div></dd><dt id="id-1.3.5.5.8.6.4.2.5"><span class="term">Squash</span></dt><dd><p>
        NFS squash option.
       </p></dd><dt id="id-1.3.5.5.8.6.4.2.6"><span class="term">FSAL</span></dt><dd><p>
        Exporting 'File System Abstraction Layer'. See
        <a class="xref" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-fsal" title="16.2.1.2. FSAL Subsection">Section 16.2.1.2, “FSAL Subsection”</a>.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-general-fsal" data-id-title="FSAL Subsection"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">16.2.1.2 </span><span class="title-name">FSAL Subsection</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-fsal">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.6.5.3.1"><span class="term">Name</span></dt><dd><p>
        Defines which back-end NFS Ganesha uses. Allowed values are
        <code class="literal">CEPH</code> for CephFS or <code class="literal">RGW</code> for
        Object Gateway. Depending on the choice, a <code class="literal">role-mds</code> or
        <code class="literal">role-rgw</code> must be defined in the
        <code class="filename">policy.cfg</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="ceph-nfsganesha-config-rgw" data-id-title="RGW Section"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.2 </span><span class="title-name">RGW Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-rgw">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.7.3.1"><span class="term">ceph_conf</span></dt><dd><p>
       Points to the <code class="filename">ceph.conf</code> file. When deploying with
       DeepSea, it is not necessary to change this value.
      </p></dd><dt id="id-1.3.5.5.8.7.3.2"><span class="term">name</span></dt><dd><p>
       The name of the Ceph client user used by NFS Ganesha.
      </p></dd><dt id="id-1.3.5.5.8.7.3.3"><span class="term">cluster</span></dt><dd><p>
       Name of the Ceph cluster. SUSE Enterprise Storage 5.5 currently only supports one
       cluster name, which is <code class="literal">ceph</code> by default.
      </p></dd></dl></div></section><section class="sect2" id="ganesha-nfsport" data-id-title="Changing Default NFS Ganesha Ports"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.3 </span><span class="title-name">Changing Default NFS Ganesha Ports</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ganesha-nfsport">#</a></h3></div></div></div><p>
    NFS Ganesha uses the port 2049 for NFS and 875 for the rquota support by
    default. To change the default port numbers, use the
    <code class="option">NFS_Port</code> and <code class="option">RQUOTA_Port</code> options inside
    the <code class="literal">NFS_CORE_PARAM</code> section, for example:
   </p><div class="verbatim-wrap"><pre class="screen">NFS_CORE_PARAM
{
 NFS_Port = 2060;
 RQUOTA_Port = 876;
}</pre></div></section></section><section class="sect1" id="ceph-nfsganesha-customrole" data-id-title="Custom NFS Ganesha Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.3 </span><span class="title-name">Custom NFS Ganesha Roles</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole">#</a></h2></div></div></div><p>
   Custom NFS Ganesha roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <code class="filename">policy.cfg</code>. The roles
   allow for:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Separated NFS Ganesha nodes for accessing Object Gateway and CephFS.
    </p></li><li class="listitem"><p>
     Assigning different Object Gateway users to NFS Ganesha nodes.
    </p></li></ul></div><p>
   Having different Object Gateway users enables NFS Ganesha nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with Ceph buckets used in the CRUSH Map.
  </p><section class="sect2" id="ceph-nfsganesha-customrole-rgw-multiusers" data-id-title="Different Object Gateway Users for NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.1 </span><span class="title-name">Different Object Gateway Users for NFS Ganesha</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole-rgw-multiusers">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create two
    NFS Ganesha roles with different Object Gateway users. In this example, the roles
    <code class="literal">gold</code> and <code class="literal">silver</code> are used, for which
    DeepSea already provides example configuration files.
   </p><div class="procedure" id="proc-ceph-nfsganesha-rgw-multiusers"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> with
      the editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/gold.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</pre></div><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/silver.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</pre></div></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 silver.conf.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 gold.conf.j2</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 silver.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 gold.j2</pre></div></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/rgw/files/
<code class="prompt user">root # </code><code class="command">cp</code> rgw.j2 silver.j2
<code class="prompt user">root # </code><code class="command">cp</code> rgw.j2 gold.j2</pre></div></li><li class="step"><p>
      Object Gateway also needs the configuration for the different roles:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/configuration/files/
<code class="prompt user">root # </code><code class="command">cp</code> ceph.conf.rgw silver.conf
<code class="prompt user">root # </code><code class="command">cp</code> ceph.conf.rgw gold.conf</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-silver/cluster/<em class="replaceable">NODE1</em>.sls
role-gold/cluster/<em class="replaceable">NODE2</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-nfsganesha-customrole-rgw-cephfs" data-id-title="Separating CephFS and Object Gateway FSAL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.2 </span><span class="title-name">Separating CephFS and Object Gateway FSAL</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole-rgw-cephfs">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create 2 new
    different roles that use CephFS and Object Gateway:
   </p><div class="procedure" id="proc-ceph-nfsganesha-customrole"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/rgw.sls</code> with the
      editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_rgw.conf.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_cfs.conf.j2</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_rgw.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_cfs.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 ganesha_rgw.j2
<code class="prompt user">root # </code><code class="command">cp</code> ganesha.j2 ganesha_cfs.j2</pre></div><p>
      The line <code class="literal">caps mds = "allow *"</code> can be removed from the
      <code class="filename">ganesha_rgw.j2</code>.
     </p></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</pre></div></li><li class="step"><p>
      Object Gateway needs the configuration for the new role:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">cp</code> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-ganesha_rgw/cluster/<em class="replaceable">NODE1</em>.sls
role-ganesha_cfs/cluster/<em class="replaceable">NODE1</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ganesha-rgw-supported-operations" data-id-title="Supported Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.3.3 </span><span class="title-name">Supported Operations</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ganesha-rgw-supported-operations">#</a></h3></div></div></div><p>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Links including symbolic links are not supported.</em></span>
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>NFS access control lists (ACLs) are not supported.</em></span>
      Unix user and group ownership and permissions <span class="emphasis"><em>are</em></span>
      supported.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Directories may not be moved or renamed.</em></span> You
      <span class="emphasis"><em>may</em></span> move files between directories.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Only full, sequential write I/O is supported.</em></span>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example some versions of GNU
      <code class="command">tar</code>), but may fail due to infrequent non-sequential
      stores. When mounting via NFS, application's sequential I/O can generally
      be forced to sequential writes to the NFS server via synchronous mounting
      (the <code class="option">-o sync</code> option). NFS clients that cannot mount
      synchronously (for example Microsoft Windows*) will not be able to upload
      files.
     </p></li><li class="listitem"><p>
      NFS RGW supports read-write operations only for block size smaller than
      4MB.
     </p></li></ul></div></section></section><section class="sect1" id="ceph-nfsganesha-services" data-id-title="Starting or Restarting NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-services">#</a></h2></div></div></div><p>
   To enable and start the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> enable nfs-ganesha
<code class="prompt user">root # </code><code class="command">systemctl</code> start nfs-ganesha</pre></div><p>
   Restart NFS Ganesha with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> restart nfs-ganesha</pre></div><p>
   When NFS Ganesha is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </p></section><section class="sect1" id="ceph-nfsganesha-loglevel" data-id-title="Setting the Log Level"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.5 </span><span class="title-name">Setting the Log Level</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-loglevel">#</a></h2></div></div></div><p>
   You change the default debug level <code class="literal">NIV_EVENT</code> by editing
   the file <code class="filename">/etc/sysconfig/nfs-ganesha</code>. Replace
   <code class="literal">NIV_EVENT</code> with <code class="literal">NIV_DEBUG</code> or
   <code class="literal">NIV_FULL_DEBUG</code>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</pre></div><p>
   A restart of the service is required when changing the log level.
  </p><div id="id-1.3.5.5.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>NFS Ganesha uses Ceph client libraries to connect to the Ceph
      cluster. By default, client libraries do not log errors or any other
      output. To see more details about NFS Ganesha interacting with the
      Ceph cluster (for example, connection issues details) logging needs
      to be explicitly defined in the <code class="filename">ceph.conf</code> configuration file
      under the <code class="literal">[client]</code> section. For example:</p><div class="verbatim-wrap"><pre class="screen">[client]
	log_file = "/var/log/ceph/ceph-client.log"</pre></div></div></section><section class="sect1" id="ceph-nfsganesha-verify" data-id-title="Verifying the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.6 </span><span class="title-name">Verifying the Exported NFS Share</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-verify">#</a></h2></div></div></div><p>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   NFS Ganesha server node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">showmount</code> -e
/ (everything)</pre></div></section><section class="sect1" id="ceph-nfsganesha-mount" data-id-title="Mounting the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.7 </span><span class="title-name">Mounting the Exported NFS Share</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-mount">#</a></h2></div></div></div><p>
   To mount the exported NFS share (as configured in
   <a class="xref" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config" title="16.2. Configuration">Section 16.2, “Configuration”</a>) on a client host, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -t nfs -o rw,noatime,sync \
 <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em></pre></div></section><section class="sect1" id="ceph-nfsganesha-more" data-id-title="Additional Resources"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.8 </span><span class="title-name">Additional Resources</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-more">#</a></h2></div></div></div><p>
   The original NFS Ganesha documentation can be found at
   <a class="link" href="https://github.com/nfs-ganesha/nfs-ganesha/wiki/Docs" target="_blank">https://github.com/nfs-ganesha/nfs-ganesha/wiki/Docs</a>.
  </p></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ceph-cephfs.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 15 </span>Clustered File System</span></a> </div><div><a class="pagination-link next" href="part-gui.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Part IV </span>Managing Cluster with GUI Tools</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-install"><span class="title-number">16.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config"><span class="title-number">16.2 </span><span class="title-name">Configuration</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole"><span class="title-number">16.3 </span><span class="title-name">Custom NFS Ganesha Roles</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-services"><span class="title-number">16.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-loglevel"><span class="title-number">16.5 </span><span class="title-name">Setting the Log Level</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-verify"><span class="title-number">16.6 </span><span class="title-name">Verifying the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-mount"><span class="title-number">16.7 </span><span class="title-name">Mounting the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-more"><span class="title-number">16.8 </span><span class="title-name">Additional Resources</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/admin_nfsganesha.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>