<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Hardware Requirements and Recommendations | Deployment Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Hardware Requirements and Recommendations | SES 6"/>
<meta name="description" content="The hardware requirements of Ceph are heavily dependent on the IO workload. The following hardware requirements and recommendations should be considered as a s…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="chapter-title" content="Chapter 2. Hardware Requirements and Recommendations"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Hardware Requirements and Recommendations | SES 6"/>
<meta property="og:description" content="The hardware requirements of Ceph are heavily dependent on the IO workload. The following hardware requirements and recommendations should be considered as a s…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hardware Requirements and Recommendations | SES 6"/>
<meta name="twitter:description" content="The hardware requirements of Ceph are heavily dependent on the IO workload. The following hardware requirements and recommendations should be considered as a s…"/>
<link rel="prev" href="cha-storage-about.html" title="Chapter 1. SUSE Enterprise Storage 6 and Ceph"/><link rel="next" href="cha-admin-ha.html" title="Chapter 3. Admin Node HA Setup"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deployment Guide</a><span> / </span><a class="crumb" href="part-ses.html">SUSE Enterprise Storage</a><span> / </span><a class="crumb" href="storage-bp-hwreq.html">Hardware Requirements and Recommendations</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Deployment Guide</div><ol><li><a href="bk02pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li class="active"><a href="part-ses.html" class="has-children you-are-here"><span class="title-number">I </span><span class="title-name">SUSE Enterprise Storage</span></a><ol><li><a href="cha-storage-about.html" class=" "><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 6 and Ceph</span></a></li><li><a href="storage-bp-hwreq.html" class=" you-are-here"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></li><li><a href="cha-admin-ha.html" class=" "><span class="title-number">3 </span><span class="title-name">Admin Node HA Setup</span></a></li><li><a href="bk02pt01ch04.html" class=" "><span class="title-number">4 </span><span class="title-name">User Privileges and Command Prompts</span></a></li></ol></li><li><a href="ses-deployment.html" class="has-children "><span class="title-number">II </span><span class="title-name">Cluster Deployment and Upgrade</span></a><ol><li><a href="ceph-install-saltstack.html" class=" "><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></li><li><a href="cha-ceph-upgrade.html" class=" "><span class="title-number">6 </span><span class="title-name">Upgrading from Previous Releases</span></a></li><li><a href="ceph-deploy-ds-custom.html" class=" "><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></li></ol></li><li><a href="additional-software.html" class="has-children "><span class="title-number">III </span><span class="title-name">Installation of Additional Services</span></a><ol><li><a href="cha-ceph-as-intro.html" class=" "><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></li><li><a href="cha-ceph-additional-software-installation.html" class=" "><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-as-iscsi.html" class=" "><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></li><li><a href="cha-ceph-as-cephfs.html" class=" "><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></li><li><a href="cha-as-ganesha.html" class=" "><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></li></ol></li><li><a href="containerized-ses-on-caasp.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Cluster Deployment on Top of SUSE CaaS Platform 4 (Technology Preview)</span></a><ol><li><a href="cha-container-kubernetes.html" class=" "><span class="title-number">13 </span><span class="title-name">SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</span></a></li></ol></li><li><a href="bk02apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk02go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-deploy-docupdate.html" class=" "><span class="title-number">B </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="storage-bp-hwreq" data-id-title="Hardware Requirements and Recommendations"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#">#</a></h2></div></div></div><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p><p>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </p><section class="sect1" id="network-overview" data-id-title="Network Overview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Network Overview</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#network-overview">#</a></h2></div></div></div><p>
   Ceph has several logical networks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A trusted internal network, the back-end network called the the
     <code class="literal">cluster network</code>.
    </p></li><li class="listitem"><p>
     A public client network called <code class="literal">public network</code>.
    </p></li><li class="listitem"><p>
     Client networks for gateways, these are optional.
    </p></li></ul></div><p>
   The trusted internal network is the back-end network between the OSD nodes
   for replication, re-balancing and recovery.Ideally, this network provides
   twice the bandwidth of the public network with default 3-way replication
   since the primary OSD sends 2 copies to other OSDs via this network. The
   public network is between clients and gateways on the one side to talk to
   monitors, managers, MDS nodes, OSD nodes. It is also used by monitors,
   managers, and MDS nodes to talk with OSD nodes.
  </p><div class="figure" id="network-overview-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/network-overview-diagram.png" target="_blank"><img src="images/network-overview-diagram.png" width="" alt="Network Overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.1: </span><span class="title-name">Network Overview </span><a title="Permalink" class="permalink" href="storage-bp-hwreq.html#network-overview-figure">#</a></h6></div></div><section class="sect2" id="ceph-install-ceph-deploy-network" data-id-title="Network Recommendations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.1 </span><span class="title-name">Network Recommendations</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ceph-install-ceph-deploy-network">#</a></h3></div></div></div><p>
    For the Ceph network environment, we recommend two bonded 25 GbE (or
    faster) network interfaces bonded using 802.3ad (LACP). The use of two
    network interfaces provides aggregation and fault-tolerance. The bond
    should then be used to provide two VLAN interfaces, one for the public
    network, and the second for the cluster network. Details on bonding the
    interfaces can be found in
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-iface-bonding" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-iface-bonding</a>.
   </p><p>
    Fault tolerance can be enhanced through isolating the components into
    failure domains. To improve fault tolerance of the network, bonding one
    interface from two separate Network Interface Cards (NIC) offers protection
    against failure of a single NIC. Similarly, creating a bond across two
    switches protects against failure of a switch. We recommend consulting with
    the network equipment vendor in order to architect the level of fault
    tolerance required.
   </p><div id="id-1.4.3.3.5.6.4" data-id-title="Administration Network not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Administration Network not Supported</h6><p>
     Additional administration network setup—that enables for example
     separating SSH, Salt, or DNS networking—is neither tested nor
     supported.
    </p></div><div id="id-1.4.3.3.5.6.5" data-id-title="Nodes Configured via DHCP" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Nodes Configured via DHCP</h6><p>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various Ceph daemons start. If this happens, the Ceph MONs and OSDs
     will not start correctly (running <code class="command">systemctl status
     ceph\*</code> will result in "unable to bind" errors). To avoid this
     issue, we recommend increasing the DHCP client timeout to at least 30
     seconds on each node in your storage cluster. This can be done by changing
     the following settings on each node:
    </p><p>
     In <code class="filename">/etc/sysconfig/network/dhcp</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">DHCLIENT_WAIT_AT_BOOT="30"</pre></div><p>
     In <code class="filename">/etc/sysconfig/network/config</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">WAIT_FOR_INTERFACES="60"</pre></div></div><section class="sect3" id="storage-bp-net-private" data-id-title="Adding a Private Network to a Running Cluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.1 </span><span class="title-name">Adding a Private Network to a Running Cluster</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#storage-bp-net-private">#</a></h4></div></div></div><p>
     If you do not specify a cluster network during Ceph deployment, it
     assumes a single public network environment. While Ceph operates fine
     with a public network, its performance and security improves when you set
     a second private cluster network. To support two networks, each Ceph
     node needs to have at least two network cards.
    </p><p>
     You need to apply the following changes to each Ceph node. It is
     relatively quick to do for a small cluster, but can be very time consuming
     if you have a cluster consisting of hundreds or thousands of nodes.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Stop Ceph related services on each cluster node.
      </p><p>
       Add a line to <code class="filename">/etc/ceph/ceph.conf</code> to define the
       cluster network, for example:
      </p><div class="verbatim-wrap"><pre class="screen">cluster network = 10.0.0.0/24</pre></div><p>
       If you need to specifically assign static IP addresses or override
       <code class="option">cluster network</code> settings, you can do so with the
       optional <code class="option">cluster addr</code>.
      </p></li><li class="step"><p>
       Check that the private cluster network works as expected on the OS
       level.
      </p></li><li class="step"><p>
       Start Ceph related services on each cluster node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph.target</pre></div></li></ol></div></div></section><section class="sect3" id="storage-bp-net-subnets" data-id-title="Monitor Nodes on Different Subnets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.2 </span><span class="title-name">Monitor Nodes on Different Subnets</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#storage-bp-net-subnets">#</a></h4></div></div></div><p>
     If the monitor nodes are on multiple subnets, for example they are located
     in different rooms and served by different switches, you need to adjust
     the <code class="filename">ceph.conf</code> file accordingly. For example, if the
     nodes have IP addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12, add the
     following lines to their <code class="literal">global</code> section:
    </p><div class="verbatim-wrap"><pre class="screen">[global]
 [...]
 mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
 mon initial members = MON1, MON2, MON3
 [...]</pre></div><p>
     Additionally, if you need to specify a per-monitor public address or
     network, you need to add a
     <code class="literal">[mon.<em class="replaceable">X</em>]</code> section for each
     monitor:
    </p><div class="verbatim-wrap"><pre class="screen">[mon.MON1]
 public network = 192.168.123.0/24

 [mon.MON2]
 public network = 1.2.3.0/24

 [mon.MON3]
 public network = 242.12.33.12/0</pre></div></section></section></section><section class="sect1" id="multi-architecture" data-id-title="Multiple Architecture Configurations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Multiple Architecture Configurations</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#multi-architecture">#</a></h2></div></div></div><p>
   SUSE Enterprise Storage supports both x86 and Arm architectures. When considering
   each architecture, it is important to note that from a cores per OSD,
   frequency, and RAM perspective, there is no real difference between CPU
   architectures for sizing.
  </p><p>
   As with smaller x86 processors (non-server), lower-performance Arm-based
   cores may not provide an optimal experience, especially when used for
   erasure coded pools.
  </p><div id="id-1.4.3.3.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Throughout the documentation, <em class="replaceable">SYSTEM-ARCH</em> is
    used in place of x86 or Arm.
   </p></div></section><section class="sect1" id="ses-hardware-config" data-id-title="Hardware Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Hardware Configuration</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-hardware-config">#</a></h2></div></div></div><p>
   For the best product experience, we recommend to start with the recommended
   cluster configuration. For a test cluster or a cluster with less performance
   requirements, we document a minimal supported cluster configuration.
  </p><section class="sect2" id="ses-bp-minimum-cluster" data-id-title="Minimum Cluster Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.1 </span><span class="title-name">Minimum Cluster Configuration</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-bp-minimum-cluster">#</a></h3></div></div></div><p>
    A minimal product cluster configuration consists of:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least four physical nodes (OSD nodes) with co-location of services
     </p></li><li class="listitem"><p>
      Dual-10 Gb Ethernet as a bonded network
     </p></li><li class="listitem"><p>
      A separate Admin Node (can be virtualized on an external node)
     </p></li></ul></div><p>
    A detailed configuration is:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Separate Admin Node with 4 GB RAM, four cores, 1 TB storage capacity. This is
      typically the Salt master node. Ceph services and gateways, such as
      Ceph Monitor, Metadata Server, Ceph OSD, Object Gateway, or NFS Ganesha are not supported on the Admin Node
      as it needs to orchestrate the cluster update and upgrade processes
      independently.
     </p></li><li class="listitem"><p>
      At least four physical OSD nodes, with eight OSD disks each, see
      <a class="xref" href="storage-bp-hwreq.html#sysreq-osd" title="2.4.1. Minimum Requirements">Section 2.4.1, “Minimum Requirements”</a> for requirements.
     </p><p>
      The total capacity of the cluster should be sized so that even with one
      node unavailable, the total used capacity (including redundancy) does not
      exceed 80%.
     </p></li><li class="listitem"><p>
      Three Ceph Monitor instances. Monitors need to be run from SSD/NVMe storage, not
      HDDs, for latency reasons.
     </p></li><li class="listitem"><p>
      Monitors, Metadata Server, and gateways can be co-located on the OSD nodes, see
      <a class="xref" href="storage-bp-hwreq.html#ses-bp-diskshare" title="2.12. OSD and Monitor Sharing One Server">Section 2.12, “OSD and Monitor Sharing One Server”</a> for monitor co-location. If you
      co-locate services, the memory and CPU requirements need to be added up.
     </p></li><li class="listitem"><p>
      iSCSI Gateway, Object Gateway, and Metadata Server require at least incremental 4 GB RAM and four
      cores.
     </p></li><li class="listitem"><p>
      If you are using CephFS, S3/Swift, iSCSI, at least two instances of
      the respective roles (Metadata Server, Object Gateway, iSCSI) are required for redundancy
      and availability.
     </p></li><li class="listitem"><p>
      The nodes are to be dedicated to SUSE Enterprise Storage and must not be used for
      any other physical, containerized, or virtualized workload.
     </p></li><li class="listitem"><p>
      If any of the gateways (iSCSI, Object Gateway, NFS Ganesha, Metadata Server, ...) are
      deployed within VMs, these VMs must not be hosted on the physical
      machines serving other cluster roles. (This is unnecessary, as they are
      supported as collocated services.)
     </p></li><li class="listitem"><p>
      When deploying services as VMs on hypervisors outside the core physical
      cluster, failure domains must be respected to ensure redundancy.
     </p><p>
      For example, do not deploy multiple roles of the same type on the same
      hypervisor, such as multiple MONs or MDSs instances.
     </p></li><li class="listitem"><p>
      When deploying inside VMs, it is particularly crucial to ensure that the
      nodes have strong network connectivity and well working time
      synchronization.
     </p></li><li class="listitem"><p>
      The hypervisor nodes must be adequately sized to avoid interference by
      other workloads consuming CPU, RAM, network, and storage resources.
     </p></li></ul></div><div class="figure" id="id-1.4.3.3.7.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/minimal-ses.png" target="_blank"><img src="images/minimal-ses.png" width="" alt="Minimum Cluster Configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.2: </span><span class="title-name">Minimum Cluster Configuration </span><a title="Permalink" class="permalink" href="storage-bp-hwreq.html#id-1.4.3.3.7.3.6">#</a></h6></div></div></section><section class="sect2" id="ses-bp-production-cluster" data-id-title="Recommended Production Cluster Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.2 </span><span class="title-name">Recommended Production Cluster Configuration</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-bp-production-cluster">#</a></h3></div></div></div><p>
    Once you grow your cluster, we recommend to relocate monitors, Metadata Server, and
    gateways on separate nodes to ensure better fault tolerance.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Seven Object Storage Nodes
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        No single node exceeds ~15% of total storage.
       </p></li><li class="listitem"><p>
        The total capacity of the cluster should be sized so that even with one
        node unavailable, the total used capacity (including redundancy) does
        not exceed 80%.
       </p></li><li class="listitem"><p>
        25 Gb Ethernet or better, bonded for internal cluster and external
        public network each.
       </p></li><li class="listitem"><p>
        56+ OSDs per storage cluster.
       </p></li><li class="listitem"><p>
        See <a class="xref" href="storage-bp-hwreq.html#sysreq-osd" title="2.4.1. Minimum Requirements">Section 2.4.1, “Minimum Requirements”</a> for further recommendation.
       </p></li></ul></div></li><li class="listitem"><p>
      Dedicated physical infrastructure nodes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Three Ceph Monitor nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="storage-bp-hwreq.html#sysreq-mon" title="2.5. Monitor Nodes">Section 2.5, “Monitor Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Object Gateway nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="storage-bp-hwreq.html#sysreq-rgw" title="2.6. Object Gateway Nodes">Section 2.6, “Object Gateway Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        iSCSI Gateway nodes: 16 GB RAM, 6-8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="storage-bp-hwreq.html#sysreq-iscsi" title="2.9. iSCSI Nodes">Section 2.9, “iSCSI Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Metadata Server nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
        RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="storage-bp-hwreq.html#sysreq-mds" title="2.7. Metadata Server Nodes">Section 2.7, “Metadata Server Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        One SES Admin Node: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p></li></ul></div></li></ul></div></section></section><section class="sect1" id="deployment-osd-recommendation" data-id-title="Object Storage Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#deployment-osd-recommendation">#</a></h2></div></div></div><section class="sect2" id="sysreq-osd" data-id-title="Minimum Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.1 </span><span class="title-name">Minimum Requirements</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-osd">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The following CPU recommendations account for devices independent of
      usage by Ceph:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        1x 2GHz CPU Thread per spinner.
       </p></li><li class="listitem"><p>
        2x 2GHz CPU Thread per SSD.
       </p></li><li class="listitem"><p>
        4x 2GHz CPU Thread per NVMe.
       </p></li></ul></div></li><li class="listitem"><p>
      Separate 10 GbE networks (public/client and internal), required 4x 10
      GbE, recommended 2x 25 GbE.
     </p></li><li class="listitem"><p>
      Total RAM required = number of OSDs x (1 GB +
      <code class="option">osd_memory_target</code>) + 16 GB
     </p><p>
      The default for <code class="option">osd_memory_target</code> is 4 GB. Refer to
      <span class="intraxref">Book “Administration Guide”, Chapter 25 “Ceph Cluster Configuration”, Section 25.2.1 “Automatic Cache Sizing”</span> for more details on
      <code class="option">osd_memory_target</code>.
     </p></li><li class="listitem"><p>
      OSD disks in JBOD configurations or or individual RAID-0 configurations.
     </p></li><li class="listitem"><p>
      OSD journal can reside on OSD disk.
     </p></li><li class="listitem"><p>
      OSD disks should be exclusively used by SUSE Enterprise Storage.
     </p></li><li class="listitem"><p>
      Dedicated disk and SSD for the operating system, preferably in a RAID 1
      configuration.
     </p></li><li class="listitem"><p>
      Allocate at least an additional 4 GB of RAM if this OSD host will host
      part of a cache pool used for cache tiering.
     </p></li><li class="listitem"><p>
      Ceph Monitors, gateway and Metadata Servers can reside on Object Storage Nodes.
     </p></li><li class="listitem"><p>
      For disk performance reasons, OSD nodes are bare metal nodes. No other
      workloads should run on an OSD node unless it is a minimal setup of
      Ceph Monitors and Ceph Managers.
     </p></li><li class="listitem"><p>
      SSDs for Journal with 6:1 ratio SSD journal to OSD.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-mindisk" data-id-title="Minimum Disk Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.2 </span><span class="title-name">Minimum Disk Size</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-bp-mindisk">#</a></h3></div></div></div><p>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal (for FileStore) or WAL/DB device (for BlueStore), and the
    primary space for the stored data. The minimum (and default) value for the
    journal/WAL/DB is 6 GB. The minimum space for data is 5 GB, as partitions
    smaller than 5 GB are automatically assigned the weight of 0.
   </p><p>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </p></section><section class="sect2" id="rec-waldb-size" data-id-title="Recommended Size for the BlueStores WAL and DB Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.3 </span><span class="title-name">Recommended Size for the BlueStore's WAL and DB Device</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#rec-waldb-size">#</a></h3></div></div></div><div id="id-1.4.3.3.8.4.2" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
     Refer to <a class="xref" href="cha-storage-about.html#about-bluestore" title="1.4. BlueStore">Section 1.4, “BlueStore”</a> for more information on
     BlueStore.
    </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      We recommend reserving 4 GB for the WAL device. While the minimal DB
      size is 64 GB for RBD-only workloads, the recommended DB size for
      Object Gateway and CephFS workloads is 2% of the main device capacity (but at
      least 196 GB).
     </p></li><li class="listitem"><p>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows Ceph to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as Ceph uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not used fully then its
      space is not wasted but used for DB operation.
     </p><p>
      To share the DB device with the WAL, do <span class="emphasis"><em>not</em></span> specify
      the WAL device, and specify only the DB device.
     </p><p>
      Find more information about specifying an OSD layout in
      <a class="xref" href="ceph-install-saltstack.html#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a>.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-share-ssd-journal" data-id-title="Using SSD for OSD Journals"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.4 </span><span class="title-name">Using SSD for OSD Journals</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-bp-share-ssd-journal">#</a></h3></div></div></div><p>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </p><p>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk.
   </p><div id="id-1.4.3.3.8.5.4" data-id-title="Sharing an SSD for Multiple Journals" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing an SSD for Multiple Journals</h6><p>
     As journal data occupies relatively little space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than six journals on the same SSD disk and 12 on
     NVMe disks.
    </p></div></section><section class="sect2" id="maximum-count-of-disks-osd" data-id-title="Maximum Recommended Number of Disks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.5 </span><span class="title-name">Maximum Recommended Number of Disks</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#maximum-count-of-disks-osd">#</a></h3></div></div></div><p>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Network bandwidth.</em></span> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Memory.</em></span> RAM above 2 GB is used for the
      BlueStore cache. With the default <code class="option">osd_memory_target</code> of
      4 GB, the system has a reasonable starting cache size for spinning
      media. If using SSD or NVME, consider increasing the cache size and RAM
      allocation per OSD to maximize performance.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Fault tolerance.</em></span> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </p></li></ul></div></section></section><section class="sect1" id="sysreq-mon" data-id-title="Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Monitor Nodes</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-mon">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     At least three Ceph Monitor nodes are required. The number of monitors should
     always be odd (1+2n).
    </p></li><li class="listitem"><p>
     At least 4 GB of RAM. For large clusters, provide 5-10 GB of RAM.
    </p></li><li class="listitem"><p>
     Processor with four logical cores.
    </p></li><li class="listitem"><p>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <code class="filename">/var/lib/ceph</code> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </p></li><li class="listitem"><p>
     There must only be one monitor process per node.
    </p></li><li class="listitem"><p>
     Mixing OSD, monitor, or Object Gateway nodes is only supported if sufficient
     hardware resources are available. That means that the requirements for all
     services need to be added up.
    </p></li><li class="listitem"><p>
     Two network interfaces bonded to multiple switches.
    </p></li></ul></div></section><section class="sect1" id="sysreq-rgw" data-id-title="Object Gateway Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Object Gateway Nodes</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-rgw">#</a></h2></div></div></div><p>
   Object Gateway nodes should have six to eight CPU cores and 32 GB of RAM (64 GB
   recommended). When other processes are co-located on the same machine, their
   requirements need to be added up.
  </p></section><section class="sect1" id="sysreq-mds" data-id-title="Metadata Server Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Metadata Server Nodes</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-mds">#</a></h2></div></div></div><p>
   Proper sizing of the Metadata Server nodes depends on the specific use case.
   Generally, the more open files the Metadata Server is to handle, the more CPU and RAM
   it needs. The following are the minimum requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     3 GB of RAM for each Metadata Server daemon.
    </p></li><li class="listitem"><p>
     Bonded network interface.
    </p></li><li class="listitem"><p>
     2.5 GHz CPU with at least 2 cores.
    </p></li></ul></div></section><section class="sect1" id="sysreq-admin-node" data-id-title="Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-admin-node">#</a></h2></div></div></div><p>
   At least 4 GB of RAM and a quad-core CPU are required. This includes running
   the Salt master on the Admin Node. For large clusters with hundreds of nodes, 6 GB
   of RAM is suggested.
  </p></section><section class="sect1" id="sysreq-iscsi" data-id-title="iSCSI Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">iSCSI Nodes</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-iscsi">#</a></h2></div></div></div><p>
   iSCSI nodes should have six to eight CPU cores and 16 GB of RAM.
  </p></section><section class="sect1" id="req-ses-other" data-id-title="SUSE Enterprise Storage 6 and Other SUSE Products"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">SUSE Enterprise Storage 6 and Other SUSE Products</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#req-ses-other">#</a></h2></div></div></div><p>
   This section contains important information about integrating SUSE Enterprise Storage
   6 with other SUSE products.
  </p><section class="sect2" id="req-ses-suma" data-id-title="SUSE Manager"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.10.1 </span><span class="title-name">SUSE Manager</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#req-ses-suma">#</a></h3></div></div></div><p>
    SUSE Manager and SUSE Enterprise Storage are not integrated, therefore SUSE Manager cannot
    currently manage a SUSE Enterprise Storage cluster.
   </p></section></section><section class="sect1" id="sysreq-naming" data-id-title="Naming Limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Naming Limitations</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#sysreq-naming">#</a></h2></div></div></div><p>
   Ceph does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a Ceph
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all Ceph
   object/configuration names.
  </p></section><section class="sect1" id="ses-bp-diskshare" data-id-title="OSD and Monitor Sharing One Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">OSD and Monitor Sharing One Server</span> <a title="Permalink" class="permalink" href="storage-bp-hwreq.html#ses-bp-diskshare">#</a></h2></div></div></div><p>
   Although it is technically possible to run Ceph OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance—the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </p><p>
   Another consideration is whether to share disks between an OSD, a monitor
   node, and the operating system on the server. The answer is simple: if
   possible, dedicate a separate disk to OSD, and a separate server to a
   monitor node.
  </p><p>
   Although Ceph supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </p><div id="id-1.4.3.3.16.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If it is <span class="emphasis"><em>really</em></span> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <code class="filename">/var/lib/ceph/mon</code> directory for slightly better
    performance.
   </p></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-storage-about.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 1 </span>SUSE Enterprise Storage 6 and Ceph</span></a> </div><div><a class="pagination-link next" href="cha-admin-ha.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 3 </span>Admin Node HA Setup</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="storage-bp-hwreq.html#network-overview"><span class="title-number">2.1 </span><span class="title-name">Network Overview</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#multi-architecture"><span class="title-number">2.2 </span><span class="title-name">Multiple Architecture Configurations</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#ses-hardware-config"><span class="title-number">2.3 </span><span class="title-name">Hardware Configuration</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#deployment-osd-recommendation"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-mon"><span class="title-number">2.5 </span><span class="title-name">Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-rgw"><span class="title-number">2.6 </span><span class="title-name">Object Gateway Nodes</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-mds"><span class="title-number">2.7 </span><span class="title-name">Metadata Server Nodes</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-admin-node"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-iscsi"><span class="title-number">2.9 </span><span class="title-name">iSCSI Nodes</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#req-ses-other"><span class="title-number">2.10 </span><span class="title-name">SUSE Enterprise Storage 6 and Other SUSE Products</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#sysreq-naming"><span class="title-number">2.11 </span><span class="title-name">Naming Limitations</span></a></span></li><li><span class="sect1"><a href="storage-bp-hwreq.html#ses-bp-diskshare"><span class="title-number">2.12 </span><span class="title-name">OSD and Monitor Sharing One Server</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/deployment_hwrecommend.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>