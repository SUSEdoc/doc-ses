<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Deploying with DeepSea/Salt | Deployment Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deploying with DeepSea/Salt | SES 6"/>
<meta name="description" content="Salt along with DeepSea is a stack of components that help you deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get r…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="chapter-title" content="Chapter 5. Deploying with DeepSea/Salt"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Deploying with DeepSea/Salt | SES 6"/>
<meta property="og:description" content="Salt along with DeepSea is a stack of components that help you deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get r…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deploying with DeepSea/Salt | SES 6"/>
<meta name="twitter:description" content="Salt along with DeepSea is a stack of components that help you deploy and manage server infrastructure. It is very scalable, fast, and relatively easy to get r…"/>
<link rel="prev" href="ses-deployment.html" title="Part II. Cluster Deployment and Upgrade"/><link rel="next" href="cha-ceph-upgrade.html" title="Chapter 6. Upgrading from Previous Releases"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deployment Guide</a><span> / </span><a class="crumb" href="ses-deployment.html">Cluster Deployment and Upgrade</a><span> / </span><a class="crumb" href="ceph-install-saltstack.html">Deploying with DeepSea/Salt</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Deployment Guide</div><ol><li><a href="bk02pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-ses.html" class="has-children "><span class="title-number">I </span><span class="title-name">SUSE Enterprise Storage</span></a><ol><li><a href="cha-storage-about.html" class=" "><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 6 and Ceph</span></a></li><li><a href="storage-bp-hwreq.html" class=" "><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></li><li><a href="cha-admin-ha.html" class=" "><span class="title-number">3 </span><span class="title-name">Admin Node HA Setup</span></a></li><li><a href="bk02pt01ch04.html" class=" "><span class="title-number">4 </span><span class="title-name">User Privileges and Command Prompts</span></a></li></ol></li><li class="active"><a href="ses-deployment.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">Cluster Deployment and Upgrade</span></a><ol><li><a href="ceph-install-saltstack.html" class=" you-are-here"><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></li><li><a href="cha-ceph-upgrade.html" class=" "><span class="title-number">6 </span><span class="title-name">Upgrading from Previous Releases</span></a></li><li><a href="ceph-deploy-ds-custom.html" class=" "><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></li></ol></li><li><a href="additional-software.html" class="has-children "><span class="title-number">III </span><span class="title-name">Installation of Additional Services</span></a><ol><li><a href="cha-ceph-as-intro.html" class=" "><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></li><li><a href="cha-ceph-additional-software-installation.html" class=" "><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-as-iscsi.html" class=" "><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></li><li><a href="cha-ceph-as-cephfs.html" class=" "><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></li><li><a href="cha-as-ganesha.html" class=" "><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></li></ol></li><li><a href="containerized-ses-on-caasp.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Cluster Deployment on Top of SUSE CaaS Platform 4 (Technology Preview)</span></a><ol><li><a href="cha-container-kubernetes.html" class=" "><span class="title-number">13 </span><span class="title-name">SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</span></a></li></ol></li><li><a href="bk02apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk02go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-deploy-docupdate.html" class=" "><span class="title-number">B </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="ceph-install-saltstack" data-id-title="Deploying with DeepSea/Salt"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#">#</a></h2></div></div></div><p>
  Salt along with DeepSea is a <span class="emphasis"><em>stack</em></span> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with Salt:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Salt minions</em></span> are the nodes controlled by a dedicated
    node called Salt master. Salt minions have roles, for example Ceph OSD, Ceph Monitor,
    Ceph Manager, Object Gateway, iSCSI Gateway, or NFS Ganesha.
   </p></li><li class="listitem"><p>
    A Salt master runs its own Salt minion. It is required for running privileged
    tasks—for example creating, authorizing, and copying keys to
    minions—so that remote minions never need to run privileged tasks.
   </p><div id="id-1.4.4.2.4.2.2" data-id-title="Sharing Multiple Roles per Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing Multiple Roles per Server</h6><p>
     You will get the best performance from your Ceph cluster when each role
     is deployed on a separate node. But real deployments sometimes require
     sharing one node for multiple roles. To avoid trouble with performance and
     the upgrade procedure, do not deploy the Ceph OSD, Metadata Server, or Ceph Monitor role to
     the Admin Node.
    </p></div></li><li class="listitem"><p>
    Salt minions need to correctly resolve the Salt master's host name over the
    network. By default, they look for the <code class="systemitem">salt</code> host
    name, but you can specify any other network-reachable host name in the
    <code class="filename">/etc/salt/minion</code> file, see
    <a class="xref" href="ceph-install-saltstack.html#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>.
   </p></li></ul></div><section class="sect1" id="cha-ceph-install-relnotes" data-id-title="Read the Release Notes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#cha-ceph-install-relnotes">#</a></h2></div></div></div><p>
   In the release notes you can find additional information on changes since
   the previous release of SUSE Enterprise Storage. Check the release notes to see
   whether:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     your hardware needs special considerations.
    </p></li><li class="listitem"><p>
     any used software packages have changed significantly.
    </p></li><li class="listitem"><p>
     special precautions are necessary for your installation.
    </p></li></ul></div><p>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </p><p>
   After having installed the package <span class="package">release-notes-ses</span>,
   find the release notes locally in the directory
   <code class="filename">/usr/share/doc/release-notes</code> or online at
   <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section><section class="sect1" id="deepsea-description" data-id-title="Introduction to DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Introduction to DeepSea</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-description">#</a></h2></div></div></div><p>
   The goal of DeepSea is to save the administrator time and confidently
   perform complex operations on a Ceph cluster.
  </p><p>
   Ceph is a very configurable software solution. It increases both the
   freedom and responsibility of system administrators.
  </p><p>
   The minimal Ceph setup is good for demonstration purposes, but does not
   show interesting features of Ceph that you can see with a big number of
   nodes.
  </p><p>
   DeepSea collects and stores data about individual servers, such as
   addresses and device names. For a distributed storage system such as Ceph,
   there can be hundreds of such items to collect and store. Collecting the
   information and entering the data manually into a configuration management
   tool is exhausting and error prone.
  </p><p>
   The steps necessary to prepare the servers, collect the configuration, and
   configure and deploy Ceph are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </p><p>
   DeepSea addresses these observations with the following strategy:
   DeepSea consolidates the administrator's decisions in a single file. The
   decisions include cluster assignment, role assignment and profile
   assignment. And DeepSea collects each set of tasks into a simple goal.
   Each goal is a <span class="emphasis"><em>stage</em></span>:
  </p><div class="itemizedlist" id="deepsea-stage-description" data-id-title="DeepSea Stages Description"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea Stages Description </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-stage-description">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Stage 0</strong></span>—the
     <span class="bold"><strong>preparation</strong></span>— during this stage, all
     required updates are applied and your system may be rebooted.
    </p><div id="id-1.4.4.2.6.8.2.2" data-id-title="Re-run Stage 0 after the Admin Node Reboot" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Re-run Stage 0 after the Admin Node Reboot</h6><p>
      If the Admin Node reboots during stage 0 to load the new kernel version, you
      need to run stage 0 again, otherwise minions will not be targeted.
     </p></div></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 1</strong></span>—the
     <span class="bold"><strong>discovery</strong></span>—here all hardware in your
     cluster is being detected and necessary information for the Ceph
     configuration is being collected. For details about configuration, refer
     to <a class="xref" href="ceph-install-saltstack.html#deepsea-pillar-salt-configuration" title="5.5. Configuration and Customization">Section 5.5, “Configuration and Customization”</a>.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 2</strong></span>—the
     <span class="bold"><strong>configuration</strong></span>—you need to prepare
     configuration data in a particular format.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 3</strong></span>—the
     <span class="bold"><strong>deployment</strong></span>—creates a basic Ceph
     cluster with mandatory Ceph services. See
     <a class="xref" href="cha-storage-about.html#storage-intro-core-nodes" title="1.2.3. Ceph Nodes and Daemons">Section 1.2.3, “Ceph Nodes and Daemons”</a> for their list.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 4</strong></span>—the
     <span class="bold"><strong>services</strong></span>—additional features of
     Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each
     is optional.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 5</strong></span>—the removal stage. This
     stage is not mandatory and during the initial setup it is usually not
     needed. In this stage the roles of minions and also the cluster
     configuration are removed. You need to run this stage when you need to
     remove a storage node from your cluster. For details refer to
     <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.3 “Removing and Reinstalling Cluster Nodes”</span>.
    </p></li></ul></div><section class="sect2" id="deepsea-organisation-locations" data-id-title="Organization and Important Locations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.1 </span><span class="title-name">Organization and Important Locations</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-organisation-locations">#</a></h3></div></div></div><p>
    Salt has several standard locations and several naming conventions used
    on your master node:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.6.9.3.1"><span class="term"><code class="filename">/srv/pillar</code></span></dt><dd><p>
       The directory stores configuration data for your cluster minions.
       <span class="emphasis"><em>Pillar</em></span> is an interface for providing global
       configuration values to all your cluster minions.
      </p></dd><dt id="id-1.4.4.2.6.9.3.2"><span class="term"><code class="filename">/srv/salt/</code></span></dt><dd><p>
       The directory stores Salt state files (also called
       <span class="emphasis"><em>sls</em></span> files). State files are formatted descriptions
       of states in which the cluster should be.
       
      </p></dd><dt id="id-1.4.4.2.6.9.3.3"><span class="term"><code class="filename">/srv/module/runners</code></span></dt><dd><p>
       The directory stores Python scripts known as runners. Runners are
       executed on the master node.
      </p></dd><dt id="id-1.4.4.2.6.9.3.4"><span class="term"><code class="filename">/srv/salt/_modules</code></span></dt><dd><p>
       The directory stores Python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </p></dd><dt id="id-1.4.4.2.6.9.3.5"><span class="term"><code class="filename">/srv/pillar/ceph</code></span></dt><dd><p>
       The directory is used by DeepSea. Collected configuration data are
       stored here.
      </p></dd><dt id="id-1.4.4.2.6.9.3.6"><span class="term"><code class="filename">/srv/salt/ceph</code></span></dt><dd><p>
       A directory used by DeepSea. It stores sls files that can be in
       different formats, but each subdirectory contains sls files. Each
       subdirectory contains only one type of sls file. For example,
       <code class="filename">/srv/salt/ceph/stage</code> contains orchestration files
       that are executed by <code class="command">salt-run state.orchestrate</code>.
      </p></dd></dl></div></section><section class="sect2" id="ds-minion-targeting" data-id-title="Targeting the Minions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.2 </span><span class="title-name">Targeting the Minions</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-minion-targeting">#</a></h3></div></div></div><p>
    DeepSea commands are executed via the Salt infrastructure. When using
    the <code class="command">salt</code> command, you need to specify a set of
    Salt minions that the command will affect. We describe the set of the minions
    as a <span class="emphasis"><em>target</em></span> for the <code class="command">salt</code> command.
    The following sections describe possible methods to target the minions.
   </p><section class="sect3" id="ds-minion-targeting-name" data-id-title="Matching the Minion Name"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.1 </span><span class="title-name">Matching the Minion Name</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-minion-targeting-name">#</a></h4></div></div></div><p>
     You can target a minion or a group of minions by matching their names. A
     minion's name is usually the short host name of the node where the minion
     runs. This is a general Salt targeting method, not related to DeepSea.
     You can use globbing, regular expressions, or lists to limit the range of
     minion names. The general syntax follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> example.module</pre></div><div id="id-1.4.4.2.6.10.3.4" data-id-title="Ceph-only Cluster" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Ceph-only Cluster</h6><p>
      If all Salt minions in your environment belong to your Ceph cluster, you
      can safely substitute <em class="replaceable">target</em> with
      <code class="literal">'*'</code> to include <span class="emphasis"><em>all</em></span> registered
      minions.
     </p></div><p>
     Match all minions in the example.net domain (assuming the minion names are
     identical to their "full" host names):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div><p>
     Match the 'web1' to 'web5' minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'web[1-5]' test.ping</pre></div><p>
     Match both 'web1-prod' and 'web1-devel' minions using a regular
     expression:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -E 'web1-(prod|devel)' test.ping</pre></div><p>
     Match a simple list of minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -L 'web1,web2,web3' test.ping</pre></div><p>
     Match all minions in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-grain" data-id-title="Targeting with a DeepSea Grain"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.2 </span><span class="title-name">Targeting with a DeepSea Grain</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-minion-targeting-grain">#</a></h4></div></div></div><p>
     In a heterogeneous Salt-managed environment where SUSE Enterprise Storage
     6 is deployed on a subset of nodes alongside other cluster
     solutions, you need to mark the relevant minions by applying a 'deepsea'
     grain to them before running DeepSea stage 0. This way, you can easily
     target DeepSea minions in environments where matching by the minion name
     is problematic.
    </p><p>
     To apply the 'deepsea' grain to a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.append deepsea default</pre></div><p>
     To remove the 'deepsea' grain from a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.delval deepsea destructive=True</pre></div><p>
     After applying the 'deepsea' grain to the relevant minions, you can target
     them as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -G 'deepsea:*' test.ping</pre></div><p>
     The following command is an equivalent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'G@deepsea:*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-dsminions" data-id-title="Set the deepsea_minions Option"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.3 </span><span class="title-name">Set the <code class="option">deepsea_minions</code> Option</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-minion-targeting-dsminions">#</a></h4></div></div></div><p>
     Setting the <code class="option">deepsea_minions</code> option's target is a
     requirement for DeepSea deployments. DeepSea uses it to instruct
     minions during the execution of stages (refer to
     <a class="xref" href="ceph-install-saltstack.html#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> for details.
    </p><p>
     To set or change the <code class="option">deepsea_minions</code> option, edit the
     <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code> file on the
     Salt master and add or replace the following line:
    </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: <em class="replaceable">target</em></pre></div><div id="id-1.4.4.2.6.10.5.5" data-id-title="deepsea_minions Target" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="option">deepsea_minions</code> Target</h6><p>
      As the <em class="replaceable">target</em> for the
      <code class="option">deepsea_minions</code> option, you can use any targeting
      method: both
      <a class="xref" href="ceph-install-saltstack.html#ds-minion-targeting-name" title="5.2.2.1. Matching the Minion Name">Matching the Minion Name</a> and
      <a class="xref" href="ceph-install-saltstack.html#ds-minion-targeting-grain" title="5.2.2.2. Targeting with a DeepSea Grain">Targeting with a DeepSea Grain</a>.
     </p><p>
      Match all Salt minions in the cluster:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: '*'</pre></div><p>
      Match all minions with the 'deepsea' grain:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: 'G@deepsea:*'</pre></div></div></section><section class="sect3" id="id-1.4.4.2.6.10.6" data-id-title="For More Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.4 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.6.10.6">#</a></h4></div></div></div><p>
     You can use more advanced ways to target minions using the Salt
     infrastructure. The 'deepsea-minions' manual page gives you more details
     about DeepSea targeting (<code class="command">man 7 deepsea_minions</code>).
    </p></section></section></section><section class="sect1" id="ceph-install-stack" data-id-title="Cluster Deployment"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.3 </span><span class="title-name">Cluster Deployment</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ceph-install-stack">#</a></h2></div></div></div><p>
   The cluster deployment process has several phases. First, you need to
   prepare all nodes of the cluster by configuring Salt and then deploy and
   configure Ceph.
  </p><div id="dev-env" data-id-title="Deploying Monitor Nodes without Defining OSD Profiles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Deploying Monitor Nodes without Defining OSD Profiles</h6><p>
    If you need to skip defining storage roles for OSD as described in
    <a class="xref" href="ceph-install-saltstack.html#policy-role-assignment" title="5.5.1.2. Role Assignment">Section 5.5.1.2, “Role Assignment”</a> and deploy Ceph Monitor nodes first, you
    can do so by setting the <code class="option">DEV_ENV</code> variable.
   </p><p>
    This allows deploying monitors without the presence of the
    <code class="filename">role-storage/</code> directory, as well as deploying a Ceph
    cluster with at least <span class="emphasis"><em>one</em></span> storage, monitor, and
    manager role.
   </p><p>
    To set the environment variable, either enable it globally by setting it in
    the <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file, or set it
    for the current shell session only:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>export DEV_ENV=true</pre></div><p>
    As an example, <code class="filename">/srv/pillar/ceph/stack/global.yml</code> can
    be created with the following contents:
   </p><div class="verbatim-wrap"><pre class="screen">DEV_ENV: <em class="replaceable">True</em></pre></div></div><p>
   The following procedure describes the cluster preparation in detail.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install and register SUSE Linux Enterprise Server 15 SP1 together with the SUSE Enterprise Storage
     6 extension on each node of the cluster.
    </p></li><li class="step"><p>
     Verify that proper products are installed and registered by listing
     existing software repositories. Run <code class="command">zypper lr -E</code> and
     compare the output with the following list:
    </p><div class="verbatim-wrap"><pre class="screen"> SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates</pre></div></li><li class="step"><p>
     Configure network settings including proper DNS name resolution on each
     node. The Salt master and all the Salt minions need to resolve each other by
     their host names. For more information on configuring a network, see
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-yast" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-yast</a>
     For more information on configuring a DNS server, see
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html</a>.
    </p><div id="id-1.4.4.2.7.5.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If cluster nodes are configured for multiple networks, DeepSea will use
      the network to which their host names (or FQDN) resolves. Consider the
      following example <code class="filename">/etc/hosts</code>:
     </p><div class="verbatim-wrap"><pre class="screen">192.168.100.1 ses1.example.com       ses1
172.16.100.1  ses1clus.cluster.lan   ses1clus</pre></div><p>
      In the above example, the <code class="literal">ses1</code> minion will resolve to
      the <code class="literal">192.168.100.x</code> network and DeepSea will use this
      network as the <span class="emphasis"><em>public network</em></span>. If the desired public
      network is <code class="literal">172.16.100.x</code>, then host name should be
      changed to <code class="literal">ses1clus</code>.
     </p></div></li><li class="step"><p>
     Install the <code class="literal">salt-master</code> and
     <code class="literal">salt-minion</code> packages on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in salt-master salt-minion</pre></div><p>
     Check that the <code class="systemitem">salt-master</code> service is enabled and
     started, and enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl enable salt-master.service
<code class="prompt user">root@master # </code>systemctl start salt-master.service</pre></div></li><li class="step"><p>
     If you intend to use firewall, verify that the Salt master node has ports
     4505 and 4506 open to all Salt minion nodes. If the ports are closed, you
     can open them using the <code class="command">yast2 firewall</code> command by
     allowing the <span class="guimenu">SaltStack</span> service.
    </p><div id="id-1.4.4.2.7.5.5.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
      DeepSea deployment stages fail when firewall is active (and even
      configured). To pass the stages correctly, you need to either turn the
      firewall off by running
     </p><div class="verbatim-wrap"><pre class="screen">    <code class="prompt user">root # </code>systemctl stop firewalld.service</pre></div><p>
      or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div></li><li class="step"><p>
     Install the package <code class="literal">salt-minion</code> on all minion nodes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div><p>
     Make sure that the <span class="emphasis"><em>fully qualified domain name</em></span> of
     each node can be resolved to the public network IP address by all other
     nodes.
    </p></li><li class="step"><p>
     Configure all minions (including the master minion) to connect to the
     master. If your Salt master is not reachable by the host name
     <code class="literal">salt</code>, edit the file
     <code class="filename">/etc/salt/minion</code> or create a new file
     <code class="filename">/etc/salt/minion.d/master.conf</code> with the following
     content:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">host_name_of_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the Salt service on all Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     Check that the <code class="systemitem">salt-minion</code> service is enabled and
     started on all nodes. Enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable salt-minion.service
<code class="prompt user">root # </code>systemctl start salt-minion.service</pre></div></li><li class="step"><p>
     Verify each Salt minion's fingerprint and accept all salt keys on the
     Salt master if the fingerprints match.
    </p><div id="id-1.4.4.2.7.5.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If the Salt minion fingerprint comes back empty, make sure the Salt minion
      has a Salt master configuration and it can communicate with the Salt master.
     </p></div><p>
     View each minion's fingerprint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     After gathering fingerprints of all the Salt minions, list fingerprints of
     all unaccepted minion keys on the Salt master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     If the minions' fingerprints match, accept them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept-all</pre></div></li><li class="step"><p>
     Verify that the keys have been accepted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --list-all</pre></div></li><li class="step"><p>
     By default, DeepSea uses the Admin Node as the time server for other cluster
     nodes. Therefore, if the Admin Node is not virtualized, select one or more time
     servers or pools, and synchronize the local time against them. Verify that
     the time synchronization service is enabled on each system start-up. Find
     more information on setting up time synchronization in
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html#sec-ntp-yast" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html#sec-ntp-yast</a>.
    </p><p>
     If the Admin Node is a virtual machine, provide better time sources for the
     cluster nodes by overriding the default NTP client configuration:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> on the
       Salt master node and add the following line:
      </p><div class="verbatim-wrap"><pre class="screen">time_server: <em class="replaceable">CUSTOM_NTP_SERVER</em></pre></div><p>
       To add multiple time servers, the format is as follows:
      </p><div class="verbatim-wrap"><pre class="screen">time_server:
  - <em class="replaceable">CUSTOM_NTP_SERVER1</em>
  - <em class="replaceable">CUSTOM_NTP_SERVER2</em>
  - <em class="replaceable">CUSTOM_NTP_SERVER3</em>
[...]</pre></div></li><li class="listitem"><p>
       Refresh the Salt pillar:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="listitem"><p>
       Verify the changed value:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' pillar.items</pre></div></li><li class="listitem"><p>
       Apply the new setting:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' state.apply ceph.time</pre></div></li></ol></div></li><li class="step" id="deploy-wiping-disk"><p>
     Prior to deploying SUSE Enterprise Storage 6, manually zap all the
     disks. Remember to replace 'X' with the correct disk letter:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Stop all processes that are using the specific disk.
      </p></li><li class="step"><p>
       Verify whether any partition on the disk is mounted, and unmount if
       needed.
      </p></li><li class="step"><p>
       If the disk is managed by LVM, deactivate and delete the whole LVM
       infrastructure. Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-lvm.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-lvm.html</a>
       for more details.
      </p></li><li class="step"><p>
       If the disk is part of MD RAID, deactivate the RAID. Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-software-raid.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-software-raid.html</a>
       for more details.
      </p></li><li class="step"><div id="id-1.4.4.2.7.5.12.2.5.1" data-id-title="Rebooting the Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Rebooting the Server</h6><p>
        If you get error messages such as 'partition in use' or 'kernel cannot
        be updated with the new partition table' during the following steps,
        reboot the server.
       </p></div><p>
       Wipe data and partitions on the disk:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-volume lvm zap /dev/sdX --destroy</pre></div></li><li class="step"><p>
       Verify that the drive is empty (with no GPT structures) using:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>parted -s /dev/sdX print free</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C</pre></div></li></ol></li><li class="step"><p>
     Optionally, if you need to preconfigure the cluster's network settings
     before the <span class="package">deepsea</span> package is installed, create
     <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> manually and
     set the <code class="option">cluster_network:</code> and
     <code class="option">public_network:</code> options. Note that the file will not be
     overwritten after you install <span class="package">deepsea</span>. Then, run:
    </p><div class="verbatim-wrap"><pre class="screen">chown -R salt:salt /srv/pillar/ceph/stack</pre></div><div id="id-1.4.4.2.7.5.13.3" data-id-title="Enabling IPv6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Enabling IPv6</h6><p>
      If you need to enable IPv6 network addressing, refer to
      <a class="xref" href="ceph-deploy-ds-custom.html#ds-modify-ipv6" title="7.2.1. Enabling IPv6 for Ceph Cluster Deployment">Section 7.2.1, “Enabling IPv6 for Ceph Cluster Deployment”</a>
     </p></div></li><li class="step"><p>
     Install DeepSea on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in deepsea</pre></div></li><li class="step"><p>
     The value of the <code class="option">master_minion</code> parameter is dynamically
     derived from the <code class="filename">/etc/salt/minion_id</code> file on the
     Salt master. If you need to override the discovered value, edit the file
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and set a relevant
     value:
    </p><div class="verbatim-wrap"><pre class="screen">master_minion: <em class="replaceable">MASTER_MINION_NAME</em></pre></div><p>
     If your Salt master is reachable via more host names, use the Salt minion name
     for the storage cluster as returned by the <code class="command">salt-key -L</code>
     command. If you used the default host name for your
     Salt master—<span class="emphasis"><em>salt</em></span>—in the
     <span class="emphasis"><em>ses</em></span> domain, then the file looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">master_minion: salt.ses</pre></div></li></ol></div></div><p>
   Now you deploy and configure Ceph. Unless specified otherwise, all steps
   are mandatory.
  </p><div id="id-1.4.4.2.7.7" data-id-title="Salt Command Conventions" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Salt Command Conventions</h6><p>
    There are two possible ways to run <code class="command">salt-run
    state.orch</code>—one is with
    'stage.<em class="replaceable">STAGE_NUMBER</em>', the other is with the name
    of the stage. Both notations have the same impact and it is fully your
    preference which command you use.
   </p></div><div class="procedure" id="ds-depl-stages" data-id-title="Running Deployment Stages"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 5.1: </span><span class="title-name">Running Deployment Stages </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-depl-stages">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensure the Salt minions belonging to the Ceph cluster are correctly
     targeted through the <code class="option">deepsea_minions</code> option in
     <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code>. Refer to
     <a class="xref" href="ceph-install-saltstack.html#ds-minion-targeting-dsminions" title="5.2.2.3. Set the deepsea_minions Option">Section 5.2.2.3, “Set the <code class="option">deepsea_minions</code> Option”</a> for more information.
    </p></li><li class="step"><p>
     By default, DeepSea deploys Ceph clusters with tuned profiles active
     on Ceph Monitor, Ceph Manager, and Ceph OSD nodes. In some cases, you may need to deploy
     without tuned profiles. To do so, put the following lines in
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> before running
     DeepSea stages:
    </p><div class="verbatim-wrap"><pre class="screen">alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off</pre></div></li><li class="step"><p>
     <span class="emphasis"><em>Optional</em></span>: create Btrfs sub-volumes for
     <code class="filename">/var/lib/ceph/</code>. This step needs to be executed before
     DeepSea stage.0. To migrate existing directories or for more details,
     see <span class="intraxref">Book “Administration Guide”, Chapter 33 “Hints and Tips”, Section 33.6 “Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes”</span>.
    </p><p>
     Apply the following commands to each of the Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'MONITOR_NODES' saltutil.sync_all
<code class="prompt user">root@master # </code>salt 'MONITOR_NODES' state.apply ceph.subvolume</pre></div><div id="id-1.4.4.2.7.8.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The Ceph.subvolume command creates <code class="filename">/var/lib/ceph</code>
      as a <code class="filename">@/var/lib/ceph</code> Btrfs subvolume.
     </p></div><p>
     The new subvolume is now mounted and <code class="literal">/etc/fstab</code> is
     updated.
    </p></li><li class="step"><p>
     Prepare your cluster. Refer to <a class="xref" href="ceph-install-saltstack.html#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>
     for more details.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.prep</pre></div><div id="id-1.4.4.2.7.8.5.5" data-id-title="Run or Monitor Stages using DeepSea CLI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Run or Monitor Stages using DeepSea CLI</h6><p>
      Using the DeepSea CLI, you can follow the stage execution progress in
      real-time, either by running the DeepSea CLI in the monitoring mode, or
      by running the stage directly through DeepSea CLI. For details refer to
      <a class="xref" href="ceph-install-saltstack.html#deepsea-cli" title="5.4. DeepSea CLI">Section 5.4, “DeepSea CLI”</a>.
     </p></div></li><li class="step"><p>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <code class="filename">/srv/pillar/ceph/proposals</code>. The data are stored in
     the YAML format in *.sls or *.yml files.
    </p><p>
     Run the following command to trigger the discovery stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.discovery</pre></div></li><li class="step"><p>
     After the previous command finishes successfully, create a
     <code class="filename">policy.cfg</code> file in
     <code class="filename">/srv/pillar/ceph/proposals</code>. For details refer to
     <a class="xref" href="ceph-install-saltstack.html#policy-configuration" title="5.5.1. The policy.cfg File">Section 5.5.1, “The <code class="filename">policy.cfg</code> File”</a>.
    </p><div id="id-1.4.4.2.7.8.7.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you need to change the cluster's network setting, edit
      <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> and adjust
      the lines starting with <code class="literal">cluster_network:</code> and
      <code class="literal">public_network:</code>.
     </p></div></li><li class="step"><p>
     The configuration stage parses the <code class="filename">policy.cfg</code> file
     and merges the included files into their final form. Cluster and role
     related content are placed in
     <code class="filename">/srv/pillar/ceph/cluster</code>, while Ceph specific
     content is placed in <code class="filename">/srv/pillar/ceph/stack/default</code>.
    </p><p>
     Run the following command to trigger the configuration stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.configure</pre></div><p>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for the specified minions (for
     example, named <code class="literal">ceph_minion1</code>,
     <code class="literal">ceph_minion2</code>, etc.) by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'ceph_minion*' pillar.items</pre></div><div id="id-1.4.4.2.7.8.8.8" data-id-title="Modifying OSDs Layout" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Modifying OSD's Layout</h6><p>
      If you want to modify the default OSD's layout and change the drive
      groups configuration, follow the procedure described in
      <a class="xref" href="ceph-install-saltstack.html#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a>.
     </p></div><div id="id-1.4.4.2.7.8.8.9" data-id-title="Overwriting Defaults" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Overwriting Defaults</h6><p>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <a class="xref" href="ceph-deploy-ds-custom.html" title="Chapter 7. Customizing the Default Configuration">Chapter 7, <em>Customizing the Default Configuration</em></a>.
     </p></div></li><li class="step"><p>
     Now you run the deployment stage. In this stage, the pillar is validated,
     and the Ceph Monitor and Ceph OSD daemons are started:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.deploy</pre></div><p>
     The command may take several minutes. If it fails, you need to fix the
     issue and run the previous stages again. After the command succeeds, run
     the following to check the status:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -s</pre></div></li><li class="step"><p>
     The last step of the Ceph cluster deployment is the
     <span class="emphasis"><em>services</em></span> stage. Here you instantiate any of the
     currently supported services: iSCSI Gateway, CephFS, Object Gateway, and NFS Ganesha. In
     this stage, the necessary pools, authorizing keyrings, and starting
     services are created. To start the stage, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.services</pre></div><p>
     Depending on the setup, the command may run for several minutes.
    </p></li><li class="step"><p>
     <span class="emphasis"><em>Disable insecure clients</em></span>. Since Nautilus v14.2.20,
     a new health warning was introduced that informs you that insecure clients
     are allowed to join the cluster. This warning is <span class="emphasis"><em>on</em></span>
     by default. The Ceph Dashboard will show the cluster in the
     <code class="literal">HEALTH_WARN</code> status and verifying the cluster status on
     the command line informs you as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]</pre></div><p>
     This warning means that the Ceph Monitors are still allowing old, unpatched
     clients to connect to the cluster. This ensures existing clients can still
     connect while the cluster is being upgraded, but warns you that there is a
     problem that needs to be addressed. When the cluster and all clients are
     upgraded to the latest version of Ceph, disallow unpatched clients by
     running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></li><li class="step"><p>
     Before you continue, we strongly recommend enabling the Ceph telemetry
     module. For more information, see <span class="intraxref">Book “Administration Guide”, Chapter 21 “Ceph Manager Modules”, Section 21.2 “Telemetry Module”</span>
     for information and instructions.
    </p></li></ol></div></div></section><section class="sect1" id="deepsea-cli" data-id-title="DeepSea CLI"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.4 </span><span class="title-name">DeepSea CLI</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-cli">#</a></h2></div></div></div><p>
   DeepSea also provides a command line interface (CLI) tool that allows the
   user to monitor or run stages while visualizing the execution progress in
   real-time. Verify that the <span class="package">deepsea-cli</span> package is
   installed before you run the <code class="command">deepsea</code> executable.
  </p><p>
   Two modes are supported for visualizing a stage's execution progress:
  </p><div class="itemizedlist" id="deepsea-cli-modes" data-id-title="DeepSea CLI Modes"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea CLI Modes </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-cli-modes">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Monitoring mode</strong></span>: visualizes the execution
     progress of a DeepSea stage triggered by the <code class="command">salt-run</code>
     command issued in another terminal session.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stand-alone mode</strong></span>: runs a DeepSea stage
     while providing real-time visualization of its component steps as they are
     executed.
    </p></li></ul></div><div id="id-1.4.4.2.8.5" data-id-title="DeepSea CLI Commands" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: DeepSea CLI Commands</h6><p>
    The DeepSea CLI commands can only be run on the Salt master node with the
    <code class="systemitem">root</code> privileges.
   </p></div><section class="sect2" id="deepsea-cli-monitor" data-id-title="DeepSea CLI: Monitor Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.1 </span><span class="title-name">DeepSea CLI: Monitor Mode</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-cli-monitor">#</a></h3></div></div></div><p>
    The progress monitor provides a detailed, real-time visualization of what
    is happening during execution of stages using <code class="command">salt-run
    state.orch</code> commands in other terminal sessions.
   </p><div id="id-1.4.4.2.8.6.3" data-id-title="Start Monitor in a New Terminal Session" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Start Monitor in a New Terminal Session</h6><p>
     You need to start the monitor in a new terminal window
     <span class="emphasis"><em>before</em></span> running any <code class="command">salt-run
     state.orch</code> so that the monitor can detect the start of the
     stage's execution.
    </p></div><p>
    If you start the monitor after issuing the <code class="command">salt-run
    state.orch</code> command, then no execution progress will be shown.
   </p><p>
    You can start the monitor mode by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea monitor</pre></div><p>
    For more information about the available command line options of the
    <code class="command">deepsea monitor</code> command, check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>man deepsea-monitor</pre></div></section><section class="sect2" id="deepsea-cli-standalone" data-id-title="DeepSea CLI: Stand-alone Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.2 </span><span class="title-name">DeepSea CLI: Stand-alone Mode</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-cli-standalone">#</a></h3></div></div></div><p>
    In the stand-alone mode, DeepSea CLI can be used to run a DeepSea
    stage, showing its execution in real-time.
   </p><p>
    The command to run a DeepSea stage from the DeepSea CLI has the
    following form:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea stage run <em class="replaceable">stage-name</em></pre></div><p>
    where <em class="replaceable">stage-name</em> corresponds to the way Salt
    orchestration state files are referenced. For example, stage
    <span class="bold"><strong>deploy</strong></span>, which corresponds to the directory
    located in <code class="filename">/srv/salt/ceph/stage/deploy</code>, is referenced
    as <span class="bold"><strong>ceph.stage.deploy</strong></span>.
   </p><p>
    This command is an alternative to the Salt-based commands for running
    DeepSea stages (or any DeepSea orchestration state file).
   </p><p>
    The command <code class="command">deepsea stage run ceph.stage.0</code> is equivalent
    to <code class="command">salt-run state.orch ceph.stage.0</code>.
   </p><p>
    For more information about the available command line options accepted by
    the <code class="command">deepsea stage run</code> command, check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>man deepsea-stage run</pre></div><p>
    In the following figure shows an example of the output of the DeepSea CLI
    when running <span class="underline">Stage 2</span>:
   </p><div class="figure" id="id-1.4.4.2.8.7.11"><div class="figure-contents"><div class="mediaobject"><a href="images/deepsea-cli-stage2-screenshot.png" target="_blank"><img src="images/deepsea-cli-stage2-screenshot.png" width="" alt="DeepSea CLI stage execution progress output"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">DeepSea CLI stage execution progress output </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.8.7.11">#</a></h6></div></div><section class="sect3" id="deepsea-cli-run-alias" data-id-title="DeepSea CLI stage run Alias"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.4.2.1 </span><span class="title-name">DeepSea CLI <code class="command">stage run</code> Alias</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-cli-run-alias">#</a></h4></div></div></div><p>
     For advanced users of Salt, we also support an alias for running a
     DeepSea stage that takes the Salt command used to run a stage, for
     example, <code class="command">salt-run state.orch
     <em class="replaceable">stage-name</em></code>, as a command of the
     DeepSea CLI.
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea salt-run state.orch <em class="replaceable">stage-name</em></pre></div></section></section></section><section class="sect1" id="deepsea-pillar-salt-configuration" data-id-title="Configuration and Customization"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.5 </span><span class="title-name">Configuration and Customization</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-pillar-salt-configuration">#</a></h2></div></div></div><section class="sect2" id="policy-configuration" data-id-title="The policy.cfg File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.1 </span><span class="title-name">The <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#policy-configuration">#</a></h3></div></div></div><p>
    The <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>
    configuration file is used to determine roles of individual cluster nodes.
    For example, which nodes act as Ceph OSDs or Ceph Monitors. Edit
    <code class="filename">policy.cfg</code> in order to reflect your desired cluster
    setup. The order of the sections is arbitrary, but the content of included
    lines overwrites matching keys from the content of previous lines.
   </p><div id="id-1.4.4.2.9.2.3" data-id-title="Examples of policy.cfg" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Examples of <code class="filename">policy.cfg</code></h6><p>
     You can find several examples of complete policy files in the
     <code class="filename">/usr/share/doc/packages/deepsea/examples/</code> directory.
    </p></div><section class="sect3" id="policy-cluster-assignment" data-id-title="Cluster Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.1 </span><span class="title-name">Cluster Assignment</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#policy-cluster-assignment">#</a></h4></div></div></div><p>
     In the <span class="bold"><strong>cluster</strong></span> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <span class="bold"><strong>ceph</strong></span> follow.
    </p><p>
     To include <span class="bold"><strong>all</strong></span> minions, add the following
     lines:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/*.sls</pre></div><p>
     To <span class="bold"><strong>whitelist</strong></span> a particular minion:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/abc.domain.sls</pre></div><p>
     or a group of minions—you can shell glob matching:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/mon*.sls</pre></div><p>
     To <span class="bold"><strong>blacklist</strong></span> minions, set the them to
     <code class="literal">unassigned</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/client*.sls</pre></div></section><section class="sect3" id="policy-role-assignment" data-id-title="Role Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.2 </span><span class="title-name">Role Assignment</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#policy-role-assignment">#</a></h4></div></div></div><p>
     This section provides you with details on assigning 'roles' to your
     cluster nodes. A 'role' in this context means the service you need to run
     on the node, such as Ceph Monitor, Object Gateway, or iSCSI Gateway. No role is assigned
     automatically, only roles added to <code class="command">policy.cfg</code> will be
     deployed.
    </p><p>
     The assignment follows this pattern:
    </p><div class="verbatim-wrap"><pre class="screen">role-<em class="replaceable">ROLE_NAME</em>/<em class="replaceable">PATH</em>/<em class="replaceable">FILES_TO_INCLUDE</em></pre></div><p>
     Where the items have the following meaning and values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <em class="replaceable">ROLE_NAME</em> is any of the following: 'master',
       'admin', 'mon', 'mgr', 'storage', 'mds', 'igw', 'rgw', 'ganesha',
       'grafana', or 'prometheus'.
      </p></li><li class="listitem"><p>
       <em class="replaceable">PATH</em> is a relative directory path to .sls or
       .yml files. In case of .sls files, it usually is
       <code class="filename">cluster</code>, while .yml files are located at
       <code class="filename">stack/default/ceph/minions</code>.
      </p></li><li class="listitem"><p>
       <em class="replaceable">FILES_TO_INCLUDE</em> are the Salt state files
       or YAML configuration files. They normally consist of Salt minions' host
       names, for example <code class="filename">ses5min2.yml</code>. Shell globbing can
       be used for more specific matching.
      </p></li></ul></div><p>
     An example for each role follows:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>master</em></span> - the node has admin keyrings to all Ceph
       clusters. Currently, only a single Ceph cluster is supported. As the
       <span class="emphasis"><em>master</em></span> role is mandatory, always add a similar line
       to the following:
      </p><div class="verbatim-wrap"><pre class="screen">role-master/cluster/master*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>admin</em></span> - the minion will have an admin keyring. You
       define the role as follows:
      </p><div class="verbatim-wrap"><pre class="screen">role-admin/cluster/abc*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mon</em></span> - the minion will provide the monitor service
       to the Ceph cluster. This role requires addresses of the assigned
       minions. From SUSE Enterprise Storage 5, the public addresses are calculated
       dynamically and are no longer needed in the Salt pillar.
      </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls</pre></div><p>
       The example assigns the monitor role to a group of minions.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>mgr</em></span> - the Ceph manager daemon which collects all
       the state information from the whole cluster. Deploy it on all minions
       where you plan to deploy the Ceph monitor role.
      </p><div class="verbatim-wrap"><pre class="screen">role-mgr/cluster/mgr*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>storage</em></span> - use this role to specify storage nodes.
      </p><div class="verbatim-wrap"><pre class="screen">role-storage/cluster/data*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mds</em></span> - the minion will provide the metadata service
       to support CephFS.
      </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mds*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>igw</em></span> - the minion will act as an iSCSI Gateway. This role
       requires addresses of the assigned minions, thus you need to also
       include the files from the <code class="filename">stack</code> directory:
      </p><div class="verbatim-wrap"><pre class="screen">role-igw/cluster/*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>rgw</em></span> - the minion will act as an Object Gateway:
      </p><div class="verbatim-wrap"><pre class="screen">role-rgw/cluster/rgw*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>ganesha</em></span> - the minion will act as an NFS Ganesha
       server. The 'ganesha' role requires either an 'rgw' or 'mds' role in
       cluster, otherwise the validation will fail in Stage 3.
      </p><div class="verbatim-wrap"><pre class="screen">role-ganesha/cluster/ganesha*.sls</pre></div><p>
       To successfully install NFS Ganesha, additional configuration is required.
       If you want to use NFS Ganesha, read <a class="xref" href="cha-as-ganesha.html" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a>
       before executing stages 2 and 4. However, it is possible to install
       NFS Ganesha later.
      </p><p>
       In some cases it can be useful to define custom roles for NFS Ganesha
       nodes. For details, see <span class="intraxref">Book “Administration Guide”, Chapter 30 “NFS Ganesha: Export Ceph Data via NFS”, Section 30.3 “Custom NFS Ganesha Roles”</span>.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>grafana</em></span>, <span class="emphasis"><em>prometheus</em></span> - this
       node adds Grafana charts based on Prometheus alerting to the
       Ceph Dashboard. Refer to <span class="intraxref">Book “Administration Guide”</span> for its detailed
       description.
      </p><div class="verbatim-wrap"><pre class="screen">role-grafana/cluster/grafana*.sls</pre></div><div class="verbatim-wrap"><pre class="screen">role-prometheus/cluster/prometheus*.sls</pre></div></li></ul></div><div id="id-1.4.4.2.9.2.5.9" data-id-title="Multiple Roles of Cluster Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Multiple Roles of Cluster Nodes</h6><p>
      You can assign several roles to a single node. For example, you can
      assign the 'mds' roles to the monitor nodes:
     </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mon[1,2]*.sls</pre></div></div></section><section class="sect3" id="policy-common-configuration" data-id-title="Common Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.3 </span><span class="title-name">Common Configuration</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#policy-common-configuration">#</a></h4></div></div></div><p>
     The common configuration section includes configuration files generated
     during the <span class="emphasis"><em>discovery (Stage 1)</em></span>. These configuration
     files store parameters like <code class="literal">fsid</code> or
     <code class="literal">public_network</code>. To include the required Ceph common
     configuration, add the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</pre></div></section><section class="sect3" id="deepsea-policy-filtering" data-id-title="Item Filtering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.4 </span><span class="title-name">Item Filtering</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-policy-filtering">#</a></h4></div></div></div><p>
     Sometimes it is not practical to include all files from a given directory
     with *.sls globbing. The <code class="filename">policy.cfg</code> file parser
     understands the following filters:
    </p><div id="id-1.4.4.2.9.2.7.3" data-id-title="Advanced Techniques" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Advanced Techniques</h6><p>
      This section describes filtering techniques for advanced users. When not
      used correctly, filtering can cause problems for example in case your
      node numbering changes.
     </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.9.2.7.4.1"><span class="term">slice=[start:end]</span></dt><dd><p>
        Use the slice filter to include only items <span class="emphasis"><em>start</em></span>
        through <span class="emphasis"><em>end-1</em></span>. Note that items in the given
        directory are sorted alphanumerically. The following line includes the
        third to fifth files from the <code class="filename">role-mon/cluster/</code>
        subdirectory:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/*.sls slice[3:6]</pre></div></dd><dt id="id-1.4.4.2.9.2.7.4.2"><span class="term">re=regexp</span></dt><dd><p>
        Use the regular expression filter to include only items matching the
        given expressions. For example:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</pre></div></dd></dl></div></section><section class="sect3" id="deepsea-example-policy-cfg" data-id-title="Example policy.cfg File"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.5 </span><span class="title-name">Example <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#deepsea-example-policy-cfg">#</a></h4></div></div></div><p>
     Following is an example of a basic <code class="filename">policy.cfg</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">## Cluster Assignment
cluster-ceph/cluster/*.sls <span class="callout" id="co-policy-1">1</span>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <span class="callout" id="co-policy-2">2</span>
role-admin/cluster/sesclient*.sls <span class="callout" id="co-policy-3">3</span>

# MON
role-mon/cluster/ses-example-[123].sls <span class="callout" id="co-policy-5">4</span>

# MGR
role-mgr/cluster/ses-example-[123].sls <span class="callout" id="co-policy-mgr">5</span>

# STORAGE
role-storage/cluster/ses-example-[5678].sls <span class="callout" id="co-policy-storage">6</span>

# MDS
role-mds/cluster/ses-example-4.sls <span class="callout" id="co-policy-6">7</span>

# IGW
role-igw/cluster/ses-example-4.sls <span class="callout" id="co-policy-10">8</span>

# RGW
role-rgw/cluster/ses-example-4.sls <span class="callout" id="co-policy-11">9</span>

# COMMON
config/stack/default/global.yml <span class="callout" id="co-policy-8">10</span>
config/stack/default/ceph/cluster.yml <span class="callout" id="co-policy-13">11</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Indicates that all minions are included in the Ceph cluster. If you
       have minions you do not want to include in the Ceph cluster, use:
      </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</pre></div><p>
       The first line marks all minions as unassigned. The second line
       overrides minions matching 'ses-example-*.sls', and assigns them to the
       Ceph cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-2"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minion called 'examplesesadmin' has the 'master' role. This, by the
       way, means it will get admin keys to the cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-3"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'sesclient*' will get admin keys as well.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-5"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (presumably three minions:
       ses-example-1, ses-example-2, and ses-example-3) will be set up as MON
       nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-mgr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (all MON nodes in the example)
       will be set up as MGR nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-storage"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[5678]' will be set up as storage
       nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-6"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the MDS role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-10"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the IGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-11"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the RGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-8"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-13"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr></table></div></section></section><section class="sect2" id="ds-drive-groups" data-id-title="DriveGroups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.2 </span><span class="title-name">DriveGroups</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-drive-groups">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>DriveGroups</em></span> specify the layouts of OSDs in the Ceph
    cluster. They are defined in a single file
    <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>.
   </p><p>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on solid state and spinners) or
    share the same deployment options (identical, for example same object
    store, same encryption option, stand-alone OSDs). To avoid explicitly
    listing devices, DriveGroups use a list of filter items that correspond to a
    few selected fields of <code class="command">ceph-volume</code>'s inventory reports.
    In the simplest case this could be the 'rotational' flag (all solid-state
    drives are to be db_devices, all rotating ones data devices) or something
    more involved such as 'model' strings, or sizes. DeepSea will provide
    code that translates these DriveGroups into actual device lists for
    inspection by the user.
   </p><div id="id-1.4.4.2.9.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Note that the filters use an <code class="literal">OR</code> gate to match against
     the drives.
    </p></div><p>
    Following is a simple procedure that demonstrates the basic workflow when
    configuring DriveGroups:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Inspect your disks' properties as seen by the
      <code class="command">ceph-volume</code> command. Only these properties are
      accepted by DriveGroups:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.details</pre></div></li><li class="step"><p>
      Open the
      <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
      YAML file and adjust to your needs. Refer to
      <a class="xref" href="ceph-install-saltstack.html#ds-drive-groups-specs" title="5.5.2.1. Specification">Section 5.5.2.1, “Specification”</a>. Remember to use spaces instead
      of tabs. Find more advanced examples in
      <a class="xref" href="ceph-install-saltstack.html#ds-drive-groups-examples" title="5.5.2.4. Examples">Section 5.5.2.4, “Examples”</a>. The following example
      includes all drives available to Ceph as OSDs:
     </p><div class="verbatim-wrap"><pre class="screen">default_drive_group_name:
  target: '*'
  data_devices:
    all: true</pre></div></li><li class="step"><p>
      Verify new layouts:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.list</pre></div><p>
      This runner returns you a structure of matching disks based on your
      DriveGroups. If you are not happy with the result, repeat the previous
      step.
     </p><div id="id-1.4.4.2.9.3.6.3.4" data-id-title="Detailed Report" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Detailed Report</h6><p>
       In addition to the <code class="command">disks.list</code> runner, there is a
       <code class="command">disks.report</code> runner that prints out a detailed report
       of what will happen in the next DeepSea stage 3 invocation.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.report</pre></div></div></li><li class="step"><p>
      Deploy OSDs. On the next DeepSea stage 3 invocation, the OSD disks will
      be deployed according to your DriveGroups specification.
     </p></li></ol></div></div><section class="sect3" id="ds-drive-groups-specs" data-id-title="Specification"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.1 </span><span class="title-name">Specification</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-drive-groups-specs">#</a></h4></div></div></div><p>
     <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
     can take one of two basic forms, depending on whether BlueStore or
     FileStore is to be used. For BlueStore setups,
     <code class="filename">drive_groups.yml</code> can be as follows:
    </p><div class="verbatim-wrap"><pre class="screen">drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  db_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  wal_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')</pre></div><p>
     For FileStore setups, <code class="filename">drive_groups.yml</code> can be as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  journal_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  format: filestore
  encryption: True</pre></div><div id="id-1.4.4.2.9.3.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If you are unsure if your OSD is encrypted, see
      <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.5 “Verify an Encrypted OSD”</span>.
     </p></div></section><section class="sect3" id="id-1.4.4.2.9.3.8" data-id-title="Matching Disk Devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.2 </span><span class="title-name">Matching Disk Devices</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.8">#</a></h4></div></div></div><p>
     You can describe the specification using the following filters:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       By a disk model:
      </p><div class="verbatim-wrap"><pre class="screen">model: <em class="replaceable">DISK_MODEL_STRING</em></pre></div></li><li class="listitem"><p>
       By a disk vendor:
      </p><div class="verbatim-wrap"><pre class="screen">vendor: <em class="replaceable">DISK_VENDOR_STRING</em></pre></div><div id="id-1.4.4.2.9.3.8.3.2.3" data-id-title="Lowercase Vendor String" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Lowercase Vendor String</h6><p>
        Always lowercase the <em class="replaceable">DISK_VENDOR_STRING</em>.
       </p></div></li><li class="listitem"><p>
       Whether a disk is rotational or not. SSDs and NVME drives are not
       rotational.
      </p><div class="verbatim-wrap"><pre class="screen">rotational: 0</pre></div></li><li class="listitem"><p>
       Deploy a node using <span class="emphasis"><em>all</em></span> available drives for OSDs:
      </p><div class="verbatim-wrap"><pre class="screen">data_devices:
  all: true</pre></div></li><li class="listitem"><p>
       Additionally, by limiting the number of matching disks:
      </p><div class="verbatim-wrap"><pre class="screen">limit: 10</pre></div></li></ul></div></section><section class="sect3" id="id-1.4.4.2.9.3.9" data-id-title="Filtering Devices by Size"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.3 </span><span class="title-name">Filtering Devices by Size</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.9">#</a></h4></div></div></div><p>
     You can filter disk devices by their size—either by an exact size,
     or a size range. The <code class="option">size:</code> parameter accepts arguments in
     the following form:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       '10G' - Includes disks of an exact size.
      </p></li><li class="listitem"><p>
       '10G:40G' - Includes disks whose size is within the range.
      </p></li><li class="listitem"><p>
       ':10G' - Includes disks less than or equal to 10 GB in size.
      </p></li><li class="listitem"><p>
       '40G:' - Includes disks equal to or greater than 40 GB in size.
      </p></li></ul></div><div class="example" id="id-1.4.4.2.9.3.9.4" data-id-title="Matching by Disk Size"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.1: </span><span class="title-name">Matching by Disk Size </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.9.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'</pre></div></div></div><div id="id-1.4.4.2.9.3.9.5" data-id-title="Quotes Required" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quotes Required</h6><p>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </p></div><div id="id-1.4.4.2.9.3.9.6" data-id-title="Unit Shortcuts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Unit Shortcuts</h6><p>
      Instead of (G)igabytes, you can specify the sizes in (M)egabytes or
      (T)erabytes as well.
     </p></div></section><section class="sect3" id="ds-drive-groups-examples" data-id-title="Examples"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.4 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#ds-drive-groups-examples">#</a></h4></div></div></div><p>
     This section includes examples of different OSD setups.
    </p><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.3" data-id-title="Simple Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.2: </span><span class="title-name">Simple Setup </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.10.3">#</a></h6></div><div class="example-contents"><p>
      This example describes two nodes with the same setup:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      The corresponding <code class="filename">drive_groups.yml</code> file will be as
      follows:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ</pre></div><p>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0</pre></div><p>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </p><p>
      If you know that drives with more than 2 TB will always be the
      slower data devices, you can filter by size:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.4" data-id-title="Advanced Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.3: </span><span class="title-name">Advanced Setup </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.10.4">#</a></h6></div><div class="example-contents"><p>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        12 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      Such a setup can be defined with two layouts as follows:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB</pre></div><p>
      Note that any drive of the size 256 GB and any drive from Samsung
      will match as a DB device with this example.
     </p></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.5" data-id-title="Advanced Setup with Non-uniform Nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.4: </span><span class="title-name">Advanced Setup with Non-uniform Nodes </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.10.5">#</a></h6></div><div class="example-contents"><p>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </p><p>
      Nodes 1-5:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      Nodes 6-10:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        5 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        20 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      You can use the 'target' key in the layout to target specific nodes.
      Salt target notation helps to keep things simple:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0</pre></div><p>
      followed by
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.6" data-id-title="Expert Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.5: </span><span class="title-name">Expert Setup </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.10.6">#</a></h6></div><div class="example-contents"><p>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.7" data-id-title="Complex (and Unlikely) Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.6: </span><span class="title-name">Complex (and Unlikely) Setup </span><a title="Permalink" class="permalink" href="ceph-install-saltstack.html#id-1.4.4.2.9.3.10.7">#</a></h6></div><div class="example-contents"><p>
      In the following setup, we are trying to define:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 HDDs backed by 1 SSD(db) and 1 NVMe(wal)
       </p></li><li class="listitem"><p>
        8 SSDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 SSDs stand-alone (encrypted)
       </p></li><li class="listitem"><p>
        1 HDD is spare and should not be deployed
       </p></li></ul></div><p>
      The summary of used drives follows:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        23 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        10 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        1 NVMe
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      The DriveGroups definition will be the following:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True</pre></div><p>
      One HDD will remain as the file is being parsed from top to bottom.
     </p></div></div></div></section></section><section class="sect2" id="adjusting-ceph-conf" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.3 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="ceph-install-saltstack.html#adjusting-ceph-conf">#</a></h3></div></div></div><p>
    If you need to put custom settings into the <code class="filename">ceph.conf</code>
    configuration file, see <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.14 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span> for more
    details.
   </p></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="ses-deployment.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part II </span>Cluster Deployment and Upgrade</span></a> </div><div><a class="pagination-link next" href="cha-ceph-upgrade.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 6 </span>Upgrading from Previous Releases</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="ceph-install-saltstack.html#cha-ceph-install-relnotes"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span></a></span></li><li><span class="sect1"><a href="ceph-install-saltstack.html#deepsea-description"><span class="title-number">5.2 </span><span class="title-name">Introduction to DeepSea</span></a></span></li><li><span class="sect1"><a href="ceph-install-saltstack.html#ceph-install-stack"><span class="title-number">5.3 </span><span class="title-name">Cluster Deployment</span></a></span></li><li><span class="sect1"><a href="ceph-install-saltstack.html#deepsea-cli"><span class="title-number">5.4 </span><span class="title-name">DeepSea CLI</span></a></span></li><li><span class="sect1"><a href="ceph-install-saltstack.html#deepsea-pillar-salt-configuration"><span class="title-number">5.5 </span><span class="title-name">Configuration and Customization</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_install_salt.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>