<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Ceph Tuning | Tuning Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Ceph Tuning | SES 6"/>
<meta name="description" content="Ceph includes a telemetry module that provides anonymized information back to the Ceph developer community. The information contained in the telemetry report p…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Tuning Guide"/>
<meta name="chapter-title" content="Chapter 6. Ceph Tuning"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Ceph Tuning | SES 6"/>
<meta property="og:description" content="Ceph includes a telemetry module that provides anonymized information back to the Ceph developer community. The information contained in the telemetry report p…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ceph Tuning | SES 6"/>
<meta name="twitter:description" content="Ceph includes a telemetry module that provides anonymized information back to the Ceph developer community. The information contained in the telemetry report p…"/>
<link rel="prev" href="tuning-os.html" title="Chapter 5. Operating System Level Tuning"/><link rel="next" href="cha-ceph-tiered.html" title="Chapter 7. Cache Tiering"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Tuning Guide</a><span> / </span><a class="crumb" href="part-tuning-guide.html">SUSE Enterprise Storage Tuning</a><span> / </span><a class="crumb" href="tuning-ceph.html">Ceph Tuning</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Tuning Guide</div><ol><li><a href="part-intro.html" class="has-children "><span class="title-number">I </span><span class="title-name">Introduction</span></a><ol><li><a href="bk03pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="tuning-how.html" class=" "><span class="title-number">2 </span><span class="title-name">General Notes on System Tuning</span></a></li><li><a href="tuning-intro.html" class=" "><span class="title-number">3 </span><span class="title-name">Introduction to Tuning SUSE Enterprise Storage Clusters</span></a></li></ol></li><li class="active"><a href="part-tuning-guide.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">SUSE Enterprise Storage Tuning</span></a><ol><li><a href="tuning-architecture.html" class=" "><span class="title-number">4 </span><span class="title-name">Architecture and Hardware Tuning</span></a></li><li><a href="tuning-os.html" class=" "><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span></a></li><li><a href="tuning-ceph.html" class=" you-are-here"><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span></a></li><li><a href="cha-ceph-tiered.html" class=" "><span class="title-number">7 </span><span class="title-name">Cache Tiering</span></a></li><li><a href="lvmcache.html" class=" "><span class="title-number">8 </span><span class="title-name">Improving Performance with LVM cache</span></a></li></ol></li><li><a href="tuning-appendix-a.html" class=" "><span class="title-number">A </span><span class="title-name">Salt State for Kernel Tuning</span></a></li><li><a href="tuning-appendix-b.html" class=" "><span class="title-number">B </span><span class="title-name">Ring Buffer Max Value Script</span></a></li><li><a href="tuning-appendix-c.html" class=" "><span class="title-number">C </span><span class="title-name">Network Tuning</span></a></li><li><a href="bk03apd.html" class=" "><span class="title-number">D </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk03go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="tuning-ceph" data-id-title="Ceph Tuning"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#">#</a></h2></div></div></div><p>
  Ceph includes a telemetry module that provides anonymized information back
  to the Ceph developer community. The information contained in the telemetry
  report provides information that helps the developers prioritize efforts and
  identify areas where more work may be needed. It may be necessary to enable
  the telemetry module before turning it on. To enable the module:
 </p><div class="verbatim-wrap"><pre class="screen">ceph mgr module enable telemetry</pre></div><p>
  To turn on telemetry reporting use the following command:
 </p><div class="verbatim-wrap"><pre class="screen">ceph telemetry on</pre></div><p>
  Additional information about the Ceph telemetry module may be found in the
  <span class="intraxref">Book “Administration Guide”</span>.
 </p><section class="sect1" id="tuning-obtaining-metrics" data-id-title="Obtaining Ceph Metrics"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Obtaining Ceph Metrics</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-obtaining-metrics">#</a></h2></div></div></div><p>
   Before adjusting Ceph tunables, it is helpful to have an understanding of
   the critical metrics to monitor and what they indicate. Many of these
   parameters are found by dumping raw data from the daemons. This is done by
   means of the <code class="command">ceph daemon dump</code> command. The following
   example shows the dump command being utilized for
   <code class="literal">osd.104</code>.
  </p><div class="verbatim-wrap"><pre class="screen">ceph --admin-daemon /var/run/ceph/ceph-osd.104.asok perf dump</pre></div><p>
   Starting with the Ceph Nautilus release, the following command may be used
   as well:
  </p><div class="verbatim-wrap"><pre class="screen">ceph daemon osd.104 perf dump</pre></div><p>
   The output of the command is quite lengthy and may benefit from being
   redirected to a file.
  </p></section><section class="sect1" id="tuning-tuning-persistent" data-id-title="Making Tuning Persistent"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Making Tuning Persistent</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-tuning-persistent">#</a></h2></div></div></div><p>
   To make parameter adjustment persistent requires modifying the
   <code class="filename">/etc/ceph/ceph.conf</code> file. This is best done through
   modifying the source component files that DeepSea uses to manage the
   cluster. Each section is identified with a header such as:
  </p><div class="verbatim-wrap"><pre class="screen">  [global]
  [osd]
  [mds]
  [mon]
  [mgr]
  [client]</pre></div><p>
   The section of the configuration is tuned by modifying the correct
   <code class="filename">[sectionname].conf</code> in the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>
   directory. After modifying the configuration file, replace
   <code class="literal">master</code> with the master minion-name (usually the admin
   node). The result is that the changes are pushed to all cluster nodes.
  </p><div class="verbatim-wrap"><pre class="screen">  salt 'master' state.apply ceph.configuration.create
  salt '*' state.apply ceph.configuration</pre></div><p>
   Changes made in this way require the affected services to be restarted
   before taking effect. It is also possible to deploy these files before
   running stage 2 of the SUSE Enterprise Storage deployment process. It is especially
   desirable to do so if changing the settings that require node or device
   re-deployment.
  </p></section><section class="sect1" id="tuning-core" data-id-title="Core"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Core</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-core">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.10.2" data-id-title="Logging"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.1 </span><span class="title-name">Logging</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.10.2">#</a></h3></div></div></div><p>
    It is possible to disable all logging to reduce latency in the various
    codepaths.
   </p><div id="id-1.5.3.4.10.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     This tuning should be used with caution and understanding that logging
     <span class="emphasis"><em>will</em></span> need to be re-enabled should support be
     required. This implies that an incident would need to be reproduced
     <span class="emphasis"><em>after</em></span> logging is re-enabled.
    </p></div><div class="verbatim-wrap"><pre class="screen">  debug ms=0
  debug mds=0
  debug osd=0
  debug optracker=0
  debug auth=0
  debug asok=0
  debug bluestore=0
  debug bluefs=0
  debug bdev=0
  debug kstore=0
  debug rocksdb=0
  debug eventtrace=0
  debug default=0
  debug rados=0
  debug client=0
  debug perfcounter=0
  debug finisher=0</pre></div></section><section class="sect2" id="id-1.5.3.4.10.3" data-id-title="Authentication Parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.2 </span><span class="title-name">Authentication Parameters</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.10.3">#</a></h3></div></div></div><p>
    Under certain conditions where the cluster is physically secure and
    isolated inside a secured network with no external exposure, it is possible
    to disable <code class="systemitem">cephx</code>. There are two levels at which <code class="systemitem">cephx</code> can be disabled.
    The first is to disable signing of authentication traffic. This can be
    accomplished with the following settings:
   </p><div class="verbatim-wrap"><pre class="screen">cephx_require_signatures = <em class="replaceable">false</em>
cephx_cluster_require_signatures = <em class="replaceable">false</em>
cephx_sign_messages = <em class="replaceable">false</em></pre></div><p>
    The second level of tuning completely disables <code class="systemitem">cephx</code> authentication. This
    should only be done on networks that are isolated from public network
    infrastructure. This change is achieved by adding the following three lines
    in the global section:
   </p><div class="verbatim-wrap"><pre class="screen">auth cluster required = none
auth service required = none
auth client required = none</pre></div></section><section class="sect2" id="id-1.5.3.4.10.4" data-id-title="RADOS Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.3 </span><span class="title-name">RADOS Operations</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.10.4">#</a></h3></div></div></div><p>
    The backend processes for performing RADOS operations show up in
    <code class="literal">throttle-*objector_ops</code> when dumping various daemons. If
    there is too much time being spent in <code class="literal">wait</code>, there may be
    some performance to gain by increasing the memory for in-flight ops or by
    increasing the total number of inflight operations overall.
   </p><div class="verbatim-wrap"><pre class="screen">objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576</pre></div></section><section class="sect2" id="id-1.5.3.4.10.5" data-id-title="OSD parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.4 </span><span class="title-name">OSD parameters</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.10.5">#</a></h3></div></div></div><p>
    Increasing the number of <code class="literal">op threads</code> may be helpful with
    SSD and NVMe devices, as it provides more work queues for operations.
   </p><div class="verbatim-wrap"><pre class="screen">osd_op_num_threads_per_shard = 4</pre></div></section><section class="sect2" id="id-1.5.3.4.10.6" data-id-title="RocksDB or WAL device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.5 </span><span class="title-name">RocksDB or WAL device</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.10.6">#</a></h3></div></div></div><p>
    In checking the performance of BlueStore, it is important to understand
    if your metadata is spilling over from the high-speed device, if defined,
    to the bulk data storage device. The parameters useful in this case are
    found under bluefs <code class="literal">slow_used_bytes</code>. If
    <code class="literal">slow_used_bytes</code> is greater than zero, the cluster is
    using the storage device instead of the RocksDB/WAL device. This is an
    indicator that more space needs to be allocated to RocksDB/WAL.
   </p><p>
    Starting with the Ceph Nautilus release, spillover is shown in the output
    of the <code class="command">ceph health</code> command.
   </p><p>
    The process of allocating more space depends on how the OSD was deployed.
    If it was deployed by a version prior to SUSE Enterprise Storage 6, the OSD will need
    to be re-deployed. If it was deployed with version 6 or after, it may be
    possible to expand the LVM that the RocksDB and WAL reside on, subject to
    available space.
   </p></section><section class="sect2" id="bluestore-parameters" data-id-title="BlueStore parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.6 </span><span class="title-name">BlueStore parameters</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#bluestore-parameters">#</a></h3></div></div></div><p>
    Ceph is thin provisioned, including the Write-Ahead Log (WAL) files. By
    pre-extending the files for the WAL, time is saved by not having to engage
    the allocator. It also potentially reduces the likelihood of fragmentation
    of the WAL files. This likely only provides benefit during the early life
    of the cluster.
   </p><div class="verbatim-wrap"><pre class="screen">bluefs_preextend_wal_files=1</pre></div><p>
    BlueStore has the ability to perform buffered writes. Buffered writes
    enable populating the read cache during the write process. This setting, in
    effect, changes the BlueStore cache into a write-through cache.
   </p><div class="verbatim-wrap"><pre class="screen">bluestore_default_buffered_write = true</pre></div><p>
    To prevent writes to the WAL when using a fast device, such as SSD and
    NVMe, set:
   </p><div class="verbatim-wrap"><pre class="screen">prefer_deferred_size_ssd=0 (pre-deployment)</pre></div></section><section class="sect2" id="bluestore-alloc-size" data-id-title="BlueStore Allocation Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.7 </span><span class="title-name">BlueStore Allocation Size</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#bluestore-alloc-size">#</a></h3></div></div></div><div id="id-1.5.3.4.10.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     The following settings are not necessary for fresh deployments. Apply only
     to upgraded deployments or early SUSE Enterprise Storage 6 deployments as they may
     still benefit.
    </p></div><p>
    The following settings have been shown to slightly improve small object
    write performance under mixed workload conditions. Reducing the
    <code class="literal">alloc_size</code> to 4 kB helps reduce write amplification
    for small objects and with erasure coded pools of smaller objects. This
    change needs to be done before OSD deployment. If done after the fact, the
    OSDs will need to be re-deployed for it to take effect.
   </p><p>
    It is advised that spinning media continue to use 64 kB while SSD/NVMe
    are likely to benefit from setting to 4 kB.
   </p><div class="verbatim-wrap"><pre class="screen">min_alloc_size_ssd=4096
min_alloc_size_hdd=65536</pre></div><div id="id-1.5.3.4.10.8.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Setting the <code class="literal">alloc_size_ssd</code> to 64 kB may reduce
     maximum throughput capability of the OSD.
    </p></div></section><section class="sect2" id="bluestore-cache" data-id-title="BlueStore Cache"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.8 </span><span class="title-name">BlueStore Cache</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#bluestore-cache">#</a></h3></div></div></div><p>
    Increasing the BlueStore cache size can improve performance with many
    workloads. While FileStore OSDs cache data in the kernel's page cache,
    BlueStore OSDs cache data within the memory allocated by the OSD daemon
    itself. The OSD daemon will allocate memory up to its memory target (as
    controlled by the <code class="literal">osd_memory_target</code> parameter), and this
    determines the potential size of the BlueStore cache. The BlueStore
    cache is a read cache that by default is populated when objects are read.
    By setting the cache's minimum size higher than the default, it is
    guaranteed that the value specified will be the minimum cache available for
    each OSD. The idea is that more low-probability cache hits may occur.
   </p><p>
    The default <code class="literal">osd_memory_target</code> value is 4 GB: For
    example, each OSD daemon running on a node can be expected to consume that
    much memory. If a node's total RAM is significantly higher than
    <code class="literal">number of OSDs × 4 GB</code> and there are no other
    daemons running on the node, performance can be increased by increasing the
    value of <code class="literal">osd_memory_target</code>. This should be done with
    care to ensure that the operating system will still have enough memory for
    its needs, while leaving a safety margin.
   </p><p>
    If you want to ensure that the BlueStore cache will not fall below a
    certain minimum, use the <code class="literal">osd_memory_cache_min</code> parameter.
    Here is an example (the values are expressed in bytes):
   </p><div class="verbatim-wrap"><pre class="screen">osd_memory_target = 6442450944
osd_memory_cache_min = 4294967296</pre></div><div id="id-1.5.3.4.10.9.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     As a best practice, start with the full memory of the node. Deduct
     16 GB or 32 GB for the OS and then deduct appropriate amounts
     for any other workloads running on the node. For example, MDS cache if the
     MDS is colocated. Divide the remainder by the number of OSDs on that host.
     Ensure you leave room for improvement. For example:
    </p><div class="verbatim-wrap"><pre class="screen">(256 GB - 32 GB ) / 20 OSDs = 11,2 GB/OSD (max)</pre></div><p>
     Using this example, configure approximately 8 or 10 GB per OSD.
    </p></div><p>
    By default, BlueStore automatically tunes cache ratios between data and
    key-value data. In some cases it may be helpful to manually tune the ratios
    or even increase the cache size. There are several relevant counters for
    the cache:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_onode_hits</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_misses</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_shard_hits</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_shard_misses</code>
     </p></li></ul></div><p>
    If the misses are high, it is worth experimenting with increasing the cache
    settings or adjusting the ratios.
   </p><p>
    Adjusting the BlueStore cache size above default has the potential to
    improve performance of small-block workloads. This can be done globally by
    adjusting the <code class="option">_cache_size</code> value. By default, the cluster
    utilizes different values for HDD and SSD/NVMe devices. The best practice
    would be to increase the specific media cache you are interested in tuning:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_cache_size_hdd</code> (default 1073741824 -
      1 GB)
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_cache_size_ssd</code> (default 3221225472 -
      3 GB)
     </p></li></ul></div><div id="id-1.5.3.4.10.9.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the cache size parameters are adjusted and auto mode is utilized,
     <code class="option">osd_memory_target</code> should be adjusted to accomodate the
     OSD base RAM and cache allocation.
    </p></div><p>
    In some cases, manually tuning the cache allocation percentage may improve
    performance. This is achieved by modifying the disabling autotuning of the
    cache with this configuration line:
   </p><div class="verbatim-wrap"><pre class="screen">bluestore_cache_autotune=0</pre></div><p>
    Changing this value invalidates tuning of the
    <code class="option">osd_memory_cache_min</code> value.
   </p><p>
    The cache allocations are modified by adjusting the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_cache_kv_ratio</code> (default .4)
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_cache_meta_ratio values</code> (default .4)
     </p></li></ul></div><p>
    Any unspecified portion is used for caching the objects themselves.
   </p></section></section><section class="sect1" id="tuning-rbd" data-id-title="RBD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.4 </span><span class="title-name">RBD</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-rbd">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.11.2" data-id-title="RBD Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4.1 </span><span class="title-name">RBD Cluster</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.11.2">#</a></h3></div></div></div><p>
    As RBD is a native protocol, the tuning is directly related to OSD or
    general Ceph core options that are covered in previous sections.
   </p></section><section class="sect2" id="id-1.5.3.4.11.3" data-id-title="RBD Client"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4.2 </span><span class="title-name">RBD Client</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.11.3">#</a></h3></div></div></div><p>
    Read ahead cache defaults to 512 kB; test by tuning up and down on the
    client nodes.
   </p><div class="verbatim-wrap"><pre class="screen">echo {bytes} &gt; /sys/block/rbd0/queue/read_ahead_kb</pre></div><p>
    If your workload performs large sequential reads such as backup and
    restore, then this can make a significant difference in restore
    performance.
   </p></section></section><section class="sect1" id="tuning-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.5 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-cephfs">#</a></h2></div></div></div><p>
   Most of the performance tuning covered in this section pertains to the
   CephFS Metadata Servers. Because CephFS is a native protocol, much of
   the performance tuning is handled at the operating system, OSD and
   BlueStore layers. Being a file system that is mounted by a client, there
   are some client options that are covered in the client section.
  </p><section class="sect2" id="id-1.5.3.4.12.3" data-id-title="MDS Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.1 </span><span class="title-name">MDS Tuning</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.12.3">#</a></h3></div></div></div><p>
    In filesystems with millions of files, there is some advantage to utilizing
    very low-latency media, such as NVMe, for the CephFS metadata pool.
   </p><p>
    Utlizing the <code class="command">ceph-daemon perf dump</code> command, there is a
    significant amount of data that can be examined for the Ceph Metadata
    Servers. It should be noted that the MDS perf counters only apply to
    metadata operations. The actual IO path is from clients straight to OSDs.
   </p><p>
    CephFS supports multiple metadata servers. These servers can operate in a
    multiple-active mode to provide load balancing of the metadata operation
    requests. To identify whether the MDS infrastructure is under-performing,
    one would examine the MDS data for request count and reply latencies. This
    should be done during an idle period on the cluster to form a baseline and
    then compared when under load. If the average time for reply latencies
    climbs too high, the MDS server needs to be examined further to identify
    whether the number of active metadata servers should be augmented, or
    whether simply increasing the metadata server cache may be sufficient. A
    sample of the output from the general MDS data for count and reply latency
    are below:
   </p><div class="verbatim-wrap"><pre class="screen">  "mds": {
    # request count, interesting to get a sense of MDS load
           "request": 0,
           "reply": 0,
    # reply and the latencies of replies can point to load issues
           "reply_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           }
          }</pre></div><p>
    Examining the <code class="literal">mds_mem</code> section of the output can help
    with understanding how the cache is utilized. High inode counters can
    indicate that a large number of files are open concurrently. This generally
    indicates that more memory may need to be provided to the MDS. If MDS
    memory cannot be increased, additional active MDS daemons should be
    deployed.
   </p><div class="verbatim-wrap"><pre class="screen">  "mds_mem": {

           "ino": 13,
           "ino+": 13,
           "ino-": 0,
           "dir": 12,
           "dir+": 12,
           "dir-": 0,
           "dn": 10,
           "dn+": 10,
           "dn-": 0,</pre></div><p>
    A high <code class="literal">cap</code> count can indicate misbehaving clients. For
    example, clients that do not hand back caps. This may indicate that some
    clients need to be upgraded to a more recent version, or that the client
    needs to be investigated for possible issues.
   </p><div class="verbatim-wrap"><pre class="screen">  "cap": 0,
  "cap+": 0,
  "cap-": 0,</pre></div><p>
    This final section shows memory utilization. The RSS value is the current
    memory size used. If this is roughly equal to the
    <code class="literal">mds_cache_memory_limit</code>, the MDS could probably use more
    memory.
   </p><div class="verbatim-wrap"><pre class="screen">  "rss": 41524,
  "heap": 314072
},</pre></div><p>
    Another important aspect of tuning a distributed file system is recognizing
    problematic workloads. The output values below provide some insight to what
    the MDS daemon is spending its time on. Each heading has the same three
    attributes as the <code class="literal">req_create_latency</code>. With this
    information, it may be possible to better tune the workloads.
   </p><div class="verbatim-wrap"><pre class="screen">  "mds_server": {
           "dispatch_client_request": 0,
           "dispatch_server_request": 0,
           "handle_client_request": 0,
           "handle_client_session": 0,
           "handle_slave_request": 0,
           "req_create_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           },
           "req_getattr_latency": {},
           "req_getfilelock_latency": {},
           "req_link_latency": {},
           "req_lookup_latency": {},
           "req_lookuphash_latency": {},
           "req_lookupino_latency": {},
           "req_lookupname_latency": {},
           "req_lookupparent_latency": {},
           "req_lookupsnap_latency": {},
           "req_lssnap_latency": {},
           "req_mkdir_latency": {},
           "req_mknod_latency": {},
           "req_mksnap_latency": {},
           "req_open_latency": {},
           "req_readdir_latency": {},
           "req_rename_latency": {},
           "req_renamesnap_latency": {},
           "req_rmdir_latency": {},
           "req_rmsnap_latency": {},
           "req_rmxattr_latency": {},
           "req_setattr_latency": {},
           "req_setdirlayout_latency": {},
           "req_setfilelock_latency": {},
           "req_setlayout_latency": {},
           "req_setxattr_latency": {},
           "req_symlink_latency": {},
           "req_unlink_latency": {},
       }</pre></div><p>
    Tuning the metadata server cache allows for more metadata operations to
    come from RAM, resulting in improved performance. The example below sets
    the cache to 16 GB.
   </p><div class="verbatim-wrap"><pre class="screen">mds_cache_memory_limit=17179869184</pre></div></section><section class="sect2" id="id-1.5.3.4.12.4" data-id-title="CephFS - Client"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.2 </span><span class="title-name">CephFS - Client</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.12.4">#</a></h3></div></div></div><p>
    From the client side, there are a number of performance affecting mount
    options that can be employed. It is important to understand the potential
    impact on the applications being utilized before employing these options.
   </p><p>
    The following mount options may be adjusted to improve performance, but we
    recommend that their impact is clearly understood prior to implementation
    in a production environment.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.3.4.12.4.4.1"><span class="term">noacl</span></dt><dd><p>
       Setting this mount option disables POSIX Access Control Lists for the
       CephFS mount, lowering potential metadata overhead.
      </p></dd><dt id="id-1.5.3.4.12.4.4.2"><span class="term">noatime</span></dt><dd><p>
       This option prevents the access time metadata for files from being
       updated.
      </p></dd><dt id="id-1.5.3.4.12.4.4.3"><span class="term">nodiratime</span></dt><dd><p>
       Setting this option prevents the metadata for access time of a directory
       from being updated.
      </p></dd><dt id="id-1.5.3.4.12.4.4.4"><span class="term">nocrc</span></dt><dd><p>
       This disables CephFS CRCs, thus relying on TCP Checksums for the
       correctness of the data to be verified.
      </p></dd><dt id="id-1.5.3.4.12.4.4.5"><span class="term">rasize</span></dt><dd><p>
       Setting a larger read-ahead for the mount may increase performance for
       large, sequential operations. Default is 8 MiB.
      </p></dd></dl></div></section></section><section class="sect1" id="tuning-rgw" data-id-title="RGW"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.6 </span><span class="title-name">RGW</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-rgw">#</a></h2></div></div></div><p>
   There are a large number of tunables for the Rados GateWay (RGW). These may
   be specific to the types of workloads being handled by the gateway and it
   may make sense to have different gateways handling distictly different
   workloads.
  </p><section class="sect2" id="id-1.5.3.4.13.3" data-id-title="Sharding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.1 </span><span class="title-name">Sharding</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.13.3">#</a></h3></div></div></div><p>
    The ideal situation is to understand how many total objects a bucket will
    host as this allows a bucket to be created with an appropriate number of
    shards at the outset. To gather information on bucket sharding, issue:
   </p><div class="verbatim-wrap"><pre class="screen">radosgw-admin bucket limit check</pre></div><p>
    The output of this command appears like the following format:
   </p><div class="verbatim-wrap"><pre class="screen">  "user_id": "myusername",
          "buckets": [
              {
                  "bucket": "mybucketname",
                  "tenant": "",
                  "num_objects": 611493,
                  "num_shards": 50,
                  "objects_per_shard": 12229,
                  "fill_status": "OK"
              }
          ]</pre></div><p>
    By default, Ceph reshards buckets to try and maintain reasonable
    performance. If it is known ahead of time how many shards a bucket may
    need, based on a ratio of 1 shard per 100 000 objects, it may be
    pre-sharded. This reduces contention and potential latency issues when
    resharding will occur. To pre-shard the bucket, it should be created and
    then submitted for sharding with the <code class="command">rgw-admin</code> command.
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">radosgw-admin bucket reshard --bucket={bucket name} --num-shards={prime number}</pre></div><p>
    Where the <code class="literal">num-shards</code> is a prime number. Each shard
    should represent about 100 000 objects.
   </p></section><section class="sect2" id="id-1.5.3.4.13.4" data-id-title="Limiting Bucket Listing Results"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.2 </span><span class="title-name">Limiting Bucket Listing Results</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.13.4">#</a></h3></div></div></div><p>
    If a process relies on listing the buckets on a frequent basis to iterate
    through results, yet only uses a small number of results for each iteration
    it is useful to set the <code class="literal">rgw_max_listing_results</code>
    parameter.
   </p></section><section class="sect2" id="id-1.5.3.4.13.5" data-id-title="Parallel I/O Requests"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.3 </span><span class="title-name">Parallel I/O Requests</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.13.5">#</a></h3></div></div></div><p>
    By default, the Object Gateway process is limited to eight simultaneous I/O
    operations for the index. This can be adjusted with the
    <code class="literal">rgw_bucket_index_max_aio</code> parameter.
   </p></section><section class="sect2" id="id-1.5.3.4.13.6" data-id-title="Window Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.4 </span><span class="title-name">Window Size</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.13.6">#</a></h3></div></div></div><p>
    When working with larger objects, increasing the size of the Object Gateway windows
    for <code class="literal">put</code> and <code class="literal">get</code> can help with
    performance. Modify the following values in the Object Gateway section of the
    configuration:
   </p><div class="verbatim-wrap"><pre class="screen">rgw put obj min window size = [size in bytes, 16MiB default]
rgw get obj min window size = [size in bytes, 16MiB default]</pre></div></section><section class="sect2" id="id-1.5.3.4.13.7" data-id-title="Nagles Algorithm"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.5 </span><span class="title-name">Nagle's Algorithm</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.13.7">#</a></h3></div></div></div><p>
    Nagle's algorithm was introduced to maximize the use of buffers and attempt
    to reduce the number of small packets transmitted over the network. While
    this is helpful in lower bandwdith environments, it can represent a
    performance degredation in high-bandwidth environments. Disabling it from
    RGW nodes can improve performance. Including the following in the Ceph
    configuation RGW section:
   </p><div class="verbatim-wrap"><pre class="screen">tcp_nodelay=1</pre></div></section></section><section class="sect1" id="tuning-admin-usage" data-id-title="Administrative and Usage Choices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.7 </span><span class="title-name">Administrative and Usage Choices</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#tuning-admin-usage">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.14.2" data-id-title="Data Protection Schemes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.1 </span><span class="title-name">Data Protection Schemes</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.14.2">#</a></h3></div></div></div><p>
    The default replication setting keeps three total copies of every object
    written. The provides a high level of data protection by allowing up to two
    devices or nodes to fail while still protecting the data.
   </p><p>
    There are use cases where protecting the data is not important, but where
    performance is. In these cases, such as HPC scratch storage, it may be
    worthwhile to lower the replication count. This can be achieved by issuing
    a command such as:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd pool set rbd size 2</pre></div></section><section class="sect2" id="id-1.5.3.4.14.3" data-id-title="Erasure Coding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.2 </span><span class="title-name">Erasure Coding</span> <a title="Permalink" class="permalink" href="tuning-ceph.html#id-1.5.3.4.14.3">#</a></h3></div></div></div><p>
    When using erasure coding, it is best to utilize optimized coding pool
    sizes. Experimental data suggests that the optimial pool sizes have either
    four or eight data chunks. It is also important to map this in relation to
    your failure domain model. If your cluster failure domain is at the node
    level, you will need at least <code class="literal">k+m</code> number of nodes.
    Similarly, if your failure domain it at the rack level, then your cluster
    needs to be spread over <code class="literal">k+m</code> racks. The key consideration
    is that distribution of the data in relation to the failure domain should
    be taken into consideration.
   </p><p>
    When using erasure coding schemes with failure domains larger than a single
    node, the use of Local Reconstruction Codes (LRC) may be beneficial due to
    lowered utilization of the network backbone, especially during failure and
    recovery scenatios.
   </p><p>
    There are particular use cases where erasure coding may even increase
    performance. These are mostly limited to large block (1 MB+)
    sequential read/write workloads. This is due to the parallelization of I/O
    requests that occurs when splitting objects into chunks to write to
    multiple OSDs.
   </p></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="tuning-os.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 5 </span>Operating System Level Tuning</span></a> </div><div><a class="pagination-link next" href="cha-ceph-tiered.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 7 </span>Cache Tiering</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="tuning-ceph.html#tuning-obtaining-metrics"><span class="title-number">6.1 </span><span class="title-name">Obtaining Ceph Metrics</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-tuning-persistent"><span class="title-number">6.2 </span><span class="title-name">Making Tuning Persistent</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-core"><span class="title-number">6.3 </span><span class="title-name">Core</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-rbd"><span class="title-number">6.4 </span><span class="title-name">RBD</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-cephfs"><span class="title-number">6.5 </span><span class="title-name">CephFS</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-rgw"><span class="title-number">6.6 </span><span class="title-name">RGW</span></a></span></li><li><span class="sect1"><a href="tuning-ceph.html#tuning-admin-usage"><span class="title-number">6.7 </span><span class="title-name">Administrative and Usage Choices</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/tuning-ceph-tuning.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>