<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Operating System Level Tuning | Tuning Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Operating System Level Tuning | SES 6"/>
<meta name="description" content="A significant amount of the performance tuning for Ceph clusters can be done at the operating system (OS) layer. This tuning involves ensuring that unnecessary…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Tuning Guide"/>
<meta name="chapter-title" content="Chapter 5. Operating System Level Tuning"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Operating System Level Tuning | SES 6"/>
<meta property="og:description" content="A significant amount of the performance tuning for Ceph clusters can be done at the operating system (OS) layer. This tuning involves ensuring that unnecessary…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Operating System Level Tuning | SES 6"/>
<meta name="twitter:description" content="A significant amount of the performance tuning for Ceph clusters can be done at the operating system (OS) layer. This tuning involves ensuring that unnecessary…"/>
<link rel="prev" href="tuning-architecture.html" title="Chapter 4. Architecture and Hardware Tuning"/><link rel="next" href="tuning-ceph.html" title="Chapter 6. Ceph Tuning"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Tuning Guide</a><span> / </span><a class="crumb" href="part-tuning-guide.html">SUSE Enterprise Storage Tuning</a><span> / </span><a class="crumb" href="tuning-os.html">Operating System Level Tuning</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Tuning Guide</div><ol><li><a href="part-intro.html" class="has-children "><span class="title-number">I </span><span class="title-name">Introduction</span></a><ol><li><a href="bk03pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="tuning-how.html" class=" "><span class="title-number">2 </span><span class="title-name">General Notes on System Tuning</span></a></li><li><a href="tuning-intro.html" class=" "><span class="title-number">3 </span><span class="title-name">Introduction to Tuning SUSE Enterprise Storage Clusters</span></a></li></ol></li><li class="active"><a href="part-tuning-guide.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">SUSE Enterprise Storage Tuning</span></a><ol><li><a href="tuning-architecture.html" class=" "><span class="title-number">4 </span><span class="title-name">Architecture and Hardware Tuning</span></a></li><li><a href="tuning-os.html" class=" you-are-here"><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span></a></li><li><a href="tuning-ceph.html" class=" "><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span></a></li><li><a href="cha-ceph-tiered.html" class=" "><span class="title-number">7 </span><span class="title-name">Cache Tiering</span></a></li><li><a href="lvmcache.html" class=" "><span class="title-number">8 </span><span class="title-name">Improving Performance with LVM cache</span></a></li></ol></li><li><a href="tuning-appendix-a.html" class=" "><span class="title-number">A </span><span class="title-name">Salt State for Kernel Tuning</span></a></li><li><a href="tuning-appendix-b.html" class=" "><span class="title-number">B </span><span class="title-name">Ring Buffer Max Value Script</span></a></li><li><a href="tuning-appendix-c.html" class=" "><span class="title-number">C </span><span class="title-name">Network Tuning</span></a></li><li><a href="bk03apd.html" class=" "><span class="title-number">D </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk03go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="tuning-os" data-id-title="Operating System Level Tuning"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span> <a title="Permalink" class="permalink" href="tuning-os.html#">#</a></h2></div></div></div><p>
  A significant amount of the performance tuning for Ceph clusters can be
  done at the operating system (OS) layer. This tuning involves ensuring that
  unnecessary services are not running, and extends down to ensuring buffers
  are not being overrun and interrupts are being spread properly. There are
  many additional tuning options for the OS that are not included here, either
  due to statistically-insignificant performance changes, or not being deemed
  candidates for significant performance improvement at the time of this work.
 </p><section class="sect1" id="tuning-sles" data-id-title="SUSE Linux Enterprise Install and Validation of Base Performance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">SUSE Linux Enterprise Install and Validation of Base Performance</span> <a title="Permalink" class="permalink" href="tuning-os.html#tuning-sles">#</a></h2></div></div></div><p>
   During the OS installation, do <span class="emphasis"><em>not</em></span> select an install
   pattern that includes an X Server. Doing so utilizes RAM and CPU resources
   that would be better allocated to tuning storage-related daemons. We
   recommend a pattern that includes the minimal pattern with the addition of
   the <code class="literal">YaST management</code> pattern.
  </p><p>
   After the OS is installed, it is proper to evaluate the various individual
   components that are critical to the overall performance of the storage
   cluster.
  </p><div id="id-1.5.3.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Check performance of individual components before SUSE Enterprise Storage is setup to
    ensure they are performing as desired.
   </p></div><section class="sect2" id="id-1.5.3.3.4.5" data-id-title="Network Performance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.1 </span><span class="title-name">Network Performance</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.4.5">#</a></h3></div></div></div><p>
    To perform <code class="command">iperf3</code> tests for network performance,
    consider increasing the window size (with the <code class="option">-w</code> option)
    and running multiple streams to fully test the bandwidth capability. If
    using the standard MTU, your NICs should be capable of running at
    approximately 70-80% of the advertised bandwidth. If you move up to jumbo
    frames, the NIC should be able to saturate the link.
   </p><p>
    This is not necessarily true for faster topologies such as 100 Gb. In
    those topologies, saturating the NIC can require substantial OS and driver
    tuning, in combination with ensuring the hardware has the appropriate CPU
    clock speeds and settings.
   </p><p>
    This is a sample of the <code class="command">iperf3</code> commands used on a
    100 Gb network. In the command line, the <code class="option">-N</code> disables
    Nagle's buffering algorithm and the <code class="option">-l</code> sets the buffer
    length to higher than the default 128 kB, resulting in slightly higher
    throughput.
   </p><div class="verbatim-wrap"><pre class="screen">  server# iperf3 -s

  client# iperf3   -c server -N -l 256k
  Connecting to host sr650-1, port 5201
  [  4] local 172.16.227.22 port 36628 connected to 172.16.227.21 port 5201
  [ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
  [  4]   0.00-1.00   sec  4.76 GBytes  40.9 Gbits/sec    0   1.48 MBytes
  [  4]   1.00-2.00   sec  4.79 GBytes  41.1 Gbits/sec    0   2.52 MBytes
  [  4]   2.00-3.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   3.00-4.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   4.00-5.00   sec  4.74 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   5.00-6.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   6.00-7.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   7.00-8.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   8.00-9.00   sec  4.73 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   9.00-10.00  sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  - - - - - - - - - - - - - - - - - - - - - - - - -
  [ ID] Interval           Transfer     Bandwidth       Retr
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec    0             sender
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec                  receiver</pre></div></section><section class="sect2" id="id-1.5.3.3.4.6" data-id-title="Storage Performance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.2 </span><span class="title-name">Storage Performance</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.4.6">#</a></h3></div></div></div><p>
    Use <code class="command">fio</code> to test individual storage devices to understand
    the per-device performance maxima. Do this for all devices to ensure that
    none are out of specification, or are connected to expanders which lower
    bandwidth. Doing an exhaustive study of different I/O sizes and patterns
    would provide the most information about performance expectations, but is
    beyond the scope of this document.
   </p><p>
    We recommend testing at least random 4 kB, random 64 kB, and
    sequential 64 kB and 1 MB buffers. This should give a reasonable
    overall view of the device’s performance characteristics. When testing, it
    is important to use the raw device
    (<code class="literal">/dev/sd<em class="replaceable">X</em></code>) and to use the
    <code class="option">direct=1</code> option with multiple jobs to maximize device
    performance under stress.
   </p><p>
    Make sure that the test size (dataset) is large enough to over-run any
    caches that may apply. We recommend using the <code class="literal">ramp-time</code>
    parameter to allow for enough time for the cache overrun to occur before
    performance measurements are made. This helps to ensure that performance
    numbers are not tainted by cache-only performance.
   </p><p>
    Run <code class="command">fio</code> against all devices in an OSD node
    simultaneously to identify bottlenecks. It should scale very near linearly;
    if not, check controller firmware, slot placement, and if necessary, split
    devices across multiple controllers. This simulates the node under heavy
    load.
   </p><p>
    We recommend using the same I/O patterns and block sizes that the
    individual devices were tested with. The job count should be a multiple of
    the total number of devices in the system, to allow for even distribution
    across all devices and buses.
   </p><section class="sect3" id="id-1.5.3.3.4.6.7" data-id-title="Latency Bound and Maximum"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.1.2.1 </span><span class="title-name">Latency Bound and Maximum</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.4.6.7">#</a></h4></div></div></div><p>
     There is value in performing both latency-bound and worst-case-scenario
     tests. Changing a particular value may not improve latency, but rather
     enable the cluster to handle even more total load, even though latencies
     continue to rise. The inverse may also be true, where a change affects
     latency of operations in a measurable way. To identify both possibilities,
     we recommend that tests be performed that represent both positions.
     Latency-bounded tests in <code class="command">fio</code> have the following set:
    </p><div class="verbatim-wrap"><pre class="screen">  latency_target=10ms
  latency_window=5s
  latency_percentile=99</pre></div><p>
     The settings above cause <code class="command">fio</code> to increase the I/O queue
     depth until 1% or more of IOPS no longer maintain a 10 ms average
     during a sliding five-second window. It then backs down the queue depth
     until the latency average is maintained.
    </p></section></section></section><section class="sect1" id="tuning-kernel-tuning" data-id-title="Kernel Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Kernel Tuning</span> <a title="Permalink" class="permalink" href="tuning-os.html#tuning-kernel-tuning">#</a></h2></div></div></div><p>
   There are several aspects of the kernel that can be tuned on both the
   cluster and some of the clients. It is important to understand that most
   tuning is about gaining very small incremental improvements, which in
   aggregate represent a measureable (and hopefully meaningful) improvement in
   performance. For this document, information on tuning comes from a variety
   of sources. The primary source is
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html</a>.
   We also used numerous other references, including documentation from
   hardware vendors.
  </p><section class="sect2" id="id-1.5.3.3.5.3" data-id-title="CPU Mitigations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.1 </span><span class="title-name">CPU Mitigations</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.3">#</a></h3></div></div></div><p>
    One key area of kernel performance tuning is to disable its side-channel
    attack mitigations. The bulk of the benefits from this occur with smaller
    I/Os, ranging in size from 4 kB to 64 kB. In particular,
    64 kB random read and sequential writes doubled in performance in a
    limited test environment using only two client nodes.
   </p><p>
    Changing these options requires a clear understanding of the security
    implications, as they involve disabling mitigations for side-channel
    attacks on some CPUs. The need to disable these mitigations may be
    minimized with newer processors. You must carefully evaluate whether this
    is something needed for the particular hardware being utilized. In the test
    configuration, a Salt state was utilized to apply these changes. The
    Salt state should be in a subdirectory of <code class="filename">/srv/salt</code>
    on the Salt master and is applied by using a <code class="command">salt
    state.apply</code> command similar to below:
   </p><div class="verbatim-wrap"><pre class="screen">salt '*' state.apply my_kerntune</pre></div><p>
    The Salt state and steps used in this testing can be found in
    <a class="xref" href="tuning-appendix-a.html" title="Appendix A. Salt State for Kernel Tuning">Appendix A, <em>Salt State for Kernel Tuning</em></a>. This needs to be adjusted to work in
    each customer environment. An example of adjusting the
    <code class="literal">grub2</code> configuration file can also be found in the
    appendix.
   </p></section><section class="sect2" id="id-1.5.3.3.5.4" data-id-title="I/O tuning - Multiqueue Block I/O"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.2 </span><span class="title-name">I/O tuning - Multiqueue Block I/O</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.4">#</a></h3></div></div></div><p>
    The first aspect to tune is to ensure that I/O is flowing in the most
    optimal pattern. For the test cluster used in this test, that means
    enabling multi-queue block I/O. This is done through adding a boot-time
    kernel parameter, as found in Section 12.4
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-tuning-io.html#cha-tuning-io-barrier" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-tuning-io.html#cha-tuning-io-barrier</a>.
    This is not an action that should be taken unilaterally on clusters which
    contain spinning media devices, due to potential performance degradation
    for those devices. The general result is that there multiple I/O queues are
    assigned to each device, allowing more jobs to be handled simultaneously by
    those device that can service large numbers of requests, such as SSD and
    NVMe drives.
   </p></section><section class="sect2" id="id-1.5.3.3.5.5" data-id-title="SSD Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.3 </span><span class="title-name">SSD Tuning</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.5">#</a></h3></div></div></div><p>
    To get the best read performance, it may be necessary to adjust the
    <code class="option">read_ahead</code> and write cache settings for the SSD devices.
    In our particular testing environment, disabling write cache and forcing
    <code class="option">read_ahead</code> to 2 MB resulted in the best overall
    performance.
   </p><p>
    Before tuning, it is important to check and record the default values, and
    measure any differences in performance compared to that baseline.
   </p><p>
    By placing the following file in <code class="filename">/etc/udev/rules.d</code>,
    devices will be detected according to the model name shown in
    <code class="filename">/sys/block/{devname}/device/model</code>, and instructed to
    disable write caching and set the <code class="option">read_ahead_kb</code> option to
    2 MB.
   </p><div class="verbatim-wrap"><pre class="screen">  /etc/udev/rules.d/99-ssd.rules

  # Setting specific kernel parameters for a subset of block devices (Intel SSDs)
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", ATTR{queue/read_ahead_kb}="2048"
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", RUN+="/sbin/hdparm -W 0 /dev/%k"</pre></div></section><section class="sect2" id="id-1.5.3.3.5.6" data-id-title="Network Stack and Device Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.4 </span><span class="title-name">Network Stack and Device Tuning</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6">#</a></h3></div></div></div><p>
    Proper tuning of the network stack can substantially assist improving the
    latency and throughput of the cluster. A full script for the testing we
    performed can be found in <a class="xref" href="tuning-appendix-c.html" title="Appendix C. Network Tuning">Appendix C, <em>Network Tuning</em></a>.
   </p><p>
    The first change, and one with the highest impact, is to utilize jumbo
    frame packets. For this to be done, all interfaces utilizing the cluster
    must be set to use an MTU of 9000 bytes. Network switches are often set to
    use 9100 or higher. This is suitable, as they are only passing packets, not
    creating them.
   </p><section class="sect3" id="id-1.5.3.3.5.6.4" data-id-title="Network Device Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.4.1 </span><span class="title-name">Network Device Tuning</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.4">#</a></h4></div></div></div><section class="sect4" id="id-1.5.3.3.5.6.4.2" data-id-title="Jumbo Frames"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.1 </span><span class="title-name">Jumbo Frames</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.4.2">#</a></h5></div></div></div><p>
      The following Salt command ensures the bonded interfaces on all nodes
      under control (including the test load generation nodes) were utilizing
      an MTU of 9000:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'ip link set bond0 mtu 9000'</pre></div><p>
      To set this persistently, utilize YaST to set the MTU for the bonded
      interface.
     </p></section><section class="sect4" id="id-1.5.3.3.5.6.4.3" data-id-title="PCIe Bus Adjustment"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.2 </span><span class="title-name">PCIe Bus Adjustment</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.4.3">#</a></h5></div></div></div><p>
      Adjusting the PCIe maximum read request size can provide a slight boost
      to performance. Be aware that this tuning is card- and slot-specific and
      must only be done in conjunction with the conditions and instructions
      supplied by the manufacturer. The maximum PCIe read request size was set
      with the following Salt commands:
     </p><div id="id-1.5.3.3.5.6.4.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       This should only be done with guidance from the NIC manufacturer and is
       specific to bus location, driver version and hardware.
      </p></div><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'setpci -s 5b:00.0 68.w=5936'
  salt '*' cmd.run 'setpci -s 5b:00.1 68.w=5936'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.4.4" data-id-title="TCP RSS"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.3 </span><span class="title-name">TCP RSS</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.4.4">#</a></h5></div></div></div><p>
      The next item on the tuning list is helpful in ensuring that a single CPU
      core is not responsible for all packet processing. A small script is used
      to spread the I/O across multiple local (from a NUMA perspective) cores.
     </p><div id="id-1.5.3.3.5.6.4.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       This is not necessary if the number of queues returned by <code class="command">ls
       /sys/class/net/{ifname}/queues/rx-*|wc -l</code> is equal to the
       number of physical cores in a single CPU socket.
      </p></div><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'for j in `cat /sys/class/net/bond0/bonding/slaves`;do \
LOCAL_CPUS=`cat /sys/class/net/$j/device/local_cpus`;echo $LOCAL_CPUS &gt; \
/sys/class/net/$j/queues/rx-0/rps_cpus;done'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.4.5" data-id-title="Ring Buffers"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.4 </span><span class="title-name">Ring Buffers</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.4.5">#</a></h5></div></div></div><p>
      Many NIC drivers start with a default value for the receive (RX) and
      transmit (TX) buffers that is not optimal for high-throughput scenarios,
      and does not allow enough time for the kernel to drain the buffer before
      it fills up.
     </p><p>
      The current and maximum settings can be revealed by issuing the following
      command to the proper NICs:
     </p><div class="verbatim-wrap"><pre class="screen">ethtool -g eth4</pre></div><p>
      The output from this command should look similar to this:
     </p><div class="verbatim-wrap"><pre class="screen">  Ring parameters for eth4:
  Pre-set maximums:
  RX:		8192
  RX Mini:	0
  RX Jumbo:	0
  TX:		8192
  Current hardware settings:
  RX:		1024
  RX Mini:	0
  RX Jumbo:	0
  TX:		1024</pre></div><p>
      Here we can see that the NIC can allocate buffers of up to 8 kB, but
      it iscurrently using ones of only 1 kB. To adjust this for the
      cluster, issue the following command:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'ethtool -G eth4 rx 8192 tx 8192'
  salt '*' cmd.run 'ethtool -G eth5 rx 8192 tx 8192'</pre></div><p>
      Setting this value persistently can be achieved via the YaST
      configuration module:
     </p><div class="figure" id="yast-config-module"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_ring_buffers.png" target="_blank"><img src="images/yast_ring_buffers.png" width="" alt="YaST Configuration Module"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">YaST Configuration Module </span><a title="Permalink" class="permalink" href="tuning-os.html#yast-config-module">#</a></h6></div></div><p>
      Additionally, the settings can be made persistent by editing the
      configuration files for the physical interfaces found in
      <code class="filename">/etc/sysconfig/network</code>. A script can be found in
      Appendix B that will change all interfaces to the maximum ring buffer
      value.
     </p></section></section><section class="sect3" id="id-1.5.3.3.5.6.5" data-id-title="Network Stack"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.4.2 </span><span class="title-name">Network Stack</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.5">#</a></h4></div></div></div><p>
     The following settings can all be made persistent by modifying
     <code class="filename">/etc/sysctl.conf</code>. They are represented as arguments
     in a Salt command to allow testing and validation in your environment
     before making them permanent.
    </p><section class="sect4" id="id-1.5.3.3.5.6.5.3" data-id-title="Lower TCP Latency"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.1 </span><span class="title-name">Lower TCP Latency</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.5.3">#</a></h5></div></div></div><p>
      Setting the <code class="literal">TCP low latency</code> option disables IPv4 TCP
      pre-queue processing and improves latency. We recommend experimenting
      with setting this to both <code class="literal">0</code> and <code class="literal">1</code>.
      In laboratory testing, setting the value to <code class="literal">1</code> provided
      slightly better performance:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_low_latency=1'</pre></div><p>
      The TCP <code class="option">fastopen</code> option allows the sending of data in
      the first <code class="literal">syn</code> packet, resulting in a slight
      improvement in latency:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_fastopen=1'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.4" data-id-title="TCP Stack Buffers"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.2 </span><span class="title-name">TCP Stack Buffers</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.5.4">#</a></h5></div></div></div><p>
      Ensure that the TCP stack has sufficent buffer space to queue both
      inbound and outbound traffic:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_rmem="10240 87380 2147483647"'
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_wmem="10240 87380 2147483647"'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.5" data-id-title="TCP Sequence and Timestamps"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.3 </span><span class="title-name">TCP Sequence and Timestamps</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.5.5">#</a></h5></div></div></div><p>
      In fast networks, TCP sequence numbers can be re-used in a very short
      timeframe. The result is that the system thinks a packet has been
      received out of order, resulting in a drop. TCP timestamps were added to
      help ensure that packet sequence could be tracked better:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_timestamps=1'</pre></div><p>
      <code class="literal">TCP Selective Acknowledgement</code> is a feature that is
      primarily useful for WAN or lower-speed networks. However, disabling it
      may have negative effects in other ways:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_sack=1'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.6" data-id-title="Kernel Network Buffers and Connections"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.4 </span><span class="title-name">Kernel Network Buffers and Connections</span> <a title="Permalink" class="permalink" href="tuning-os.html#id-1.5.3.3.5.6.5.6">#</a></h5></div></div></div><p>
      Providing plenty of buffer space is a recurring theme in tuning networks
      for high performance. The <code class="literal">netdev_max_backlog</code> is where
      traffic is queued after it has been received by the NIC, but before it is
      processed by the protocol stack (IP, TCP, etc):
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.netdev_max_backlog=250000'</pre></div><p>
      Other preventative measures for the system and gateway nodes include
      ensuring that the maximum connection count is high enough to prevent the
      generation of <code class="literal">syn</code> cookies. This is useful to set on
      all nodes involved:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.somaxconn=2048'</pre></div><p>
      Increasing the network stack buffers is useful to ensure that sufficient
      buffers exist for all transactions:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.rmem_max=2147483647'
salt '*' cmd.run 'sysctl -w net.core.wmem_max=2147483647'</pre></div></section></section></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="tuning-architecture.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 4 </span>Architecture and Hardware Tuning</span></a> </div><div><a class="pagination-link next" href="tuning-ceph.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 6 </span>Ceph Tuning</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="tuning-os.html#tuning-sles"><span class="title-number">5.1 </span><span class="title-name">SUSE Linux Enterprise Install and Validation of Base Performance</span></a></span></li><li><span class="sect1"><a href="tuning-os.html#tuning-kernel-tuning"><span class="title-number">5.2 </span><span class="title-name">Kernel Tuning</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/tuning-os-tuning.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>