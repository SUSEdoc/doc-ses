<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Hints and Tips | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Hints and Tips | SES 6"/>
<meta name="description" content="The chapter provides information to help you enhance performance of your Ceph cluster and provides tips how to set the cluster up."/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 33. Hints and Tips"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Hints and Tips | SES 6"/>
<meta property="og:description" content="The chapter provides information to help you enhance performance of your Ceph cluster and provides tips how to set the cluster up."/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hints and Tips | SES 6"/>
<meta name="twitter:description" content="The chapter provides information to help you enhance performance of your Ceph cluster and provides tips how to set the cluster up."/>
<link rel="prev" href="part-troubleshooting.html" title="Part VI. FAQs, Tips and Troubleshooting"/><link rel="next" href="storage-faqs.html" title="Chapter 34. Frequently Asked Questions"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-troubleshooting.html">FAQs, Tips and Troubleshooting</a><span> / </span><a class="crumb" href="storage-tips.html">Hints and Tips</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li><a href="part-operate.html" class="has-children "><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li><a href="part-dataccess.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li class="active"><a href="part-troubleshooting.html" class="has-children you-are-here"><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" you-are-here"><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="storage-tips" data-id-title="Hints and Tips"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">33 </span><span class="title-name">Hints and Tips</span> <a title="Permalink" class="permalink" href="storage-tips.html#">#</a></h2></div></div></div><p>
  The chapter provides information to help you enhance performance of your
  Ceph cluster and provides tips how to set the cluster up.
 </p><section class="sect1" id="tips-orphaned-partitions" data-id-title="Identifying Orphaned Partitions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.1 </span><span class="title-name">Identifying Orphaned Partitions</span> <a title="Permalink" class="permalink" href="storage-tips.html#tips-orphaned-partitions">#</a></h2></div></div></div><p>
   To identify possibly orphaned journal/WAL/DB devices, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Pick the device that may have orphaned partitions and save the list of its
     partitions to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ls /dev/sdd?* &gt; /tmp/partitions</pre></div></li><li class="step"><p>
     Run <code class="command">readlink</code> against all block.wal, block.db, and
     journal devices, and compare the output to the previously saved list of
     partitions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -</pre></div><p>
     The output is the list of partitions that are <span class="emphasis"><em>not</em></span>
     used by Ceph.
    </p></li><li class="step"><p>
     Remove the orphaned partitions that do not belong to Ceph with your
     preferred command (for example, <code class="command">fdisk</code>,
     <code class="command">parted</code>, or <code class="command">sgdisk</code>).
    </p></li></ol></div></div></section><section class="sect1" id="tips-scrubbing" data-id-title="Adjusting Scrubbing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.2 </span><span class="title-name">Adjusting Scrubbing</span> <a title="Permalink" class="permalink" href="storage-tips.html#tips-scrubbing">#</a></h2></div></div></div><p>
   By default, Ceph performs light scrubbing daily (find more details in
   <a class="xref" href="cha-storage-datamgm.html#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) and deep scrubbing weekly.
   <span class="emphasis"><em>Light</em></span> scrubbing checks object sizes and checksums to
   ensure that placement groups are storing the same object data.
   <span class="emphasis"><em>Deep</em></span> scrubbing checks an object’s content with that of
   its replicas to ensure that the actual contents are the same. The price for
   checking data integrity is increased I/O load on the cluster during the
   scrubbing procedure.
  </p><p>
   The default settings allow Ceph OSDs to initiate scrubbing at inappropriate
   times, such as during periods of heavy loads. Customers may experience
   latency and poor performance when scrubbing operations conflict with their
   operations. Ceph provides several scrubbing settings that can limit
   scrubbing to periods with lower loads or during off-peak hours.
  </p><p>
   If the cluster experiences high loads during the day and low loads late at
   night, consider restricting scrubbing to night time hours, such as 11pm till
   6am:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6</pre></div><p>
   If time restriction is not an effective method of determining a scrubbing
   schedule, consider using the <code class="option">osd_scrub_load_threshold</code>
   option. The default value is 0.5, but it could be modified for low load
   conditions:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_load_threshold = 0.25</pre></div></section><section class="sect1" id="tips-stopping-osd-without-rebalancing" data-id-title="Stopping OSDs without Rebalancing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span> <a title="Permalink" class="permalink" href="storage-tips.html#tips-stopping-osd-without-rebalancing">#</a></h2></div></div></div><p>
   You may need to stop OSDs for maintenance periodically. If you do not want
   CRUSH to automatically rebalance the cluster in order to avoid huge data
   transfers, set the cluster to <code class="literal">noout</code> first:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ceph osd set noout</pre></div><p>
   When the cluster is set to <code class="literal">noout</code>, you can begin stopping
   the OSDs within the failure domain that requires maintenance work:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl stop ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   Find more information in
   <a class="xref" href="ceph-operating-services.html#ceph-operating-services-individual" title="16.1.2. Starting, Stopping, and Restarting Individual Services">Section 16.1.2, “Starting, Stopping, and Restarting Individual Services”</a>.
  </p><p>
   After you complete the maintenance, start OSDs again:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl start ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   After OSD services are started, unset the cluster from
   <code class="literal">noout</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd unset noout</pre></div></section><section class="sect1" id="Cluster-Time-Setting" data-id-title="Time Synchronization of Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.4 </span><span class="title-name">Time Synchronization of Nodes</span> <a title="Permalink" class="permalink" href="storage-tips.html#Cluster-Time-Setting">#</a></h2></div></div></div><p>
   Ceph requires precise time synchronization between all nodes.
  </p><p>
   We recommend synchronizing all Ceph cluster nodes with at least three
   reliable time sources that are located on the internal network. The internal
   time sources can point to a public time server or have their own time
   source.
  </p><div id="id-1.3.8.2.7.4" data-id-title="Public Time Servers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Public Time Servers</h6><p>
    Do not synchronize all Ceph cluster nodes directly with remote public
    time servers. With such a configuration, each node in the cluster has its
    own NTP daemon that communicates continually over the Internet with a set
    of three or four time servers that may provide slightly different times.
    This solution introduces a large degree of latency variability that makes
    it difficult or impossible to keep the clock drift under 0.05 seconds,
    which is what the Ceph monitors require.
   </p></div><p>
   For details how to set up the NTP server refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html" target="_blank">SUSE Linux Enterprise Server
   Administration Guide</a>.
  </p><p>
   Then to change the time on your cluster, do the following:
  </p><div id="id-1.3.8.2.7.7" data-id-title="Setting Time" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Setting Time</h6><p>
    You may face a situation when you need to set the time back, for example if
    the time changes from the summer to the standard time. We do not recommend
    to move the time backward for a longer period than the cluster is down.
    Moving the time forward does not cause any trouble.
   </p></div><div class="procedure" id="id-1.3.8.2.7.8" data-id-title="Time Synchronization on the Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 33.1: </span><span class="title-name">Time Synchronization on the Cluster </span><a title="Permalink" class="permalink" href="storage-tips.html#id-1.3.8.2.7.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop all clients accessing the Ceph cluster, especially those using
     iSCSI.
    </p></li><li class="step"><p>
     Shut down your Ceph cluster. On each node run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph.target</pre></div><div id="id-1.3.8.2.7.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If you use Ceph and SUSE OpenStack Cloud, stop also the SUSE OpenStack Cloud.
     </p></div></li><li class="step"><p>
     Verify that your NTP server is set up correctly—all
     <code class="systemitem">chronyd</code> daemons get their time
     from a source or sources in the local network.
    </p></li><li class="step"><p>
     Set the correct time on your NTP server.
    </p></li><li class="step"><p>
     Verify that NTP is running and working properly, on all nodes run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl status chronyd.service</pre></div></li><li class="step"><p>
     Start all monitoring nodes and verify that there is no clock skew:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start <em class="replaceable">target</em></pre></div></li><li class="step"><p>
     Start all OSD nodes.
    </p></li><li class="step"><p>
     Start other Ceph services.
    </p></li><li class="step"><p>
     Start the SUSE OpenStack Cloud if you have it.
    </p></li></ol></div></div></section><section class="sect1" id="storage-bp-cluster-mntc-unbalanced" data-id-title="Checking for Unbalanced Data Writing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-cluster-mntc-unbalanced">#</a></h2></div></div></div><p>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <span class="emphasis"><em>weight</em></span>. The
   weight is a relative number and tells Ceph how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </p><p>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </p><p>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given ruleset, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written—you can increase their weight to
   have Ceph write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </p><div class="verbatim-wrap"><pre class="screen">$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</pre></div><div id="id-1.3.8.2.8.7" data-id-title="OSD Reweight by Utilization" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: OSD Reweight by Utilization</h6><p>
    The <code class="command">ceph osd reweight-by-utilization</code>
    <em class="replaceable">threshold</em> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </p></div></section><section class="sect1" id="storage-tips-ceph-btrfs-subvol" data-id-title="Btrfs Subvolume for /var/lib/ceph on Ceph Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.6 </span><span class="title-name">Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-tips-ceph-btrfs-subvol">#</a></h2></div></div></div><p>
   SUSE Linux Enterprise is by default installed on a Btrfs partition. Ceph Monitors store their state
   and database in the <code class="filename">/var/lib/ceph</code> directory. To prevent
   corruption of a Ceph Monitor from a system rollback of a previous snapshot, create
   a Btrfs subvolume for <code class="filename">/var/lib/ceph</code>. A dedicated
   subvolume excludes the monitor data from snapshots of the root subvolume.
  </p><div id="id-1.3.8.2.9.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    Create the <code class="filename">/var/lib/ceph</code> subvolume before running
    DeepSea stage 0 because stage 0 installs Ceph related packages and
    creates the <code class="filename">/var/lib/ceph</code> directory.
   </p></div><p>
   DeepSea stage 3 then verifies whether <code class="filename">@/var/lib/ceph</code>
   is a Btrfs subvolume and fails if it is a normal directory.
  </p><section class="sect2" id="btrfs-subvol-requirements" data-id-title="Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.1 </span><span class="title-name">Requirements</span> <a title="Permalink" class="permalink" href="storage-tips.html#btrfs-subvol-requirements">#</a></h3></div></div></div><section class="sect3" id="tips-ceph-btrfs-subvol-new" data-id-title="New Deployments"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.1.1 </span><span class="title-name">New Deployments</span> <a title="Permalink" class="permalink" href="storage-tips.html#tips-ceph-btrfs-subvol-new">#</a></h4></div></div></div><p>
     Salt and DeepSea need to be properly installed and working.
    </p></section><section class="sect3" id="storage-tips-ceph-btrfs-subvol-req-existing" data-id-title="Existing Deployments"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.1.2 </span><span class="title-name">Existing Deployments</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-tips-ceph-btrfs-subvol-req-existing">#</a></h4></div></div></div><p>
     If your cluster is already installed, the following requirements must be
     met:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Nodes are upgraded to SUSE Enterprise Storage 6 and cluster is under
       DeepSea control.
      </p></li><li class="listitem"><p>
       Ceph cluster is up and healthy.
      </p></li><li class="listitem"><p>
       Upgrade process has synchronized Salt and DeepSea modules to all
       minion nodes.
      </p></li></ul></div></section></section><section class="sect2" id="storage-tips-ceph-btrfs-subvol-automatic" data-id-title="Steps Required during a New Cluster Deployment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.2 </span><span class="title-name">Steps Required during a New Cluster Deployment</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-tips-ceph-btrfs-subvol-automatic">#</a></h3></div></div></div><section class="sect3" id="var-lib-ceph-stage0" data-id-title="Before Running DeepSea stage 0"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.2.1 </span><span class="title-name">Before Running DeepSea stage 0</span> <a title="Permalink" class="permalink" href="storage-tips.html#var-lib-ceph-stage0">#</a></h4></div></div></div><p>
     Prior to running DeepSea stage 0, apply the following commands to each
     of the Salt minions that will become Ceph Monitors:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">MONITOR_NODES</em>' saltutil.sync_all
<code class="prompt user">root@master # </code>salt '<em class="replaceable">MONITOR_NODES</em>' state.apply ceph.subvolume</pre></div><p>
     The <code class="command">ceph.subvolume</code> command does the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creates <code class="filename">/var/lib/ceph</code> as a
       <code class="literal">@/var/lib/ceph</code> Btrfs subvolume.
      </p></li><li class="listitem"><p>
       Mounts the new subvolume and updates <code class="filename">/etc/fstab</code>
       appropriately.
      </p></li></ul></div></section><section class="sect3" id="id-1.3.8.2.9.6.3" data-id-title="DeepSea stage 3 Validation Fails"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.2.2 </span><span class="title-name">DeepSea stage 3 Validation Fails</span> <a title="Permalink" class="permalink" href="storage-tips.html#id-1.3.8.2.9.6.3">#</a></h4></div></div></div><p>
     If you forgot to run the commands mentioned in
     <a class="xref" href="storage-tips.html#var-lib-ceph-stage0" title="33.6.2.1. Before Running DeepSea stage 0">Section 33.6.2.1, “Before Running DeepSea stage 0”</a> before running stage 0, the
     <code class="filename">/var/lib/ceph</code> subdirectory already exists, causing
     DeepSea stage 3 validation failure. To convert it into a subvolume, do
     the following:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Change directory to <code class="filename">/var/lib</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib</pre></div></li><li class="step"><p>
       Back up the current content of the <code class="filename">ceph</code>
       subdirectory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>sudo mv ceph ceph-</pre></div></li><li class="step"><p>
       Create the subvolume and, mount it, and update
       <code class="filename">/etc/fstab</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'MONITOR_NODES' state.apply ceph.subvolume</pre></div></li><li class="step"><p>
       Change to the backup subdirectory, synchronize its content with the new
       subvolume, then remove it:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib/ceph-
<code class="prompt user">cephadm@mon &gt; </code>rsync -av . ../ceph
<code class="prompt user">cephadm@mon &gt; </code>cd ..
<code class="prompt user">cephadm@mon &gt; </code>rm -rf ./ceph-</pre></div></li></ol></div></div></section></section><section class="sect2" id="btrfs-subvol-upgrades" data-id-title="Steps Required during Cluster Upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.3 </span><span class="title-name">Steps Required during Cluster Upgrade</span> <a title="Permalink" class="permalink" href="storage-tips.html#btrfs-subvol-upgrades">#</a></h3></div></div></div><p>
    On SUSE Enterprise Storage 5.5, the <code class="filename">/var</code>
    directory is not on a Btrfs subvolume, but its subfolders (such as
    <code class="filename">/var/log</code> or <code class="filename">/var/cache</code>) are Btrfs
    subvolumes under '@'. Creating <code class="filename">@/var/lib/ceph</code>
    subvolumes requires mounting the '@' subvolume first (it is not mounted by
    default) and creating the <code class="filename">@/var/lib/ceph</code> subvolume
    under it.
   </p><p>
    Following are example commands that illustrate the process:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /mnt/btrfs
<code class="prompt user">root # </code>mount -o subvol=@ <em class="replaceable">ROOT_DEVICE</em> /mnt/btrfs
<code class="prompt user">root # </code>btrfs subvolume create /mnt/btrfs/var/lib/ceph
<code class="prompt user">root # </code>umount /mnt/btrfs</pre></div><p>
    At this point the <code class="filename">@/var/lib/ceph</code> subvolume is created
    and you can continue as described in
    <a class="xref" href="storage-tips.html#storage-tips-ceph-btrfs-subvol-automatic" title="33.6.2. Steps Required during a New Cluster Deployment">Section 33.6.2, “Steps Required during a New Cluster Deployment”</a>.
   </p></section><section class="sect2" id="var-lib-ceph-subvol-manual" data-id-title="Manual Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.4 </span><span class="title-name">Manual Setup</span> <a title="Permalink" class="permalink" href="storage-tips.html#var-lib-ceph-subvol-manual">#</a></h3></div></div></div><p>
    Automatic setup of the <code class="filename">@/var/lib/ceph</code> Btrfs subvolume
    on the Ceph Monitor nodes may not be suitable for all scenarios. You can migrate
    your <code class="filename">/var/lib/ceph</code> directory to a
    <code class="filename">@/var/lib/ceph</code> subvolume by following these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Terminate running Ceph processes.
     </p></li><li class="step"><p>
      Unmount OSDs on the node.
     </p></li><li class="step"><p>
      Change to the backup subdirectory, synchronize its content with the new
      subvolume, then remove it:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib/ceph-
<code class="prompt user">cephadm@mon &gt; </code>rsync -av . ../ceph
<code class="prompt user">cephadm@mon &gt; </code>cd ..
<code class="prompt user">cephadm@mon &gt; </code>rm -rf ./ceph-</pre></div></li><li class="step"><p>
      Remount OSDs.
     </p></li><li class="step"><p>
      Restart Ceph daemons.
     </p></li></ol></div></div></section><section class="sect2" id="var-lib-ceph-subvol-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.5 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="storage-tips.html#var-lib-ceph-subvol-moreinfo">#</a></h3></div></div></div><p>
    Find more detailed information about manual setup in the file
    <code class="filename">/srv/salt/ceph/subvolume/README.md</code> on the Salt master
    node.
   </p></section></section><section class="sect1" id="storage-bp-srv-maint-fds-inc" data-id-title="Increasing File Descriptors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.7 </span><span class="title-name">Increasing File Descriptors</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-srv-maint-fds-inc">#</a></h2></div></div></div><p>
   For OSD daemons, the read/write operations are critical to keep the Ceph
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </p><p>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <code class="filename">/etc/ceph/ceph.conf</code>, for example:
  </p><div class="verbatim-wrap"><pre class="screen">max_open_files = 131072</pre></div><p>
   After you change <code class="option">max_open_files</code>, you need to restart the
   OSD service on the relevant Ceph node.
  </p></section><section class="sect1" id="storage-admin-integration" data-id-title="Integration with Virtualization Software"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.8 </span><span class="title-name">Integration with Virtualization Software</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-admin-integration">#</a></h2></div></div></div><section class="sect2" id="storage-bp-integration-kvm" data-id-title="Storing KVM Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.1 </span><span class="title-name">Storing KVM Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-integration-kvm">#</a></h3></div></div></div><p>
    You can create a disk image for KVM-driven virtual machine, store it in a
    Ceph pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <code class="command">qemu-kvm</code> making use of
    the disk image stored in the cluster. For more detailed information, see
    <a class="xref" href="cha-ceph-kvm.html" title="Chapter 32. Ceph as a Back-end for QEMU KVM Instance">Chapter 32, <em>Ceph as a Back-end for QEMU KVM Instance</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-libvirt" data-id-title="Storing libvirt Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.2 </span><span class="title-name">Storing <code class="systemitem">libvirt</code> Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-integration-libvirt">#</a></h3></div></div></div><p>
    Similar to KVM (see <a class="xref" href="storage-tips.html#storage-bp-integration-kvm" title="33.8.1. Storing KVM Disks in Ceph Cluster">Section 33.8.1, “Storing KVM Disks in Ceph Cluster”</a>), you
    can use Ceph to store virtual machines driven by <code class="systemitem">libvirt</code>. The advantage
    is that you can run any <code class="systemitem">libvirt</code>-supported virtualization solution, such
    as KVM, Xen, or LXC. For more information, see
    <a class="xref" href="cha-ceph-libvirt.html" title="Chapter 31. Using libvirt with Ceph">Chapter 31, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-xen" data-id-title="Storing Xen Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.3 </span><span class="title-name">Storing Xen Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-integration-xen">#</a></h3></div></div></div><p>
    One way to use Ceph for storing Xen disks is to make use of <code class="systemitem">libvirt</code>
    as described in <a class="xref" href="cha-ceph-libvirt.html" title="Chapter 31. Using libvirt with Ceph">Chapter 31, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p><p>
    Another option is to make Xen talk to the <code class="systemitem">rbd</code>
    block device driver directly:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you have no disk image prepared for Xen, create a new one:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create myimage --size 8000 --pool mypool</pre></div></li><li class="step"><p>
      List images in the pool <code class="literal">mypool</code> and check if your new
      image is there:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
      Create a new block device by mapping the <code class="literal">myimage</code> image
      to the <code class="systemitem">rbd</code> kernel module:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.8.2.11.4.4.3.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
       To specify a user name, use <code class="option">--id
       <em class="replaceable">user-name</em></code>. Moreover, if you use
       <code class="systemitem">cephx</code> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="systemitem">cephadm</code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
      List all mapped devices:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="command">rbd showmapped</code>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div></li><li class="step"><p>
      Now you can configure Xen to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <code class="command">xl</code>-style domain configuration file:
     </p><div class="verbatim-wrap"><pre class="screen">disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</pre></div></li></ol></div></div></section></section><section class="sect1" id="storage-bp-net-firewall" data-id-title="Firewall Settings for Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.9 </span><span class="title-name">Firewall Settings for Ceph</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-net-firewall">#</a></h2></div></div></div><div id="id-1.3.8.2.12.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
    DeepSea deployment stages fail when firewall is active (and even
    configured). To pass the stages correctly, you need to either turn the
    firewall off by running
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop SuSEfirewall2.service</pre></div><p>
    or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div><p>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <span class="guimenu">YaST</span> / <span class="guimenu">Security and
   Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
   Services</span>.
  </p><p>
   Following is a list of Ceph related services and numbers of the ports that
   they normally use:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.8.2.12.5.1"><span class="term">Ceph Monitor</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph MON</span> service or port 6789 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.2"><span class="term">Ceph OSD or Metadata Server</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph OSD/MDS</span> service or ports 6800-7300
      (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.3"><span class="term">iSCSI Gateway</span></dt><dd><p>
      Open port 3260 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.4"><span class="term">Object Gateway</span></dt><dd><p>
      Open the port where Object Gateway communication occurs. It is set in
      <code class="filename">/etc/ceph.conf</code> on the line starting with
      <code class="literal">rgw frontends =</code>. Default is 80 for HTTP and 443 for
      HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.5"><span class="term">NFS Ganesha</span></dt><dd><p>
      By default, NFS Ganesha uses ports 2049 (NFS service, TCP) and 875 (rquota
      support, TCP). Refer to <a class="xref" href="cha-ceph-nfsganesha.html#ganesha-nfsport" title="30.2.1.4. Changing Default NFS Ganesha Ports">Section 30.2.1.4, “Changing Default NFS Ganesha Ports”</a> for more
      information on changing the default NFS Ganesha ports.
     </p></dd><dt id="id-1.3.8.2.12.5.6"><span class="term">Apache based services, such as SMT, or SUSE Manager</span></dt><dd><p>
      Open ports 80 for HTTP and 443 for HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.7"><span class="term">SSH</span></dt><dd><p>
      Open port 22 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.8"><span class="term">NTP</span></dt><dd><p>
      Open port 123 (UDP).
     </p></dd><dt id="id-1.3.8.2.12.5.9"><span class="term">Salt</span></dt><dd><p>
      Open ports 4505 and 4506 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.10"><span class="term">Grafana</span></dt><dd><p>
      Open port 3000 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.11"><span class="term">Prometheus</span></dt><dd><p>
      Open port 9100 (TCP).
     </p></dd></dl></div></section><section class="sect1" id="storage-bp-network-test" data-id-title="Testing Network Performance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.10 </span><span class="title-name">Testing Network Performance</span> <a title="Permalink" class="permalink" href="storage-tips.html#storage-bp-network-test">#</a></h2></div></div></div><p>
   To test the network performance, DeepSea's <code class="literal">net</code> runner
   provides the following commands:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A simple ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</pre></div></li><li class="listitem"><p>
     A jumbo ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</pre></div></li><li class="listitem"><p>
     A bandwidth test:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</pre></div><div id="id-1.3.8.2.13.3.3.3" data-id-title="Stop iperf3 Processes Manually" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Stop 'iperf3' Processes Manually</h6><p>
      When running a test using the <code class="command">net.iperf</code> runner, the
      'iperf3' server processes that are started do not stop automatically when
      a test is completed. To stop the processes, use the following runner:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' multi.kill_iperf_cmd</pre></div></div></li></ul></div></section><section class="sect1" id="bp-flash-led-lights" data-id-title="How to Locate Physical Disks Using LED Lights"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.11 </span><span class="title-name">How to Locate Physical Disks Using LED Lights</span> <a title="Permalink" class="permalink" href="storage-tips.html#bp-flash-led-lights">#</a></h2></div></div></div><p>
   This section describes using <code class="systemitem">libstoragemgmt</code> and/or
   third party tools to adjust the LED lights on physical disks. This
   capability may not be available for all hardware platforms.
  </p><p>
   Matching an OSD disk to a physical disk can be challenging, especially on
   nodes with a high density of disks. Some hardware environments include LED
   lights that can be adjusted via software to flash or illuminate a different
   color for identification purposes. SUSE Enterprise Storage offers support for this
   capability through Salt, <code class="systemitem">libstoragemgmt</code>, and
   third party tools specific to the hardware in use. The configuration for
   this capability is defined in the
   <code class="filename">/srv/pillar/ceph/disk_led.sls</code> Salt pillar:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> cat /srv/pillar/ceph/disk_led.sls
# This is the default configuration for the storage enclosure LED blinking.
# The placeholder {device_file} will be replaced with the device file of
# the disk when the command is executed.
#
# Have a look into the /srv/pillar/ceph/README file to find out how to
# customize this configuration per minion/host.

disk_led:
  cmd:
    ident:
      'on': lsmcli local-disk-ident-led-on --path '{device_file}'
      'off': lsmcli local-disk-ident-led-off --path '{device_file}'
    fault:
      'on': lsmcli local-disk-fault-led-on --path '{device_file}'
      'off': lsmcli local-disk-fault-led-off --path '{device_file}'</pre></div><p>
   The default configuration for <code class="filename">disk_led.sls</code> offers disk
   LED support through the <code class="systemitem">libstoragemgmt</code> layer. The
   <code class="systemitem">libstoragemgmt</code> layer provides this support through
   a hardware-specific plug-in and third party tools. The default behavior can
   be customized using DeepSea by adding the following for the
   <code class="literal">ledmon</code> package and <code class="literal">ledctl</code> tool to
   <code class="filename">/srv/pillar/ceph/stack/global.yml</code> (or any other YAML
   file in <code class="filename">/stack/ceph/</code>):
  </p><div class="verbatim-wrap"><pre class="screen">  disk_led:
    cmd:
      ident:
        'on': ledctl locate='{device_file}'
        'off': ledctl locate_off='{device_file}'
      fault:
        'on': ledctl locate='{device_file}'
        'off': ledctl locate_off='{device_file}'</pre></div><p>
   If the customization should only apply to a special node (minion), then the
   file <code class="filename">stack/ceph/minions/{{minion}}.yml</code> needs to be
   used.
  </p><p>
   With or without <code class="systemitem">libstoragemgmt</code>, third party tools
   may be required to adjust LED lights. These third party tools are available
   through various hardware vendors. Some of the common vendors and tools are:
  </p><div class="table" id="id-1.3.8.2.14.9" data-id-title="Third Party Storage Tools"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 33.1: </span><span class="title-name">Third Party Storage Tools </span><a title="Permalink" class="permalink" href="storage-tips.html#id-1.3.8.2.14.9">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Vendor/Disk Controller</th><th style="border-bottom: 1px solid ; ">Tool</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HPE SmartArray</td><td style="border-bottom: 1px solid ; ">hpssacli</td></tr><tr><td style="border-right: 1px solid ; ">LSI MegaRAID</td><td>storcli</td></tr></tbody></table></div></div><p>
   SUSE Linux Enterprise Server also provides the <span class="package">ledmon</span> package and
   <code class="command">ledctl</code> tool. This tool may also work for hardware
   environments utilizing Intel storage enclosures. Proper syntax when using
   this tool is as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'
    fault:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'</pre></div><p>
   If you are on supported hardware, with all required third party tools, LEDs
   can be enabled or disabled using the following command syntax from the
   Salt master node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run disk_led.device <em class="replaceable">NODE</em> <em class="replaceable">DISK</em> <em class="replaceable">fault|ident</em> <em class="replaceable">on|off</em></pre></div><p>
   For example, to enable or disable LED identification or fault lights on
   <code class="filename">/dev/sdd</code> on OSD node <code class="filename">srv16.ceph</code>,
   run the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd ident on
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd ident off
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd fault on
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd fault off</pre></div><div id="id-1.3.8.2.14.16" data-id-title="Device Naming" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Device Naming</h6><p>
    The device name used in the <code class="command">salt-run</code> command needs to
    match the name recognized by Salt. The following command can be used to
    display these names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">minion_name</em>' grains.get disks</pre></div></div><p>
   In many environments, the <code class="filename">/srv/pillar/ceph/disk_led.sls</code>
   configuration will require changes in order to adjust the LED lights for
   specific hardware needs. Simple changes may be performed by replacing
   <code class="command">lsmcli</code> with another tool, or adjusting command line
   parameters. Complex changes may be accomplished by calling an external
   script in place of the <code class="filename">lsmcli</code> command. When making any
   changes to <code class="filename">/srv/pillar/ceph/disk_led.sls</code>, follow these
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make required changes to
     <code class="filename">/srv/pillar/ceph/disk_led.sls</code> on the Salt master node.
    </p></li><li class="step"><p>
     Verify that the changes are reflected correctly in the pillar data:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt '<em class="replaceable">SALT MASTER</em>*' pillar.get disk_led</pre></div></li><li class="step"><p>
     Refresh the pillar data on all nodes using:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt '*' saltutil.pillar_refresh</pre></div></li></ol></div></div><p>
   It is possible to use an external script to directly use third-party tools
   to adjust LED lights. The following examples show how to adjust
   <code class="filename">/srv/pillar/ceph/disk_led.sls</code> to support an external
   script, and two sample scripts for HP and LSI environments.
  </p><p>
   Modified <code class="filename">/srv/pillar/ceph/disk_led.sls</code> which calls an
   external script:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off
    fault:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off</pre></div><p>
   Sample script for flashing LED lights on HP hardware using the
   <code class="systemitem">hpssacli</code> utilities:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /usr/local/bin/flash_led_hp.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

FOUND=0
MAX_CTRLS=10
MAX_DISKS=50

for i in $(seq 0 $MAX_CTRLS); do
  # Search for valid controllers
  if hpssacli ctrl slot=$i show summary &gt;/dev/null; then
    # Search all disks on the current controller
    for j in $(seq 0 $MAX_DISKS); do
      if hpssacli ctrl slot=$i ld $j show | grep -q $1; then
        FOUND=1
        echo "Found $1 on ctrl=$i, ld=$j. Turning LED $2."
        hpssacli ctrl slot=$i ld $j modify led=$2
        break;
      fi
    done
    [[ "$FOUND" = "1" ]] &amp;&amp; break
  fi
done</pre></div><p>
   Sample script for flashing LED lights on LSI hardware using the
   <code class="systemitem">storcli</code> utilities:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /usr/local/bin/flash_led_lsi.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

[[ "$2" = "on" ]] &amp;&amp; ACTION="start" || ACTION="stop"

# Determine serial number for the disk
SERIAL=$(lshw -class disk | grep -A2 $1 | grep serial | awk '{print $NF}')
if [ ! -z "$SERIAL" ]; then
  # Search for disk serial number across all controllers and enclosures
  DEVICE=$(/opt/MegaRAID/storcli/storcli64 /call/eall/sall show all | grep -B6 $SERIAL | grep Drive | awk '{print $2}')
  if [ ! -z "$DEVICE" ]; then
    echo "Found $1 on device $DEVICE. Turning LED $2."
    /opt/MegaRAID/storcli/storcli64 $DEVICE $ACTION locate
  else
    echo "Device not found!"
    exit -1
  fi
else
  echo "Disk serial number not found!"
  exit -1
fi</pre></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="part-troubleshooting.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part VI </span>FAQs, Tips and Troubleshooting</span></a> </div><div><a class="pagination-link next" href="storage-faqs.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 34 </span>Frequently Asked Questions</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="storage-tips.html#tips-orphaned-partitions"><span class="title-number">33.1 </span><span class="title-name">Identifying Orphaned Partitions</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#tips-scrubbing"><span class="title-number">33.2 </span><span class="title-name">Adjusting Scrubbing</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#tips-stopping-osd-without-rebalancing"><span class="title-number">33.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#Cluster-Time-Setting"><span class="title-number">33.4 </span><span class="title-name">Time Synchronization of Nodes</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-bp-cluster-mntc-unbalanced"><span class="title-number">33.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-tips-ceph-btrfs-subvol"><span class="title-number">33.6 </span><span class="title-name">Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-bp-srv-maint-fds-inc"><span class="title-number">33.7 </span><span class="title-name">Increasing File Descriptors</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-admin-integration"><span class="title-number">33.8 </span><span class="title-name">Integration with Virtualization Software</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-bp-net-firewall"><span class="title-number">33.9 </span><span class="title-name">Firewall Settings for Ceph</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#storage-bp-network-test"><span class="title-number">33.10 </span><span class="title-name">Testing Network Performance</span></a></span></li><li><span class="sect1"><a href="storage-tips.html#bp-flash-led-lights"><span class="title-number">33.11 </span><span class="title-name">How to Locate Physical Disks Using LED Lights</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_ceph_tips.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>