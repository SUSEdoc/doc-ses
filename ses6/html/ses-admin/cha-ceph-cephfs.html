<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Clustered File System | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Clustered File System | SES 6"/>
<meta name="description" content="This chapter describes administration tasks that are normally performed after the cluster is set up and CephFS exported. If you need more information on settin…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 28. Clustered File System"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Clustered File System | SES 6"/>
<meta property="og:description" content="This chapter describes administration tasks that are normally performed after the cluster is set up and CephFS exported. If you need more information on settin…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Clustered File System | SES 6"/>
<meta name="twitter:description" content="This chapter describes administration tasks that are normally performed after the cluster is set up and CephFS exported. If you need more information on settin…"/>
<link rel="prev" href="cha-ceph-iscsi.html" title="Chapter 27. Ceph iSCSI Gateway"/><link rel="next" href="cha-ses-cifs.html" title="Chapter 29. Exporting Ceph Data via Samba"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-dataccess.html">Accessing Cluster Data</a><span> / </span><a class="crumb" href="cha-ceph-cephfs.html">Clustered File System</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li><a href="part-operate.html" class="has-children "><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li class="active"><a href="part-dataccess.html" class="has-children you-are-here"><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" you-are-here"><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-cephfs" data-id-title="Clustered File System"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">28 </span><span class="title-name">Clustered File System</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#">#</a></h2></div></div></div><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
 </p><section class="sect1" id="ceph-cephfs-cephfs-mount" data-id-title="Mounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.1 </span><span class="title-name">Mounting CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-mount">#</a></h2></div></div></div><p>
   When the file system is created and the MDS is active, you are ready to
   mount the file system from a client host.
  </p><section class="sect2" id="cephfs-client-preparation" data-id-title="Client Preparation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.1 </span><span class="title-name">Client Preparation</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-client-preparation">#</a></h3></div></div></div><p>
    If the client host is running SUSE Linux Enterprise 12 SP2 or SP3, you can skip this
    section as the system is ready to mount CephFS 'out of the box'.
   </p><p>
    If the client host is running SUSE Linux Enterprise 12 SP1, you need to apply all the
    latest patches before mounting CephFS.
   </p><p>
    In any case, everything needed to mount CephFS is included in SUSE Linux Enterprise. The
    SUSE Enterprise Storage 6 product is not needed.
   </p><p>
    To support the full <code class="command">mount</code> syntax, the
    <span class="package">ceph-common</span> package (which is shipped with SUSE Linux Enterprise) should
    be installed before trying to mount CephFS.
   </p><div id="id-1.3.6.4.4.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Without the <span class="package">ceph-common</span> package (and thus without the
     <code class="command">mount.ceph</code> helper), the monitors' IPs will need to be
     used instead of their names. This is because the kernel client will be
     unable to perform name resolution.
    </p><p>
     The basic mount syntax is:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>sudo mount -t ceph monip[:port][,monip2[:port]...]:/[subdir] mnt</pre></div></div></section><section class="sect2" id="Creating-Secret-File" data-id-title="Create a Secret File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.2 </span><span class="title-name">Create a Secret File</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#Creating-Secret-File">#</a></h3></div></div></div><p>
    The Ceph cluster runs with authentication turned on by default. You
    should create a file that stores your secret key (not the keyring itself).
    To obtain the secret key for a particular user and then create the file, do
    the following:
   </p><div class="procedure" id="id-1.3.6.4.4.4.3" data-id-title="Creating a Secret Key"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 28.1: </span><span class="title-name">Creating a Secret Key </span><a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#id-1.3.6.4.4.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      View the key for the particular user in a keyring file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>cat /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
      Copy the key of the user who will be using the mounted Ceph FS file
      system. Usually, the key looks similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen">AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</pre></div></li><li class="step"><p>
      Create a file with the user name as a file name part, for example
      <code class="filename">/etc/ceph/admin.secret</code> for the user
      <span class="emphasis"><em>admin</em></span>.
     </p></li><li class="step"><p>
      Paste the key value to the file created in the previous step.
     </p></li><li class="step"><p>
      Set proper access rights to the file. The user should be the only one who
      can read the file—others may not have any access rights.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-cephfs-krnldrv" data-id-title="Mount CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.3 </span><span class="title-name">Mount CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-krnldrv">#</a></h3></div></div></div><p>
    You can mount CephFS with the <code class="command">mount</code> command. You need
    to specify the monitor host name or IP address. Because the
    <code class="systemitem">cephx</code> authentication is enabled by default in
    SUSE Enterprise Storage, you need to specify a user name and their related secret as
    well:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><p>
    As the previous command remains in the shell history, a more secure
    approach is to read the secret from a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    Note that the secret file should only contain the actual keyring secret. In
    our example, the file will then contain only the following line:
   </p><div class="verbatim-wrap"><pre class="screen">AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><div id="id-1.3.6.4.4.5.8" data-id-title="Specify Multiple Monitors" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Specify Multiple Monitors</h6><p>
     It is a good idea to specify multiple monitors separated by commas on the
     <code class="command">mount</code> command line in case one monitor happens to be
     down at the time of mount. Each monitor address takes the form
     <code class="literal">host[:port]</code>. If the port is not specified, it defaults
     to 6789.
    </p></div><p>
    Create the mount point on the local host:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /mnt/cephfs</pre></div><p>
    Mount the CephFS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    A subdirectory <code class="filename">subdir</code> may be specified if a subset of
    the file system is to be mounted:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    You can specify more than one monitor host in the <code class="command">mount</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><div id="id-1.3.6.4.4.5.17" data-id-title="Read Access to the Root Directory" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Read Access to the Root Directory</h6><p>
     If clients with path restriction are used, the MDS capabilities need to
     include read access to the root directory. For example, a keyring may look
     as follows:
    </p><div class="verbatim-wrap"><pre class="screen">client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</pre></div><p>
     The <code class="literal">allow r path=/</code> part means that path-restricted
     clients are able to see the root volume, but cannot write to it. This may
     be an issue for use cases where complete isolation is a requirement.
    </p></div></section></section><section class="sect1" id="ceph-cephfs-cephfs-unmount" data-id-title="Unmounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.2 </span><span class="title-name">Unmounting CephFS</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-unmount">#</a></h2></div></div></div><p>
   To unmount the CephFS, use the <code class="command">umount</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>umount /mnt/cephfs</pre></div></section><section class="sect1" id="ceph-cephfs-cephfs-fstab" data-id-title="CephFS in /etc/fstab"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-fstab">#</a></h2></div></div></div><p>
   To mount CephFS automatically upon client start-up, insert the
   corresponding line in its file systems table
   <code class="filename">/etc/fstab</code>:
  </p><div class="verbatim-wrap"><pre class="screen">mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</pre></div></section><section class="sect1" id="ceph-cephfs-activeactive" data-id-title="Multiple Active MDS Daemons (Active-Active MDS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-activeactive">#</a></h2></div></div></div><p>
   CephFS is configured for a single active MDS daemon by default. To scale
   metadata performance for large-scale systems, you can enable multiple active
   MDS daemons, which will share the metadata workload with one another.
  </p><section class="sect2" id="id-1.3.6.4.7.3" data-id-title="When to Use Active-Active MDS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.1 </span><span class="title-name">When to Use Active-Active MDS</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#id-1.3.6.4.7.3">#</a></h3></div></div></div><p>
    Consider using multiple active MDS daemons when your metadata performance
    is bottlenecked on the default single MDS.
   </p><p>
    Adding more daemons does not increase performance on all workload types.
    For example, a single application running on a single client will not
    benefit from an increased number of MDS daemons unless the application is
    doing a lot of metadata operations in parallel.
   </p><p>
    Workloads that typically benefit from a larger number of active MDS daemons
    are those with many clients, perhaps working on many separate directories.
   </p></section><section class="sect2" id="cephfs-activeactive-increase" data-id-title="Increasing the MDS Active Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.2 </span><span class="title-name">Increasing the MDS Active Cluster Size</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-activeactive-increase">#</a></h3></div></div></div><p>
    Each CephFS file system has a <code class="option">max_mds</code> setting, which
    controls how many ranks will be created. The actual number of ranks in the
    file system will only be increased if a spare daemon is available to take
    on the new rank. For example, if there is only one MDS daemon running and
    <code class="option">max_mds</code> is set to two, no second rank will be created.
   </p><p>
    In the following example, we set the <code class="option">max_mds</code> option to 2
    to create a new rank apart from the default one. To see the changes, run
    <code class="command">ceph status</code> before and after you set
    <code class="option">max_mds</code>, and watch the line containing
    <code class="literal">fsmap</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 2
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</pre></div><p>
    The newly created rank (1) passes through the 'creating' state and then
    enter its 'active' state.
   </p><div id="id-1.3.6.4.7.4.6" data-id-title="Standby Daemons" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Standby Daemons</h6><p>
     Even with multiple active MDS daemons, a highly available system still
     requires standby daemons to take over if any of the servers running an
     active daemon fail.
    </p><p>
     Consequently, the practical maximum of <code class="option">max_mds</code> for highly
     available systems is one less than the total number of MDS servers in your
     system. To remain available in the event of multiple server failures,
     increase the number of standby daemons in the system to match the number
     of server failures you need to survive.
    </p></div></section><section class="sect2" id="cephfs-activeactive-decrease" data-id-title="Decreasing the Number of Ranks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.3 </span><span class="title-name">Decreasing the Number of Ranks</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-activeactive-decrease">#</a></h3></div></div></div><p>
    All ranks—including the ranks to be removed—must first be
    active. This means that you need to have at least <code class="option">max_mds</code>
    MDS daemons available.
   </p><p>
    First, set <code class="option">max_mds</code> to a lower number. For example, go back
    to having a single active MDS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 1
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</pre></div><p>
    Note that we still have two active MDSs. The ranks still exist even though
    we have decreased <code class="option">max_mds</code>, because
    <code class="option">max_mds</code> only restricts the creation of new ranks.
   </p><p>
    To reduce the number of ranks, reduce the <code class="option">max_mds</code> option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 1</pre></div><p>
    The deactivated rank will first enter the stopping state, for a period of
    time while it hands off its share of the metadata to the remaining active
    daemons. This phase can take from seconds to minutes. If the MDS appears to
    be stuck in the stopping state then that should be investigated as a
    possible bug.
   </p><p>
    If an MDS daemon crashes or is terminated while in the 'stopping' state, a
    standby will take over and the rank will go back to 'active'. You can try
    to deactivate it again when it has come back up.
   </p><p>
    When a daemon finishes stopping, it will start again and go back to being a
    standby.
   </p></section><section class="sect2" id="cephfs-activeactive-pinning" data-id-title="Manually Pinning Directory Trees to a Rank"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.4 </span><span class="title-name">Manually Pinning Directory Trees to a Rank</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-activeactive-pinning">#</a></h3></div></div></div><p>
    In multiple active metadata server configurations, a balancer runs, which
    works to spread metadata load evenly across the cluster. This usually works
    well enough for most users, but sometimes it is desirable to override the
    dynamic balancer with explicit mappings of metadata to particular ranks.
    This can allow the administrator or users to evenly spread application load
    or limit impact of users' metadata requests on the entire cluster.
   </p><p>
    The mechanism provided for this purpose is called an 'export pin'. It is an
    extended attribute of directories. The name of this extended attribute is
    <code class="literal">ceph.dir.pin</code>. Users can set this attribute using
    standard commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>setfattr -n ceph.dir.pin -v 2 <em class="replaceable">/path/to/dir</em></pre></div><p>
    The value (<code class="option">-v</code>) of the extended attribute is the rank to
    assign the directory sub-tree to. A default value of -1 indicates that the
    directory is not pinned.
   </p><p>
    A directory export pin is inherited from its closest parent with a set
    export pin. Therefore, setting the export pin on a directory affects all of
    its children. However, the parent's pin can be overridden by setting the
    child directory export pin. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</pre></div></section></section><section class="sect1" id="ceph-cephfs-failover" data-id-title="Managing Failover"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.5 </span><span class="title-name">Managing Failover</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-failover">#</a></h2></div></div></div><p>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <code class="option">mds_beacon_grace</code> seconds (default 15 seconds) before
   marking the daemon as <span class="emphasis"><em>laggy</em></span>. You can configure one or
   more 'standby' daemons that will take over during the MDS daemon failover.
  </p><section class="sect2" id="ceph-cephfs-failover-standby" data-id-title="Configuring Standby Replay"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.1 </span><span class="title-name">Configuring Standby Replay</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-failover-standby">#</a></h3></div></div></div><p>
    Each CephFS file system may be configured to add standby-replay daemons.
    These standby daemons follow the active MDS's metadata journal to reduce
    failover time in the event that the active MDS becomes unavailable. Each
    active MDS may have only one standby-replay daemon following it.
   </p><p>
    Configure standby-replay on a file system with the following comamnd:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">FS-NAME</em> allow_standby_replay <em class="replaceable">BOOL</em></pre></div><p>
    Once set the monitors will assign available standby daemons to follow the
    active MDSs in that file system.
   </p><p>
    Once an MDS has entered the standby-replay state, it will only be used as a
    standby for the rank that it is following. If another rank fails, this
    standby-replay daemon will not be used as a replacement, even if no other
    standbys are available. For this reason, it is advised that if
    standby-replay is used then every active MDS should have a standby-replay
    daemon.
   </p></section><section class="sect2" id="ceph-cephfs-failover-examples" data-id-title="Examples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.2 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#ceph-cephfs-failover-examples">#</a></h3></div></div></div><p>
    Several example <code class="filename">ceph.conf</code> configurations follow. You
    can either copy a <code class="filename">ceph.conf</code> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains that server's daemon configuration.
   </p><section class="sect3" id="id-1.3.6.4.8.4.3" data-id-title="Simple Pair"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.2.1 </span><span class="title-name">Simple Pair</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#id-1.3.6.4.8.4.3">#</a></h4></div></div></div><p>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </p><div class="verbatim-wrap"><pre class="screen">[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</pre></div></section></section></section><section class="sect1" id="cephfs-quotas" data-id-title="Setting CephFS Quotas"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.6 </span><span class="title-name">Setting CephFS Quotas</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-quotas">#</a></h2></div></div></div><p>
   You can set quotas on any subdirectory of the Ceph file system. The quota
   restricts either the number of <span class="bold"><strong>bytes</strong></span> or
   <span class="bold"><strong>files</strong></span> stored beneath the specified point in
   the directory hierarchy.
  </p><section class="sect2" id="cephfs-quotas-limitation" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.6.1 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-quotas-limitation">#</a></h3></div></div></div><p>
    Using quotas with CephFS has the following limitations:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.9.3.3.1"><span class="term">Quotas are cooperative and non-competing.</span></dt><dd><p>
       Ceph quotas rely on the client that is mounting the file system to
       stop writing to it when a limit is reached. The server part cannot
       prevent a malicious client from writing as much data as it needs. Do not
       use quotas to prevent filling the file system in environments where the
       clients are fully untrusted.
      </p></dd><dt id="id-1.3.6.4.9.3.3.2"><span class="term">Quotas are imprecise.</span></dt><dd><p>
       Processes that are writing to the file system will be stopped shortly
       after the quota limit is reached. They will inevitably be allowed to
       write some amount of data over the configured limit. Client writers will
       be stopped within tenths of seconds after crossing the configured limit.
      </p></dd><dt id="id-1.3.6.4.9.3.3.3"><span class="term">Quotas are implemented in the kernel client from version 4.17.</span></dt><dd><p>
       Quotas are supported by the user space client (libcephfs, ceph-fuse).
       Linux kernel clients 4.17 and higher support CephFS quotas on
       SUSE Enterprise Storage 6 clusters. Kernel clients (even recent
       versions) will fail to handle quotas on older clusters, even if they are
       able to set the quotas extended attributes.
      </p></dd><dt id="id-1.3.6.4.9.3.3.4"><span class="term">Configure quotas carefully when used with path-based mount restrictions.</span></dt><dd><p>
       The client needs to have access to the directory inode on which quotas
       are configured in order to enforce them. If the client has restricted
       access to a specific path (for example <code class="filename">/home/user</code>)
       based on the MDS capability, and a quota is configured on an ancestor
       directory they do not have access to (<code class="filename">/home</code>), the
       client will not enforce it. When using path-based access restrictions,
       be sure to configure the quota on the directory that the client can
       access (for example <code class="filename">/home/user</code> or
       <code class="filename">/home/user/quota_dir</code>).
      </p></dd></dl></div></section><section class="sect2" id="cephfs-quotas-config" data-id-title="Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.6.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-quotas-config">#</a></h3></div></div></div><p>
    You can configure CephFS quotas by using virtual extended attributes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.9.4.3.1"><span class="term"><code class="option">ceph.quota.max_files</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>file</em></span> limit.
      </p></dd><dt id="id-1.3.6.4.9.4.3.2"><span class="term"><code class="option">ceph.quota.max_bytes</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>byte</em></span> limit.
      </p></dd></dl></div><p>
    If the attributes appear on a directory inode, a quota is configured there.
    If they are not present then no quota is set on that directory (although
    one may still be configured on a parent directory).
   </p><p>
    To set a 100 MB quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 100000000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To set a 10,000 files quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_files -v 10000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To view quota setting, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>getfattr -n ceph.quota.max_bytes <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>getfattr -n ceph.quota.max_files <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div id="id-1.3.6.4.9.4.12" data-id-title="Quota Not Set" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quota Not Set</h6><p>
     If the value of the extended attribute is '0', the quota is not set.
    </p></div><p>
    To remove a quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 0 <em class="replaceable">/SOME/DIRECTORY</em>
<code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_files -v 0 <em class="replaceable">/SOME/DIRECTORY</em></pre></div></section></section><section class="sect1" id="cephfs-snapshots" data-id-title="Managing CephFS Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.7 </span><span class="title-name">Managing CephFS Snapshots</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-snapshots">#</a></h2></div></div></div><p>
   CephFS snapshots create a read-only view of the file system at the point
   in time they are taken. You can create a snapshot in any directory. The
   snapshot will cover all data in the file system under the specified
   directory. After creating a snapshot, the buffered data is flushed out
   asynchronously from various clients. As a result, creating a snapshot is
   very fast.
  </p><div id="id-1.3.6.4.10.3" data-id-title="Multiple File Systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Multiple File Systems</h6><p>
    If you have multiple CephFS file systems sharing a single pool (via name
    spaces), their snapshots will collide, and deleting one snapshot will
    result in missing file data for other snapshots sharing the same pool.
   </p></div><section class="sect2" id="cephfs-snapshots-create" data-id-title="Creating Snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.7.1 </span><span class="title-name">Creating Snapshots</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-snapshots-create">#</a></h3></div></div></div><p>
    The CephFS snapshot feature is enabled by default on new file systems. To
    enable it on existing file systems, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">CEPHFS_NAME</em> allow_new_snaps true</pre></div><p>
    After you enable snapshots, all directories in the CephFS will have a
    special <code class="filename">.snap</code> subdirectory.
   </p><div id="id-1.3.6.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This is a <span class="emphasis"><em>virtual</em></span> subdirectory. It does not appear in
     the directory listing of the parent directory, but the name
     <code class="filename">.snap</code> cannot be used as a file or directory name. To
     access the <code class="filename">.snap</code> directory one needs to explicitly
     access it, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls -la /<em class="replaceable">CEPHFS_MOUNT</em>/.snap/</pre></div></div><div id="id-1.3.6.4.10.4.6" data-id-title="Kernel clients limitation" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Kernel clients limitation</h6><p>
     CephFS kernel clients have a limitation: they cannot handle more than
     400 snapshots in a file system. The number of snapshots should always be
     kept below this limit, regardless of which client you are using. If using
     older CephFS clients, such as SLE12-SP3, keep in mind that going above
     400 snapshots is harmful to operations as the client will crash.
    </p></div><div id="id-1.3.6.4.10.4.7" data-id-title="Custom Snapshot Subdirectory Name" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom Snapshot Subdirectory Name</h6><p>
     You may configure a different name for the snapshots subdirectory by
     setting the <code class="option">client snapdir</code> setting.
    </p></div><p>
    To create a snapshot, create a subdirectory under the
    <code class="filename">.snap</code> directory with a custom name. For example, to
    create a snapshot of the directory
    <code class="filename">/<em class="replaceable">CEPHFS_MOUNT</em>/2/3/</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section><section class="sect2" id="cephfs-snapshots-delete" data-id-title="Deleting Snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.7.2 </span><span class="title-name">Deleting Snapshots</span> <a title="Permalink" class="permalink" href="cha-ceph-cephfs.html#cephfs-snapshots-delete">#</a></h3></div></div></div><p>
    To delete a snapshot, remove its subdirectory inside the
    <code class="filename">.snap</code> directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>rmdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ceph-iscsi.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 27 </span>Ceph iSCSI Gateway</span></a> </div><div><a class="pagination-link next" href="cha-ses-cifs.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 29 </span>Exporting Ceph Data via Samba</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-mount"><span class="title-number">28.1 </span><span class="title-name">Mounting CephFS</span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-unmount"><span class="title-number">28.2 </span><span class="title-name">Unmounting CephFS</span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#ceph-cephfs-cephfs-fstab"><span class="title-number">28.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#ceph-cephfs-activeactive"><span class="title-number">28.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#ceph-cephfs-failover"><span class="title-number">28.5 </span><span class="title-name">Managing Failover</span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#cephfs-quotas"><span class="title-number">28.6 </span><span class="title-name">Setting CephFS Quotas</span></a></span></li><li><span class="sect1"><a href="cha-ceph-cephfs.html#cephfs-snapshots"><span class="title-number">28.7 </span><span class="title-name">Managing CephFS Snapshots</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_ceph_cephfs.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>