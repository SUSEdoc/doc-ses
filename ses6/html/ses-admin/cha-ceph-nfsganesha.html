<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>NFS Ganesha: Export Ceph Data via NFS | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="NFS Ganesha: Export Ceph Data via NFS | SES 6"/>
<meta name="description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 30. NFS Ganesha: Export Ceph Data via NFS"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="NFS Ganesha: Export Ceph Data via NFS | SES 6"/>
<meta property="og:description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NFS Ganesha: Export Ceph Data via NFS | SES 6"/>
<meta name="twitter:description" content="NFS Ganesha is an NFS server (refer to Sharing File Systems with NFS ) that runs in a user address space instead of as part of the operating system kernel. Wit…"/>
<link rel="prev" href="cha-ses-cifs.html" title="Chapter 29. Exporting Ceph Data via Samba"/><link rel="next" href="part-virt.html" title="Part V. Integration with Virtualization Tools"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-dataccess.html">Accessing Cluster Data</a><span> / </span><a class="crumb" href="cha-ceph-nfsganesha.html">NFS Ganesha: Export Ceph Data via NFS</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li><a href="part-operate.html" class="has-children "><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li class="active"><a href="part-dataccess.html" class="has-children you-are-here"><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" you-are-here"><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-nfsganesha" data-id-title="NFS Ganesha: Export Ceph Data via NFS"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#">#</a></h2></div></div></div><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p><p>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/<em class="replaceable">USERNAME</em>/<em class="replaceable">BUCKETNAME</em></code>.
 </p><p>
  A CephFS is exported by default via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/cephfs</code>.
 </p><div id="id-1.3.6.6.6" data-id-title="NFS Ganesha Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha Performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing Ceph via
   an NFS Gateway may significantly reduce application performance when
   compared to native CephFS or Object Gateway clients.
  </p></div><section class="sect1" id="ceph-nfsganesha-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-install">#</a></h2></div></div></div><p>
   For installation instructions, see <span class="intraxref">Book “Deployment Guide”, Chapter 12 “Installation of NFS Ganesha”</span>.
  </p></section><section class="sect1" id="ceph-nfsganesha-config" data-id-title="Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config">#</a></h2></div></div></div><p>
   For a list of all parameters available within the configuration file, see:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">man ganesha-config</code>
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-ceph-config</code> for CephFS File System
     Abstraction Layer (FSAL) options.
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-rgw-config</code> for Object Gateway FSAL options.
    </p></li></ul></div><p>
   This section includes information to help you configure the NFS Ganesha server
   to export the cluster data accessible via Object Gateway and CephFS.
  </p><div id="id-1.3.6.6.8.5" data-id-title="Restart the NFS Ganesha Service" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Restart the NFS Ganesha Service</h6><p>
    When you configure NFS Ganesha via the Ceph Dashboard, the
    <code class="systemitem">nfs-ganesha.service</code> service is
    restarted automatically for the changes to take effect.
   </p><p>
    When you configure NFS Ganesha manually, you need to restart the
    <code class="systemitem">nfs-ganesha.service</code> service on the
    NFS Ganesha node to re-read the new configuration:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart nfs-ganesha.service</pre></div></div><p>
   NFS Ganesha configuration consists of two parts: service configuration and
   exports configuration. The service configuration is controlled by
   <code class="filename">/etc/ganesha/ganesha.conf</code>. Note that changes to this
   file are overwritten when DeepSea stage 4 is executed. To persistently
   change the settings, edit the file
   <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> located on
   the Salt master. The exports configuration is stored in the Ceph cluster as
   RADOS objects.
  </p><section class="sect2" id="ceph-nfsganesha-config-service-general" data-id-title="Service Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.1 </span><span class="title-name">Service Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-service-general">#</a></h3></div></div></div><p>
    The service configuration is stored in
    <code class="filename">/etc/ganesha/ganesha.conf</code> and controls all NFS Ganesha
    daemon settings, including where the exports configuration are stored in
    the Ceph cluster. Note that changes to this file are overwritten when
    DeepSea stage 4 is executed. To persistently change the settings, edit
    the file <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code>
    located on the Salt master.
   </p><section class="sect3" id="ceph-nfsganesha-config-service-rados" data-id-title="RADOS_URLS Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.1 </span><span class="title-name">RADOS_URLS Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-service-rados">#</a></h4></div></div></div><p>
     The <code class="literal">RADOS_URLS</code> section configures the Ceph cluster
     access for reading NFS Ganesha configuration from RADOS objects.
    </p><div class="verbatim-wrap"><pre class="screen">RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<em class="replaceable">MINION_ID</em>";
  watch_url = "rados://<em class="replaceable">RADOS_POOL</em>/ganesha/conf-<em class="replaceable">MINION_ID</em>";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.7.3.4.1"><span class="term">Ceph_Conf</span></dt><dd><p>
        Ceph configuration file path location.
       </p></dd><dt id="id-1.3.6.6.8.7.3.4.2"><span class="term">UserId</span></dt><dd><p>
        The cephx user ID.
       </p></dd><dt id="id-1.3.6.6.8.7.3.4.3"><span class="term">watch_url</span></dt><dd><p>
        The RADOS object URL to watch for reload notifications.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-service-rgw" data-id-title="RGW Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.2 </span><span class="title-name">RGW Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-service-rgw">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.7.4.3.1"><span class="term">ceph_conf</span></dt><dd><p>
        Points to the <code class="filename">ceph.conf</code> file. When deploying with
        DeepSea, it is not necessary to change this value.
       </p></dd><dt id="id-1.3.6.6.8.7.4.3.2"><span class="term">name</span></dt><dd><p>
        The name of the Ceph client user used by NFS Ganesha.
       </p></dd><dt id="id-1.3.6.6.8.7.4.3.3"><span class="term">cluster</span></dt><dd><p>
        The name of the Ceph cluster. SUSE Enterprise Storage 6 currently
        only supports one cluster name, which is <code class="literal">ceph</code> by
        default.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-service-url" data-id-title="RADOS Object URL"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.3 </span><span class="title-name">RADOS Object URL</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-service-url">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">%url rados://<em class="replaceable">RADOS_POOL</em>/ganesha/conf-<em class="replaceable">MINION_ID</em></pre></div><p>
     NFS Ganesha supports reading the configuration from a RADOS object. The
     <code class="literal">%url</code> directive allows to specify a RADOS URL that
     identifies the location of the RADOS object.
    </p><p>
     A RADOS URL can be of two forms:
     <code class="literal">rados://&lt;POOL&gt;/&lt;OBJECT&gt;</code> or
     <code class="literal">rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</code>,
     where <code class="literal">POOL</code> is the RADOS pool where the object is
     stored, <code class="literal">NAMESPACE</code> the pool namespace where the object
     is stored, and <code class="literal">OBJECT</code> the object name.
    </p><p>
     To support the Ceph Dashboard's NFS Ganesha management capabilities, you need
     to follow a convention on the name of the RADOS object for each service
     daemon. The name of the object must be of the form
     <code class="literal">conf-<em class="replaceable">MINION_ID</em></code> where
     MINION_ID corresponds to the Salt minion ID of the node where this service
     is running.
    </p><p>
     DeepSea already takes care of correctly generating this URL, and you do
     not need to make any change.
    </p></section><section class="sect3" id="ganesha-nfsport" data-id-title="Changing Default NFS Ganesha Ports"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.4 </span><span class="title-name">Changing Default NFS Ganesha Ports</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ganesha-nfsport">#</a></h4></div></div></div><p>
     NFS Ganesha uses the port 2049 for NFS and 875 for the rquota support by
     default. To change the default port numbers, use the
     <code class="option">NFS_Port</code> and <code class="option">RQUOTA_Port</code> options inside
     the <code class="literal">NFS_CORE_PARAM</code> section, for example:
    </p><div class="verbatim-wrap"><pre class="screen">NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}</pre></div></section></section><section class="sect2" id="ceph-nfsganesha-config-exports-general" data-id-title="Exports Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.2 </span><span class="title-name">Exports Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-exports-general">#</a></h3></div></div></div><p>
    Exports configuration is stored as RADOS objects in the Ceph cluster.
    Each export block is stored in its own RADOS object named
    <code class="literal">export-<em class="replaceable">ID</em></code>, where
    <em class="replaceable">ID</em> must match the <code class="literal">Export_ID</code>
    attribute of the export configuration. The association between exports and
    NFS Ganesha services is done through the
    <code class="literal">conf-<em class="replaceable">MINION_ID</em></code> objects. Each
    service object contains a list of RADOS URLs for each export exported by
    that service. An export block looks like the following:
   </p><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><p>
    To create the RADOS object for the above export block, we first need to
    store the export block code in a file. Then we can use the RADOS CLI tool
    to store the contents of the previously saved file in a RADOS object.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p <em class="replaceable">POOL</em> -N <em class="replaceable">NAMESPACE</em> put export-<em class="replaceable">EXPORT_ID</em> <em class="replaceable">EXPORT_FILE</em></pre></div><p>
    After creating the export object, we can associate the export with a
    service instance by adding the corresponding RADOS URL of the export
    object to the service object. The following sections describe how to
    configure an export block.
   </p><section class="sect3" id="ceph-nfsganesha-config-general-export" data-id-title="Export Main Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.1 </span><span class="title-name">Export Main Section</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-export">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.8.7.2.1"><span class="term">Export_Id</span></dt><dd><p>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.2"><span class="term">Path</span></dt><dd><p>
        Export path in the related CephFS pool (mandatory). This allows
        subdirectories to be exported from the CephFS.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.3"><span class="term">Pseudo</span></dt><dd><p>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </p><p>
        Example: with the value <code class="literal">/cephfs/</code> and after executing
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount <em class="replaceable">GANESHA_IP</em>:/cephfs/ /mnt/</pre></div><p>
        The CephFS data is available in the directory
        <code class="filename">/mnt/cephfs/</code> on the client.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.4"><span class="term">Access_Type</span></dt><dd><p>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </p><div id="id-1.3.6.6.8.8.7.2.4.2.2" data-id-title="Limit Access to Clients" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Limit Access to Clients</h6><p>
         If you leave <code class="literal">Access_Type = RW</code> in the main
         <code class="literal">EXPORT</code> section and limit access to a specific
         client in the <code class="literal">CLIENT</code> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <code class="literal">Access_Type =
         None</code> in the <code class="literal">EXPORT</code> section and then
         specify less restrictive access mode for one or more clients in the
         <code class="literal">CLIENT</code> section:
        </p><div class="verbatim-wrap"><pre class="screen">EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}</pre></div></div></dd><dt id="id-1.3.6.6.8.8.7.2.5"><span class="term">Squash</span></dt><dd><p>
        NFS squash option.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.6"><span class="term">FSAL</span></dt><dd><p>
        Exporting 'File System Abstraction Layer'. See
        <a class="xref" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-fsal" title="30.2.2.2. FSAL Subsection">Section 30.2.2.2, “FSAL Subsection”</a>.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-general-fsal" data-id-title="FSAL Subsection"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.2 </span><span class="title-name">FSAL Subsection</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config-general-fsal">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.8.8.3.1"><span class="term">Name</span></dt><dd><p>
        Defines which back-end NFS Ganesha uses. Allowed values are
        <code class="literal">CEPH</code> for CephFS or <code class="literal">RGW</code> for
        Object Gateway. Depending on the choice, a <code class="literal">role-mds</code> or
        <code class="literal">role-rgw</code> must be defined in the
        <code class="filename">policy.cfg</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="ganesha-getting-exports" data-id-title="Obtaining Exports Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.3 </span><span class="title-name">Obtaining Exports Configuration</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ganesha-getting-exports">#</a></h3></div></div></div><p>
    To obtain existing exports configuration, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Find the RADOS pool name and namespace for NFS Ganesha exports. The
      following command outputs a string of the
      <em class="replaceable">POOL_NAME</em>/<em class="replaceable">NAMESPACE</em>
      form.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard get-ganesha-clusters-rados-pool-namespace
cephfs_data/ganesha</pre></div></li><li class="step"><p>
      By using the obtained pool name and namespace, list the RADOS objects
      available on that pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p cephfs_data -N ganesha ls
conf-osd-node1
export-1
conf-osd-node2</pre></div><div id="id-1.3.6.6.8.9.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
       To see how each node is configured, view its content using the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>cat conf-osd-node1
%url "rados://cephfs_data/ganesha/export-1"
cat conf-osd-node2
%url "rados://cephfs_data/ganesha/export-1"</pre></div><p>
       In this case, both nodes will use
       <code class="literal">rados://cephfs_data/ganesha/export-1</code>. If there are
       multiple configurations, each node can use a different configuration.
      </p></div></li><li class="step"><p>
      Each export configuration is stored in a single object with the name
      <code class="literal">export-<em class="replaceable">ID</em></code>. Use the
      following command to obtain the contents of the object and save it to
      <code class="filename">/tmp/export-1</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p cephfs_data -N ganesha get export-1 /tmp/export-1
<code class="prompt user">cephadm@adm &gt; </code>cat /tmp/export-1
EXPORT {
    export_id = 1;
    path = "/";
    pseudo = "/cephfs";
    access_type = "RW";
    squash = "no_root_squash";
    protocols = 3, 4;
    transports = "UDP", "TCP";
    FSAL {
        name = "CEPH";
        user_id = "admin";
        filesystem = "cephfs";
        secret_access_key = "<em class="replaceable">SECRET_KEY</em>";
    }

    CLIENT {
        clients = 192.168.3.105;
        access_type = "RW";
        squash = "no_root_squash";
    }
}</pre></div></li></ol></div></div></section></section><section class="sect1" id="ceph-nfsganesha-customrole" data-id-title="Custom NFS Ganesha Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.3 </span><span class="title-name">Custom NFS Ganesha Roles</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole">#</a></h2></div></div></div><p>
   Custom NFS Ganesha roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <code class="filename">policy.cfg</code>. The roles
   allow for:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Separated NFS Ganesha nodes for accessing Object Gateway and CephFS.
    </p></li><li class="listitem"><p>
     Assigning different Object Gateway users to NFS Ganesha nodes.
    </p></li></ul></div><p>
   Having different Object Gateway users enables NFS Ganesha nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with Ceph buckets used in the CRUSH Map.
  </p><section class="sect2" id="ceph-nfsganesha-customrole-rgw-multiusers" data-id-title="Different Object Gateway Users for NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.1 </span><span class="title-name">Different Object Gateway Users for NFS Ganesha</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole-rgw-multiusers">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create two
    NFS Ganesha roles with different Object Gateway users. In this example, the roles
    <code class="literal">gold</code> and <code class="literal">silver</code> are used, for which
    DeepSea already provides example configuration files.
   </p><div class="procedure" id="proc-ceph-nfsganesha-rgw-multiusers"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> with
      the editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/gold.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</pre></div><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/silver.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</pre></div></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 silver.conf.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 gold.conf.j2</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 silver.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 gold.j2</pre></div></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/rgw/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> rgw.j2 silver.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> rgw.j2 gold.j2</pre></div></li><li class="step"><p>
      Object Gateway also needs the configuration for the different roles:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/configuration/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ceph.conf.rgw silver.conf
<code class="prompt user">root@master # </code><code class="command">cp</code> ceph.conf.rgw gold.conf</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-silver/cluster/<em class="replaceable">NODE1</em>.sls
role-gold/cluster/<em class="replaceable">NODE2</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-nfsganesha-customrole-rgw-cephfs" data-id-title="Separating CephFS and Object Gateway FSAL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.2 </span><span class="title-name">Separating CephFS and Object Gateway FSAL</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole-rgw-cephfs">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create 2 new
    different roles that use CephFS and Object Gateway:
   </p><div class="procedure" id="proc-ceph-nfsganesha-customrole"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/rgw.sls</code> with the
      editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_rgw.conf.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_cfs.conf.j2</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_rgw.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_cfs.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 ganesha_rgw.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 ganesha_cfs.j2</pre></div><p>
      The line <code class="literal">caps mds = "allow *"</code> can be removed from the
      <code class="filename">ganesha_rgw.j2</code>.
     </p></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</pre></div></li><li class="step"><p>
      Object Gateway needs the configuration for the new role:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-ganesha_rgw/cluster/<em class="replaceable">NODE1</em>.sls
role-ganesha_cfs/cluster/<em class="replaceable">NODE1</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ganesha-rgw-supported-operations" data-id-title="Supported Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.3 </span><span class="title-name">Supported Operations</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ganesha-rgw-supported-operations">#</a></h3></div></div></div><p>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Links including symbolic links are not supported.</em></span>
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>NFS access control lists (ACLs) are not supported.</em></span>
      Unix user and group ownership and permissions <span class="emphasis"><em>are</em></span>
      supported.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Directories may not be moved or renamed.</em></span> You
      <span class="emphasis"><em>may</em></span> move files between directories.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Only full, sequential write I/O is supported.</em></span>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example, some versions of GNU
      <code class="command">tar</code>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <code class="option">-o sync</code> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </p></li><li class="listitem"><p>
      NFS RGW supports read-write operations only for block sizes smaller than
      4 MB.
     </p></li></ul></div></section></section><section class="sect1" id="ceph-nfsganesha-services" data-id-title="Starting or Restarting NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-services">#</a></h2></div></div></div><p>
   To enable and start the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> enable nfs-ganesha
<code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> start nfs-ganesha</pre></div><p>
   Restart NFS Ganesha with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> restart nfs-ganesha</pre></div><p>
   When NFS Ganesha is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </p></section><section class="sect1" id="ceph-nfsganesha-loglevel" data-id-title="Setting the Log Level"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.5 </span><span class="title-name">Setting the Log Level</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-loglevel">#</a></h2></div></div></div><p>
   You change the default debug level <code class="literal">NIV_EVENT</code> by editing
   the file <code class="filename">/etc/sysconfig/ganesha</code>. Replace
   <code class="literal">NIV_EVENT</code> with <code class="literal">NIV_DEBUG</code> or
   <code class="literal">NIV_FULL_DEBUG</code>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</pre></div><p>
   A restart of the service is required when changing the log level.
  </p><div id="id-1.3.6.6.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    NFS Ganesha uses Ceph client libraries to connect to the Ceph cluster. By
    default, client libraries do not log errors or any other output. To see
    more details about NFS Ganesha interacting with the Ceph cluster (for
    example, connection issues details) logging needs to be explicitly defined
    in the <code class="filename">ceph.conf</code> configuration file under the
    <code class="literal">[client]</code> section. For example:
   </p><div class="verbatim-wrap"><pre class="screen">[client]
	log_file = "/var/log/ceph/ceph-client.log"</pre></div></div></section><section class="sect1" id="ceph-nfsganesha-verify" data-id-title="Verifying the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.6 </span><span class="title-name">Verifying the Exported NFS Share</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-verify">#</a></h2></div></div></div><p>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   NFS Ganesha server node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">showmount</code> -e
/ (everything)</pre></div></section><section class="sect1" id="ceph-nfsganesha-mount" data-id-title="Mounting the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.7 </span><span class="title-name">Mounting the Exported NFS Share</span> <a title="Permalink" class="permalink" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-mount">#</a></h2></div></div></div><p>
   To mount the exported NFS share (as configured in
   <a class="xref" href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config" title="30.2. Configuration">Section 30.2, “Configuration”</a>) on a client host, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -t nfs -o rw,noatime,sync \
 <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em></pre></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-ses-cifs.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 29 </span>Exporting Ceph Data via Samba</span></a> </div><div><a class="pagination-link next" href="part-virt.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Part V </span>Integration with Virtualization Tools</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-install"><span class="title-number">30.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-config"><span class="title-number">30.2 </span><span class="title-name">Configuration</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-customrole"><span class="title-number">30.3 </span><span class="title-name">Custom NFS Ganesha Roles</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-services"><span class="title-number">30.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-loglevel"><span class="title-number">30.5 </span><span class="title-name">Setting the Log Level</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-verify"><span class="title-number">30.6 </span><span class="title-name">Verifying the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="cha-ceph-nfsganesha.html#ceph-nfsganesha-mount"><span class="title-number">30.7 </span><span class="title-name">Mounting the Exported NFS Share</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_nfsganesha.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>