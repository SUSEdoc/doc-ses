<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>RADOS Block Device | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="RADOS Block Device | SES 6"/>
<meta name="description" content="A block is a sequence of bytes, for example a 4 MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, suc…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 23. RADOS Block Device"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="RADOS Block Device | SES 6"/>
<meta property="og:description" content="A block is a sequence of bytes, for example a 4 MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, suc…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RADOS Block Device | SES 6"/>
<meta name="twitter:description" content="A block is a sequence of bytes, for example a 4 MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, suc…"/>
<link rel="prev" href="ceph-pools.html" title="Chapter 22. Managing Storage Pools"/><link rel="next" href="cha-ceph-erasure.html" title="Chapter 24. Erasure Coded Pools"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-operate.html">Operating a Cluster</a><span> / </span><a class="crumb" href="ceph-rbd.html">RADOS Block Device</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li class="active"><a href="part-operate.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" you-are-here"><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li><a href="part-dataccess.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="ceph-rbd" data-id-title="RADOS Block Device"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#">#</a></h2></div></div></div><p>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like Ceph.
 </p><p>
  Ceph block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a Ceph cluster. Ceph block
  devices leverage RADOS capabilities such as snapshotting, replication, and
  consistency. Ceph's RADOS Block Devices (RBD) interact with OSDs using kernel modules or
  the <code class="systemitem">librbd</code> library.
 </p><div class="figure" id="id-1.3.5.10.5"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_rbd_schema.png" target="_blank"><img src="images/ceph_rbd_schema.png" width="" alt="RADOS Protocol"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 23.1: </span><span class="title-name">RADOS Protocol </span><a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.5">#</a></h6></div></div><p>
  Ceph's block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as QEMU, or
  cloud-based computing systems such as OpenStack that rely on <code class="systemitem">libvirt</code>. You
  can use the same cluster to operate the Object Gateway, CephFS, and RADOS Block Devices
  simultaneously.
 </p><section class="sect1" id="ceph-rbd-commands" data-id-title="Block Device Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.1 </span><span class="title-name">Block Device Commands</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-commands">#</a></h2></div></div></div><p>
   The <code class="command">rbd</code> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </p><section class="sect2" id="ceph-rbd-cmds-create" data-id-title="Creating a Block Device Image in a Replicated Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.1 </span><span class="title-name">Creating a Block Device Image in a Replicated Pool</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-create">#</a></h3></div></div></div><p>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <a class="xref" href="ceph-pools.html" title="Chapter 22. Managing Storage Pools">Chapter 22, <em>Managing Storage Pools</em></a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create --size <em class="replaceable">MEGABYTES</em> <em class="replaceable">POOL-NAME</em>/<em class="replaceable">IMAGE-NAME</em></pre></div><p>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create --size 1024 mypool/myimage</pre></div><div id="id-1.3.5.10.7.3.6" data-id-title="Image Size Units" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Image Size Units</h6><p>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </p></div></section><section class="sect2" id="ceph-rbd-cmds-create-ec" data-id-title="Creating a Block Device Image in an Erasure Coded Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.2 </span><span class="title-name">Creating a Block Device Image in an Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-create-ec">#</a></h3></div></div></div><p>
    As of SUSE Enterprise Storage 5, it is possible to store data of a block device image
    directly in erasure coded (EC) pools. A RADOS Block Device image consists of
    <span class="emphasis"><em>data</em></span> and <span class="emphasis"><em>metadata</em></span> parts. You can
    store only the 'data' part of a RADOS Block Device image in an EC pool. The pool needs
    to have the 'overwrite' flag set to <span class="emphasis"><em>true</em></span>, and that is
    only possible if all OSDs where the pool is stored use BlueStore.
   </p><p>
    You cannot store the image's 'metadata' part in an EC pool. You need to
    specify the replicated pool for storing the image's metadata with the
    <code class="option">--pool=</code> option of the <code class="command">rbd create</code>
    command.
   </p><p>
    Use the following steps to create an RBD image in a newly created EC pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool create <em class="replaceable">POOL_NAME</em> 12 12 erasure
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> allow_ec_overwrites true

#Metadata will reside in pool "<em class="replaceable">OTHER_POOL</em>", and data in pool "<em class="replaceable">POOL_NAME</em>"
<code class="prompt user">cephadm@adm &gt; </code><code class="command">rbd</code> create <em class="replaceable">IMAGE_NAME</em> --size=1G --data-pool <em class="replaceable">POOL_NAME</em> --pool=<em class="replaceable">OTHER_POOL</em></pre></div></section><section class="sect2" id="ceph-rbd-cmds-list" data-id-title="Listing Block Device Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.3 </span><span class="title-name">Listing Block Device Images</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-list">#</a></h3></div></div></div><p>
    To list block devices in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd ls mypool</pre></div></section><section class="sect2" id="ceph-rbd-cmds-info" data-id-title="Retrieving Image Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.4 </span><span class="title-name">Retrieving Image Information</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-info">#</a></h3></div></div></div><p>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd info mypool/myimage</pre></div></section><section class="sect2" id="ceph-rbd-cmds-resize" data-id-title="Resizing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.5 </span><span class="title-name">Resizing a Block Device Image</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-resize">#</a></h3></div></div></div><p>
    RADOS Block Device images are thin provisioned—they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <code class="option">--size</code> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> # to increase
<code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --allow-shrink # to decrease</pre></div></section><section class="sect2" id="ceph-rbd-cmds-rm" data-id-title="Removing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.6 </span><span class="title-name">Removing a Block Device Image</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-cmds-rm">#</a></h3></div></div></div><p>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd rm mypool/myimage</pre></div></section></section><section class="sect1" id="storage-bp-integration-mount-rbd" data-id-title="Mounting and Unmounting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.2 </span><span class="title-name">Mounting and Unmounting</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#storage-bp-integration-mount-rbd">#</a></h2></div></div></div><p>
   After you create a RADOS Block Device, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure your Ceph cluster includes a pool with the disk image you want
     to map. Assume the pool is called <code class="literal">mypool</code> and the image
     is <code class="literal">myimage</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
     Map the image to a new block device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.5.10.8.3.2.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
      To specify a user name, use <code class="option">--id
      <em class="replaceable">user-name</em></code>. If you use
      <code class="systemitem">cephx</code> authentication, you also need to specify a
      secret. It may come from a keyring or a file containing the secret:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
      or
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
     List all mapped devices:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div><p>
     The device we want to work on is <code class="filename">/dev/rbd0</code>.
    </p><div id="id-1.3.5.10.8.3.3.4" data-id-title="RBD Device Path" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: RBD Device Path</h6><p>
      Instead of
      <code class="filename">/dev/rbd<em class="replaceable">DEVICE_NUMBER</em></code>,
      you can use
      <code class="filename">/dev/rbd/<em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></code>
      as a persistent device path. For example:
     </p><div class="verbatim-wrap"><pre class="screen">/dev/rbd/mypool/myimage</pre></div></div></li><li class="step"><p>
     Make an XFS file system on the <code class="filename">/dev/rbd0</code> device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div></li><li class="step"><p>
     Mount the device and check it is correctly mounted. Replace
     <code class="filename">/mnt</code> with your mount point.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount /dev/rbd0 /mnt
<code class="prompt user">root # </code>mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</pre></div><p>
     Now you can move data to and from the device as if it was a local
     directory.
    </p><div id="id-1.3.5.10.8.3.5.4" data-id-title="Increasing the Size of RBD Device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increasing the Size of RBD Device</h6><p>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
        Increase the size of the RBD image, for example up to 10 GB.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
        Grow the file system to fill up the new size of the device.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></div></li><li class="step"><p>
     After you finish accessing the device, you can unmap and unmount it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd unmap /dev/rbd0
<code class="prompt user">root # </code>unmount /mnt</pre></div></li></ol></div></div><div id="id-1.3.5.10.8.4" data-id-title="Manual (Un)mounting" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Manual (Un)mounting</h6><p>
    Since manually mapping and mounting RBD images after boot and unmounting
    and unmapping them before shutdown can be tedious, an
    <code class="command">rbdmap</code> script and <code class="systemitem">systemd</code> unit is provided. Refer to
    <a class="xref" href="ceph-rbd.html#ceph-rbd-rbdmap" title="23.2.1. rbdmap: Map RBD Devices at Boot Time">Section 23.2.1, “rbdmap: Map RBD Devices at Boot Time”</a>.
   </p></div><section class="sect2" id="ceph-rbd-rbdmap" data-id-title="rbdmap: Map RBD Devices at Boot Time"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.2.1 </span><span class="title-name">rbdmap: Map RBD Devices at Boot Time</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-rbdmap">#</a></h3></div></div></div><p>
    <code class="command">rbdmap</code> is a shell script that automates <code class="command">rbd
    map</code> and <code class="command">rbd unmap</code> operations on one or more
    RBD images. Although you can run the script manually at any time, the main
    advantage is automatic mapping and mounting of RBD images at boot time (and
    unmounting and unmapping at shutdown), as triggered by the Init system. A
    <code class="systemitem">systemd</code> unit file, <code class="filename">rbdmap.service</code> is included with
    the <code class="systemitem">ceph-common</code> package for this purpose.
   </p><p>
    The script takes a single argument, which can be either
    <code class="option">map</code> or <code class="option">unmap</code>. In either case, the script
    parses a configuration file. It defaults to
    <code class="filename">/etc/ceph/rbdmap</code>, but can be overridden via an
    environment variable <code class="literal">RBDMAPFILE</code>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </p><p>
    The configuration file has the following format:
   </p><div class="verbatim-wrap"><pre class="screen">image_specification rbd_options</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.8.5.6.1"><span class="term"><code class="option">image_specification</code></span></dt><dd><p>
       Path to an image within a pool. Specify as
       <em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>.
      </p></dd><dt id="id-1.3.5.10.8.5.6.2"><span class="term"><code class="option">rbd_options</code></span></dt><dd><p>
       An optional list of parameters to be passed to the underlying
       <code class="command">rbd map</code> command. These parameters and their values
       should be specified as a comma-separated string, for example:
      </p><div class="verbatim-wrap"><pre class="screen">PARAM1=VAL1,PARAM2=VAL2,...</pre></div><p>
       The example makes the <code class="command">rbdmap</code> script run the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --PARAM1 VAL1 --PARAM2 VAL2</pre></div><p>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbdmap map mypool/myimage id=rbd_user,keyring=/etc/ceph/ceph.client.rbd.keyring</pre></div></dd></dl></div><p>
    When run as <code class="command">rbdmap map</code>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using <code class="command">the rbd map</code> command) and then mount
    the image.
   </p><p>
    When run as <code class="command">rbdmap unmap</code>, images listed in the
    configuration file will be unmounted and unmapped.
   </p><p>
    <code class="command">rbdmap unmap-all</code> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </p><p>
    If successful, the rbd map operation maps the image to a /dev/rbdX device,
    at which point a udev rule is triggered to create a friendly device name
    symbolic link
    <code class="filename">/dev/rbd/<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em></code>
    pointing to the real mapped device.
   </p><p>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <code class="filename">/etc/fstab</code>.
    When writing <code class="filename">/etc/fstab</code> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early—before the device in
    question even exists, as <code class="filename">rbdmap.service</code> is typically
    triggered quite late in the boot sequence.
   </p><p>
    For a complete list of <code class="command">rbd</code> options, see the
    <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>).
   </p><p>
    For examples of the <code class="command">rbdmap</code> usage, see the
    <code class="command">rbdmap</code> manual page (<code class="command">man 8 rbdmap</code>).
   </p></section><section class="sect2" id="id-1.3.5.10.8.6" data-id-title="Increasing the Size of RBD Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.2.2 </span><span class="title-name">Increasing the Size of RBD Device</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.8.6">#</a></h3></div></div></div><p>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Increase the size of the RBD image, for example up to 10GB.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
      Grow the file system to fill up the new size of the device.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></section></section><section class="sect1" id="cha-ceph-snapshots-rbd" data-id-title="Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.3 </span><span class="title-name">Snapshots</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#cha-ceph-snapshots-rbd">#</a></h2></div></div></div><p>
   An RBD snapshot is a snapshot of a RADOS Block Device image. With snapshots, you retain a
   history of the image's state. Ceph also supports snapshot layering, which
   allows you to clone VM images quickly and easily. Ceph supports block
   device snapshots using the <code class="command">rbd</code> command and many
   higher-level interfaces, including QEMU, <code class="systemitem">libvirt</code>,
   OpenStack, and CloudStack.
  </p><div id="id-1.3.5.10.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </p></div><section class="sect2" id="id-1.3.5.10.9.4" data-id-title="Cephx Notes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.1 </span><span class="title-name">Cephx Notes</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.4">#</a></h3></div></div></div><p>
    When <code class="systemitem">cephx</code> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <a class="xref" href="cha-storage-cephx.html" title="Chapter 19. Authentication with cephx">Chapter 19, <em>Authentication with <code class="systemitem">cephx</code></em></a> for more details. You may
    also add the <code class="systemitem">CEPH_ARGS</code> environment variable to
    avoid re-entry of the following parameters.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --id <em class="replaceable">user-ID</em> --keyring=/path/to/secret <em class="replaceable">commands</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --name <em class="replaceable">username</em> --keyring=/path/to/secret <em class="replaceable">commands</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --id admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em></pre></div><div id="id-1.3.5.10.9.4.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Add the user and secret to the <code class="systemitem">CEPH_ARGS</code>
     environment variable so that you do not need to enter them each time.
    </p></div></section><section class="sect2" id="id-1.3.5.10.9.5" data-id-title="Snapshot Basics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.2 </span><span class="title-name">Snapshot Basics</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <code class="command">rbd</code> command on the command line.
   </p><section class="sect3" id="id-1.3.5.10.9.5.3" data-id-title="Create Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.1 </span><span class="title-name">Create Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5.3">#</a></h4></div></div></div><p>
     To create a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     create</code> option, the pool name, and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap create --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap create <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool rbd snap create --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap create rbd/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.5.4" data-id-title="List Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.2 </span><span class="title-name">List Snapshots</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5.4">#</a></h4></div></div></div><p>
     To list snapshots of an image, specify the pool name and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap ls <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap ls <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool rbd snap ls image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap ls rbd/image1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.5.5" data-id-title="Rollback Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.3 </span><span class="title-name">Rollback Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5.5">#</a></h4></div></div></div><p>
     To rollback to a snapshot with <code class="command">rbd</code>, specify the
     <code class="option">snap rollback</code> option, the pool name, the image name, and
     the snapshot name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rollback --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rollback <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap rollback --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rollback pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.5.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <span class="emphasis"><em>faster to
      clone</em></span> from a snapshot <span class="emphasis"><em>than to rollback</em></span> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.5.6" data-id-title="Delete a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.4 </span><span class="title-name">Delete a Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5.6">#</a></h4></div></div></div><p>
     To delete a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     rm</code> option, the pool name, the image name, and the user name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rm --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rm <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap rm --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rm pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.5.6.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.5.7" data-id-title="Purge Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.5 </span><span class="title-name">Purge Snapshots</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.5.7">#</a></h4></div></div></div><p>
     To delete all snapshots for an image with <code class="command">rbd</code>, specify
     the <code class="option">snap purge</code> option and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap purge <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap purge <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap purge image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap purge pool1/image1</pre></div></section></section><section class="sect2" id="ceph-snapshoti-layering" data-id-title="Layering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.3 </span><span class="title-name">Layering</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-snapshoti-layering">#</a></h3></div></div></div><p>
    Ceph supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables Ceph block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics—making it possible to create clones rapidly.
   </p><div id="id-1.3.5.10.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a Ceph block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </p></div><p>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </p><p>
    A COW clone of a snapshot behaves exactly like any other Ceph block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <span class="emphasis"><em>must</em></span> protect the snapshot before you clone it.
   </p><div id="id-1.3.5.10.9.6.6" data-id-title="--image-format 1 Not Supported" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">--image-format 1</code> Not Supported</h6><p>
     You cannot create snapshots of images created with the deprecated
     <code class="command">rbd create --image-format 1</code> option. Ceph only
     supports cloning of the default <span class="emphasis"><em>format 2</em></span> images.
    </p></div><section class="sect3" id="id-1.3.5.10.9.6.7" data-id-title="Getting Started with Layering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.1 </span><span class="title-name">Getting Started with Layering</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.6.7">#</a></h4></div></div></div><p>
     Ceph block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </p><p>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Image Template</em></span>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, SUSE Linux Enterprise Server), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <code class="command">zypper ref &amp;&amp; zypper patch</code>
       followed by <code class="command">rbd snap create</code>). As the image matures,
       the user can clone any one of the snapshots.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Extended Template</em></span>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Template Pool</em></span>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Image Migration/Recovery</em></span>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </p></li></ul></div></section><section class="sect3" id="id-1.3.5.10.9.6.8" data-id-title="Protecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.2 </span><span class="title-name">Protecting a Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.6.8">#</a></h4></div></div></div><p>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap protect \
 --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap protect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap protect pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.6.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You cannot delete a protected snapshot.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.6.9" data-id-title="Cloning a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.3 </span><span class="title-name">Cloning a Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.6.9">#</a></h4></div></div></div><p>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd clone --pool <em class="replaceable">pool-name</em> --image <em class="replaceable">parent-image</em> \
 --snap <em class="replaceable">snap-name</em> --dest-pool <em class="replaceable">pool-name</em> \
 --dest <em class="replaceable">child-image</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd clone <em class="replaceable">pool-name</em>/<em class="replaceable">parent-image</em>@<em class="replaceable">snap-name</em> \
<em class="replaceable">pool-name</em>/<em class="replaceable">child-image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd clone pool1/image1@snapshot1 pool1/image2</pre></div><div id="id-1.3.5.10.9.6.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.6.10" data-id-title="Unprotecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.4 </span><span class="title-name">Unprotecting a Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.6.10">#</a></h4></div></div></div><p>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <span class="emphasis"><em>not</em></span> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap unprotect --image <em class="replaceable">image-name</em> \
 --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap unprotect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap unprotect pool1/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.6.11" data-id-title="Listing Children of a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.5 </span><span class="title-name">Listing Children of a Snapshot</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.9.6.11">#</a></h4></div></div></div><p>
     To list the children of a snapshot, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> children --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snap-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd children <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 children --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd children pool1/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-flatten" data-id-title="Flattening a Cloned Image"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.6 </span><span class="title-name">Flattening a Cloned Image</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-flatten">#</a></h4></div></div></div><p>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> flatten --image <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd flatten <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 flatten --image image1
<code class="prompt user">cephadm@adm &gt; </code>rbd flatten pool1/image1</pre></div><div id="id-1.3.5.10.9.6.12.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </p></div></section></section></section><section class="sect1" id="ceph-rbd-mirror" data-id-title="Mirroring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.4 </span><span class="title-name">Mirroring</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-mirror">#</a></h2></div></div></div><p>
   RBD images can be asynchronously mirrored between two Ceph clusters. This
   capability uses the RBD journaling image feature to ensure crash-consistent
   replication between clusters. Mirroring is configured on a per-pool basis
   within peer clusters and can be configured to automatically mirror all
   images within a pool or only a specific subset of images. Mirroring is
   configured using the <code class="command">rbd</code> command. The
   <code class="systemitem">rbd-mirror</code> daemon is responsible for pulling image
   updates from the remote peer cluster and applying them to the image within
   the local cluster.
  </p><div id="id-1.3.5.10.10.3" data-id-title="rbd-mirror Daemon" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: rbd-mirror Daemon</h6><p>
    To use RBD mirroring, you need to have two Ceph clusters, each running
    the <code class="systemitem">rbd-mirror</code> daemon.
   </p></div><div id="id-1.3.5.10.10.4" data-id-title="RADOS Block Devices Exported via iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: RADOS Block Devices Exported via iSCSI</h6><p>
    You cannot mirror RBD devices that are exported via iSCSI using
    kernel-based iSCSI Gateway.
   </p><p>
    Refer to <a class="xref" href="cha-ceph-iscsi.html" title="Chapter 27. Ceph iSCSI Gateway">Chapter 27, <em>Ceph iSCSI Gateway</em></a> for more details on iSCSI.
   </p></div><section class="sect2" id="rbd-mirror-daemon" data-id-title="rbd-mirror Daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.1 </span><span class="title-name">rbd-mirror Daemon</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-mirror-daemon">#</a></h3></div></div></div><p>
    The two <code class="systemitem">rbd-mirror</code> daemons are responsible for
    watching image journals on the remote, peer cluster and replaying the
    journal events against the local cluster. The RBD image journaling feature
    records all modifications to the image in the order they occur. This
    ensures that a crash-consistent mirror of the remote image is available
    locally.
   </p><p>
    The <code class="systemitem">rbd-mirror</code> daemon is available in the
    <span class="package">rbd-mirror</span> package. You can install the package on OSD
    nodes, gateway nodes, or even on dedicated nodes. We do not recommend
    installing the <span class="package">rbd-mirror</span> on the Admin Node. Install, enable,
    and start <span class="package">rbd-mirror</span>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper install rbd-mirror
<code class="prompt user">root@minion &gt; </code>systemctl enable ceph-rbd-mirror@<em class="replaceable">server_name</em>.service
<code class="prompt user">root@minion &gt; </code>systemctl start ceph-rbd-mirror@<em class="replaceable">server_name</em>.service</pre></div><div id="id-1.3.5.10.10.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Each <code class="systemitem">rbd-mirror</code> daemon requires the ability to
     connect to both clusters simultaneously.
    </p></div></section><section class="sect2" id="ceph-rbd-mirror-poolconfig" data-id-title="Pool Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.2 </span><span class="title-name">Pool Configuration</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#ceph-rbd-mirror-poolconfig">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <code class="command">rbd</code> command. Mirroring is configured on a per-pool basis
    within the Ceph clusters.
   </p><p>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named 'local' and 'remote', are
    accessible from a single host for clarity.
   </p><p>
    See the <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>)
    for additional details on how to connect to different Ceph clusters.
   </p><div id="id-1.3.5.10.10.6.5" data-id-title="Multiple Clusters" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple Clusters</h6><p>
     The cluster name in the following examples corresponds to a Ceph
     configuration file of the same name
     <code class="filename">/etc/ceph/remote.conf</code>.
    </p></div><section class="sect3" id="id-1.3.5.10.10.6.6" data-id-title="Enable Mirroring on a Pool"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.1 </span><span class="title-name">Enable Mirroring on a Pool</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.6.6">#</a></h4></div></div></div><p>
     To enable mirroring on a pool, specify the <code class="command">mirror pool
     enable</code> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.10.6.6.3.1"><span class="term">pool</span></dt><dd><p>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </p></dd><dt id="id-1.3.5.10.10.6.6.3.2"><span class="term">image</span></dt><dd><p>
        Mirroring needs to be explicitly enabled on each image. See
        <a class="xref" href="ceph-rbd.html#rbd-mirror-enable-image-mirroring" title="23.4.3.2. Enable Image Mirroring">Section 23.4.3.2, “Enable Image Mirroring”</a> for more
        information.
       </p></dd></dl></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool enable <em class="replaceable">POOL_NAME</em> pool
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool enable <em class="replaceable">POOL_NAME</em> pool</pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.7" data-id-title="Disable Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.2 </span><span class="title-name">Disable Mirroring</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.6.7">#</a></h4></div></div></div><p>
     To disable mirroring on a pool, specify the <code class="command">mirror pool
     disable</code> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool disable <em class="replaceable">POOL_NAME</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool disable <em class="replaceable">POOL_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.8" data-id-title="Add Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.3 </span><span class="title-name">Add Cluster Peer</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.6.8">#</a></h4></div></div></div><p>
     In order for the <code class="systemitem">rbd-mirror</code> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <code class="command">mirror pool peer
     add</code> subcommand, the pool name, and a cluster specification:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool peer add <em class="replaceable">POOL_NAME</em> client.remote@remote
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool peer add <em class="replaceable">POOL_NAME</em> client.local@local</pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.9" data-id-title="Remove Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.4 </span><span class="title-name">Remove Cluster Peer</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.6.9">#</a></h4></div></div></div><p>
     To remove a mirroring peer cluster, specify the <code class="command">mirror pool peer
     remove</code> subcommand, the pool name, and the peer UUID (available
     from the <code class="command">rbd mirror pool info</code> command):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 55672766-c02b-4729-8567-f13a66893445
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 60c0e299-b38f-4234-91f6-eed0a367be08</pre></div></section></section><section class="sect2" id="rbd-mirror-imageconfig" data-id-title="Image Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.3 </span><span class="title-name">Image Configuration</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-mirror-imageconfig">#</a></h3></div></div></div><p>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer Ceph cluster.
   </p><p>
    Mirrored RBD images are designated as either <span class="emphasis"><em>primary</em></span>
    or <span class="emphasis"><em>non-primary</em></span>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </p><p>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <a class="xref" href="ceph-rbd.html#rbd-mirror-enable-image-mirroring" title="23.4.3.2. Enable Image Mirroring">Section 23.4.3.2, “Enable Image Mirroring”</a>) by the
    <code class="command">rbd</code> command).
   </p><section class="sect3" id="id-1.3.5.10.10.7.5" data-id-title="Image Journaling Support"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.1 </span><span class="title-name">Image Journaling Support</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.7.5">#</a></h4></div></div></div><p>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. Before an image can be
     mirrored to a peer cluster, the journaling feature must be enabled. The
     feature can be enabled at the time of image creation by providing the
     <code class="option">--image-feature exclusive-lock,journaling</code> option to the
     <code class="command">rbd</code> command.
    </p><p>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <code class="command">feature enable</code> subcommand, the pool and image name, and
     the feature name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local feature enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> journaling</pre></div><div id="id-1.3.5.10.10.7.5.5" data-id-title="Option Dependency" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Option Dependency</h6><p>
      The <code class="option">journaling</code> feature is dependent on the
      <code class="option">exclusive-lock</code> feature. If the
      <code class="option">exclusive-lock</code> feature is not already enabled, you need
      to enable it prior to enabling the <code class="option">journaling</code> feature.
     </p></div><div id="id-1.3.5.10.10.7.5.6" data-id-title="Journaling on All New Images" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Journaling on All New Images</h6><p>
      You can enable journaling on all new images by default by appending the
      <code class="literal">journaling</code> value to the <code class="option">rbd default
      features</code> option in the Ceph configuration file. For example:
     </p><div class="verbatim-wrap"><pre class="screen">rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</pre></div><p>
      Before applying such a change, carefully consider if enabling journaling
      on all new images is good for your deployment, because it can have a
      negative performance impact.
     </p></div></section><section class="sect3" id="rbd-mirror-enable-image-mirroring" data-id-title="Enable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.2 </span><span class="title-name">Enable Image Mirroring</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-mirror-enable-image-mirroring">#</a></h4></div></div></div><p>
     If mirroring is configured in the 'image' mode, then it is necessary to
     explicitly enable mirroring for each image within the pool. To enable
     mirroring for a specific image, specify the <code class="command">mirror image
     enable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.7.7" data-id-title="Disable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.3 </span><span class="title-name">Disable Image Mirroring</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.7.7">#</a></h4></div></div></div><p>
     To disable mirroring for a specific image, specify the <code class="command">mirror
     image disable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image disable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.7.8" data-id-title="Image Promotion and Demotion"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.4 </span><span class="title-name">Image Promotion and Demotion</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.7.8">#</a></h4></div></div></div><p>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </p><div id="id-1.3.5.10.10.7.8.3" data-id-title="Forced Promotion" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Forced Promotion</h6><p>
      Promotion can be forced using the <code class="option">--force</code> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <code class="command">resync</code> subcommand is issued.
     </p></div><p>
     To demote a specific image to non-primary, specify the <code class="command">mirror
     image demote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image demote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To demote all primary images within a pool to non-primary, specify the
     <code class="command">mirror pool demote</code> subcommand along with the pool name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool demote <em class="replaceable">POOL_NAME</em></pre></div><p>
     To promote a specific image to primary, specify the <code class="command">mirror image
     promote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror image promote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To promote all non-primary images within a pool to primary, specify the
     <code class="command">mirror pool promote</code> subcommand along with the pool
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool promote <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.5.10.10.7.8.12" data-id-title="Split I/O Load" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Split I/O Load</h6><p>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </p></div></section><section class="sect3" id="id-1.3.5.10.10.7.9" data-id-title="Force Image Resync"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.5 </span><span class="title-name">Force Image Resync</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#id-1.3.5.10.10.7.9">#</a></h4></div></div></div><p>
     If a split-brain event is detected by the
     <code class="systemitem">rbd-mirror</code> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out of date and then request a
     resync to the primary image. To request an image resync, specify the
     <code class="command">mirror image resync</code> subcommand along with the pool and
     image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror image resync <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section></section><section class="sect2" id="rbd-mirror-status" data-id-title="Mirror Status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.4 </span><span class="title-name">Mirror Status</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-mirror-status">#</a></h3></div></div></div><p>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <code class="command">mirror image
    status</code> and <code class="command">mirror pool status</code> subcommands:
   </p><p>
    To request the mirror image status, specify the <code class="command">mirror image
    status</code> subcommand along with the pool and image name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror image status <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
    To request the mirror pool summary status, specify the <code class="command">mirror pool
    status</code> subcommand along with the pool name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror pool status <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.5.10.10.8.7" data-id-title="" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: </h6><p>
     Adding the <code class="option">--verbose</code> option to the <code class="command">mirror pool
     status</code> subcommand will additionally output status details for
     every mirroring image in the pool.
    </p></div></section></section><section class="sect1" id="rbd-cache-settings" data-id-title="Cache Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.5 </span><span class="title-name">Cache Settings</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-cache-settings">#</a></h2></div></div></div><p>
   The user space implementation of the Ceph block device
   (<code class="systemitem">librbd</code>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <span class="emphasis"><em>Least
   Recently Used</em></span> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </p><p>
   Ceph supports write-back caching for RBD. To enable it, add
  </p><div class="verbatim-wrap"><pre class="screen">[client]
...
rbd cache = true</pre></div><p>
   to the <code class="literal">[client]</code> section of your
   <code class="filename">ceph.conf</code> file. By default,
   <code class="systemitem">librbd</code> does not perform any caching. Writes and
   reads go directly to the storage cluster, and writes return only when the
   data is on disk on all replicas. With caching enabled, writes return
   immediately, unless there are more unflushed bytes than set in the
   <code class="option">rbd cache max dirty</code> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </p><p>
   Ceph supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, set
  </p><div class="verbatim-wrap"><pre class="screen">rbd cache max dirty = 0</pre></div><p>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </p><p>
   The <code class="filename">ceph.conf</code> file settings for RBD should be set in
   the <code class="literal">[client]</code> section of your configuration file. The
   settings include:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.11.10.1"><span class="term"><code class="option">rbd cache</code></span></dt><dd><p>
      Enable caching for RADOS Block Device (RBD). Default is 'true'.
     </p></dd><dt id="id-1.3.5.10.11.10.2"><span class="term"><code class="option">rbd cache size</code></span></dt><dd><p>
      The RBD cache size in bytes. Default is 32 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.3"><span class="term"><code class="option">rbd cache max dirty</code></span></dt><dd><p>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <code class="option">rbd cache max dirty</code> needs to be less than <code class="option">rbd
      cache size</code>. If set to 0, uses write-through caching. Default is
      24 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.4"><span class="term"><code class="option">rbd cache target dirty</code></span></dt><dd><p>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.5"><span class="term"><code class="option">rbd cache max dirty age</code></span></dt><dd><p>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </p></dd><dt id="id-1.3.5.10.11.10.6"><span class="term"><code class="option">rbd cache writethrough until flush</code></span></dt><dd><p>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <code class="systemitem">rbd</code>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-qos" data-id-title="QoS Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.6 </span><span class="title-name">QoS Settings</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-qos">#</a></h2></div></div></div><p>
   Generallyi, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </p><div id="id-1.3.5.10.12.3" data-id-title="Not Supported by iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Not Supported by iSCSI</h6><p>
    The following QoS settings are used only by the userspace RBD
    implementation <code class="systemitem">librbd</code> and
    <span class="emphasis"><em>not</em></span> used by the <code class="systemitem">kRBD</code>
    implementation. Because iSCSI uses <code class="systemitem">kRBD</code>, it does
    not use the QoS settings. However, for iSCSI you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.12.4.1"><span class="term"><code class="option">rbd qos iops limit</code></span></dt><dd><p>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.2"><span class="term"><code class="option">rbd qos bps limit</code></span></dt><dd><p>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.3"><span class="term"><code class="option">rbd qos read iops limit</code></span></dt><dd><p>
      The desired limit of read operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.4"><span class="term"><code class="option">rbd qos write iops limit</code></span></dt><dd><p>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </p></dd><dt id="id-1.3.5.10.12.4.5"><span class="term"><code class="option">rbd qos read bps limit</code></span></dt><dd><p>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.6"><span class="term"><code class="option">rbd qos write bps limit</code></span></dt><dd><p>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.7"><span class="term"><code class="option">rbd qos iops burst</code></span></dt><dd><p>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.8"><span class="term"><code class="option">rbd qos bps burst</code></span></dt><dd><p>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.9"><span class="term"><code class="option">rbd qos read iops burst</code></span></dt><dd><p>
      The desired burst limit of read operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.10"><span class="term"><code class="option">rbd qos write iops burst</code></span></dt><dd><p>
      The desired burst limit of write operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.11"><span class="term"><code class="option">rbd qos read bps burst</code></span></dt><dd><p>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.12"><span class="term"><code class="option">rbd qos write bps burst</code></span></dt><dd><p>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.13"><span class="term"><code class="option">rbd qos schedule tick min</code></span></dt><dd><p>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </p></dd></dl></div></section><section class="sect1" id="rbd-readahead-settings" data-id-title="Read-ahead Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.7 </span><span class="title-name">Read-ahead Settings</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-readahead-settings">#</a></h2></div></div></div><p>
   RADOS Block Device supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.13.3.1"><span class="term"><code class="option">rbd readahead trigger requests</code></span></dt><dd><p>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </p></dd><dt id="id-1.3.5.10.13.3.2"><span class="term"><code class="option">rbd readahead max bytes</code></span></dt><dd><p>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </p></dd><dt id="id-1.3.5.10.13.3.3"><span class="term"><code class="option">rbd readahead disable after bytes</code></span></dt><dd><p>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </p></dd></dl></div></section><section class="sect1" id="rbd-features" data-id-title="Advanced Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.8 </span><span class="title-name">Advanced Features</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-features">#</a></h2></div></div></div><p>
   RADOS Block Device supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the Ceph configuration file by using the
   <code class="option">rbd_default_features</code> option.
  </p><p>
   You can specify the values of the <code class="option">rbd_default_features</code>
   option in two ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     As a sum of features' internal values. Each feature has its own internal
     value—for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = 17</pre></div></li><li class="listitem"><p>
     As a comma-separated list of features. The previous example will look as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = layering,fast-diff</pre></div></li></ul></div><div id="id-1.3.5.10.14.5" data-id-title="Features Not Supported by iSCSI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Features Not Supported by iSCSI</h6><p>
    RBD images with the following features will not be supported by iSCSI:
    <code class="option">deep-flatten</code>, <code class="option">object-map</code>,
    <code class="option">journaling</code>, <code class="option">fast-diff</code>,
    <code class="option">striping</code>
   </p></div><p>
   A list of advanced RBD features follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.14.7.1"><span class="term"><code class="option">layering</code></span></dt><dd><p>
      Layering enables you to use cloning.
     </p><p>
      Internal value is 1, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.2"><span class="term"><code class="option">striping</code></span></dt><dd><p>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy RADOS Block Devices.
     </p><p>
      Internal value is 2, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.3"><span class="term"><code class="option">exclusive-lock</code></span></dt><dd><p>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.4"><span class="term"><code class="option">object-map</code></span></dt><dd><p>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </p><p>
      Internal value is 8, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.5"><span class="term"><code class="option">fast-diff</code></span></dt><dd><p>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </p><p>
      Internal value is 16, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.6"><span class="term"><code class="option">deep-flatten</code></span></dt><dd><p>
      Deep-flatten makes the <code class="command">rbd flatten</code> (see
      <a class="xref" href="ceph-rbd.html#rbd-flatten" title="23.3.3.6. Flattening a Cloned Image">Section 23.3.3.6, “Flattening a Cloned Image”</a>) work on all the snapshots of an image, in
      addition to the image itself. Without it, snapshots of an image will
      still rely on the parent, therefore you will not be able to delete the
      parent image until the snapshots are deleted. Deep-flatten makes a parent
      independent of its clones, even if they have snapshots.
     </p><p>
      Internal value is 32, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.7"><span class="term"><code class="option">journaling</code></span></dt><dd><p>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <a class="xref" href="ceph-rbd.html#ceph-rbd-mirror" title="23.4. Mirroring">Section 23.4, “Mirroring”</a>) uses the journal to replicate a crash
      consistent image to a remote cluster.
     </p><p>
      Internal value is 64, default is 'no'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-old-clients-map" data-id-title="Mapping RBD Using Old Kernel Clients"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.9 </span><span class="title-name">Mapping RBD Using Old Kernel Clients</span> <a title="Permalink" class="permalink" href="ceph-rbd.html#rbd-old-clients-map">#</a></h2></div></div></div><p>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with SUSE Enterprise Storage 6 forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000</pre></div><div id="id-1.3.5.10.15.4" data-id-title="Changing CRUSH Map Bucket Types Causes Massive Rebalancing" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Changing CRUSH Map Bucket Types Causes Massive Rebalancing</h6><p>
    If you intend to switch the CRUSH Map bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Disable any RBD image features that are not supported. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd feature disable pool1/image1 object-map
<code class="prompt user">cephadm@adm &gt; </code>rbd feature disable pool1/image1 exclusive-lock</pre></div></li><li class="step"><p>
     Change the CRUSH Map bucket types from 'straw2' to 'straw':
    </p><ol type="a" class="substeps"><li class="step"><p>
       Save the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap -o crushmap.original</pre></div></li><li class="step"><p>
       Decompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -d crushmap.original -o crushmap.txt</pre></div></li><li class="step"><p>
       Edit the CRUSH Map and replace 'straw2' with 'straw'.
      </p></li><li class="step"><p>
       Recompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -c crushmap.txt -o crushmap.new</pre></div></li><li class="step"><p>
       Set the new CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd setcrushmap -i crushmap.new</pre></div></li></ol></li></ol></div></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="ceph-pools.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 22 </span>Managing Storage Pools</span></a> </div><div><a class="pagination-link next" href="cha-ceph-erasure.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 24 </span>Erasure Coded Pools</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="ceph-rbd.html#ceph-rbd-commands"><span class="title-number">23.1 </span><span class="title-name">Block Device Commands</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#storage-bp-integration-mount-rbd"><span class="title-number">23.2 </span><span class="title-name">Mounting and Unmounting</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#cha-ceph-snapshots-rbd"><span class="title-number">23.3 </span><span class="title-name">Snapshots</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#ceph-rbd-mirror"><span class="title-number">23.4 </span><span class="title-name">Mirroring</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#rbd-cache-settings"><span class="title-number">23.5 </span><span class="title-name">Cache Settings</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#rbd-qos"><span class="title-number">23.6 </span><span class="title-name">QoS Settings</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#rbd-readahead-settings"><span class="title-number">23.7 </span><span class="title-name">Read-ahead Settings</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#rbd-features"><span class="title-number">23.8 </span><span class="title-name">Advanced Features</span></a></span></li><li><span class="sect1"><a href="ceph-rbd.html#rbd-old-clients-map"><span class="title-number">23.9 </span><span class="title-name">Mapping RBD Using Old Kernel Clients</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_rbd.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>