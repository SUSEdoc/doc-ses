<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Monitoring and Alerting | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Monitoring and Alerting | SES 6"/>
<meta name="description" content="In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting stack on the Salt master. Users have to define the Prometheus role for Promet…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 18. Monitoring and Alerting"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Monitoring and Alerting | SES 6"/>
<meta property="og:description" content="In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting stack on the Salt master. Users have to define the Prometheus role for Promet…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Monitoring and Alerting | SES 6"/>
<meta name="twitter:description" content="In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting stack on the Salt master. Users have to define the Prometheus role for Promet…"/>
<link rel="prev" href="ceph-monitor.html" title="Chapter 17. Determining Cluster State"/><link rel="next" href="cha-storage-cephx.html" title="Chapter 19. Authentication with cephx"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-operate.html">Operating a Cluster</a><span> / </span><a class="crumb" href="monitoring-alerting.html">Monitoring and Alerting</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li><a href="part-cluster-managment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li class="active"><a href="part-operate.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" you-are-here"><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li><a href="part-dataccess.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="monitoring-alerting" data-id-title="Monitoring and Alerting"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#">#</a></h2></div></div></div><p>
  In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting
  stack on the Salt master. Users have to define the Prometheus role for
  Prometheus and Alertmanager, and the Grafana role for Grafana. When
  multiple nodes are assigned with the Prometheus or Grafana role, a highly
  available setup is deployed.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Prometheus</strong></span> is the monitoring and
    alerting toolkit.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Alertmanager</strong></span> handles alerts sent by the
    Prometheus server.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Grafana</strong></span> is the visualization and
    alerting software.
   </p></li><li class="listitem"><p>
    The <code class="systemitem">prometheus-node_exporter</code> is the
    service running on all Salt minions.
   </p></li></ul></div><p>
  The Prometheus configuration and <span class="emphasis"><em>scrape</em></span> targets
  (exporting daemons) are setup automatically by DeepSea. DeepSea also
  deploys a list of default alerts, for example <code class="literal">health
  error</code>, <code class="literal">10% OSDs down</code>, or <code class="literal">pgs
  inactive</code>.
 </p><section class="sect1" id="pillar-variables" data-id-title="Pillar Variables"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.1 </span><span class="title-name">Pillar Variables</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#pillar-variables">#</a></h2></div></div></div><p>
   The Salt pillar is a key-value store that provides information and
   configuration values to minions. It is available to all minions, each with
   differing content. The Salt pillar is pre-populated with default values and
   can be customized in two different ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong><code class="filename">/srv/pillar/ceph/stack/global.yml</code></strong></span>:
     to change pillar variables for all nodes.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">CLUSTER_NAME</em>/minions/<em class="replaceable">HOST</em></code></strong></span>:
     to change specific minion configurations.
    </p></li></ul></div><p>
   The pillar variables below are available to all nodes by default:
  </p><div class="verbatim-wrap"><pre class="screen">  monitoring:
    alertmanager:
      config: salt://path/to/config
      additional_flags: ''
    grafana:
      ssl_cert: False # self-signed certs are created by default
      ssl_key: False # self-signed certs are created by default
    prometheus:
      # pass additional configration to prometheus
      additional_flags: ''
      alert_relabel_config: []
      rule_files: []
      # per exporter config variables
      scrape_interval:
        ceph: 10
        node_exporter: 10
        prometheus: 10
        grafana: 10
      relabel_config:
        alertmanager: []
        ceph: []
        node_exporter: []
        prometheus: []
        grafana: []
      metric_relabel_config:
        ceph: []
        node_exporter: []
        prometheus: []
        grafana: []
      target_partition:
        ceph: '1/1'
        node_exporter: '1/1'
        prometheus: '1/1'
        grafana: '1/1'</pre></div></section><section class="sect1" id="grafana" data-id-title="Grafana"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.2 </span><span class="title-name">Grafana</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#grafana">#</a></h2></div></div></div><section class="sect2" id="grafana-certs" data-id-title="Grafana SSL/TLS certificates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.2.1 </span><span class="title-name">Grafana SSL/TLS certificates</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#grafana-certs">#</a></h3></div></div></div><p>
    All traffic is encrypted through Grafana. You can either supply your own
    SSL certs or create self-signed one.
   </p><p>
    Grafana uses the following variables:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="bold"><strong><code class="literal">ssl_cert</code></strong></span>
     </p></li><li class="listitem"><p>
      <span class="bold"><strong><code class="literal">ssl_key</code></strong></span>
     </p></li></ul></div><p>
    The Ceph Dashboard embeds the Grafana dashboards via HTML
    <code class="literal">iframe</code> elements. If Grafana is configured without
    SSL/TLS support, or if SSL is using self-signed certificates, and if the
    SSL support in the dashboard has been enabled (which is the default
    configuration), then most browsers will block the embedding of insecure
    content into a secured web page. If you can not see the embedded Grafana
    dashboards in Ceph Dashboard, check your browser's documentation on how to
    unblock mixed content or how to accept self-signed certificates.
    Alternatively, consider enabling SSL/TLS support in Grafana, using a
    certificate that is issued by a certificate authority (CA) known to the
    browser.
   </p><p>
    For more information on supplying your own SSL certificates, see
    <a class="xref" href="dashboard-initial-configuration.html#cert-sign-CA" title="13.1.3. Certificates Signed by CA">Section 13.1.3, “Certificates Signed by CA”</a>. For generating a self-signed or trusted
    third-party certificate using OpenSSL, see
    <a class="xref" href="dashboard-initial-configuration.html#self-sign-certificates-openssl" title="13.1.2. Self-signed or Trusted Third-party Certificate with OpenSSL">Section 13.1.2, “Self-signed or Trusted Third-party Certificate with OpenSSL”</a>. For creating your own
    CA-signed certificate, see <a class="xref" href="dashboard-initial-configuration.html#self-sign-certificates" title="13.1.1. Self-signed Certificates">Section 13.1.1, “Self-signed Certificates”</a>. For
    creating your own custom CA signed certificate, see
    <a class="xref" href="dashboard-initial-configuration.html#cert-sign-custom-CA" title="13.1.4. Certificates Signed with a Custom CA">Section 13.1.4, “Certificates Signed with a Custom CA”</a>.
   </p></section><section class="sect2" id="grafana-url" data-id-title="Configuring Grafana frontend URL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.2.2 </span><span class="title-name">Configuring Grafana frontend URL</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#grafana-url">#</a></h3></div></div></div><p>
    The Ceph Dashboard backend requires the Grafana URL to be able to verify the
    existence of Grafana dashboards before the frontend even loads them. Due
    to the nature of how Grafana is implemented in Ceph Dashboard, this means
    that two working connections are required in order to be able to see
    Grafana graphs in Ceph Dashboard:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The backend (Ceph Manager module) needs to verify the existence of the requested
      graph. If this request succeeds, it lets the frontend know that it can
      safely access Grafana.
     </p></li><li class="listitem"><p>
      The frontend then requests the Grafana graphs directly from the user's
      browser using an <code class="literal">iframe</code>. The Grafana instance is
      accessed directly without any detour through Ceph Dashboard.
     </p></li></ul></div><p>
    Now, it might be the case that your environment makes it difficult for the
    user's browser to directly access the URL configured in Ceph Dashboard. To
    solve this issue, a separate URL can be configured which will solely be
    used to tell the frontend (the user's browser) which URL it should use to
    access Grafana.
   </p><p>
    To change the URL that is returned to the frontend issue the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-grafana-frontend-api-url <em class="replaceable">GRAFANA-SERVER-URL</em></pre></div><p>
    If no value is set for that option, it will simply fall back to the value
    of the <em class="replaceable">GRAFANA_API_URL</em> option, which is set
    automatically by DeepSea. If set, it will instruct the browser to use
    this URL to access Grafana.
   </p></section></section><section class="sect1" id="prometheus" data-id-title="Prometheus"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.3 </span><span class="title-name">Prometheus</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#prometheus">#</a></h2></div></div></div><p>
   The exporter based configuration that can be passed through the pillar.
   These groups map to exporters that provide data. The node exporter is
   present on all nodes, Ceph is exported by the Ceph Manager nodes, Prometheus
   and Grafana is exported by the respective Prometheus and Grafana
   nodes.
  </p><p>
   Prometheus uses the following variables:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">scrape_interval</code></strong></span>:
     change the scrape interval, how often an exporter is to be scraped.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">target_partition</code></strong></span>:
     partition scrape targets when multiple Prometheus instnaces are deployed
     and have some instances scrape only part of all exporter instances.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">relabel_config</code></strong></span>:
     dynamically rewrites the label set of a target before it gets scraped.
     Multiple relabeling steps can be configured per scrape configuration.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">metrics_relabel_config</code></strong></span>:
     applied to samples as the last step before ingestion.
    </p></li></ul></div><section class="sect2" id="prometheus-security-model" data-id-title="Security Model"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.3.1 </span><span class="title-name">Security Model</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#prometheus-security-model">#</a></h3></div></div></div><p>
    Prometheus' security model presumes that untrusted users have access to
    the Prometheus HTTP endpoint and logs. Untrusted users have access to all
    the (meta-)data Prometheus collects that is contained in the database,
    plus a variety of operational and debugging information.
   </p><p>
    However, Prometheus' HTTP API is limited to read-only operations.
    Configurations cannot be changed using the API, and secrets are not
    exposed. Moreover, Prometheus has some built-in measures to mitigate the
    impact of denial of service attacks.
   </p></section></section><section class="sect1" id="alerting-alertmanager" data-id-title="Alertmanager"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.4 </span><span class="title-name">Alertmanager</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#alerting-alertmanager">#</a></h2></div></div></div><p>
   The Alertmanager handles alerts sent by the Prometheus server. It takes
   care of deduplicating, grouping, and routing them to the correct receiver.
   It also takes care of silencing of alerts. Alertmanager is configured via
   the command line flags and a configuration file that defines inhibition
   rules, notification routing and notification receivers.
  </p><section class="sect2" id="id-1.3.5.5.8.3" data-id-title="Configuration File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.1 </span><span class="title-name">Configuration File</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3">#</a></h3></div></div></div><p>
    Alertmanager's configuration is different for each deployment. Therefore,
    DeepSea does not ship any related defaults. You need to provide your own
    <code class="filename">alertmanager.yml</code> configuration file. The
    <span class="package">alertmanager</span> package by default installs a configuration
    file <code class="filename">/etc/prometheus/alertmanager.yml</code> which can serve
    as an example configuration. If you prefer to have your Alertmanager
    configuration managed by DeepSea, add the following key to your pillar,
    for example to the
    <code class="filename">/srv/pillar/ceph/stack/ceph/minions/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
    file:
   </p><p>
    For a complete example of Alertmanager's configuration file, see
    <a class="xref" href="monitoring-alerting.html#troubleshooting-alerts" title="18.5. Troubleshooting Alerts">Section 18.5, “Troubleshooting Alerts”</a>.
   </p><div class="verbatim-wrap"><pre class="screen">monitoring:
  alertmanager:
    /path/to/your/alertmanager/config.yml</pre></div><p>
    Alertmanager's configuration file is written in the YAML format. It
    follows the scheme described below. Parameters in brackets are optional.
    For non-list parameters the default value is used. The following generic
    placeholders are used in the scheme:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.3.6.1"><span class="term"><em class="replaceable">DURATION</em></span></dt><dd><p>
       A duration matching the regular expression
       <code class="literal">[0-9]+(ms|[smhdwy])</code>
      </p></dd><dt id="id-1.3.5.5.8.3.6.2"><span class="term"><em class="replaceable">LABELNAME</em></span></dt><dd><p>
       A string matching the regular expression
       <code class="literal">[a-zA-Z_][a-zA-Z0-9_]*</code>
      </p></dd><dt id="id-1.3.5.5.8.3.6.3"><span class="term"><em class="replaceable">LABELVALUE</em></span></dt><dd><p>
       A string of Unicode characters.
      </p></dd><dt id="id-1.3.5.5.8.3.6.4"><span class="term"><em class="replaceable">FILEPATH</em></span></dt><dd><p>
       A valid path in the current working directory.
      </p></dd><dt id="id-1.3.5.5.8.3.6.5"><span class="term"><em class="replaceable">BOOLEAN</em></span></dt><dd><p>
       A Boolean that can take the values 'true' or 'false'.
      </p></dd><dt id="id-1.3.5.5.8.3.6.6"><span class="term"><em class="replaceable">STRING</em></span></dt><dd><p>
       A regular string.
      </p></dd><dt id="id-1.3.5.5.8.3.6.7"><span class="term"><em class="replaceable">SECRET</em></span></dt><dd><p>
       A regular string that is a secret, for example a password.
      </p></dd><dt id="id-1.3.5.5.8.3.6.8"><span class="term"><em class="replaceable">TMPL_STRING</em></span></dt><dd><p>
       A string which is template-expanded before usage.
      </p></dd><dt id="id-1.3.5.5.8.3.6.9"><span class="term"><em class="replaceable">TMPL_SECRET</em></span></dt><dd><p>
       A secret string which is template-expanded before usage.
      </p></dd></dl></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.7" data-id-title="Global Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.1: </span><span class="title-name">Global Configuration </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.7">#</a></h6></div><div class="example-contents"><p>
     Parameters in the <code class="literal">global:</code> configuration are valid in
     all other configuration contexts. They also serve as defaults for other
     configuration sections.
    </p><div class="verbatim-wrap"><pre class="screen">global:
# the time after which an alert is declared resolved if it has not been updated
[ resolve_timeout: <em class="replaceable">DURATION</em> | default = 5m ]

# The default SMTP From header field.
[ smtp_from: <em class="replaceable">TMPL_STRING</em> ]
# The default SMTP smarthost used for sending emails, including port number.
# Port number usually is 25, or 587 for SMTP over TLS
# (sometimes referred to as STARTTLS).
# Example: smtp.example.org:587
[ smtp_smarthost: <em class="replaceable">STRING</em> ]
# The default host name to identify to the SMTP server.
[ smtp_hello: <em class="replaceable">STRING</em> | default = "localhost" ]
[ smtp_auth_username: <em class="replaceable">STRING</em> ]
# SMTP Auth using LOGIN and PLAIN.
[ smtp_auth_password: <em class="replaceable">SECRET</em> ]
# SMTP Auth using PLAIN.
[ smtp_auth_identity: <em class="replaceable">STRING</em> ]
# SMTP Auth using CRAM-MD5.
[ smtp_auth_secret: <em class="replaceable">SECRET</em> ]
# The default SMTP TLS requirement.
[ smtp_require_tls: <em class="replaceable">BOOL</em> | default = true ]

# The API URL to use for Slack notifications.
[ slack_api_url: <em class="replaceable">STRING</em> ]
[ victorops_api_key: <em class="replaceable">STRING</em> ]
[ victorops_api_url: <em class="replaceable">STRING</em> | default = "https://victorops.example.com/integrations/alert/" ]
[ pagerduty_url: <em class="replaceable">STRING</em> | default = "https://pagerduty.example.com/v2/enqueue" ]
[ opsgenie_api_key: <em class="replaceable">STRING</em> ]
[ opsgenie_api_url: <em class="replaceable">STRING</em> | default = "https://opsgenie.example.com/" ]
[ hipchat_api_url: <em class="replaceable">STRING</em> | default = "https://hipchat.example.com/" ]
[ hipchat_auth_token: <em class="replaceable">SECRET</em> ]
[ wechat_api_url: <em class="replaceable">STRING</em> | default = "https://wechat.example.com/cgi-bin/" ]
[ wechat_api_secret: <em class="replaceable">SECRET</em> ]
[ wechat_api_corp_id: <em class="replaceable">STRING</em> ]

# The default HTTP client configuration
[ http_config: <em class="replaceable">HTTP_CONFIG</em> ]

# Files from which custom notification template definitions are read.
# The last component may use a wildcard matcher, e.g. 'templates/*.tmpl'.
templates:
[ - <em class="replaceable">FILEPATH</em> ... ]

# The root node of the routing tree.
route: <em class="replaceable">ROUTE</em>

# A list of notification receivers.
receivers:
- <em class="replaceable">RECEIVER</em> ...

# A list of inhibition rules.
inhibit_rules:
[ - <em class="replaceable">INHIBIT_RULE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.8" data-id-title="ROUTE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.2: </span><span class="title-name"><em class="replaceable">ROUTE</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.8">#</a></h6></div><div class="example-contents"><p>
     A <em class="replaceable">ROUTE</em> block defines a node in a routing tree.
     Unspecified parameters are inherited from its parent node. Every alert
     enters the routing tree at the configured top-level route, which needs to
     match all alerts. It then traverses the child nodes. If the
     <code class="option">continue</code> option is set to 'false', the traversing stops
     after the first matched child. Setting the option to 'true' on a matched
     node, the alert will continue matching against subsequent siblings. If an
     alert does not match any children of a node, the alert is handled based on
     the configuration parameters of the current node.
    </p><div class="verbatim-wrap"><pre class="screen">[ receiver: <em class="replaceable">STRING</em> ]
[ group_by: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]

# If an alert should continue matching subsequent sibling nodes.
[ continue: <em class="replaceable">BOOLEAN</em> | default = false ]

# A set of equality matchers an alert has to fulfill to match a node.
match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]

# A set of regex-matchers an alert has to fulfill to match a node.
match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Time to wait before sending a notification for a group of alerts.
[ group_wait: <em class="replaceable">DURATION</em> | default = 30s ]

# Time to wait before sending a notification about new alerts
# added to a group of alerts for which an initial notification has
# already been sent.
[ group_interval: <em class="replaceable">DURATION</em> | default = 5m ]

# Time to wait before re-sending a notification
[ repeat_interval: <em class="replaceable">DURATION</em> | default = 4h ]

# Possible child routes.
routes:
 [ - <em class="replaceable">ROUTE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.9" data-id-title="INHIBIT_RULE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.3: </span><span class="title-name"><em class="replaceable">INHIBIT_RULE</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.9">#</a></h6></div><div class="example-contents"><p>
     An inhibition rule mutes a target alert that matches a set of matchers
     when a source alert exists that matches another set of matchers. Both
     alerts need to share the same label values for the label names in the
     <code class="option">equal</code> list.
    </p><p>
     Alerts can match and therefore inhibit themselves. Do not write inhibition
     rules where an alert matches both source and target.
    </p><div class="verbatim-wrap"><pre class="screen"># Matchers that need to be fulfilled for the alerts to be muted.
target_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
target_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Matchers for which at least one alert needs to exist so that the
# inhibition occurs.
source_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
source_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Labels with an equal value in the source and target
# alert for the inhibition to take effect.
[ equal: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.10" data-id-title="HTTP_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.4: </span><span class="title-name"><em class="replaceable">HTTP_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.10">#</a></h6></div><div class="example-contents"><p>
     <em class="replaceable">HTTP_CONFIG</em> configures the HTTP client used by
     the receiver to communicate with API services.
    </p><p>
     Note that <code class="option">basic_auth</code>, <code class="option">bearer_token</code> and
     <code class="option">bearer_token_file</code> options are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Sets the 'Authorization' header with the user name and password.
basic_auth:
[ username: <em class="replaceable">STRING</em> ]
[ password: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token.
[ bearer_token: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token read from a file.
[ bearer_token_file: <em class="replaceable">FILEPATH</em> ]

# TLS settings.
tls_config:
# CA certificate to validate the server certificate with.
[ ca_file: <em class="replaceable">FILEPATH</em> ]
# Certificate and key files for client cert authentication to the server.
[ cert_file: <em class="replaceable">FILEPATH</em> ]
[ key_file: <em class="replaceable">FILEPATH</em> ]
# ServerName extension to indicate the name of the server.
# http://tools.ietf.org/html/rfc4366#section-3.1
[ server_name: <em class="replaceable">STRING</em> ]
# Disable validation of the server certificate.
[ insecure_skip_verify: <em class="replaceable">BOOLEAN</em> | default = false]

# Optional proxy URL.
[ proxy_url: <em class="replaceable">STRING</em> ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.11" data-id-title="RECEIVER"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.5: </span><span class="title-name"><em class="replaceable">RECEIVER</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.11">#</a></h6></div><div class="example-contents"><p>
     Receiver is a named configuration for one or more notification
     integrations.
    </p><p>
     Instead of adding new receivers, we recommend implementing custom
     notification integrations using the webhook receiver (see
     <a class="xref" href="monitoring-alerting.html#alert-webhook" title="WEBHOOK_CONFIG">Example 18.15, “<em class="replaceable">WEBHOOK_CONFIG</em>”</a>).
    </p><div class="verbatim-wrap"><pre class="screen"># The unique name of the receiver.
name: <em class="replaceable">STRING</em>

# Configurations for several notification integrations.
email_configs:
[ - <em class="replaceable">EMAIL_CONFIG</em>, ... ]
hipchat_configs:
[ - <em class="replaceable">HIPCHAT_CONFIG</em>, ... ]
pagerduty_configs:
[ - <em class="replaceable">PAGERDUTY_CONFIG</em>, ... ]
pushover_configs:
[ - <em class="replaceable">PUSHOVER_CONFIG</em>, ... ]
slack_configs:
[ - <em class="replaceable">SLACK_CONFIG</em>, ... ]
opsgenie_configs:
[ - <em class="replaceable">OPSGENIE_CONFIG</em>, ... ]
webhook_configs:
[ - <em class="replaceable">WEBHOOK_CONFIG</em>, ... ]
victorops_configs:
[ - <em class="replaceable">VICTOROPS_CONFIG</em>, ... ]
wechat_configs:
[ - <em class="replaceable">WECHAT_CONFIG</em>, ... ]</pre></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.12" data-id-title="EMAIL_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.6: </span><span class="title-name"><em class="replaceable">EMAIL_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.12">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The email address to send notifications to.
to: <em class="replaceable">TMPL_STRING</em>

# The sender address.
[ from: <em class="replaceable">TMPL_STRING</em> | default = global.smtp_from ]

# The SMTP host through which emails are sent.
[ smarthost: <em class="replaceable">STRING</em> | default = global.smtp_smarthost ]

# The host name to identify to the SMTP server.
[ hello: <em class="replaceable">STRING</em> | default = global.smtp_hello ]

# SMTP authentication details.
[ auth_username: <em class="replaceable">STRING</em> | default = global.smtp_auth_username ]
[ auth_password: <em class="replaceable">SECRET</em> | default = global.smtp_auth_password ]
[ auth_secret: <em class="replaceable">SECRET</em> | default = global.smtp_auth_secret ]
[ auth_identity: <em class="replaceable">STRING</em> | default = global.smtp_auth_identity ]

# The SMTP TLS requirement.
[ require_tls: <em class="replaceable">BOOL</em> | default = global.smtp_require_tls ]

# The HTML body of the email notification.
[ html: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "email.default.html" . }}' ]
# The text body of the email notification.
[ text: <em class="replaceable">TMPL_STRING</em> ]

# Further headers email header key/value pairs. Overrides any headers
# previously set by the notification implementation.
[ headers: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.13" data-id-title="HIPCHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.7: </span><span class="title-name"><em class="replaceable">HIPCHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.13">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The HipChat Room ID.
room_id: <em class="replaceable">TMPL_STRING</em>
# The authentication token.
[ auth_token: <em class="replaceable">SECRET</em> | default = global.hipchat_auth_token ]
# The URL to send API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.hipchat_api_url ]

# A label to be shown in addition to the sender's name.
[ from:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.from" . }}' ]
# The message body.
[ message:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.message" . }}' ]
# Whether this message will trigger a user notification.
[ notify:  <em class="replaceable">BOOLEAN</em> | default = false ]
# Determines how the message is treated by the alertmanager and rendered inside HipChat. Valid values are 'text' and 'html'.
[ message_format:  <em class="replaceable">STRING</em> | default = 'text' ]
# Background color for message.
[ color:  <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}red{{ else }}green{{ end }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.14" data-id-title="PAGERDUTY_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.8: </span><span class="title-name"><em class="replaceable">PAGERDUTY_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.14">#</a></h6></div><div class="example-contents"><p>
     The <code class="option">routing_key</code> and <code class="option">service_key</code> options
     are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The PagerDuty integration key (when using 'Events API v2').
routing_key: <em class="replaceable">TMPL_SECRET</em>
# The PagerDuty integration key (when using 'Prometheus').
service_key: <em class="replaceable">TMPL_SECRET</em>

# The URL to send API requests to.
[ url: <em class="replaceable">STRING</em> | default = global.pagerduty_url ]

# The client identification of the Alertmanager.
[ client:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.client" . }}' ]
# A backlink to the notification sender.
[ client_url:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.clientURL" . }}' ]

# The incident description.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.description" .}}' ]

# Severity of the incident.
[ severity: <em class="replaceable">TMPL_STRING</em> | default = 'error' ]

# A set of arbitrary key/value pairs that provide further details.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } | default = {
 firing:       '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
 resolved:     '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
 num_firing:   '{{ .Alerts.Firing | len }}'
 num_resolved: '{{ .Alerts.Resolved | len }}'
} ]

# The HTTP client's configuration.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.15" data-id-title="PUSHOVER_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.9: </span><span class="title-name"><em class="replaceable">PUSHOVER_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.15">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The recipient user key.
user_key: <em class="replaceable">SECRET</em>

# Registered application’s API token.
token: <em class="replaceable">SECRET</em>

# Notification title.
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.title" . }}' ]

# Notification message.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.message" . }}' ]

# A supplementary URL displayed together with the message.
[ url: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.url" . }}' ]

# Priority.
[ priority: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}2{{ else }}0{{ end }}' ]

# How often the Pushover servers will send the same notification (at least 30 seconds).
[ retry: <em class="replaceable">DURATION</em> | default = 1m ]

# How long your notification will continue to be retried (unless the user
# acknowledges the notification).
[ expire: <em class="replaceable">DURATION</em> | default = 1h ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.16" data-id-title="SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.10: </span><span class="title-name"><em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.16">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The Slack webhook URL.
[ api_url: <em class="replaceable">SECRET</em> | default = global.slack_api_url ]

# The channel or user to send notifications to.
channel: <em class="replaceable">TMPL_STRING</em>

# API request data as defined by the Slack webhook API.
[ icon_emoji: <em class="replaceable">TMPL_STRING</em> ]
[ icon_url: <em class="replaceable">TMPL_STRING</em> ]
[ link_names: <em class="replaceable">BOOLEAN</em> | default = false ]
[ username: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.username" . }}' ]
# The following parameters define the attachment.
actions:
[ <em class="replaceable">ACTION_CONFIG</em> ... ]
[ color: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}' ]
[ fallback: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.fallback" . }}' ]
fields:
[ <em class="replaceable">FIELD_CONFIG</em> ... ]
[ footer: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.footer" . }}' ]
[ pretext: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.pretext" . }}' ]
[ short_fields: <em class="replaceable">BOOLEAN</em> | default = false ]
[ text: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.text" . }}' ]
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.title" . }}' ]
[ title_link: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.titlelink" . }}' ]
[ image_url: <em class="replaceable">TMPL_STRING</em> ]
[ thumb_url: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.17" data-id-title="ACTION_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.11: </span><span class="title-name"><em class="replaceable">ACTION_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.17">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Provide a button to tell Slack you want to render a button.
type: <em class="replaceable">TMPL_STRING</em>
# Label for the button.
text: <em class="replaceable">TMPL_STRING</em>
# http or https URL to deliver users to. If you specify invalid URLs, the message will be posted with no button.
url: <em class="replaceable">TMPL_STRING</em>
#  If set to 'primary', the button will be green, indicating the best forward action to take
#  'danger' turns the button red, indicating a destructive action.
[ style: <em class="replaceable">TMPL_STRING</em> [ default = '' ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.18" data-id-title="FIELD_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.12: </span><span class="title-name"><em class="replaceable">FIELD_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.18">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># A bold heading without markup above the <code class="option">value</code> text.
title: <em class="replaceable">TMPL_STRING</em>
# The text of the field. It can span across several lines.
value: <em class="replaceable">TMPL_STRING</em>
# A flag indicating if <code class="option">value</code> is short enough to be displayed together with other values.
[ short: <em class="replaceable">BOOLEAN</em> | default = slack_config.short_fields ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.19" data-id-title="OPSGENIE_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.13: </span><span class="title-name"><em class="replaceable">OPSGENIE_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.19">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key to use with the OpsGenie API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.opsgenie_api_key ]

# The host to send OpsGenie API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.opsgenie_api_url ]

# Alert text (maximum is 130 characters).
[ message: <em class="replaceable">TMPL_STRING</em> ]

# A description of the incident.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.description" . }}' ]

# A backlink to the sender.
[ source: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.source" . }}' ]

# A set of arbitrary key/value pairs that provide further detail.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]

# Comma separated list of team responsible for notifications.
[ teams: <em class="replaceable">TMPL_STRING</em> ]

# Comma separated list of tags attached to the notifications.
[ tags: <em class="replaceable">TMPL_STRING</em> ]

# Additional alert note.
[ note: <em class="replaceable">TMPL_STRING</em> ]

# Priority level of alert, one of P1, P2, P3, P4, and P5.
[ priority: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.20" data-id-title="VICTOROPS_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.14: </span><span class="title-name"><em class="replaceable">VICTOROPS_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.20">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key for talking to the VictorOps API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.victorops_api_key ]

# The VictorOps API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.victorops_api_url ]

# A key used to map the alert to a team.
routing_key: <em class="replaceable">TMPL_STRING</em>

# Describes the behavior of the alert (one of 'CRITICAL', 'WARNING', 'INFO').
[ message_type: <em class="replaceable">TMPL_STRING</em> | default = 'CRITICAL' ]

# Summary of the alerted problem.
[ entity_display_name: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.entity_display_name" . }}' ]

# Long explanation of the alerted problem.
[ state_message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.state_message" . }}' ]

# The monitoring tool the state message is from.
[ monitoring_tool: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.monitoring_tool" . }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="alert-webhook" data-id-title="WEBHOOK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.15: </span><span class="title-name"><em class="replaceable">WEBHOOK_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#alert-webhook">#</a></h6></div><div class="example-contents"><p>
     You can use the webhook receiver to configure a generic receiver.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The endpoint for sending HTTP POST requests.
url: <em class="replaceable">STRING</em>

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div><p>
     Alertmanager sends HTTP POST requests in the following JSON format:
    </p><div class="verbatim-wrap"><pre class="screen">{
 "version": "4",
 "groupKey": <em class="replaceable">STRING</em>, // identifycation of the group of alerts (to deduplicate)
 "status": "&lt;resolved|firing&gt;",
 "receiver": <em class="replaceable">STRING</em>,
 "groupLabels": <em class="replaceable">OBJECT</em>,
 "commonLabels": <em class="replaceable">OBJECT</em>,
 "commonAnnotations": <em class="replaceable">OBJECT</em>,
 "externalURL": <em class="replaceable">STRING</em>, // backlink to Alertmanager.
 "alerts": [
   {
     "status": "&lt;resolved|firing&gt;",
     "labels": <em class="replaceable">OBJECT</em>,
     "annotations": <em class="replaceable">OBJECT</em>,
     "startsAt": "&lt;rfc3339&gt;",
     "endsAt": "&lt;rfc3339&gt;",
     "generatorURL": <em class="replaceable">STRING</em> // identifies the entity that caused the alert
   },
   ...
 ]
}</pre></div><p>
     The webhook receiver allows for integration with the following
     notification mechanisms:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       DingTalk (https://github.com/timonwong/prometheus-webhook-dingtalk)
      </p></li><li class="listitem"><p>
       IRC Bot (https://github.com/multimfi/bot)
      </p></li><li class="listitem"><p>
       JIRAlert (https://github.com/free/jiralert)
      </p></li><li class="listitem"><p>
       Phabricator / Maniphest (https://github.com/knyar/phalerts)
      </p></li><li class="listitem"><p>
       prom2teams: forwards notifications to Microsoft Teams
       (https://github.com/idealista/prom2teams)
      </p></li><li class="listitem"><p>
       SMS: supports multiple providers (https://github.com/messagebird/sachet)
      </p></li><li class="listitem"><p>
       Telegram bot (https://github.com/inCaller/prometheus_bot)
      </p></li><li class="listitem"><p>
       SNMP trap (https://github.com/SUSE/prometheus-webhook-snmp)
      </p></li></ul></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.22" data-id-title="WECHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.16: </span><span class="title-name"><em class="replaceable">WECHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.3.22">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The API key to use for the WeChat API.
[ api_secret: <em class="replaceable">SECRET</em> | default = global.wechat_api_secret ]

# The WeChat API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.wechat_api_url ]

# The corp id used to authenticate.
[ corp_id: <em class="replaceable">STRING</em> | default = global.wechat_api_corp_id ]

# API request data as defined by the WeChat API.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "wechat.default.message" . }}' ]
[ agent_id: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.agent_id" . }}' ]
[ to_user: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_user" . }}' ]
[ to_party: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_party" . }}' ]
[ to_tag: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_tag" . }}' ]</pre></div></div></div></section><section class="sect2" id="id-1.3.5.5.8.4" data-id-title="Custom Alerts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.2 </span><span class="title-name">Custom Alerts</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.4">#</a></h3></div></div></div><p>
    You can define your custom alert conditions to send notifications to an
    external service. Prometheus uses its own expression language for
    defining custom alerts. Following is an example of a rule with an alert:
   </p><div class="verbatim-wrap"><pre class="screen">groups:
- name: example
 rules:
  # alert on high deviation from average PG count
  - alert: high pg count deviation
   expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
   for: 5m
   labels:
    severity: warning
    type: ses_default
   annotations:
   description: &gt;
    OSD {{ $labels.osd }} deviates by more then 30% from average PG count</pre></div><p>
    The optional <code class="literal">for</code> clause specifies the time Prometheus
    will wait between first encountering a new expression output vector element
    and counting an alert as firing. In this case, Prometheus will check that
    the alert continues to be active for 5 minutes before firing the alert.
    Elements in a pending state are active, but not firing yet.
   </p><p>
    The <code class="literal">labels</code> clause specifies a set of additional labels
    attached to the alert. Conflicting labels will be overwritten. Labels can
    be templated (see <a class="xref" href="monitoring-alerting.html#alertmanager-templates" title="18.4.2.1. Templates">Section 18.4.2.1, “Templates”</a> for more details
    on templating).
   </p><p>
    The <code class="literal">annotations</code> clause specifies informational labels.
    You can use them to store additional information, for example alert
    descriptions or runbook links. Annotations can be templated (see
    <a class="xref" href="monitoring-alerting.html#alertmanager-templates" title="18.4.2.1. Templates">Section 18.4.2.1, “Templates”</a> for more details on templating).
   </p><p>
    To add your custom alerts to SUSE Enterprise Storage 6, either
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      place your YAML files with custom alerts in the
      <code class="filename">/etc/prometheus/alerts</code> directory
     </p></li></ul></div><p>
    or
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      provide a list of paths to your custom alert files in the Pillar under
      the <code class="option">monitoring:custom_alerts</code> key. DeepSea Stage 2 or
      the <code class="command">salt <em class="replaceable">SALT_MASTER</em> state.apply
      ceph.monitoring.prometheus</code> command will add your alert files in
      the right place.
     </p><div class="complex-example"><div class="example" id="id-1.3.5.5.8.4.10.1.2" data-id-title="Adding Custom Alerts to SUSE Enterprise Storage"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.17: </span><span class="title-name">Adding Custom Alerts to SUSE Enterprise Storage </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.4.10.1.2">#</a></h6></div><div class="example-contents"><p>
       A file with custom alerts is in
       <code class="filename">/root/my_alerts/my_alerts.yml</code> on the Salt master.
       If you add
      </p><div class="verbatim-wrap"><pre class="screen">monitoring:
 custom_alerts:
   - /root/my_alerts/my_alerts.yml</pre></div><p>
       to the
       <code class="filename">/srv/pillar/ceph/cluster/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
       file, DeepSea will create the
       <code class="filename">/etc/prometheus/alerts/my_alerts.yml</code> file and
       restart Prometheus.
      </p></div></div></div></li></ul></div><section class="sect3" id="alertmanager-templates" data-id-title="Templates"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">18.4.2.1 </span><span class="title-name">Templates</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#alertmanager-templates">#</a></h4></div></div></div><p>
     You can use templates for label and annotation values. The
     <code class="varname">$labels</code> variable includes the label key/value pairs of
     an alert instance, while <code class="varname">$value</code> holds the evaluated
     value of an alert instance.
    </p><p>
     The following example inserts a firing element label and value:
    </p><div class="verbatim-wrap"><pre class="screen">{{ $labels.<em class="replaceable">LABELNAME</em> }}
{{ $value }}</pre></div></section><section class="sect3" id="id-1.3.5.5.8.4.12" data-id-title="Inspecting Alerts at Runtime"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">18.4.2.2 </span><span class="title-name">Inspecting Alerts at Runtime</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.4.12">#</a></h4></div></div></div><p>
     If you need to verify which alerts are active, you have several options:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Navigate to the <span class="guimenu">Alerts</span> tab of Prometheus. It will
       show you the exact label sets for which defined alerts are active.
       Prometheus also stores synthetic time series for pending and firing
       alerts. They have the following form:
      </p><div class="verbatim-wrap"><pre class="screen">ALERTS{alertname="<em class="replaceable">ALERT_NAME</em>", alertstate="pending|firing", <em class="replaceable">ADDITIONAL_ALERT_LABELS</em>}</pre></div><p>
       The sample value is 1 if the alert is active (pending or firing). The
       series is marked 'stale' when the alert is inactive.
      </p></li><li class="listitem"><p>
       In the Prometheus Web interface at the URL address
       http://<em class="replaceable">PROMETHEUS_HOST_IP</em>:9090/alerts,
       inspect alerts and their state (INACTIVE, PENDING or FIRING).
      </p></li><li class="listitem"><p>
       In the Alertmanager Web interface at the URL address
       http://:<em class="replaceable">PROMETHEUS_HOST_IP</em>:9093/#/alerts,
       inspect alerts and silence them if desired.
      </p></li></ul></div></section></section><section class="sect2" id="snmp-trap-receiver" data-id-title="SNMP Trap Receiver"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.3 </span><span class="title-name">SNMP Trap Receiver</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#snmp-trap-receiver">#</a></h3></div></div></div><p>
    If you want to get notified about Prometheus alerts via SNMP traps, then
    you can install the Prometheus Alertmanager SNMP trap receiver via
    DeepSea. To do so you need to enable it in the pillar under the
    <code class="option">monitoring:alertmanager_receiver_snmp:enabled</code> key in your
    <code class="filename">global.yml</code> file. The configuration of the receiver
    must be set under the
    <code class="option">monitoring:alertmanager_receiver_snmp:config</code> key.
   </p><div class="complex-example"><div class="example" id="id-1.3.5.5.8.5.3" data-id-title="SNMP Trap Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.18: </span><span class="title-name">SNMP Trap Configuration </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.8.5.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">monitoring:
 alertmanager:
   receiver:
      snmp:
        enabled: True
        config:
          host: localhost
          port: 9099
          snmp_host: snmp.foo-bar.com
          snmp_community: private
          metrics: True</pre></div><p>
     Refer to the receiver manual at
     <a class="link" href="https://github.com/SUSE/prometheus-webhook-snmp#global-configuration-file" target="_blank">https://github.com/SUSE/prometheus-webhook-snmp#global-configuration-file</a>.
     for more details about the configuration options.
    </p></div></div></div><p>
    DeepSea Stage 2 or the <code class="command">salt
    <em class="replaceable">SALT_MASTER</em> state.apply
    ceph.monitoring.alertmanager</code> command will install and configure
    the receiver in the appropriate location. Verify your settings with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call pillar.get 'monitoring:alertmanager_receiver_snmp:enabled'
<code class="prompt user">root@master # </code>salt-call pillar.get 'monitoring:alertmanager_receiver_snmp:config'</pre></div></section></section><section class="sect1" id="troubleshooting-alerts" data-id-title="Troubleshooting Alerts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.5 </span><span class="title-name">Troubleshooting Alerts</span> <a title="Permalink" class="permalink" href="monitoring-alerting.html#troubleshooting-alerts">#</a></h2></div></div></div><p>
   The following section details the alert that has been triggered and actions
   to take when the alert is displayed.
  </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MONITOR </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.3.2"><span class="term"><code class="option">MON_DOWN</code></span></dt><dd><p>
      One or more monitor daemons are down. The cluster requires a majority of
      the monitors in order to function. When one or more monitors are down,
      clients will initially have difficulty connecting to the cluster.
     </p><p>
      Restart the monitor daemon that is down as soon as possible to reduce the
      risk of a subsequent monitor failure.
     </p></dd><dt id="id-1.3.5.5.9.3.3"><span class="term"><code class="option">MON_CLOCK_SKEW</code></span></dt><dd><p>
      The clocks on the hosts running the
      <code class="systemitem">ceph-mon</code> monitor daemons are not
      well synchronized. This health alert is raised if the cluster detects a
      clock skew greater than <code class="option">mon_clock_drift_allowed</code>. Resolve
      this by synchronizing the clocks using either <code class="literal">ntpd</code> or
      <code class="literal">chrony</code>. If it is impractical to keep the clocks
      closely synchronized, the <code class="option">mon_clock_drift_allowed</code>
      threshold can be increased, but this value must stay well below the
      <code class="option">mon_lease</code> interval in order for monitor cluster to
      function properly.
     </p></dd><dt id="id-1.3.5.5.9.3.4"><span class="term"><code class="option">MON_MSGR2_NOT_ENABLED</code></span></dt><dd><p>
      The <code class="option">ms_bind_msgr2</code> option is enabled but one or more
      monitors is not configured to bind to a v2 port in the cluster’s monmap.
      This means that features specific to the msgr2 protocol (for example,
      encryption) are not available on some or all connections. In most cases
      this can be corrected by issuing the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon enable-msgr2</pre></div><p>
      This command changes any monitor configured for the old default port 6789
      to continue to listen for v1 connections on 6789 and also listen for v2
      connections on the new default 3300 port. If a monitor is configured to
      listen for v1 connections on a non-standard port (not 6789), then the
      monmap needs to be modified manually.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MANAGER </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.4.2"><span class="term"><code class="option">MGR_MODULE_DEPENDENCY</code></span></dt><dd><p>
      An enabled manager module is failing its dependency check. This health
      check should come with a message from the module about the problem. For
      example, a module might report that a required package is not installed.
      In which case, the message will read: "Install the required package and
      restart your manager daemons." This health check only applies to enabled
      modules. If a module is not enabled, you can see whether it is reporting
      dependency issues in the output of <code class="command">ceph module ls</code>.
     </p></dd><dt id="id-1.3.5.5.9.4.3"><span class="term"><code class="option">MGR_MODULE_ERROR</code></span></dt><dd><p>
      A manager module has experienced an unexpected error. Typically, this
      means an unhandled exception was raised from the module’s serve function.
      The human readable description of the error may be obscurely worded if
      the exception did not provide a useful description of itself. This health
      check may indicate a bug. Open a bug report if you think you have
      encountered a bug. If you believe the error is transient, you may restart
      your manager daemon(s), or use <code class="command">ceph mgr fail</code> on the
      active daemon to prompt a failover to another daemon.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">OSDS </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.5.2"><span class="term"><code class="option">OSD_DOWN</code></span></dt><dd><p>
      One or more OSDs are marked down. The
      <code class="systemitem">ceph-osd</code> daemon may have been
      stopped, or peer OSDs may be unable to reach the OSD over the network.
      Common causes include a stopped or crashed daemon, a down host, or a
      network outage. Verify the host is healthy, the daemon is started, and
      network is functioning. If the daemon has crashed, the daemon log file
      (<code class="filename">/var/log/ceph/ceph-osd.*</code>) may contain debugging
      information.
     </p></dd><dt id="id-1.3.5.5.9.5.3"><span class="term"><code class="option">OSD_<em class="replaceable">CRUSH TYPE</em>_DOWN</code></span></dt><dd><p>
      For example, <code class="filename">OSD_HOST_DOWN</code> or
      <code class="filename">OSD_ROOT_DOWN</code>. All the OSDs within a particular
      CRUSH subtree are marked down, for example all OSDs on a host.
     </p></dd><dt id="id-1.3.5.5.9.5.4"><span class="term"><code class="option">OSD_ORPHAN</code></span></dt><dd><p>
      An OSD is referenced in the CRUSH Map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rm osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.5.5.9.5.5"><span class="term"><code class="option">OSD_OUT_OF_ORDER_FULL</code></span></dt><dd><p>
      The utilization thresholds for <code class="literal">backfillfull</code>,
      <code class="literal">nearfull</code>, <code class="literal">full</code>, and
      <code class="option">failsafe_full</code> are not ascending. The thresholds can be
      adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">RATIO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">RATIO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">RATIO</em></pre></div></dd><dt id="id-1.3.5.5.9.5.6"><span class="term"><code class="option">OSD_FULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">full</code> threshold and
      is preventing the cluster from servicing writes. Utilization by pool can
      be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div><p>
      The currently defined <code class="literal">full</code> ratio can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep full_ratio</pre></div><p>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">RATIO</em></pre></div><p>
      New storage should be added to the cluster by deploying more OSDs or
      existing data should be deleted in order to free up space.
     </p></dd><dt id="id-1.3.5.5.9.5.7"><span class="term"><code class="option">OSD_BACKFILLFULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">backfillfull</code>
      threshold, preventing data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching <code class="literal">full</code>.
      Utilization by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.5.9.5.8"><span class="term"><code class="option">OSD_NEARFULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">nearfull</code> threshold.
      This is an early warning that the cluster is approaching
      <code class="literal">full</code>. Utilization by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.5.9.5.9"><span class="term"><code class="option">OSDMAP_FLAGS</code></span></dt><dd><p>
      One or more cluster flags of interest has been set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.9.5.9.2.2.1"><span class="term">full</span></dt><dd><p>
         The cluster is flagged as <code class="literal">full</code> and cannot serve
         writes
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.2"><span class="term">pauserd, pausewr</span></dt><dd><p>
         Paused reads or writes
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.3"><span class="term">noup</span></dt><dd><p>
         OSDs are not allowed to start
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.4"><span class="term">nodown</span></dt><dd><p>
         OSD failure reports are being ignored and the monitors are not marking
         OSDs down
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.5"><span class="term">noin</span></dt><dd><p>
         OSDs that were previously marked out are not being marked back in when
         they start
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.6"><span class="term">noout</span></dt><dd><p>
         Down OSDs are not automatically marked out after the configured
         interval
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.7"><span class="term">nobackfill, norecover, norebalance</span></dt><dd><p>
         Recovery or data rebalancing is suspended
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.8"><span class="term">noscrub, nodeep_scrub</span></dt><dd><p>
         Scrubbing is disabled
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.9"><span class="term">notieragent</span></dt><dd><p>
         Cache tiering activity is suspended
        </p></dd></dl></div><p>
      With the exception of <code class="literal">full</code>, these flags can be set or
      cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set <em class="replaceable">FLAG</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset <em class="replaceable">FLAG</em></pre></div></dd><dt id="id-1.3.5.5.9.5.10"><span class="term"><code class="option">OSD_FLAGS</code></span></dt><dd><p>
      One or more OSDs or CRUSH <code class="literal">{nodes,device classes}</code> has a
      flag of interest set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.9.5.10.2.2.1"><span class="term">noup</span></dt><dd><p>
         These OSDs are not allowed to start
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.2"><span class="term">nodown</span></dt><dd><p>
         Failure reports for these OSDs are ignored
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.3"><span class="term">noin</span></dt><dd><p>
         If these OSDs were previously marked out automatically after a
         failure, they are not to be marked in when they start
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.4"><span class="term">noout</span></dt><dd><p>
         If these OSDs are down they are not automatically marked out after the
         configured interval
        </p></dd></dl></div><p>
      These flags can be set and cleared in batch with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group <em class="replaceable">FLAG</em> <em class="replaceable">WHO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group <em class="replaceable">FLAG</em> <em class="replaceable">WHO</em></pre></div><p>
      For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout osd.0 osd.1
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout osd.0 osd.1
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout host-foo
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout host-foo
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout class-hdd
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout class-hdd</pre></div></dd><dt id="id-1.3.5.5.9.5.11"><span class="term"><code class="option">OLD_CRUSH_TUNABLES</code></span></dt><dd><p>
      The CRUSH Map is using old settings and should be updated. The oldest
      tunables that can be used (for example, the oldest client version that
      can connect to the cluster) without triggering this health warning are
      determined by the <code class="option">mon_crush_min_required_version</code> config
      option.
     </p></dd><dt id="id-1.3.5.5.9.5.12"><span class="term"><code class="option">OLD_CRUSH_STRAW_CALC_VERSION</code></span></dt><dd><p>
      The CRUSH Map is using an older, sub-optimal method for calculating
      intermediate weight values for straw buckets. The CRUSH Map requires an
      update to use the newer method (<code class="literal">straw_calc_version=1</code>).
     </p></dd><dt id="id-1.3.5.5.9.5.13"><span class="term"><code class="option">CACHE_POOL_NO_HIT_SET</code></span></dt><dd><p>
      One or more cache pools are not configured with a hit set to track
      utilization. This prevents the tiering agent from identifying cold
      objects to flush and evict from the cache. Hit sets can be configured on
      the cache pool with the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_type <em class="replaceable">TYPE</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_period <em class="replaceable">PERIOD-IN-SECONDS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_count <em class="replaceable">NUMBER-OF-HITSETS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_fpp <em class="replaceable">TARGET-FALSE-POSITIVE-RATE</em></pre></div></dd><dt id="id-1.3.5.5.9.5.14"><span class="term"><code class="option">OSD_NO_SORTBITWISE</code></span></dt><dd><p>
      No SUSE Enterprise Storage 5.5 v12.y.z OSDs are running but the
      <code class="option">sortbitwise</code> flag has not been set. Set the
      <code class="option">sortbitwise</code> flag before v12.y.z or newer OSDs can start.
      You can safely set the flag with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set sortbitwise</pre></div></dd><dt id="id-1.3.5.5.9.5.15"><span class="term"><code class="option">POOL_FULL</code></span></dt><dd><p>
      One or more pools have reached the quota and are no longer allowing
      writes. Pool quotas and utilization can be seen with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df detail</pre></div><p>
      You can either raise the pool quota with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOLNAME</em> max_objects <em class="replaceable">NUM-OBJECTS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOLNAME</em> max_bytes <em class="replaceable">NUM-BYTES</em></pre></div><p>
      Or, you can delete existing data to reduce utilization.
     </p></dd><dt id="id-1.3.5.5.9.5.16"><span class="term"><code class="option">BLUEFS_SPILLOVER</code></span></dt><dd><p>
      One or more OSDs that use the BlueStore backend have been allocated db
      partitions (storage space for metadata, normally on a faster device) but
      that space has filled, such that metadata has overflowed onto the normal
      slow device. This is not necessarily an error condition or even
      unexpected, but if the administrator’s expectation was that all metadata
      would fit on the faster device, it indicates that not enough space was
      provided. This warning can be disabled on all OSDs with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd bluestore_warn_on_bluefs_spillover false</pre></div><p>
      Alternatively, it can be disabled on a specific OSD with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd.123 bluestore_warn_on_bluefs_spillover false</pre></div><p>
      To provide more metadata space, the OSD in question can be destroyed and
      reprovisioned. This involves data migration and recovery. It is possible
      to expand the LVM logical volume backing the db storage. If the
      underlying LV has been expanded, the OSD daemon needs to be stopped and
      BlueFS informed of the device size change with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool bluefs-bdev-expand --path /var/lib/ceph/osd/ceph-$ID</pre></div></dd><dt id="id-1.3.5.5.9.5.17"><span class="term"><code class="option">BLUEFS_AVAILABLE_SPACE</code></span></dt><dd><p>
      To check how much space is free for BlueFS, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore bluefs available</pre></div><p>
      This provides output for up to 3 values; <code class="literal">BDEV_DB free</code>,
      <code class="literal">BDEV_SLOW free</code> and
      <code class="option">available_from_bluestore</code>. <code class="literal">BDEV_DB</code> and
      <code class="literal">BDEV_SLOW</code> report the amount of space that has been
      acquired by BlueFS and is considered free. Value
      <code class="option">available_from_bluestore</code> denotes ability of BlueStore
      to leave more space to BlueFS. It is normal that this value is different
      from amount of BlueStore free space, as BlueFS allocation unit is
      typically larger than BlueStore allocation unit. This means that only
      part of BlueStore free space is acceptable for BlueFS.
     </p></dd><dt id="id-1.3.5.5.9.5.18"><span class="term"><code class="option">BLUEFS_LOW_SPACE</code></span></dt><dd><p>
      If BlueFS is running low on available free space and there is little
      <code class="option">available_from_bluestore</code>, consider reducing BlueFS'
      allocation unit size. To simulate available space when the allocation
      unit is different, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore bluefs available <em class="replaceable">ALLOC-UNIT-SIZE</em></pre></div></dd><dt id="id-1.3.5.5.9.5.19"><span class="term"><code class="option">BLUESTORE_FRAGMENTATION</code></span></dt><dd><p>
      As BlueStore works, free space on underlying storage becomes
      fragmented. This is normal and unavoidable, but excessive fragmentation
      can cause slowdown. To inspect BlueStore fragmentation, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore allocator score block</pre></div><p>
      Score is given in [0-1] range. [0.0 .. 0.4] tiny fragmentation [0.4 ..
      0.7] small, acceptable fragmentation [0.7 .. 0.9] considerable, but safe
      fragmentation [0.9 .. 1.0] severe fragmentation, can impact BlueFS'
      ability to get space from BlueStore. If detailed report of free
      fragments is required, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore allocator dump block</pre></div><p>
      If the OSD process does not perform fragmentation, inspect with
      <code class="command">ceph-bluestore-tool</code>. Get the fragmentation score:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-score</pre></div><p>
      Dump detailed free chunks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-dump</pre></div></dd><dt id="id-1.3.5.5.9.5.20"><span class="term"><code class="option">BLUESTORE_LEGACY_STATFS</code></span></dt><dd><p>
      As of SUSE Enterprise Storage 6, BlueStore tracks its internal usage statistics
      on a per-pool granular basis and one or more OSDs have BlueStore
      volumes that were created prior to SUSE Enterprise Storage 6. If all
      OSDs are older than SUSE Enterprise Storage 6, the per-pool metrics
      are not available. However, if there is a mix of pre-SUSE Enterprise Storage
      6 and post-SUSE Enterprise Storage 6 OSDs, the cluster
      usage statistics reported by <code class="command">ceph df</code> will not be
      accurate. The old OSDs can be updated to use the new usage tracking
      scheme by stopping each OSD, running a repair operation, and the
      restarting it. For example, if <code class="literal">osd.123</code> requires an
      update:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph-osd@123
<code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-123
<code class="prompt user">root # </code>systemctl start ceph-osd@123</pre></div><p>
      This warning can be disabled with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global bluestore_warn_on_legacy_statfs false</pre></div></dd><dt id="id-1.3.5.5.9.5.21"><span class="term"><code class="option">BLUESTORE_DISK_SIZE_MISMATCH</code></span></dt><dd><p>
      One or more OSDs using BlueStore has an internal inconsistency between
      the size of the physical device and the metadata tracking its size. This
      can lead to the OSD crashing in the future. The OSDs in question should
      be destroyed and re-deployed. To avoid putting any data at risk,
      re-deploy only one OSD at a time. For example, if
      <em class="replaceable">OSD_ID</em> has the error:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd out osd.$N
while ! ceph osd safe-to-destroy osd.$N ; do sleep 1m ; done
ceph osd destroy osd.$N
ceph-volume lvm zap /path/to/device
ceph-volume lvm create --osd-id $N --data /path/to/device</pre></div></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">DEVICE HEALTH </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.6.2"><span class="term"><code class="option">DEVICE_HEALTH</code></span></dt><dd><p>
      One or more devices are expected to fail. The warning threshold is
      controlled by the <code class="literal">mgr/devicehealth/warn_threshold</code>
      configuration option. This warning only applies to OSDs that are
      currently marked <code class="option">in</code>. The expected response to this
      failure is to mark the device <code class="option">out</code>. The data is then
      migrated off of the device and the hardware is removed from the system.
      Marking out is normally done automatically if
      <code class="literal">mgr/devicehealth/self_heal</code> is enabled based on the
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>. Device health can
      be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph device info <em class="replaceable">DEVICE-ID</em></pre></div><p>
      Device life expectancy is set by a prediction model run by the Ceph Manager or
      by an external tool via the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph device set-life-expectancy <em class="replaceable">DEVICE-ID</em> <em class="replaceable">FROM</em> <em class="replaceable">TO</em></pre></div><p>
      You can change the stored life expectancy manually, but that usually does
      not persist—the tool that originally set it reset and changing the
      stored value does not affect the actual health of the hardware device.
     </p></dd><dt id="id-1.3.5.5.9.6.3"><span class="term"><code class="option">DEVICE_HEALTH_IN_USE</code></span></dt><dd><p>
      One or more devices are expected to fail and has been marked
      <code class="option">out</code> of the cluster based on
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, but the devices
      are still participating in one more PGs. This may be because it was only
      recently marked as <code class="option">out</code> and the data is still migrating,
      or because the data cannot be migrated off for some reason (for example,
      the cluster is nearly full, or the CRUSH hierarchy is such that there is
      not another suitable OSD to migrate the data to). This message can be
      silenced by disabling the self heal behavior (setting
      <code class="literal">mgr/devicehealth/self_heal</code> to false), by adjusting the
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, or by addressing
      what is preventing data from being migrated off of the ailing device.
     </p></dd><dt id="id-1.3.5.5.9.6.4"><span class="term"><code class="option">DEVICE_HEALTH_TOOMANY</code></span></dt><dd><p>
      Too many devices are expected to fail and the
      <code class="literal">mgr/devicehealth/self_heal</code> behavior is enabled, such
      that marking <code class="option">out</code> all of the ailing devices would exceed
      the clusters <code class="option">mon_osd_min_in_ratio</code> ratio that prevents
      too many OSDs from being automatically marked <code class="option">out</code>. This
      can indicates that too many devices in the cluster are expected to fail
      and action is required to add newer (healthier) devices before too many
      devices fail and data is lost. The health message can also be silenced by
      adjusting parameters like <code class="option">mon_osd_min_in_ratio</code> or
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, but be warned
      that this increases the likelihood of unrecoverable data loss in the
      cluster.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">DATA HEALTH (POOLS AND PLACEMENT GROUPS) </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.7">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.7.2"><span class="term"><code class="option">PG_AVAILABILITY</code></span></dt><dd><p>
      Data availability is reduced and the cluster is unable to service
      potential read or write requests for some data in the cluster.
      Specifically, if one or more PGs are in a state that does not allow IO
      requests to be serviced. Problematic PG states include peering, stale,
      incomplete, and in-active (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs are currently down;
      see the discussion for <code class="option">OSD_DOWN</code> above. The state of
      specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">PG_ID</em> query</pre></div></dd><dt id="id-1.3.5.5.9.7.3"><span class="term"><code class="option">PG_DEGRADED</code></span></dt><dd><p>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, if one
      or more PGs:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        have a degraded or undersized flag set, meaning there are not enough
        instances of that placement group in the cluster;
       </p></li><li class="listitem"><p>
        have not had the clean flag set for some time.
       </p></li></ul></div></dd><dt id="id-1.3.5.5.9.7.4"><span class="term"><code class="option">PG_RECOVERY_FULL</code></span></dt><dd><p>
      Data redundancy can be reduced or at risk for some data due to a lack of
      free space in the cluster. Specifically, one or more PGs have the
      <code class="option">recovery_toofull</code> flag set, meaning that the cluster is
      unable to migrate or recover data because one or more OSDs are above the
      full threshold. See the discussion for <code class="option">OSD_FULL</code> above
      for steps to resolve this condition.
     </p></dd><dt id="id-1.3.5.5.9.7.5"><span class="term"><code class="option">PG_BACKFILL_FULL</code></span></dt><dd><p>
      Data redundancy can be reduced or at risk for some data due to a lack of
      free space in the cluster. Specifically, one or more PGs have the
      <code class="option">backfill_toofull</code> flag set, meaning that the cluster is
      unable to migrate or recover data because one or more OSDs are above the
      backfillfull threshold. See the discussion for
      <code class="option">OSD_BACKFILLFULL</code> above for steps to resolve this
      condition.
     </p></dd><dt id="id-1.3.5.5.9.7.6"><span class="term"><code class="option">PG_DAMAGED</code></span></dt><dd><p>
      Data scrubbing has discovered some problems with data consistency in the
      cluster. Specifically, one or more PGs have the inconsistent or
      <code class="option">snaptrim_error</code> flag is set, indicating an earlier scrub
      operation found a problem, or that the repair flag is set and a repair
      for such an inconsistency is currently in progress.
     </p></dd><dt id="id-1.3.5.5.9.7.7"><span class="term"><code class="option">OSD_SCRUB_ERRORS</code></span></dt><dd><p>
      Recent OSD scrubs have uncovered inconsistencies. This error is generally
      paired with <code class="option">PG_DAMAGED</code>.
     </p></dd><dt id="id-1.3.5.5.9.7.8"><span class="term"><code class="option">LARGE_OMAP_OBJECTS</code></span></dt><dd><p>
      One or more pools contain large omap objects as determined by
      <code class="option">osd_deep_scrub_large_omap_object_key_threshold</code>
      (threshold for number of keys to determine a large omap object) or
      <code class="option">osd_deep_scrub_large_omap_object_value_sum_threshold</code>
      (the threshold for summed size (bytes) of all key values to determine a
      large omap object) or both. More information on the object name, key
      count, and size in bytes can be found by searching the cluster log for
      ‘Large omap object found’. Large omap objects can be caused by RGW bucket
      index objects that do not have automatic resharding enabled. The
      thresholds can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd osd_deep_scrub_large_omap_object_key_threshold <em class="replaceable">KEYS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph config set osd osd_deep_scrub_large_omap_object_value_sum_threshold <em class="replaceable">BYTES</em></pre></div></dd><dt id="id-1.3.5.5.9.7.9"><span class="term"><code class="option">CACHE_POOL_NEAR_FULL</code></span></dt><dd><p>
      A cache tier pool is nearly full. Full is determined by the
      <code class="option">target_max_bytes</code> and <code class="option">target_max_objects</code>
      properties on the cache pool. Once the pool reaches the target threshold,
      write requests to the pool may block while data is flushed and evicted
      from the cache, a state that normally leads to very high latencies and
      poor performance. The cache pool target size can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">CACHE-POOL-NAME</em> target_max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">CACHE-POOL-NAME</em> target_max_objects <em class="replaceable">OBJECTS</em></pre></div><p>
      Normal cache flush and eviction activity can also be throttled due to
      reduced availability, performance of the base tier, or overall cluster
      load.
     </p></dd><dt id="id-1.3.5.5.9.7.10"><span class="term"><code class="option">POOL_TOO_FEW_PGS</code></span></dt><dd><p>
      One or more pools should probably have more PGs, based on the amount of
      data that is currently stored in the pool. This can lead to sub-optimal
      distribution and balance of data across the OSDs in the cluster, and
      similarly reduce overall performance. This warning is generated if the
      <code class="option">pg_autoscale_mode</code> property on the pool is set to warn.
      To disable the warning, you can disable auto-scaling of PGs for the pool
      entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode off</pre></div><p>
      To allow the cluster to automatically adjust the number of PGs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode on</pre></div><p>
      You can also manually set the number of PGs for the pool to the
      recommended amount with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div></dd><dt id="id-1.3.5.5.9.7.11"><span class="term"><code class="option">TOO_MANY_PGS</code></span></dt><dd><p>
      The number of PGs in use in the cluster is above the configurable
      threshold of <code class="option">mon_max_pg_per_osd</code> PGs per OSD. If this
      threshold is exceeded, the cluster does not allow new pools to be
      created, pool <code class="option">pg_num</code> to be increased, or pool
      replication to be increased (any of which would lead to more PGs in the
      cluster). A large number of PGs can lead to higher memory utilization for
      OSD daemons, slower peering after cluster state changes (like OSD
      restarts, additions, or removals), and higher load on the Ceph Manager and Ceph Monitor
      daemons. The simplest way to mitigate the problem is to increase the
      number of OSDs in the cluster by adding more hardware. The OSD count used
      for the purposes of this health check is the number of
      <code class="option">in</code> OSDs, marking <code class="option">out</code> OSDs
      <code class="option">in</code> (if there are any) can also help:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd in <em class="replaceable">OSD_IDs</em></pre></div></dd><dt id="id-1.3.5.5.9.7.12"><span class="term"><code class="option">POOL_TOO_MANY_PGS</code></span></dt><dd><p>
      One or more pools require more PGs based on the amount of data that is
      currently stored in the pool. This can lead to higher memory utilization
      for OSD daemons, slower peering after cluster state changes (like OSD
      restarts, additions, or removals), and higher load on the manager and
      monitor daemons. This warning is generated if the
      <code class="option">pg_autoscale_mode</code> property on the pool is set to warn.
      To disable the warning, you can disable auto-scaling of PGs for the pool
      entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode off</pre></div><p>
      To allow the cluster to automatically adjust the number of PGs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode on</pre></div><p>
      You can also manually set the number of PGs for the pool to the
      recommended amount with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div></dd><dt id="id-1.3.5.5.9.7.13"><span class="term"><code class="option">POOL_TARGET_SIZE_RATIO_OVERCOMMITTED</code></span></dt><dd><p>
      One or more pools have a <code class="option">target_size_ratio</code> property set
      to estimate the expected size of the pool as a fraction of total storage,
      but the value(s) exceed the total available storage (either by themselves
      or in combination with other pools’ actual usage). This can indicate that
      the <code class="option">target_size_ratio</code> value for the pool is too large
      and should be reduced or set to zero with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> target_size_ratio 0</pre></div></dd><dt id="id-1.3.5.5.9.7.14"><span class="term"><code class="option">POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</code></span></dt><dd><p>
      One or more pools have a <code class="option">target_size_bytes</code> property set
      to estimate the expected size of the pool, but the value(s) exceed the
      total available storage (either by themselves or in combination with
      other pools’ actual usage). This indicates that the
      <code class="option">target_size_bytes</code> value for the pool is too large and
      should be reduced or set to zero with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> target_size_bytes 0</pre></div></dd><dt id="id-1.3.5.5.9.7.15"><span class="term"><code class="option">TOO_FEW_OSDS</code></span></dt><dd><p>
      The number of OSDs in the cluster is below the configurable threshold of
      <code class="option">osd_pool_default_size</code>.
     </p></dd><dt id="id-1.3.5.5.9.7.16"><span class="term"><code class="option">SMALLER_PGP_NUM</code></span></dt><dd><p>
      One or more pools have a <code class="option">pgp_num</code> value less than
      <code class="option">pg_num</code>, indicating that the PG count was increased
      without also increasing the placement behavior. To adjust the placement
      group number, adjust <code class="option">pgp_num</code> and
      <code class="option">pg_num</code>. Ensure that changing <code class="option">pgp_num</code> is
      performed first and does not trigger the rebalance. To resolve, set
      <code class="option">pgp_num</code> to match <code class="option">pg_num</code> and trigger the
      data migration with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL</em> pgp_num <em class="replaceable">PG-NUM-VALUE</em></pre></div></dd><dt id="id-1.3.5.5.9.7.17"><span class="term"><code class="option">MANY_OBJECTS_PER_PG</code></span></dt><dd><p>
      One or more pools has an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration value. This
      indicates that the pool(s) containing most of the data in the cluster
      have too few PGs, or that other pools that do not contain as much data
      have too many PGs. The threshold can be raised to silence the health
      warning by adjusting the <code class="option">mon_pg_warn_max_object_skew</code>
      configuration option on the monitors.
     </p></dd><dt id="id-1.3.5.5.9.7.18"><span class="term"><code class="option">POOL_APP_NOT_ENABLED</code></span></dt><dd><p>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd pool init <em class="replaceable">POOLNAME</em></pre></div><p>
      If the pool is being used by a custom application
      <em class="replaceable">FOO</em>, you can also label via the low-level
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable <em class="replaceable">FOO</em></pre></div></dd><dt id="id-1.3.5.5.9.7.19"><span class="term"><code class="option">POOL_FULL</code></span></dt><dd><p>
      One or more pools has reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <code class="option">mon_pool_quota_crit_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_objects <em class="replaceable">OBJECTS</em></pre></div><p>
      Setting the quota value to 0 disables the quota.
     </p></dd><dt id="id-1.3.5.5.9.7.20"><span class="term"><code class="option">POOL_NEAR_FULL</code></span></dt><dd><p>
      One or more pools are approaching its quota. The threshold to trigger
      this warning condition is controlled by the
      <code class="option">mon_pool_quota_warn_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_objects <em class="replaceable">OBJECTS</em></pre></div></dd><dt id="id-1.3.5.5.9.7.21"><span class="term"><code class="option">OBJECT_MISPLACED</code></span></dt><dd><p>
      One or more objects in the cluster is not stored on the node the cluster
      would like it to be stored on. This is an indication that data migration
      due to some recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in and of itself. Data consistency is not at
      risk and old copies of objects are not removed until the desired number
      of new copies (in the desired locations) are present.
     </p></dd><dt id="id-1.3.5.5.9.7.22"><span class="term"><code class="option">OBJECT_UNFOUND</code></span></dt><dd><p>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to unfound objects will block.
      Ideally, a down OSD can be brought back online that has the more recent
      copy of the unfound object. Candidate OSDs can be identified from the
      peering state for the PG(s) responsible for the unfound object:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">PG_ID</em> query</pre></div><p>
      If the latest copy of the object is not available, the cluster can be
      told to roll back to a previous version of the object.
     </p></dd><dt id="id-1.3.5.5.9.7.23"><span class="term"><code class="option">SLOW_OPS</code></span></dt><dd><p>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. The
      request queue on the OSD(s) in question can be queried with the following
      command, executed from the OSD host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> ops</pre></div><p>
      A summary of the slowest recent requests can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_historic_ops</pre></div><p>
      The location of an OSD can be found with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd find osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.5.5.9.7.24"><span class="term"><code class="option">PG_NOT_SCRUBBED</code></span></dt><dd><p>
      One or more PGs have not been scrubbed recently. PGs are normally
      scrubbed every <code class="option">mon_scrub_interval</code> seconds and this
      warning triggers when
      <code class="option">mon_warn_pg_not_deep_scrubbed_ratio</code> percentage of
      interval has elapsed without a scrub since it was due. PGs do not scrub
      if they are not flagged as clean. This can happen if they are misplaced
      or degraded (see <code class="option">PG_AVAILABILITY</code> and
      <code class="option">PG_DEGRADED</code> above). You can manually initiate a scrub of
      a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg scrub <em class="replaceable">PG_ID</em></pre></div></dd><dt id="id-1.3.5.5.9.7.25"><span class="term"><code class="option">PG_NOT_DEEP_SCRUBBED</code></span></dt><dd><p>
      One or more PGs have not been deep scrubbed recently. PGs are normally
      scrubbed every <code class="option">osd_deep_scrub_interval</code> seconds and this
      warning triggers when
      <code class="option">mon_warn_pg_not_deep_scrubbed_ratio</code> percentage of
      interval has elapsed without a scrub since it was due. PGs do not (deep)
      scrub if they are not flagged as clean. This can happen if they are
      misplaced or degraded (see <code class="option">PG_AVAILABILITY</code> and
      <code class="option">PG_DEGRADED</code> above). You can manually initiate a scrub of
      a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg deep-scrub <em class="replaceable">PG_ID</em></pre></div></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MISCELLANEOUS </span><a title="Permalink" class="permalink" href="monitoring-alerting.html#id-1.3.5.5.9.8">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.8.2"><span class="term"><code class="option">RECENT_CRASH</code></span></dt><dd><p>
      One or more Ceph daemons have crashed recently, and the crash has not
      yet been archived or acknowledged by the administrator. This may indicate
      a software bug, a hardware problem (for example, a failing disk), or some
      other problem.
     </p><div id="id-1.3.5.5.9.8.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       Encountering a crash is not normal, but can be observed on occasion.
       When a crash occurs, <code class="command">ceph crash</code> will alert the
       administrator. If <code class="command">ceph crash</code> is reporting an abnormal
       number of crashes, contact SUSE support for further assistance.
       <code class="command">supportconfig</code> reports from the affected nodes will
       help SUSE address the issue. Also consider patching the cluster at a
       regular interval.
      </p></div><p>
      New crashes can be listed with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash ls-new</pre></div><p>
      Information about a specific crash can be examined with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash info <em class="replaceable">CRASH-ID</em></pre></div><p>
      This warning can be silenced by archiving the crash (perhaps after being
      examined by an administrator) so that it does not generate this warning:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive <em class="replaceable">CRASH-ID</em></pre></div><p>
      Similarly, all new crashes can be archived with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive-all</pre></div><p>
      Archived crashes are still visible via <code class="command">ceph crash ls</code>
      but not <code class="command">ceph crash ls-new</code>. The time period for what
      recent means is controlled by the option
      <code class="literal">mgr/crash/warn_recent_interval</code> (default: two weeks).
      These warnings can be disabled entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr/crash/warn_recent_interval 0</pre></div></dd><dt id="id-1.3.5.5.9.8.3"><span class="term"><code class="option">TELEMETRY_CHANGED</code></span></dt><dd><p>
      Telemetry has been enabled but the contents of the telemetry report have
      changed since that time, so telemetry reports are not sent. The Ceph
      developers periodically revise the telemetry feature to include new and
      useful information, or to remove information found to be useless or
      sensitive. If any new information is included in the report, Ceph
      requires the administrator to re-enable telemetry to ensure they have an
      opportunity to (re)review what information is shared. To review the
      contents of the telemetry report:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show</pre></div><p>
      The telemetry report consists of several optional channels that are
      independently enabled or disabled. To re-enable telemetry (and make this
      warning go away):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
      To disable telemetry (and make this warning go away):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry off</pre></div></dd></dl></div><div class="verbatim-wrap"><pre class="screen"> groups:
  - name: cluster health
   rules:
    - alert: health error
     expr: ceph_health_status == 2
     for: 5m
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Ceph in error for &gt; 5m
    - alert: unhealthy
     expr: ceph_health_status != 0
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: Ceph not healthy for &gt; 5m
  - name: mon
   rules:
    - alert: low monitor quorum count
     expr: ceph_monitor_quorum_count &lt; 3
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Monitor count in quorum is low
  - name: osd
   rules:
    - alert: 10% OSDs down
     expr: sum(ceph_osd_down) / count(ceph_osd_in) &gt;= 0.1
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: More then 10% of OSDS are down
    - alert: OSD down
     expr: sum(ceph_osd_down) &gt; 1
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: One or more OSDS down for more then 15 minutes
    - alert: OSDs near full
     expr: (ceph_osd_utilization unless on(osd) ceph_osd_down) &gt; 80
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: OSD {{ $labels.osd }} is dangerously full, over 80%
    # alert on single OSDs flapping
    - alert: flap osd
     expr: rate(ceph_osd_up[5m])*60 &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
        OSD {{ $label.osd }} was marked down at back up at least once a
        minute for 5 minutes.
    # alert on high deviation from average PG count
    - alert: high pg count deviation
     expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
     for: 5m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
        OSD {{ $labels.osd }} deviates by more then 30% from
        average PG count
    # alert on high commit latency...but how high is too high
  - name: mds
   rules:
   # no mds metrics are exported yet
  - name: mgr
   rules:
   # no mgr metrics are exported yet
  - name: pgs
   rules:
    - alert: pgs inactive
     expr: ceph_total_pgs - ceph_active_pgs &gt; 0
     for: 5m
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: One or more PGs are inactive for more then 5 minutes.
    - alert: pgs unclean
     expr: ceph_total_pgs - ceph_clean_pgs &gt; 0
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: One or more PGs are not clean for more then 15 minutes.
  - name: nodes
   rules:
    - alert: root volume full
     expr: node_filesystem_avail{mountpoint="/"} / node_filesystem_size{mountpoint="/"} &lt; 0.1
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Root volume (OSD and MON store) is dangerously full (&lt; 10% free)
    # alert on nic packet errors and drops rates &gt; 1 packet/s
    - alert: network packets dropped
     expr: irate(node_network_receive_drop{device!="lo"}[5m]) + irate(node_network_transmit_drop{device!="lo"}[5m]) &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Node {{ $labels.instance }} experiences packet drop &gt; 1
       packet/s on interface {{ $lables.device }}
    - alert: network packet errors
     expr: irate(node_network_receive_errs{device!="lo"}[5m]) + irate(node_network_transmit_errs{device!="lo"}[5m]) &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Node {{ $labels.instance }} experiences packet errors &gt; 1
       packet/s on interface {{ $lables.device }}
    # predict fs fillup times
    - alert: storage filling
     expr: ((node_filesystem_free - node_filesystem_size) / deriv(node_filesystem_free[2d]) &lt;= 5) &gt; 0
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Mountpoint {{ $lables.mountpoint }} will be full in less then 5 days
       assuming the average fillup rate of the past 48 hours.
  - name: pools
   rules:
    - alert: pool full
     expr: ceph_pool_used_bytes / ceph_pool_available_bytes &gt; 0.9
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Pool {{ $labels.pool }} at 90% capacity or over
    - alert: pool filling up
     expr: (-ceph_pool_used_bytes / deriv(ceph_pool_available_bytes[2d]) &lt;= 5 ) &gt; 0
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Pool {{ $labels.pool }} will be full in less then 5 days
       assuming the average fillup rate of the past 48 hours.</pre></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="ceph-monitor.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 17 </span>Determining Cluster State</span></a> </div><div><a class="pagination-link next" href="cha-storage-cephx.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 19 </span>Authentication with <code class="systemitem">cephx</code></span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="monitoring-alerting.html#pillar-variables"><span class="title-number">18.1 </span><span class="title-name">Pillar Variables</span></a></span></li><li><span class="sect1"><a href="monitoring-alerting.html#grafana"><span class="title-number">18.2 </span><span class="title-name">Grafana</span></a></span></li><li><span class="sect1"><a href="monitoring-alerting.html#prometheus"><span class="title-number">18.3 </span><span class="title-name">Prometheus</span></a></span></li><li><span class="sect1"><a href="monitoring-alerting.html#alerting-alertmanager"><span class="title-number">18.4 </span><span class="title-name">Alertmanager</span></a></span></li><li><span class="sect1"><a href="monitoring-alerting.html#troubleshooting-alerts"><span class="title-number">18.5 </span><span class="title-name">Troubleshooting Alerts</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_monitoring_alerting.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>