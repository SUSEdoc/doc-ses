<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Salt Cluster Administration | Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Salt Cluster Administration | SES 6"/>
<meta name="description" content="After you deploy a Ceph cluster, you will probably need to perform several modifications to it occasionally. These include adding or removing new nodes, disks,…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="chapter-title" content="Chapter 2. Salt Cluster Administration"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Salt Cluster Administration | SES 6"/>
<meta property="og:description" content="After you deploy a Ceph cluster, you will probably need to perform several modifications to it occasionally. These include adding or removing new nodes, disks,…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Salt Cluster Administration | SES 6"/>
<meta name="twitter:description" content="After you deploy a Ceph cluster, you will probably need to perform several modifications to it occasionally. These include adding or removing new nodes, disks,…"/>
<link rel="prev" href="bk01pt01ch01.html" title="Chapter 1. User Privileges and Command Prompts"/><link rel="next" href="cha-deployment-backup.html" title="Chapter 3. Backing Up Cluster Configuration and Data"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration Guide</a><span> / </span><a class="crumb" href="part-cluster-managment.html">Cluster Management</a><span> / </span><a class="crumb" href="storage-salt-cluster.html">Salt Cluster Administration</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration Guide</div><ol><li><a href="bk01pr01.html" class=" "><span class="title-number"> </span><span class="title-name">About This Guide</span></a></li><li class="active"><a href="part-cluster-managment.html" class="has-children you-are-here"><span class="title-number">I </span><span class="title-name">Cluster Management</span></a><ol><li><a href="bk01pt01ch01.html" class=" "><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></li><li><a href="storage-salt-cluster.html" class=" you-are-here"><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></li></ol></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></li></ol></li><li><a href="part-operate.html" class="has-children "><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a><ol><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">15 </span><span class="title-name">Introduction</span></a></li><li><a href="ceph-operating-services.html" class=" "><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></li><li><a href="ceph-monitor.html" class=" "><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></li><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></li></ol></li><li><a href="part-dataccess.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></li></ol></li><li><a href="part-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></li></ol></li><li><a href="part-troubleshooting.html" class="has-children "><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a><ol><li><a href="storage-tips.html" class=" "><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></li><li><a href="storage-troubleshooting.html" class=" "><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></li></ol></li><li><a href="app-stage1-custom.html" class=" "><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></li><li><a href="bk01apb.html" class=" "><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li><li><a href="ap-adm-docupdate.html" class=" "><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="storage-salt-cluster" data-id-title="Salt Cluster Administration"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#">#</a></h2></div></div></div><p>
  After you deploy a Ceph cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </p><section class="sect1" id="salt-adding-nodes" data-id-title="Adding New Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Adding New Cluster Nodes</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-adding-nodes">#</a></h2></div></div></div><p>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”</span>:
  </p><div id="id-1.3.3.3.4.3" data-id-title="Prevent Rebalancing" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Prevent Rebalancing</h6><p>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, add all OSDs you intend to add at the same time.
   </p><p>
    An additional way is to set the <code class="option">osd crush initial weight =
    0</code> option in the <code class="filename">ceph.conf</code> file before adding
    the OSDs:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Add <code class="option">osd crush initial weight = 0</code> to
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>.
     </p></li><li class="step"><p>
      Create the new configuration on the Salt master node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">SALT_MASTER_NODE</em>' state.apply ceph.configuration.create</pre></div><p>
      Or:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call state.apply ceph.configuration.create</pre></div></li><li class="step"><p>
      Apply the new configuration to the targeted OSD minions:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">OSD_MINIONS</em>' state.apply ceph.configuration</pre></div><div id="id-1.3.3.3.4.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If this is <span class="emphasis"><em>not</em></span> a new node, but you want to proceed
       as if it were, ensure you remove the
       <code class="filename">/etc/ceph/destroyedOSDs.yml</code> file from the node.
       Otherwise, any devices from the first attempt will be restored with
       their previous OSD ID and reweight.
      </p><p>
       Run the following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt 'node*' state.apply ceph.osd</pre></div></div></li><li class="step"><p>
      After the new OSDs are added, adjust their weights as required with the
      <code class="command">ceph osd crush reweight</code> command in small increments.
      This allows the cluster to rebalance and become healthy between
      increasing increments so it does not overwhelm the cluster and clients
      accessing the cluster.
     </p></li></ol></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install SUSE Linux Enterprise Server 15 SP1 on the new node and configure its network setting so that
     it resolves the Salt master host name correctly. Verify that it has a proper
     connection to both public and cluster networks, and that time
     synchronization is correctly configured. Then install the
     <code class="systemitem">salt-minion</code> package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div><p>
     If the Salt master's host name is different from <code class="literal">salt</code>,
     edit <code class="filename">/etc/salt/minion</code> and add the following:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">DNS_name_of_your_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the <code class="systemitem">salt.minion</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     On the Salt master, accept the salt key of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept <em class="replaceable">NEW_NODE_KEY</em></pre></div></li><li class="step"><p>
     Verify that <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code>
     targets the new Salt minion and/or set the proper DeepSea grain. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2.2.1 “Matching the Minion Name”</span> or
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”, Running Deployment Stages</span> for more details.
    </p></li><li class="step"><p>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information DeepSea expects.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><div id="id-1.3.3.3.4.4.4.3" data-id-title="Possible Restart of DeepSea stage 0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Possible Restart of DeepSea stage 0</h6><p>
      If the Salt master rebooted after its kernel update, you need to restart
      DeepSea stage 0.
     </p></div></li><li class="step"><p>
     Run the discovery stage. It will write new file entries in the
     <code class="filename">/srv/pillar/ceph/proposals</code> directory, where you can
     edit relevant .yml files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     Optionally, change
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="step"><p>
     Run the configuration stage. It reads everything under
     <code class="filename">/srv/pillar/ceph</code> and updates the pillar accordingly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     Pillar stores data which you can access with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div><div id="id-1.3.3.3.4.4.7.5" data-id-title="Modifying OSDs Layout" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Modifying OSD's Layout</h6><p>
      If you want to modify the default OSD's layout and change the drive
      groups configuration, follow the procedure described in
      <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span>.
     </p></div></li><li class="step"><p>
     The configuration and deployment stages include newly added nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div></section><section class="sect1" id="salt-adding-services" data-id-title="Adding New Roles to Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Adding New Roles to Nodes</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-adding-services">#</a></h2></div></div></div><p>
   You can deploy all types of supported roles with DeepSea. See
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1.2 “Role Assignment”</span> for more information on supported
   role types and examples of matching them.
  </p><p>
   To add a new service to an existing node, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Adapt <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> to match
     the existing host with a new role. For more details, refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>. For example, if you need to run an
     Object Gateway on a MON node, the line is similar to:
    </p><div class="verbatim-wrap"><pre class="screen">role-rgw/xx/x/example.mon-1.sls</pre></div></li><li class="step"><p>
     Run stage 2 to update the pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div></li><li class="step"><p>
     Run stage 3 to deploy core services, or stage 4 to deploy optional
     services. Running both stages does not hurt.
    </p></li></ol></div></div></section><section class="sect1" id="salt-node-removing" data-id-title="Removing and Reinstalling Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-node-removing">#</a></h2></div></div></div><div id="id-1.3.3.3.6.2" data-id-title="Removing a Cluster Node Temporarily" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a Cluster Node Temporarily</h6><p>
    The Salt master expects all minions to be present in the cluster and
    responsive. If a minion breaks and is not responsive anymore, it causes
    problems to the Salt infrastructure, mainly to DeepSea and Ceph Dashboard.
   </p><p>
    Before you fix the minion, delete its key from the Salt master temporarily:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">MINION_HOST_NAME</em></pre></div><p>
    After the minion is fixed, add its key to the Salt master again:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -a <em class="replaceable">MINION_HOST_NAME</em></pre></div></div><p>
   To remove a role from a cluster, edit
   <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and remove the
   corresponding line(s). Then run stages 2 and 5 as described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
  </p><div id="id-1.3.3.3.6.4" data-id-title="Removing OSDs from Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Removing OSDs from Cluster</h6><p>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </p><p>
    Before running stage 5 to do the actual removal, always check which OSDs
    are going to be removed by DeepSea:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run rescinded.ids</pre></div></div><p>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </p><p>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </p><div id="id-1.3.3.3.6.7" data-id-title="Preserving Partitions Created by Other Methods" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Preserving Partitions Created by Other Methods</h6><p>
    Disk drives previously configured by other methods, such as
    <code class="command">ceph-deploy</code>, may still contain partitions. DeepSea
    will not automatically destroy these. The administrator must reclaim these
    drives manually.
   </p></div><div class="complex-example"><div class="example" id="ex-ds-rmnode" data-id-title="Removing a Salt minion from the Cluster"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.1: </span><span class="title-name">Removing a Salt minion from the Cluster </span><a title="Permalink" class="permalink" href="storage-salt-cluster.html#ex-ds-rmnode">#</a></h6></div><div class="example-contents"><p>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <code class="filename">policy.cfg</code> are similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
role-storage/cluster/data*.sls
[...]</pre></div><p>
    Then to remove the Salt minion 'data2.ceph', change the lines to the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
role-storage/cluster/data[1,3-6]*.sls
[...]</pre></div><p>
    Also keep in mind to adapt your drive_groups.yml file to match the new
    targets.
   </p><div class="verbatim-wrap"><pre class="screen">    [...]
    drive_group_name:
      target: 'data[1,3-6]*'
    [...]</pre></div><p>
    Then run stage 2, check which OSDs are going to be removed, and finish by
    running stage 5:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-ds-mignode" data-id-title="Migrating Nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.2: </span><span class="title-name">Migrating Nodes </span><a title="Permalink" class="permalink" href="storage-salt-cluster.html#ex-ds-mignode">#</a></h6></div><div class="example-contents"><p>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    Object Gateway while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </p><p>
    After running stages 0 and 1 (see <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”, Running Deployment Stages</span>) for the
    new hardware, you named the new gateway <code class="literal">rgw1</code>. If the
    node <code class="literal">data8</code> needs the Object Gateway role removed and the storage
    role added, and the current <code class="filename">policy.cfg</code> looks like
    this:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
role-storage/cluster/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</pre></div><p>
    Then change it to:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
role-storage/cluster/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</pre></div><p>
    Run stages 2 to 4, check which OSDs are going to be possibly removed, and
    finish by running stage 5. Stage 3 will add <code class="literal">data8</code> as a
    storage node. For a moment, <code class="literal">data8</code> will have both roles.
    Stage 4 will add the Object Gateway role to <code class="literal">rgw1</code> and stage 5 will
    remove the Object Gateway role from <code class="literal">data8</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-failed-node" data-id-title="Removal of a Failed Node"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.3: </span><span class="title-name">Removal of a Failed Node </span><a title="Permalink" class="permalink" href="storage-salt-cluster.html#ex-failed-node">#</a></h6></div><div class="example-contents"><p>
    If the Salt minion is not responding and the administrator is unable to
    resolve the issue, we recommend removing the Salt key:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">MINION_ID</em></pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-failed-storage-node" data-id-title="Removal of a Failed Storage Node"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.4: </span><span class="title-name">Removal of a Failed Storage Node </span><a title="Permalink" class="permalink" href="storage-salt-cluster.html#ex-failed-storage-node">#</a></h6></div><div class="example-contents"><p>
    When a server fails (due to network, power, or other issues), it means that
    all the OSDs are dead. Issue the following commands for
    <span class="emphasis"><em>each</em></span> OSD on the failed storage node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd purge-actual $id --yes-i-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph auth del osd.$id</pre></div><p>
    Running the <code class="command">ceph osd purge-actual</code> command is equivalent
    to the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph destroy $id
<code class="prompt user">cephadm@adm &gt; </code>ceph osd rm $id
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush remove osd.$id</pre></div></div></div></div></section><section class="sect1" id="ds-mon" data-id-title="Redeploying Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Redeploying Monitor Nodes</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#ds-mon">#</a></h2></div></div></div><p>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </p><div id="id-1.3.3.3.7.3" data-id-title="The Minimum Is Three Monitor Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: The Minimum Is Three Monitor Nodes</h6><p>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only two monitor nodes, you need to
    temporarily assign the monitor role to other cluster nodes before you
    redeploy the failed monitor nodes. After you redeploy the failed monitor
    nodes, you can uninstall the temporary monitor roles.
   </p><p>
    For more information on adding new nodes/roles to the Ceph cluster, see
    <a class="xref" href="storage-salt-cluster.html#salt-adding-nodes" title="2.1. Adding New Cluster Nodes">Section 2.1, “Adding New Cluster Nodes”</a> and
    <a class="xref" href="storage-salt-cluster.html#salt-adding-services" title="2.2. Adding New Roles to Nodes">Section 2.2, “Adding New Roles to Nodes”</a>.
   </p><p>
    For more information on removing cluster nodes, refer to
    <a class="xref" href="storage-salt-cluster.html#salt-node-removing" title="2.3. Removing and Reinstalling Cluster Nodes">Section 2.3, “Removing and Reinstalling Cluster Nodes”</a>.
   </p></div><p>
   There are two basic degrees of a Ceph node failure:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The Salt minion host is broken either physically or on the OS level, and
     does not respond to the <code class="command">salt
     '<em class="replaceable">minion_name</em>' test.ping</code> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p></li><li class="listitem"><p>
     The monitor related services failed and refuse to recover, but the host
     responds to the <code class="command">salt '<em class="replaceable">minion_name</em>'
     test.ping</code> call. In such case, follow these steps:
    </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> on the
     Salt master, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]</pre></div></li><li class="step"><p>
     Run DeepSea stages 2 to 5 to apply the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.3
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.4
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.5</pre></div></li></ol></div></div></section><section class="sect1" id="salt-verify-encrypt-osd" data-id-title="Verify an Encrypted OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Verify an Encrypted OSD</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-verify-encrypt-osd">#</a></h2></div></div></div><p>
   After using DeepSea to deploy an OSD, you may want to verify that the OSD
   is encrypted.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check the output of <code class="command">ceph-volume lvm list</code> (it should be
     run as root on the node where the OSDs in question are located):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-volume lvm list

  ====== osd.3 =======

    [block]       /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7

        block device              /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7
        block uuid                m5F10p-tUeo-6ZGP-UjxJ-X3cd-Ec5B-dNGXvG
        cephx lockbox secret
        cluster fsid              413d9116-e4f6-4211-a53b-89aa219f1cf2
        cluster name              ceph
        crush device class        None
        encrypted                 0
        osd fsid                  f8596bf7-000f-4186-9378-170b782359dc
        osd id                    3
        type                      block
        vdo                       0
        devices                   /dev/vdb

  ====== osd.7 =======

    [block]       /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987

        block device              /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987
        block uuid                1y3qcS-ZG01-Y7Z1-B3Kv-PLr6-jbm6-8B79g6
        cephx lockbox secret
        cluster fsid              413d9116-e4f6-4211-a53b-89aa219f1cf2
        cluster name              ceph
        crush device class        None
        encrypted                 0
        osd fsid                  0f9a8002-4c81-4f5f-93a6-255252cac2c4
        osd id                    7
        type                      block
        vdo                       0
        devices                   /dev/vdc</pre></div><p>
     Note the line that says <code class="literal">encrypted 0</code>. This means the OSD
     is not encrypted. The possible values are as follows:
    </p><div class="verbatim-wrap"><pre class="screen">  encrypted                 0  = not encrypted
  encrypted                 1  = encrypted</pre></div><p>
     If you get the following error, it means the node where you are running
     the command does not have any OSDs on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-volume lvm list
No valid Ceph lvm devices found</pre></div><p>
     If you have deployed a cluster with an OSD for which <code class="command">ceph-volume
     lvm list</code> shows <code class="literal">encrypted 1</code>, the OSD is
     encrypted. If you are unsure, proceed to step two.
    </p></li><li class="step"><p>
     Ceph OSD encryption-at-rest relies on the Linux kernel's
     <code class="literal">dm-crypt</code> subsystem and the Linux Unified Key Setup
     ("LUKS"). When creating an encrypted OSD, <code class="command">ceph-volume</code>
     creates an encrypted logical volume and saves the corresponding
     <code class="literal">dm-crypt</code> secret key in the Ceph Monitor data store. When the
     OSD is to be started, <code class="command">ceph-volume</code> ensures the device is
     mounted, retrieves the <code class="literal">dm-crypt</code> secret key from the
     Ceph Monitor's, and decrypts the underlying device. This creates a new device,
     containing the unencrypted data, and this is the device the Ceph OSD
     daemon is started on.
    </p><p>
     The OSD does not know whether the underlying logical volume is encrypted
     or not, there is no <code class="command">ceph osd command</code> that returns this
     information. However, it is possible to query LUKS for it, as follows.
    </p><p>
     First, get the device of the OSD logical volume you are interested in.
     This can be obtained from the <code class="command">ceph-volume lvm list</code>
     output:
    </p><div class="verbatim-wrap"><pre class="screen">block device              /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7</pre></div><p>
     Then, dump the LUKS header from that device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cryptsetup luksDump OSD_BLOCK_DEVICE</pre></div><p>
     if the OSD is <span class="emphasis"><em>not</em></span> encrypted, the output is as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">Device /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987 is not a valid LUKS device.</pre></div><p>
     If the OSD <span class="emphasis"><em>is</em></span> encrypted, the output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cryptsetup luksDump /dev/ceph-1ce61157-81be-427d-83ad-7337f05d8514/osd-data-89230c92-3ace-4685-97ff-6fa059cef63a
  LUKS header information for /dev/ceph-1ce61157-81be-427d-83ad-7337f05d8514/osd-data-89230c92-3ace-4685-97ff-6fa059cef63a

  Version:        1
  Cipher name:    aes
  Cipher mode:    xts-plain64
  Hash spec:      sha256
  Payload offset: 4096
  MK bits:        256
  MK digest:      e9 41 85 f1 1b a3 54 e2 48 6a dc c2 50 26 a5 3b 79 b0 f2 2e
  MK salt:        4c 8c 9d 1f 72 1a 88 6c 06 88 04 72 81 7b e4 bb
                  b1 70 e1 c2 7c c5 3b 30 6d f7 c8 9c 7c ca 22 7d
  MK iterations:  118940
  UUID:           7675f03b-58e3-47f2-85fc-3bafcf1e589f

  Key Slot 0: ENABLED
          Iterations:             1906500
          Salt:                   8f 1f 7f f4 eb 30 5a 22 a5 b4 14 07 cc da dc 48
                                  b5 e9 87 ef 3b 9b 24 72 59 ea 1a 0a ec 61 e6 42
          Key material offset:    8
          AF stripes:             4000
  Key Slot 1: DISABLED
  Key Slot 2: DISABLED
  Key Slot 3: DISABLED
  Key Slot 4: DISABLED
  Key Slot 5: DISABLED
  Key Slot 6: DISABLED
  Key Slot 7: DISABLED</pre></div><p>
     Since decrypting the data on an encrypted OSD disk requires knowledge of
     the corresponding <code class="literal">dm-crypt</code> secret key, OSD encryption
     provides protection for cases when a disk drive that was used as an OSD is
     decommissioned, lost, or stolen.
    </p></li></ol></div></div></section><section class="sect1" id="salt-node-add-disk" data-id-title="Adding an OSD Disk to a Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Adding an OSD Disk to a Node</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-node-add-disk">#</a></h2></div></div></div><p>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <span class="intraxref">Step 12</span> in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span> for more details. Adapt
   <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
   accordingly (refer to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span> for details). After
   saving the file, run DeepSea's stage 3:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.3</pre></div></section><section class="sect1" id="salt-removing-osd" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-removing-osd">#</a></h2></div></div></div><p>
   You can remove a Ceph OSD from the cluster by running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_ID</em></pre></div><p>
   <em class="replaceable">OSD_ID</em> needs to be a number of the OSD without
   the <code class="literal">osd.</code> prefix. For example, from
   <code class="literal">osd.3</code> only use the digit <code class="literal">3</code>.
  </p><section class="sect2" id="osd-removal-multiple" data-id-title="Removing Multiple OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.1 </span><span class="title-name">Removing Multiple OSDs</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#osd-removal-multiple">#</a></h3></div></div></div><p>
    Use the same procedure as mentioned in <a class="xref" href="storage-salt-cluster.html#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>
    but simply supply multiple OSD IDs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove 2 6 11 15
Removing osd 2 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.2 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 6 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.6 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 11 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.11 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 15 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.15 is safe to destroy
Purging from the crushmap
Zapping the device


2:
True
6:
True
11:
True
15:
True</pre></div></section><section class="sect2" id="remove-all-osds-per-host" data-id-title="Removing All OSDs on a Host"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.2 </span><span class="title-name">Removing All OSDs on a Host</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#remove-all-osds-per-host">#</a></h3></div></div></div><p>
    To remove all OSDs on a specific host, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_HOST_NAME</em></pre></div></section><section class="sect2" id="osd-forced-removal" data-id-title="Removing Broken OSDs Forcefully"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.3 </span><span class="title-name">Removing Broken OSDs Forcefully</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#osd-forced-removal">#</a></h3></div></div></div><p>
    There are cases when removing an OSD gracefully (see
    <a class="xref" href="storage-salt-cluster.html#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>) fails. This may happen, for example,
    if the OSD or its journal, WAL or DB are broken, when it suffers from
    hanging I/O operations, or when the OSD disk fails to unmount.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_ID</em> force=True</pre></div><div id="id-1.3.3.3.10.7.4" data-id-title="Hanging Mounts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Hanging Mounts</h6><p>
     If a partition is still mounted on the disk being removed, the command
     will exit with the 'Unmount failed - check for processes on
     <em class="replaceable">DEVICE</em>' message. You can then list all
     processes that access the file system with the <code class="command">fuser -m
     <em class="replaceable">DEVICE</em></code>. If <code class="command">fuser</code>
     returns nothing, try manual <code class="command">unmount
     <em class="replaceable">DEVICE</em></code> and watch the output of
     <code class="command">dmesg</code> or <code class="command">journalctl</code> commands.
    </p></div></section><section class="sect2" id="validate-osd-lvm" data-id-title="Validating OSD LVM Metadata"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.4 </span><span class="title-name">Validating OSD LVM Metadata</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#validate-osd-lvm">#</a></h3></div></div></div><p>
    After removing an OSD using the <code class="command">salt-run osd.remove
    <em class="replaceable">ID</em></code> or through other ceph commands, LVM
    metadata may not be completely removed. This means that if you want to
    re-deploy a new OSD, old LVM metadata would be used.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      First, check if the OSD has been removed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm list</pre></div><p>
      Even if one of the OSD's has been removed successfully, it can still be
      listed. For example, if you removed <code class="literal">osd.2</code>, the
      following would be the output:
     </p><div class="verbatim-wrap"><pre class="screen">  ====== osd.2 =======

  [block] /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380

  block device /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380
  block uuid kH9aNy-vnCT-ExmQ-cAsI-H7Gw-LupE-cvSJO9
  cephx lockbox secret
  cluster fsid 6b6bbac4-eb11-45cc-b325-637e3ff9fa0c
  cluster name ceph
  crush device class None
  encrypted 0
  osd fsid aac51485-131c-442b-a243-47c9186067db
  osd id 2
  type block
  vdo 0
  devices /dev/sda</pre></div><p>
      In this example, you can see that <code class="literal">osd.2</code> is still
      located in <code class="filename">/dev/sda</code>.
     </p></li><li class="step"><p>
      Validate the LVM metadata on the OSD node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume inventory</pre></div><p>
      The output from running <code class="command">ceph-volume inventory</code> marks
      the <code class="filename">/dev/sda</code> availablity as
      <code class="literal">False</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">  Device Path Size rotates available Model name
  /dev/sda 40.00 GB True False QEMU HARDDISK
  /dev/sdb 40.00 GB True False QEMU HARDDISK
  /dev/sdc 40.00 GB True False QEMU HARDDISK
  /dev/sdd 40.00 GB True False QEMU HARDDISK
  /dev/sde 40.00 GB True False QEMU HARDDISK
  /dev/sdf 40.00 GB True False QEMU HARDDISK
  /dev/vda 25.00 GB True False</pre></div></li><li class="step"><p>
      Run the following command on the OSD node to remove the LVM metadata
      completely:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm zap --osd-id <em class="replaceable">ID</em> --destroy</pre></div></li><li class="step"><p>
      Run the <code class="command">inventory</code> command again to verify that the
      <code class="filename">/dev/sda</code> availability returns
      <code class="literal">True</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume inventory
Device Path Size rotates available Model name
/dev/sda 40.00 GB True True QEMU HARDDISK
/dev/sdb 40.00 GB True False QEMU HARDDISK
/dev/sdc 40.00 GB True False QEMU HARDDISK
/dev/sdd 40.00 GB True False QEMU HARDDISK
/dev/sde 40.00 GB True False QEMU HARDDISK
/dev/sdf 40.00 GB True False QEMU HARDDISK
/dev/vda 25.00 GB True False</pre></div><p>
      LVM metadata has been removed. You can safely run the
      <code class="command">dd</code> command on the device.
     </p></li><li class="step"><p>
      The OSD can now be re-deployed without needing to reboot the OSD node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div></section></section><section class="sect1" id="ds-osd-replace" data-id-title="Replacing an OSD Disk"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Replacing an OSD Disk</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#ds-osd-replace">#</a></h2></div></div></div><p>
   There are several reasons why you may need to replace an OSD disk, for
   example:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The OSD disk failed or is soon going to fail based on SMART information,
     and can no longer be used to store data safely.
    </p></li><li class="listitem"><p>
     You need to upgrade the OSD disk, for example to increase its size.
    </p></li><li class="listitem"><p>
     You need to change the OSD disk layout.
    </p></li><li class="listitem"><p>
     You plan to move from a non-LVM to a LVM-based layout.
    </p></li></ul></div><p>
   The replacement procedure is the same for both cases. It is also valid for
   both default and customized CRUSH Maps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Suppose that, for example, '5' is the ID of the OSD whose disk needs to be
     replaced. The following command marks it as
     <span class="bold"><strong>destroyed</strong></span> in the CRUSH Map but leaves
     its original ID:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.replace 5</pre></div><div id="id-1.3.3.3.11.5.1.3" data-id-title="osd.replace and osd.remove" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">osd.replace</code> and <code class="command">osd.remove</code></h6><p>
      The Salt's <code class="command">osd.replace</code> and
      <code class="command">osd.remove</code> (see <a class="xref" href="storage-salt-cluster.html#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>)
      commands are identical except that <code class="command">osd.replace</code> leaves
      the OSD as 'destroyed' in the CRUSH Map while
      <code class="command">osd.remove</code> removes all traces from the CRUSH Map.
     </p></div></li><li class="step"><p>
     Manually replace the failed/upgraded OSD drive.
    </p></li><li class="step"><p>
     If you want to modify the default OSD's layout and change the DriveGroups
     configuration, follow the procedure described in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span>.
    </p></li><li class="step"><p>
     Run the deployment stage 3 to deploy the replaced OSD disk:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><div id="id-1.3.3.3.11.6" data-id-title="Shared device failure" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Shared device failure</h6><p>
    If a shared device for DB/WAL fails, you need to perform the replacement
    procedure for all OSDs that share the failed device.
   </p></div></section><section class="sect1" id="ds-osd-recover" data-id-title="Recovering a Reinstalled OSD Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">Recovering a Reinstalled OSD Node</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#ds-osd-recover">#</a></h2></div></div></div><p>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Reinstall the base SUSE Linux Enterprise operating system on the node where the OS broke.
     Install the <span class="package">salt-minion</span> packages on the OSD node,
     delete the old Salt minion key on the Salt master, and register the new
     Salt minion's key with the Salt master. For more information on the initial
     deployment, see <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p></li><li class="step"><p>
     Instead of running the whole of stage 0, run the following parts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.sync
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.packages.common
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.mines
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.updates</pre></div></li><li class="step"><p>
     Copy the <code class="filename">ceph.conf</code> to the OSD node, and then activate
     the OSD:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.configuration
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' cmd.run "ceph-volume lvm activate --all"</pre></div></li><li class="step"><p>
     Verify activation with one of the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph -s
# OR
<code class="prompt user">root@master # </code>ceph osd tree</pre></div></li><li class="step"><p>
     To ensure consistency across the cluster, run the DeepSea stages in the
     following order:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
     Run DeepSea stage 0:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </p></li><li class="step"><p>
     Get Prometheus' node exporter installed/running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">RECOVERED_MINION</em>' \
 state.apply ceph.monitoring.prometheus.exporters.node_exporter</pre></div></li><li class="step"><p>
     Remove unnecessary Salt grains (best after all OSDs have been migrated
     to LVM):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I roles:storage grains.delkey ceph</pre></div></li></ol></div></div></section><section class="sect1" id="moving-saltmaster" data-id-title="Moving the Admin Node to a New Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">Moving the Admin Node to a New Server</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#moving-saltmaster">#</a></h2></div></div></div><p>
   If you need to replace the Admin Node host with a new one, you need to move the
   Salt master and DeepSea files. Use your favorite synchronization tool for
   transferring the files. In this procedure, we use <code class="command">rsync</code>
   because it is a standard tool available in SUSE Linux Enterprise Server 15 SP1 software repositories.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop <code class="systemitem">salt-master</code> and
     <code class="systemitem">salt-minion</code> services on the old
     Admin Node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-master.service
<code class="prompt user">root@master # </code>systemctl stop salt-minion.service</pre></div></li><li class="step"><p>
     Configure Salt on the new Admin Node so that the Salt master and Salt minions
     communicate. Find more details in <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p><div id="id-1.3.3.3.13.3.2.2" data-id-title="Transition of Salt Minions" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Transition of Salt Minions</h6><p>
      To ease the transition of Salt minions to the new Admin Node, remove the
      original Salt master's public key from each of them:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>rm /etc/salt/pki/minion/minion_master.pub
<code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></div></li><li class="step"><p>
     Verify that the <span class="package">deepsea</span> package is installed and
     install it if required.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install deepsea</pre></div></li><li class="step"><p>
     Customize the <code class="filename">policy.cfg</code> file by changing the
     <code class="literal">role-master</code> line. Find more details in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="step"><p>
     Synchronize <code class="filename">/srv/pillar</code> and
     <code class="filename">/srv/salt</code> directories from the old Admin Node to the new
     one.
    </p><div id="id-1.3.3.3.13.3.5.2" data-id-title="rsync Dry Run and Symbolic Links" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">rsync</code> Dry Run and Symbolic Links</h6><p>
      If possible, try synchronizing the files in a dry run first to see which
      files will be transferred (<code class="command">rsync</code>'s option
      <code class="option">-n</code>). Also, include symbolic links
      (<code class="command">rsync</code>'s option <code class="option">-a</code>). For
      <code class="command">rsync</code>, the synchronize command will look as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>rsync -avn /srv/pillar/ <em class="replaceable">NEW-ADMIN-HOSTNAME:</em>/srv/pillar</pre></div></div></li><li class="step"><p>
     If you made custom changes to files outside of
     <code class="filename">/srv/pillar</code> and <code class="filename">/srv/salt</code>, for
     example in <code class="filename">/etc/salt/master</code> or
     <code class="filename">/etc/salt/master.d</code>, synchronize them as well.
    </p></li><li class="step"><p>
     Now you can run DeepSea stages from the new Admin Node. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2 “Introduction to DeepSea”</span> for their detailed description.
    </p></li></ol></div></div></section><section class="sect1" id="salt-automated-installation" data-id-title="Automated Installation via Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Automated Installation via Salt</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#salt-automated-installation">#</a></h2></div></div></div><p>
   The installation can be automated by using the Salt reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a Ceph cluster with the specified behavior.
  </p><div id="id-1.3.3.3.14.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Salt cannot perform dependency checks based on reactor events. There is a
    real risk of putting your Salt master into a death spiral.
   </p></div><p>
   The automated installation requires the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A properly created
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>.
    </p></li><li class="listitem"><p>
     Prepared custom <code class="filename">global.yml</code> placed to the
     <code class="filename">/srv/pillar/ceph/stack</code> directory.
    </p></li></ul></div><p>
   The default reactor configuration will only run stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </p><p>
   When the first salt-minion starts, stage 0 will begin. A lock prevents
   multiple instances. When all minions complete stage 0, stage 1 will begin.
  </p><p>
   If the operation is performed properly, edit the file
  </p><div class="verbatim-wrap"><pre class="screen">/etc/salt/master.d/reactor.conf</pre></div><p>
   and replace the following line
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/discovery.sls</pre></div><p>
   with
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/all_stages.sls</pre></div><p>
   Verify that the line is not commented out.
  </p></section><section class="sect1" id="deepsea-rolling-updates" data-id-title="Updating the Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">Updating the Cluster Nodes</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#deepsea-rolling-updates">#</a></h2></div></div></div><p>
   Keep the Ceph cluster nodes up-to-date by applying rolling updates
   regularly.
  </p><section class="sect2" id="rolling-updates-repos" data-id-title="Software Repositories"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.1 </span><span class="title-name">Software Repositories</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#rolling-updates-repos">#</a></h3></div></div></div><p>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Upgrading from Previous Releases”, Section 6.5.1 “Manual Node Upgrade Using the Installer DVD”</span> for a complete list of the
    required repositories.
   </p></section><section class="sect2" id="rolling-upgrades-staging" data-id-title="Repository Staging"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.2 </span><span class="title-name">Repository Staging</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#rolling-upgrades-staging">#</a></h3></div></div></div><p>
    If you use a staging tool—for example, SUSE Manager, Subscription Management Tool, or
    Repository Mirroring Tool—that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for SUSE Linux Enterprise Server and SUSE Enterprise Storage are
    created at the same point in time.
   </p><p>
    We strongly recommend to use a staging tool to apply patches which have
    <code class="literal">frozen</code> or <code class="literal">staged</code> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </p></section><section class="sect2" id="rolling-updates-patch-or-dup" data-id-title="zypper patch or zypper dup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.3 </span><span class="title-name"><code class="command">zypper patch</code> or <code class="command">zypper dup</code></span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#rolling-updates-patch-or-dup">#</a></h3></div></div></div><p>
    By default, cluster nodes are upgraded using the <code class="command">zypper
    dup</code> command. If you prefer to update the system using
    <code class="command">zypper patch</code> instead, edit
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
    following line:
   </p><div class="verbatim-wrap"><pre class="screen">update_method_init: zypper-patch</pre></div></section><section class="sect2" id="rolling-updates-reboots" data-id-title="Cluster Node Reboots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.4 </span><span class="title-name">Cluster Node Reboots</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#rolling-updates-reboots">#</a></h3></div></div></div><p>
    During the update, cluster nodes may be optionally rebooted if their kernel
    was upgraded by the update. If you want to eliminate the possibility of a
    forced reboot of potentially all nodes, either verify that the latest
    kernel is installed and running on Ceph nodes, or disable automatic node
    reboots as described in <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.1.5 “Updates and Reboots during Stage 0”</span>.
   </p></section><section class="sect2" id="id-1.3.3.3.15.7" data-id-title="Downtime of Ceph Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.5 </span><span class="title-name">Downtime of Ceph Services</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#id-1.3.3.3.15.7">#</a></h3></div></div></div><p>
    Depending on the configuration, cluster nodes may be rebooted during the
    update as described in <a class="xref" href="storage-salt-cluster.html#rolling-updates-reboots" title="2.12.4. Cluster Node Reboots">Section 2.12.4, “Cluster Node Reboots”</a>. If there
    is a single point of failure for services such as Object Gateway, Samba Gateway, NFS Ganesha,
    or iSCSI, the client machines may be temporarily disconnected from
    services whose nodes are being rebooted.
   </p></section><section class="sect2" id="rolling-updates-running" data-id-title="Running the Update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.6 </span><span class="title-name">Running the Update</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#rolling-updates-running">#</a></h3></div></div></div><p>
    To update the software packages on all cluster nodes to the latest version,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the <span class="package">deepsea</span>, <span class="package">salt-master</span>,
      and <span class="package">salt-minion</span> packages and restart relevant services
      on the Salt master:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'roles:master' state.apply ceph.updates.master</pre></div></li><li class="step"><p>
      Update and restart the <span class="package">salt-minion</span> package on all
      cluster nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'cluster:ceph' state.apply ceph.updates.salt</pre></div></li><li class="step"><p>
      Update all other software packages on the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
      Restart Ceph related services:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-salt-cluster-reboot" data-id-title="Halting or Rebooting Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.13 </span><span class="title-name">Halting or Rebooting Cluster</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#sec-salt-cluster-reboot">#</a></h2></div></div></div><p>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Tell the Ceph cluster not to mark OSDs as out:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd set noout</pre></div></li><li class="step"><p>
     Stop daemons and nodes in the following order:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Storage clients
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph Monitor
      </p></li></ol></div></li><li class="step"><p>
     If required, perform maintenance tasks.
    </p></li><li class="step"><p>
     Start the nodes and servers in the reverse order of the shutdown process:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Ceph Monitor
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Storage clients
      </p></li></ol></div></li><li class="step"><p>
     Remove the noout flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd unset noout</pre></div></li></ol></div></div></section><section class="sect1" id="ds-custom-cephconf" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.14 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#ds-custom-cephconf">#</a></h2></div></div></div><p>
   If you need to put custom settings into the <code class="filename">ceph.conf</code>
   file, you can do so by modifying the configuration files in the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d</code>
   directory:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     global.conf
    </p></li><li class="listitem"><p>
     mon.conf
    </p></li><li class="listitem"><p>
     mgr.conf
    </p></li><li class="listitem"><p>
     mds.conf
    </p></li><li class="listitem"><p>
     osd.conf
    </p></li><li class="listitem"><p>
     client.conf
    </p></li><li class="listitem"><p>
     rgw.conf
    </p></li></ul></div><div id="id-1.3.3.3.17.4" data-id-title="Unique rgw.conf" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unique <code class="filename">rgw.conf</code></h6><p>
    The Object Gateway offers a lot of flexibility and is unique compared to the other
    <code class="filename">ceph.conf</code> sections. All other Ceph components have
    static headers such as <code class="literal">[mon]</code> or
    <code class="literal">[osd]</code>. The Object Gateway has unique headers such as
    <code class="literal">[client.rgw.rgw1]</code>. This means that the
    <code class="filename">rgw.conf</code> file needs a header entry. For examples, see
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw.conf</code></pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw-ssl.conf</code></pre></div><p>
    See <a class="xref" href="cha-ceph-gw.html#ceph-rgw-https" title="26.7. Enabling HTTPS/SSL for Object Gateways">Section 26.7, “Enabling HTTPS/SSL for Object Gateways”</a> for more examples.
   </p></div><div id="id-1.3.3.3.17.5" data-id-title="Run stage 3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Run stage 3</h6><p>
    After you make custom changes to the above mentioned configuration files,
    run stages 3 and 4 to apply these changes to the cluster nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></div><p>
   These files are included from the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
   template file, and correspond to the different sections that the Ceph
   configuration file accepts. Putting a configuration snippet in the correct
   file enables DeepSea to place it into the correct section. You do not need
   to add any of the section headers.
  </p><div id="id-1.3.3.3.17.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <code class="literal">[osd.1]</code>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </p></div><section class="sect2" id="id-1.3.3.3.17.8" data-id-title="Overriding the Defaults"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.14.1 </span><span class="title-name">Overriding the Defaults</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#id-1.3.3.3.17.8">#</a></h3></div></div></div><p>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>
    file:
   </p><div class="verbatim-wrap"><pre class="screen">auth cluster required = none
auth service required = none
auth client required = none</pre></div><p>
    When redefining the default values, Ceph related tools such as
    <code class="command">rados</code> may issue warnings that specific values from the
    <code class="filename">ceph.conf.j2</code> were redefined in
    <code class="filename">global.conf</code>. These warnings are caused by one
    parameter assigned twice in the resulting <code class="filename">ceph.conf</code>.
   </p><p>
    As a workaround for this specific case, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Change the current directory to
      <code class="filename">/srv/salt/ceph/configuration/create</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/create</pre></div></li><li class="step"><p>
      Copy <code class="filename">default.sls</code> to <code class="filename">custom.sls</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp default.sls custom.sls</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom.sls</code> and change
      <code class="option">ceph.conf.j2</code> to <code class="option">custom-ceph.conf.j2</code>.
     </p></li><li class="step"><p>
      Change current directory to
      <code class="filename">/srv/salt/ceph/configuration/files</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/files</pre></div></li><li class="step"><p>
      Copy <code class="filename">ceph.conf.j2</code> to
      <code class="filename">custom-ceph.conf.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp ceph.conf.j2 custom-ceph.conf.j2</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom-ceph.conf.j2</code> and delete the following
      line:
     </p><div class="verbatim-wrap"><pre class="screen">{% include "ceph/configuration/files/rbd.conf" %}</pre></div><p>
      Edit <code class="filename">global.yml</code> and add the following line:
     </p><div class="verbatim-wrap"><pre class="screen">configuration_create: custom</pre></div></li><li class="step"><p>
      Refresh the pillar:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh</pre></div></li><li class="step"><p>
      Run stage 3:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><p>
    Now you should have only one entry for each value definition. To re-create
    the configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.configuration.create</pre></div><p>
    and then verify the contents of
    <code class="filename">/srv/salt/ceph/configuration/cache/ceph.conf</code>.
   </p></section><section class="sect2" id="id-1.3.3.3.17.9" data-id-title="Including Configuration Files"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.14.2 </span><span class="title-name">Including Configuration Files</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#id-1.3.3.3.17.9">#</a></h3></div></div></div><p>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <code class="filename">osd.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</pre></div><p>
    In the previous example, the <code class="filename">osd1.conf</code>,
    <code class="filename">osd2.conf</code>, <code class="filename">osd3.conf</code>, and
    <code class="filename">osd4.conf</code> files contain the configuration options
    specific to the related OSD.
   </p><div id="id-1.3.3.3.17.9.5" data-id-title="Runtime Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Runtime Configuration</h6><p>
     Changes made to Ceph configuration files take effect after the related
     Ceph daemons restart. See <a class="xref" href="cha-ceph-configuration.html#ceph-config-runtime" title="25.1. Runtime Configuration">Section 25.1, “Runtime Configuration”</a> for more
     information on changing the Ceph runtime configuration.
    </p></div></section></section><section class="sect1" id="admin-apparmor" data-id-title="Enabling AppArmor Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.15 </span><span class="title-name">Enabling AppArmor Profiles</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#admin-apparmor">#</a></h2></div></div></div><p>
   AppArmor is a security solution that confines programs by a specific profile.
   For more details, refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-apparmor.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-apparmor.html</a>.
  </p><p>
   DeepSea provides three states for AppArmor profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular AppArmor state, run:
  </p><div class="verbatim-wrap"><pre class="screen">salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<em class="replaceable">STATE</em></pre></div><p>
   To put the AppArmor profiles in an 'enforce' state:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce</pre></div><p>
   To put the AppArmor profiles in a 'complain' state:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain</pre></div><p>
   To disable the AppArmor profiles:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable</pre></div><div id="id-1.3.3.3.18.11" data-id-title="Enabling the AppArmor Service" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Enabling the AppArmor Service</h6><p>
    Each of these three calls verifies if AppArmor is installed and installs it if
    not, and starts and enables the related <code class="systemitem">systemd</code> service. DeepSea will
    warn you if AppArmor was installed and started/enabled in another way and
    therefore runs without DeepSea profiles.
   </p></div></section><section class="sect1" id="deactivate-tuned-profiles" data-id-title="Deactivating Tuned Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.16 </span><span class="title-name">Deactivating Tuned Profiles</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#deactivate-tuned-profiles">#</a></h2></div></div></div><p>
   By default, DeepSea deploys Ceph clusters with tuned profiles active on
   Ceph Monitor, Ceph Manager, and Ceph OSD nodes. In some cases, you may need to permanently
   deactivate tuned profiles. To do so, put the following lines in
   <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and re-run stage 3:
  </p><div class="verbatim-wrap"><pre class="screen">alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></section><section class="sect1" id="deepsea-ceph-purge" data-id-title="Removing an Entire Ceph Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.17 </span><span class="title-name">Removing an Entire Ceph Cluster</span> <a title="Permalink" class="permalink" href="storage-salt-cluster.html#deepsea-ceph-purge">#</a></h2></div></div></div><p>
   The <code class="command">ceph.purge</code> runner removes the entire Ceph cluster.
   This way you can clean the cluster environment when testing different
   setups. After the <code class="command">ceph.purge</code> completes, the Salt
   cluster is reverted back to the state at the end of DeepSea stage 1. You
   can then either change the <code class="filename">policy.cfg</code> (see
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>), or proceed to DeepSea stage 2
   with the same setup.
  </p><p>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the Ceph
   cluster by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety
<code class="prompt user">root@master # </code>salt-run state.orch ceph.purge</pre></div><div id="id-1.3.3.3.20.5" data-id-title="Disabling Ceph Cluster Removal" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling Ceph Cluster Removal</h6><p>
    If you want to prevent anyone from running the
    <code class="command">ceph.purge</code> runner, create a file named
    <code class="filename">disabled.sls</code> in the
    <code class="filename">/srv/salt/ceph/purge</code> directory and insert the
    following line in the
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">purge_init: disabled</pre></div></div><div id="id-1.3.3.3.20.6" data-id-title="Rescind Custom Roles" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Rescind Custom Roles</h6><p>
    If you previously created custom roles for Ceph Dashboard (refer to
    <a class="xref" href="dashboard-user-mgmt.html#dashboard-adding-roles" title="6.6. Adding Custom Roles">Section 6.6, “Adding Custom Roles”</a> and
    <a class="xref" href="dashboard-user-roles.html#dashboard-permissions" title="14.2. User Roles and Permissions">Section 14.2, “User Roles and Permissions”</a> for detailed information), you need
    to take manual steps to purge them before running the
    <code class="command">ceph.purge</code> runner. For example, if the custom role for
    Object Gateway is named 'us-east-1', then follow these steps:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/rescind
<code class="prompt user">root@master # </code>rsync -a rgw/ us-east-1
<code class="prompt user">root@master # </code>sed -i 's!rgw!us-east-1!' us-east-1/*.sls</pre></div></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="bk01pt01ch01.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 1 </span>User Privileges and Command Prompts</span></a> </div><div><a class="pagination-link next" href="cha-deployment-backup.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 3 </span>Backing Up Cluster Configuration and Data</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="storage-salt-cluster.html#salt-adding-nodes"><span class="title-number">2.1 </span><span class="title-name">Adding New Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-adding-services"><span class="title-number">2.2 </span><span class="title-name">Adding New Roles to Nodes</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-node-removing"><span class="title-number">2.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#ds-mon"><span class="title-number">2.4 </span><span class="title-name">Redeploying Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-verify-encrypt-osd"><span class="title-number">2.5 </span><span class="title-name">Verify an Encrypted OSD</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-node-add-disk"><span class="title-number">2.6 </span><span class="title-name">Adding an OSD Disk to a Node</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-removing-osd"><span class="title-number">2.7 </span><span class="title-name">Removing an OSD</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#ds-osd-replace"><span class="title-number">2.8 </span><span class="title-name">Replacing an OSD Disk</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#ds-osd-recover"><span class="title-number">2.9 </span><span class="title-name">Recovering a Reinstalled OSD Node</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#moving-saltmaster"><span class="title-number">2.10 </span><span class="title-name">Moving the Admin Node to a New Server</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#salt-automated-installation"><span class="title-number">2.11 </span><span class="title-name">Automated Installation via Salt</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#deepsea-rolling-updates"><span class="title-number">2.12 </span><span class="title-name">Updating the Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#sec-salt-cluster-reboot"><span class="title-number">2.13 </span><span class="title-name">Halting or Rebooting Cluster</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#ds-custom-cephconf"><span class="title-number">2.14 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#admin-apparmor"><span class="title-number">2.15 </span><span class="title-name">Enabling AppArmor Profiles</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#deactivate-tuned-profiles"><span class="title-number">2.16 </span><span class="title-name">Deactivating Tuned Profiles</span></a></span></li><li><span class="sect1"><a href="storage-salt-cluster.html#deepsea-ceph-purge"><span class="title-number">2.17 </span><span class="title-name">Removing an Entire Ceph Cluster</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/admin_saltcluster.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>