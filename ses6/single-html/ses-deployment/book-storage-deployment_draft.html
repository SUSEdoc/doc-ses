<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Deployment Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deployment Guide | SES 6"/>
<meta name="description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Deployment Guide | SES 6"/>
<meta property="og:description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deployment Guide | SES 6"/>
<meta name="twitter:description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-deployment">Deployment Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-deployment" data-id-title="Deployment Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h1 class="title">Deployment Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Alexandra</span> <span class="surname">Settle</span>, <span class="firstname">Liam</span> <span class="surname">Proven</span>, and <span class="firstname">Sven</span> <span class="surname">Seeberg</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>06/27/2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#id-1.4.2"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.2.7"><span class="title-name">Available Documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.8"><span class="title-name">Feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.9"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.10"><span class="title-name">About the Making of This Manual</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.11"><span class="title-name">Ceph Contributors</span></a></span></li></ul></li><li><span class="part"><a href="#part-ses"><span class="title-number">I </span><span class="title-name">SUSE Enterprise Storage</span></a></span><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 6 and Ceph</span></a></span><ul><li><span class="sect1"><a href="#storage-intro-features"><span class="title-number">1.1 </span><span class="title-name">Ceph Features</span></a></span></li><li><span class="sect1"><a href="#storage-intro-core"><span class="title-number">1.2 </span><span class="title-name">Core Components</span></a></span></li><li><span class="sect1"><a href="#storage-intro-structure"><span class="title-number">1.3 </span><span class="title-name">Storage Structure</span></a></span></li><li><span class="sect1"><a href="#about-bluestore"><span class="title-number">1.4 </span><span class="title-name">BlueStore</span></a></span></li><li><span class="sect1"><a href="#storage-moreinfo"><span class="title-number">1.5 </span><span class="title-name">Additional Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></span><ul><li><span class="sect1"><a href="#network-overview"><span class="title-number">2.1 </span><span class="title-name">Network Overview</span></a></span></li><li><span class="sect1"><a href="#multi-architecture"><span class="title-number">2.2 </span><span class="title-name">Multiple Architecture Configurations</span></a></span></li><li><span class="sect1"><a href="#ses-hardware-config"><span class="title-number">2.3 </span><span class="title-name">Hardware Configuration</span></a></span></li><li><span class="sect1"><a href="#deployment-osd-recommendation"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mon"><span class="title-number">2.5 </span><span class="title-name">Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-rgw"><span class="title-number">2.6 </span><span class="title-name">Object Gateway Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mds"><span class="title-number">2.7 </span><span class="title-name">Metadata Server Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-admin-node"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span></a></span></li><li><span class="sect1"><a href="#sysreq-iscsi"><span class="title-number">2.9 </span><span class="title-name">iSCSI Nodes</span></a></span></li><li><span class="sect1"><a href="#req-ses-other"><span class="title-number">2.10 </span><span class="title-name">SUSE Enterprise Storage 6 and Other SUSE Products</span></a></span></li><li><span class="sect1"><a href="#sysreq-naming"><span class="title-number">2.11 </span><span class="title-name">Naming Limitations</span></a></span></li><li><span class="sect1"><a href="#ses-bp-diskshare"><span class="title-number">2.12 </span><span class="title-name">OSD and Monitor Sharing One Server</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Admin Node HA Setup</span></a></span><ul><li><span class="sect1"><a href="#admin-ha-architecture"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA Cluster for Admin Node</span></a></span></li><li><span class="sect1"><a href="#admin-ha-cluster"><span class="title-number">3.2 </span><span class="title-name">Building a HA Cluster with Admin Node</span></a></span></li></ul></li><li><span class="chapter"><a href="#id-1.4.3.5"><span class="title-number">4 </span><span class="title-name">User Privileges and Command Prompts</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.3.5.4"><span class="title-number">4.1 </span><span class="title-name">Salt/DeepSea Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.4.3.5.5"><span class="title-number">4.2 </span><span class="title-name">Ceph Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.4.3.5.6"><span class="title-number">4.3 </span><span class="title-name">General Linux Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.4.3.5.7"><span class="title-number">4.4 </span><span class="title-name">Additional Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#ses-deployment"><span class="title-number">II </span><span class="title-name">Cluster Deployment and Upgrade</span></a></span><ul><li><span class="chapter"><a href="#ceph-install-saltstack"><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-install-relnotes"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span></a></span></li><li><span class="sect1"><a href="#deepsea-description"><span class="title-number">5.2 </span><span class="title-name">Introduction to DeepSea</span></a></span></li><li><span class="sect1"><a href="#ceph-install-stack"><span class="title-number">5.3 </span><span class="title-name">Cluster Deployment</span></a></span></li><li><span class="sect1"><a href="#deepsea-cli"><span class="title-number">5.4 </span><span class="title-name">DeepSea CLI</span></a></span></li><li><span class="sect1"><a href="#deepsea-pillar-salt-configuration"><span class="title-number">5.5 </span><span class="title-name">Configuration and Customization</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">6 </span><span class="title-name">Upgrading from Previous Releases</span></a></span><ul><li><span class="sect1"><a href="#upgrade-general-considerations"><span class="title-number">6.1 </span><span class="title-name">General Considerations</span></a></span></li><li><span class="sect1"><a href="#before-upgrade"><span class="title-number">6.2 </span><span class="title-name">Steps to Take before Upgrading the First Node</span></a></span></li><li><span class="sect1"><a href="#upgrade-backup-order-of-nodes"><span class="title-number">6.3 </span><span class="title-name">Order in Which Nodes Must Be Upgraded</span></a></span></li><li><span class="sect1"><a href="#upgrade-offline-ctdb-cluster"><span class="title-number">6.4 </span><span class="title-name">Offline Upgrade of CTDB Clusters</span></a></span></li><li><span class="sect1"><a href="#upgrade-one-node"><span class="title-number">6.5 </span><span class="title-name">Per-Node Upgrade Instructions</span></a></span></li><li><span class="sect1"><a href="#upgrade-adm"><span class="title-number">6.6 </span><span class="title-name">Upgrade the Admin Node</span></a></span></li><li><span class="sect1"><a href="#upgrade-mons"><span class="title-number">6.7 </span><span class="title-name">Upgrade Ceph Monitor/Ceph Manager Nodes</span></a></span></li><li><span class="sect1"><a href="#upgrade-mds"><span class="title-number">6.8 </span><span class="title-name">Upgrade Metadata Servers</span></a></span></li><li><span class="sect1"><a href="#upgrade-main-osd"><span class="title-number">6.9 </span><span class="title-name">Upgrade Ceph OSDs</span></a></span></li><li><span class="sect1"><a href="#upgrade-appnodes-order"><span class="title-number">6.10 </span><span class="title-name">Upgrade Gateway Nodes</span></a></span></li><li><span class="sect1"><a href="#final-steps"><span class="title-number">6.11 </span><span class="title-name">Steps to Take after the Last Node Has Been Upgraded</span></a></span></li><li><span class="sect1"><a href="#upgrade-main-policy"><span class="title-number">6.12 </span><span class="title-name">Update <code class="filename">policy.cfg</code> and Deploy Ceph Dashboard Using DeepSea</span></a></span></li><li><span class="sect1"><a href="#upgrade-drive-groups"><span class="title-number">6.13 </span><span class="title-name">Migration from Profile-based Deployments to DriveGroups</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-deploy-ds-custom"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></span><ul><li><span class="sect1"><a href="#using-customized-files"><span class="title-number">7.1 </span><span class="title-name">Using Customized Configuration Files</span></a></span></li><li><span class="sect1"><a href="#discovered-configuration-modification"><span class="title-number">7.2 </span><span class="title-name">Modifying Discovered Configuration</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#additional-software"><span class="title-number">III </span><span class="title-name">Installation of Additional Services</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-as-intro"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></span></li><li><span class="chapter"><a href="#cha-ceph-additional-software-installation"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#rgw-installation"><span class="title-number">9.1 </span><span class="title-name">Object Gateway Manual Installation</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-as-iscsi"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></span><ul><li><span class="sect1"><a href="#ceph-iscsi-iscsi"><span class="title-number">10.1 </span><span class="title-name">iSCSI Block Storage</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-lrbd"><span class="title-number">10.2 </span><span class="title-name">General Information about <code class="systemitem">ceph-iscsi</code></span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-deploy"><span class="title-number">10.3 </span><span class="title-name">Deployment Considerations</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-install"><span class="title-number">10.4 </span><span class="title-name">Installation and Configuration</span></a></span></li><li><span class="sect1"><a href="#iscsi-tcmu"><span class="title-number">10.5 </span><span class="title-name">Exporting RADOS Block Device Images Using <code class="systemitem">tcmu-runner</code></span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-as-cephfs"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></span><ul><li><span class="sect1"><a href="#ceph-cephfs-limitations"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-mds"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs"><span class="title-number">11.3 </span><span class="title-name">CephFS</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-as-ganesha"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></span><ul><li><span class="sect1"><a href="#sec-as-ganesha-preparation"><span class="title-number">12.1 </span><span class="title-name">Preparation</span></a></span></li><li><span class="sect1"><a href="#sec-as-ganesha-basic-example"><span class="title-number">12.2 </span><span class="title-name">Example Installation</span></a></span></li><li><span class="sect1"><a href="#sec-ganesha-active-active"><span class="title-number">12.3 </span><span class="title-name">Active-Active Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-as-ganesha-info"><span class="title-number">12.4 </span><span class="title-name">More Information</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#containerized-ses-on-caasp"><span class="title-number">IV </span><span class="title-name">Cluster Deployment on Top of SUSE CaaS Platform 4 (Technology Preview)</span></a></span><ul><li><span class="chapter"><a href="#cha-container-kubernetes"><span class="title-number">13 </span><span class="title-name">SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</span></a></span><ul><li><span class="sect1"><a href="#kube-consider"><span class="title-number">13.1 </span><span class="title-name">Considerations</span></a></span></li><li><span class="sect1"><a href="#kube-prereq"><span class="title-number">13.2 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="sect1"><a href="#kube-rook-manifests"><span class="title-number">13.3 </span><span class="title-name">Get Rook Manifests</span></a></span></li><li><span class="sect1"><a href="#kube-install"><span class="title-number">13.4 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#kube-using-rook"><span class="title-number">13.5 </span><span class="title-name">Using Rook as Storage for Kubernetes Workload</span></a></span></li><li><span class="sect1"><a href="#kube-uninstall"><span class="title-number">13.6 </span><span class="title-name">Uninstalling Rook</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#id-1.4.7"><span class="title-number">A </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></span></li><li><span class="glossary"><a href="#id-1.4.8"><span class="title-name">Glossary</span></a></span></li><li><span class="appendix"><a href="#ap-deploy-docupdate"><span class="title-number">B </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-depl-docupdates-6mu"><span class="title-number">B.1 </span><span class="title-name">Maintenance update of SUSE Enterprise Storage 6 documentation</span></a></span></li><li><span class="sect1"><a href="#sec-depl-docupdates-6"><span class="title-number">B.2 </span><span class="title-name">June 2019 (Release of SUSE Enterprise Storage 6)</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#storage-intro-core-rados-figure"><span class="number">1.1 </span><span class="name">Interfaces to the Ceph Object Store</span></a></span></li><li><span class="figure"><a href="#storage-intro-structure-example-figure"><span class="number">1.2 </span><span class="name">Small Scale Ceph Example</span></a></span></li><li><span class="figure"><a href="#network-overview-figure"><span class="number">2.1 </span><span class="name">Network Overview</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.7.3.6"><span class="number">2.2 </span><span class="name">Minimum Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.6.6"><span class="number">3.1 </span><span class="name">2-Node HA Cluster for Admin Node</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.2.8.7.11"><span class="number">5.1 </span><span class="name">DeepSea CLI stage execution progress output</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.3.9.5.2.4.2"><span class="number">6.1 </span><span class="name">Select the Migration Target</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.3.14.3.2.2.3"><span class="number">6.2 </span><span class="name">Dependency Conflict Resolution</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.6.3"><span class="number">10.1 </span><span class="name">Ceph Cluster with a Single iSCSI Gateway</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.4.6.5"><span class="number">10.2 </span><span class="name">Ceph Cluster with Multiple iSCSI Gateways</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#id-1.4.4.2.9.3.9.4"><span class="number">5.1 </span><span class="name">Matching by Disk Size</span></a></span></li><li><span class="example"><a href="#id-1.4.4.2.9.3.10.3"><span class="number">5.2 </span><span class="name">Simple Setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.2.9.3.10.4"><span class="number">5.3 </span><span class="name">Advanced Setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.2.9.3.10.5"><span class="number">5.4 </span><span class="name">Advanced Setup with Non-uniform Nodes</span></a></span></li><li><span class="example"><a href="#id-1.4.4.2.9.3.10.6"><span class="number">5.5 </span><span class="name">Expert Setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.2.9.3.10.7"><span class="number">5.6 </span><span class="name">Complex (and Unlikely) Setup</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.4.1.6"><p>
  Copyright ©
2022

  SUSE LLC
 </p><p>
  Copyright © 2016, RedHat, Inc, and contributors.

 </p><p>
  The text of and illustrations in this document are licensed under a Creative
  Commons Attribution-Share Alike 4.0 International ("CC-BY-SA"). An
  explanation of CC-BY-SA is available at
  <a class="link" href="http://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">http://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
  In accordance with CC-BY-SA, if you distribute this document or an adaptation
  of it, you must provide the URL for the original version.
 </p><p>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International Corp.
  or its subsidiaries in the United States and/or other countries. All other
  trademarks are the property of their respective owners.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="id-1.4.2" data-id-title="About This Guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About This Guide</span> <a title="Permalink" class="permalink" href="#id-1.4.2">#</a></h1></div></div></div><p>
  SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the
  capabilities of the Ceph (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>) storage
  project with the enterprise engineering and support of SUSE. SUSE Enterprise Storage
  6 provides IT organizations with the ability to deploy a
  distributed storage architecture that can support a number of use cases using
  commodity hardware platforms.
 </p><p>
  This guide helps you understand the concept of the SUSE Enterprise Storage
  6 with the main focus on managing and administrating the Ceph
  infrastructure. It also demonstrates how to use Ceph with other related
  solutions, such as OpenStack or KVM.
 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system as well as documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><section class="sect1" id="id-1.4.2.7" data-id-title="Available Documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available Documentation</span> <a title="Permalink" class="permalink" href="#id-1.4.2.7">#</a></h2></div></div></div><p>
  
  The following manuals are available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.7.4.1"><span class="term"><span class="intraxref">Book “Administration Guide”</span></span></dt><dd><p>
     The guide describes various administration tasks that are typically
     performed after the installation. The guide also introduces steps to
     integrate Ceph with virtualization solutions such as <code class="systemitem">libvirt</code>, Xen,
     or KVM, and ways to access objects stored in the cluster via iSCSI and
     RADOS gateways.
    </p></dd><dt id="id-1.4.2.7.4.2"><span class="term"><span class="intraxref">Book “Deployment Guide”</span></span></dt><dd><p>
     Guides you through the installation steps of the Ceph cluster and all
     services related to Ceph. The guide also illustrates a basic Ceph
     cluster structure and provides you with related terminology.
    </p></dd></dl></div><p>
  HTML versions of the product manuals can be found in the installed system
  under <code class="filename">/usr/share/doc/manual</code>. Find the latest
  documentation updates at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>
  where you can download the manuals for your product in multiple formats.
 </p></section><section class="sect1" id="id-1.4.2.8" data-id-title="Feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Feedback</span> <a title="Permalink" class="permalink" href="#id-1.4.2.8">#</a></h2></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.8.4.1"><span class="term">Bugs and Enhancement Requests</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To report bugs for a product component, log in to the Novell Customer Center from
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a> and select <span class="guimenu">My Support</span> / <span class="guimenu">Service Request</span>.
    </p></dd><dt id="id-1.4.2.8.4.2"><span class="term">User Comments</span></dt><dd><p>
     We want to hear your comments and suggestions for this manual and the
     other documentation included with this product. If you have questions,
     suggestions, or corrections, contact doc-team@suse.com, or you can also
     click the <code class="literal">Report Documentation Bug</code> link beside each
     chapter or section heading.
    </p></dd><dt id="id-1.4.2.8.4.3"><span class="term">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.de</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a concise
     description of the problem and refer to the respective section number and
     page (or URL).
    </p></dd></dl></div></section><section class="sect1" id="id-1.4.2.9" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#id-1.4.2.9">#</a></h2></div></div></div><p>
  The following typographical conventions are used in this manual:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">placeholder</em>: replace
    <em class="replaceable">placeholder</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination; keys
    are shown in uppercase as on a keyboard
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> / <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a reference
    to a chapter in another manual.
   </p></li></ul></div></section><section class="sect1" id="id-1.4.2.10" data-id-title="About the Making of This Manual"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">About the Making of This Manual</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10">#</a></h2></div></div></div><p>
  This book is written in GeekoDoc, a subset of DocBook (see
  <a class="link" href="http://www.docbook.org" target="_blank">http://www.docbook.org</a>). The XML source files were
  validated by <code class="command">xmllint</code>, processed by
  <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
  version of Norman Walsh's stylesheets. The final PDF can be formatted through
  FOP from Apache or through XEP from RenderX. The authoring and publishing
  tools used to produce this manual are available in the package
  <code class="systemitem">daps</code>. The DocBook Authoring and
  Publishing Suite (DAPS) is developed as open source software. For more
  information, see <a class="link" href="http://daps.sf.net/" target="_blank">http://daps.sf.net/</a>.
 </p></section><section class="sect1" id="id-1.4.2.11" data-id-title="Ceph Contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph Contributors</span> <a title="Permalink" class="permalink" href="#id-1.4.2.11">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section></section><div class="part" id="part-ses" data-id-title="SUSE Enterprise Storage"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">SUSE Enterprise Storage </span><a title="Permalink" class="permalink" href="#part-ses">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 6 and Ceph</span></a></span></li><dd class="toc-abstract"><p>SUSE Enterprise Storage 6 is a distributed storage system designed for scalability, reliability and performance which is based on the Ceph technology. A Ceph cluster can be run on commodity servers in a common network like Ethernet. The cluster scales up well to thousands of servers (later on referr…</p></dd><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span></a></span></li><dd class="toc-abstract"><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p></dd><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Admin Node HA Setup</span></a></span></li><dd class="toc-abstract"><p>The Admin Node is a Ceph cluster node where the Salt master service runs. The Admin Node is a central point of the Ceph cluster because it manages the rest of the cluster nodes by querying and instructing their Salt minion services. It usually includes other services as well, for example the Grafana…</p></dd><li><span class="chapter"><a href="#id-1.4.3.5"><span class="title-number">4 </span><span class="title-name">User Privileges and Command Prompts</span></a></span></li><dd class="toc-abstract"><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p></dd></ul></div><section class="chapter" id="cha-storage-about" data-id-title="SUSE Enterprise Storage 6 and Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">SUSE Enterprise Storage 6 and Ceph</span> <a title="Permalink" class="permalink" href="#cha-storage-about">#</a></h2></div></div></div><p>
  SUSE Enterprise Storage 6 is a distributed storage system designed for
  scalability, reliability and performance which is based on the Ceph
  technology. A Ceph cluster can be run on commodity servers in a common
  network like Ethernet. The cluster scales up well to thousands of servers
  (later on referred to as nodes) and into the petabyte range. As opposed to
  conventional systems which have allocation tables to store and fetch data,
  Ceph uses a deterministic algorithm to allocate storage for data and has no
  centralized information structure. Ceph assumes that in storage clusters
  the addition or removal of hardware is the rule, not the exception. The
  Ceph cluster automates management tasks such as data distribution and
  redistribution, data replication, failure detection and recovery. Ceph is
  both self-healing and self-managing which results in a reduction of
  administrative and budget overhead.
 </p><p>
  This chapter provides a high level overview of SUSE Enterprise Storage 6
  and briefly describes the most important components.
 </p><div id="id-1.4.3.2.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
   Since SUSE Enterprise Storage 5, the only cluster deployment method is DeepSea.
   Refer to <a class="xref" href="#ceph-install-saltstack" title="Chapter 5. Deploying with DeepSea/Salt">Chapter 5, <em>Deploying with DeepSea/Salt</em></a> for details about the
   deployment process.
  </p></div><section class="sect1" id="storage-intro-features" data-id-title="Ceph Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Ceph Features</span> <a title="Permalink" class="permalink" href="#storage-intro-features">#</a></h2></div></div></div><p>
   The Ceph environment has the following features:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.6.3.1"><span class="term">Scalability</span></dt><dd><p>
      Ceph can scale to thousands of nodes and manage storage in the range of
      petabytes.
     </p></dd><dt id="id-1.4.3.2.6.3.2"><span class="term">Commodity Hardware</span></dt><dd><p>
      No special hardware is required to run a Ceph cluster. For details, see
      <a class="xref" href="#storage-bp-hwreq" title="Chapter 2. Hardware Requirements and Recommendations">Chapter 2, <em>Hardware Requirements and Recommendations</em></a>
     </p></dd><dt id="id-1.4.3.2.6.3.3"><span class="term">Self-managing</span></dt><dd><p>
      The Ceph cluster is self-managing. When nodes are added, removed or
      fail, the cluster automatically redistributes the data. It is also aware
      of overloaded disks.
     </p></dd><dt id="id-1.4.3.2.6.3.4"><span class="term">No Single Point of Failure</span></dt><dd><p>
      No node in a cluster stores important information alone. The number of
      redundancies can be configured.
     </p></dd><dt id="id-1.4.3.2.6.3.5"><span class="term">Open Source Software</span></dt><dd><p>
      Ceph is an open source software solution and independent of specific
      hardware or vendors.
     </p></dd></dl></div></section><section class="sect1" id="storage-intro-core" data-id-title="Core Components"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Core Components</span> <a title="Permalink" class="permalink" href="#storage-intro-core">#</a></h2></div></div></div><p>
   To make full use of Ceph's power, it is necessary to understand some of
   the basic components and concepts. This section introduces some parts of
   Ceph that are often referenced in other chapters.
  </p><section class="sect2" id="storage-intro-core-rados" data-id-title="RADOS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.1 </span><span class="title-name">RADOS</span> <a title="Permalink" class="permalink" href="#storage-intro-core-rados">#</a></h3></div></div></div><p>
    The basic component of Ceph is called <span class="emphasis"><em>RADOS</em></span>
    <span class="emphasis"><em>(Reliable Autonomic Distributed Object Store)</em></span>. It is
    responsible for managing the data stored in the cluster. Data in Ceph is
    usually stored as objects. Each object consists of an identifier and the
    data.
   </p><p>
    RADOS provides the following access methods to the stored objects that
    cover many use cases:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.3.4.1"><span class="term">Object Gateway</span></dt><dd><p>
       Object Gateway is an HTTP REST gateway for the RADOS object store. It enables
       direct access to objects stored in the Ceph cluster.
      </p></dd><dt id="id-1.4.3.2.7.3.4.2"><span class="term">RADOS Block Device</span></dt><dd><p>
       RADOS Block Devices (RBD) can be accessed like any other block device.
       These can be used for example in combination with <code class="systemitem">libvirt</code> for
       virtualization purposes.
      </p></dd><dt id="id-1.4.3.2.7.3.4.3"><span class="term">CephFS</span></dt><dd><p>
       The Ceph File System is a POSIX-compliant file system.
      </p></dd><dt id="id-1.4.3.2.7.3.4.4"><span class="term"><code class="systemitem">librados</code></span></dt><dd><p>
       <code class="systemitem">librados</code> is a library that can
       be used with many programming languages to create an application capable
       of directly interacting with the storage cluster.
      </p></dd></dl></div><p>
    <code class="systemitem">librados</code> is used by Object Gateway and RBD
    while CephFS directly interfaces with RADOS
    <a class="xref" href="#storage-intro-core-rados-figure" title="Interfaces to the Ceph Object Store">Figure 1.1, “Interfaces to the Ceph Object Store”</a>.
   </p><div class="figure" id="storage-intro-core-rados-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/rados-structure.png" target="_blank"><img src="images/rados-structure.png" width="" alt="Interfaces to the Ceph Object Store"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.1: </span><span class="title-name">Interfaces to the Ceph Object Store </span><a title="Permalink" class="permalink" href="#storage-intro-core-rados-figure">#</a></h6></div></div></section><section class="sect2" id="storage-intro-core-crush" data-id-title="CRUSH"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.2 </span><span class="title-name">CRUSH</span> <a title="Permalink" class="permalink" href="#storage-intro-core-crush">#</a></h3></div></div></div><p>
    At the core of a Ceph cluster is the <span class="emphasis"><em>CRUSH</em></span>
    algorithm. CRUSH is the acronym for <span class="emphasis"><em>Controlled Replication Under
    Scalable Hashing</em></span>. CRUSH is a function that handles the storage
    allocation and needs comparably few parameters. That means only a small
    amount of information is necessary to calculate the storage position of an
    object. The parameters are a current map of the cluster including the
    health state, some administrator-defined placement rules and the name of
    the object that needs to be stored or retrieved. With this information, all
    nodes in the Ceph cluster are able to calculate where an object and its
    replicas are stored. This makes writing or reading data very efficient.
    CRUSH tries to evenly distribute data over all nodes in the cluster.
   </p><p>
    The <span class="emphasis"><em>CRUSH map</em></span> contains all storage nodes and
    administrator-defined placement rules for storing objects in the cluster.
    It defines a hierarchical structure that usually corresponds to the
    physical structure of the cluster. For example, the data-containing disks
    are in hosts, hosts are in racks, racks in rows and rows in data centers.
    This structure can be used to define <span class="emphasis"><em>failure domains</em></span>.
    Ceph then ensures that replications are stored on different branches of a
    specific failure domain.
   </p><p>
    If the failure domain is set to rack, replications of objects are
    distributed over different racks. This can mitigate outages caused by a
    failed switch in a rack. If one power distribution unit supplies a row of
    racks, the failure domain can be set to row. When the power distribution
    unit fails, the replicated data is still available on other rows.
   </p></section><section class="sect2" id="storage-intro-core-nodes" data-id-title="Ceph Nodes and Daemons"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.3 </span><span class="title-name">Ceph Nodes and Daemons</span> <a title="Permalink" class="permalink" href="#storage-intro-core-nodes">#</a></h3></div></div></div><p>
    In Ceph, nodes are servers working for the cluster. They can run several
    different types of daemons. We recommend running only one type of daemon on
    each node, except for Ceph Manager daemons which can be co-located with Ceph Monitors.
    Each cluster requires at least Ceph Monitor, Ceph Manager, and Ceph OSD daemons:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.5.3.1"><span class="term">Admin Node</span></dt><dd><p>
       <span class="emphasis"><em>Admin Node</em></span> is a Ceph cluster node where the Salt master
       service is running. The Admin Node is a central point of the Ceph cluster
       because it manages the rest of the cluster nodes by querying and
       instructing their Salt minion services.
      </p></dd><dt id="id-1.4.3.2.7.5.3.2"><span class="term">Ceph Monitor</span></dt><dd><p>
       <span class="emphasis"><em>Ceph Monitor</em></span> (often abbreviated as
       <span class="emphasis"><em>MON</em></span>) nodes maintain information about the cluster
       health state, a map of all nodes and data distribution rules (see
       <a class="xref" href="#storage-intro-core-crush" title="1.2.2. CRUSH">Section 1.2.2, “CRUSH”</a>).
      </p><p>
       If failures or conflicts occur, the Ceph Monitor nodes in the cluster decide by
       majority which information is correct. To form a qualified majority, it
       is recommended to have an odd number of Ceph Monitor nodes, and at least three
       of them.
      </p><p>
       If more than one site is used, the Ceph Monitor nodes should be distributed
       over an odd number of sites. The number of Ceph Monitor nodes per site should
       be such that more than 50% of the Ceph Monitor nodes remain functional if one
       site fails.
      </p></dd><dt id="id-1.4.3.2.7.5.3.3"><span class="term">Ceph Manager</span></dt><dd><p>
       The Ceph Manager collects the state information from the whole cluster. The
       Ceph Manager daemon runs alongside the monitor daemons. It provides additional
       monitoring, and interfaces the external monitoring and management
       systems. It includes other services as well, for example the Ceph Dashboard
       Web UI. The Ceph Dashboard Web UI runs on the same node as the Ceph Manager.
      </p><p>
       The Ceph manager requires no additional configuration, beyond ensuring
       it is running. You can deploy it as a separate role using DeepSea.
      </p></dd><dt id="id-1.4.3.2.7.5.3.4"><span class="term">Ceph OSD</span></dt><dd><p>
       A <span class="emphasis"><em>Ceph OSD</em></span> is a daemon handling <span class="emphasis"><em>Object
       Storage Devices</em></span> which are a physical or logical storage units
       (hard disks or partitions). Object Storage Devices can be physical
       disks/partitions or logical volumes. The daemon additionally takes care
       of data replication and rebalancing in case of added or removed nodes.
      </p><p>
       Ceph OSD daemons communicate with monitor daemons and provide them
       with the state of the other OSD daemons.
      </p></dd></dl></div><p>
    To use CephFS, Object Gateway, NFS Ganesha, or iSCSI Gateway, additional nodes are required:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.2.7.5.5.1"><span class="term">Metadata Server (MDS)</span></dt><dd><p>
       The Metadata Servers store metadata for the CephFS. By using an MDS you can
       execute basic file system commands such as <code class="command">ls</code> without
       overloading the cluster.
      </p></dd><dt id="id-1.4.3.2.7.5.5.2"><span class="term">Object Gateway</span></dt><dd><p>
       The Object Gateway is an HTTP REST gateway for the RADOS object store. It is
       compatible with OpenStack Swift and Amazon S3 and has its own user
       management.
      </p></dd><dt id="id-1.4.3.2.7.5.5.3"><span class="term">NFS Ganesha</span></dt><dd><p>
       NFS Ganesha provides an NFS access to either the Object Gateway or the CephFS. It
       runs in the user instead of the kernel space and directly interacts with
       the Object Gateway or CephFS.
      </p></dd><dt id="id-1.4.3.2.7.5.5.4"><span class="term">iSCSI Gateway</span></dt><dd><p>
       iSCSI is a storage network protocol that allows clients to send SCSI
       commands to SCSI storage devices (targets) on remote servers.
      </p></dd><dt id="id-1.4.3.2.7.5.5.5"><span class="term">Samba Gateway</span></dt><dd><p>
       The Samba Gateway provides a SAMBA access to data stored on CephFS.
      </p></dd></dl></div></section></section><section class="sect1" id="storage-intro-structure" data-id-title="Storage Structure"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">Storage Structure</span> <a title="Permalink" class="permalink" href="#storage-intro-structure">#</a></h2></div></div></div><section class="sect2" id="storage-intro-structure-pool" data-id-title="Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.1 </span><span class="title-name">Pool</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pool">#</a></h3></div></div></div><p>
    Objects that are stored in a Ceph cluster are put into
    <span class="emphasis"><em>pools</em></span>. Pools represent logical partitions of the
    cluster to the outside world. For each pool a set of rules can be defined,
    for example, how many replications of each object must exist. The standard
    configuration of pools is called <span class="emphasis"><em>replicated pool</em></span>.
   </p><p>
    Pools usually contain objects but can also be configured to act similar to
    a RAID 5. In this configuration, objects are stored in chunks along with
    additional coding chunks. The coding chunks contain the redundant
    information. The number of data and coding chunks can be defined by the
    administrator. In this configuration, pools are referred to as
    <span class="emphasis"><em>erasure coded pools</em></span>.
   </p></section><section class="sect2" id="storage-intro-structure-pg" data-id-title="Placement Group"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.2 </span><span class="title-name">Placement Group</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pg">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Placement Groups</em></span> (PGs) are used for the distribution
    of data within a pool. When creating a pool, a certain number of placement
    groups is set. The placement groups are used internally to group objects
    and are an important factor for the performance of a Ceph cluster. The PG
    for an object is determined by the object's name.
   </p></section><section class="sect2" id="storage-intro-structure-example" data-id-title="Example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.3 </span><span class="title-name">Example</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-example">#</a></h3></div></div></div><p>
    This section provides a simplified example of how Ceph manages data (see
    <a class="xref" href="#storage-intro-structure-example-figure" title="Small Scale Ceph Example">Figure 1.2, “Small Scale Ceph Example”</a>). This example
    does not represent a recommended configuration for a Ceph cluster. The
    hardware setup consists of three storage nodes or Ceph OSDs
    (<code class="literal">Host 1</code>, <code class="literal">Host 2</code>, <code class="literal">Host
    3</code>). Each node has three hard disks which are used as OSDs
    (<code class="literal">osd.1</code> to <code class="literal">osd.9</code>). The Ceph Monitor nodes are
    neglected in this example.
   </p><div id="id-1.4.3.2.8.4.3" data-id-title="Difference between Ceph OSD and OSD" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Difference between Ceph OSD and OSD</h6><p>
     While <span class="emphasis"><em>Ceph OSD</em></span> or <span class="emphasis"><em>Ceph OSD
     daemon</em></span> refers to a daemon that is run on a node, the word
     <span class="emphasis"><em>OSD</em></span> refers to the logical disk that the daemon
     interacts with.
    </p></div><p>
    The cluster has two pools, <code class="literal">Pool A</code> and <code class="literal">Pool
    B</code>. While Pool A replicates objects only two times, resilience for
    Pool B is more important and it has three replications for each object.
   </p><p>
    When an application puts an object into a pool, for example via the REST
    API, a Placement Group (<code class="literal">PG1</code> to <code class="literal">PG4</code>)
    is selected based on the pool and the object name. The CRUSH algorithm then
    calculates on which OSDs the object is stored, based on the Placement Group
    that contains the object.
   </p><p>
    In this example the failure domain is set to host. This ensures that
    replications of objects are stored on different hosts. Depending on the
    replication level set for a pool, the object is stored on two or three OSDs
    that are used by the Placement Group.
   </p><p>
    An application that writes an object only interacts with one Ceph OSD,
    the primary Ceph OSD. The primary Ceph OSD takes care of replication
    and confirms the completion of the write process after all other OSDs have
    stored the object.
   </p><p>
    If <code class="literal">osd.5</code> fails, all object in <code class="literal">PG1</code> are
    still available on <code class="literal">osd.1</code>. As soon as the cluster
    recognizes that an OSD has failed, another OSD takes over. In this example
    <code class="literal">osd.4</code> is used as a replacement for
    <code class="literal">osd.5</code>. The objects stored on <code class="literal">osd.1</code>
    are then replicated to <code class="literal">osd.4</code> to restore the replication
    level.
   </p><div class="figure" id="storage-intro-structure-example-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/data-structure-example.png" target="_blank"><img src="images/data-structure-example.png" width="" alt="Small Scale Ceph Example"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.2: </span><span class="title-name">Small Scale Ceph Example </span><a title="Permalink" class="permalink" href="#storage-intro-structure-example-figure">#</a></h6></div></div><p>
    If a new node with new OSDs is added to the cluster, the cluster map is
    going to change. The CRUSH function then returns different locations for
    objects. Objects that receive new locations will be relocated. This process
    results in a balanced usage of all OSDs.
   </p></section></section><section class="sect1" id="about-bluestore" data-id-title="BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">BlueStore</span> <a title="Permalink" class="permalink" href="#about-bluestore">#</a></h2></div></div></div><p>
   BlueStore is a new default storage back-end for Ceph from SUSE Enterprise Storage
   5. It has better performance than FileStore, full data check-summing, and
   built-in compression.
  </p><p>
   BlueStore manages either one, two, or three storage devices. In the
   simplest case, BlueStore consumes a single primary storage device. The
   storage device is normally partitioned into two parts:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     A small partition named BlueFS that implements file system-like
     functionalities required by RocksDB.
    </p></li><li class="listitem"><p>
     The rest of the device is normally a large partition occupying the rest of
     the device. It is managed directly by BlueStore and contains all of the
     actual data. This primary device is normally identified by a block
     symbolic link in the data directory.
    </p></li></ol></div><p>
   It is also possible to deploy BlueStore across two additional devices:
  </p><p>
   A <span class="emphasis"><em>WAL device</em></span> can be used for BlueStore’s internal
   journal or write-ahead log. It is identified by the
   <code class="literal">block.wal</code> symbolic link in the data directory. It is only
   useful to use a separate WAL device if the device is faster than the primary
   device or the DB device, for example when:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The WAL device is an NVMe, and the DB device is an SSD, and the data
     device is either SSD or HDD.
    </p></li><li class="listitem"><p>
     Both the WAL and DB devices are separate SSDs, and the data device is an
     SSD or HDD.
    </p></li></ul></div><p>
   A <span class="emphasis"><em>DB device</em></span> can be used for storing BlueStore’s
   internal metadata. BlueStore (or rather, the embedded RocksDB) will put as
   much metadata as it can on the DB device to improve performance. Again, it
   is only helpful to provision a shared DB device if it is faster than the
   primary device.
  </p><div id="id-1.4.3.2.9.9" data-id-title="Plan for the DB Size" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Plan for the DB Size</h6><p>
    Plan thoroughly to ensure sufficient size of the DB device. If the DB
    device fills up, metadata will spill over to the primary device, which
    badly degrades the OSD's performance.
   </p><p>
    You can check if a WAL/DB partition is getting full and spilling over with
    the <code class="command">ceph daemon osd<em class="replaceable">.ID</em> perf
    dump</code> command. The <code class="option">slow_used_bytes</code> value shows
    the amount of data being spilled out:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd<em class="replaceable">.ID</em> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,</pre></div></div></section><section class="sect1" id="storage-moreinfo" data-id-title="Additional Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#storage-moreinfo">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ceph as a community project has its own extensive online documentation.
     For topics not found in this manual, refer to
     <a class="link" href="http://docs.ceph.com/docs/master/" target="_blank">http://docs.ceph.com/docs/master/</a>.
    </p></li><li class="listitem"><p>
     The original publication <span class="emphasis"><em>CRUSH: Controlled, Scalable,
     Decentralized Placement of Replicated Data</em></span> by <span class="emphasis"><em>S.A.
     Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</em></span> provides helpful
     insight into the inner workings of Ceph. Especially when deploying large
     scale clusters it is a recommended reading. The publication can be found
     at <a class="link" href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf" target="_blank">http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf</a>.
    </p></li><li class="listitem"><p>
     SUSE Enterprise Storage can be used with non-SUSE OpenStack distributions. The Ceph
     clients need to be at a level that is compatible with SUSE Enterprise Storage.
    </p><div id="id-1.4.3.2.10.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      SUSE supports the server component of the Ceph deployment and the
      client is supported by the OpenStack distribution vendor.
     </p></div></li></ul></div></section></section><section class="chapter" id="storage-bp-hwreq" data-id-title="Hardware Requirements and Recommendations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Hardware Requirements and Recommendations</span> <a title="Permalink" class="permalink" href="#storage-bp-hwreq">#</a></h2></div></div></div><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p><p>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </p><section class="sect1" id="network-overview" data-id-title="Network Overview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Network Overview</span> <a title="Permalink" class="permalink" href="#network-overview">#</a></h2></div></div></div><p>
   Ceph has several logical networks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A trusted internal network, the back-end network called the the
     <code class="literal">cluster network</code>.
    </p></li><li class="listitem"><p>
     A public client network called <code class="literal">public network</code>.
    </p></li><li class="listitem"><p>
     Client networks for gateways, these are optional.
    </p></li></ul></div><p>
   The trusted internal network is the back-end network between the OSD nodes
   for replication, re-balancing and recovery.Ideally, this network provides
   twice the bandwidth of the public network with default 3-way replication
   since the primary OSD sends 2 copies to other OSDs via this network. The
   public network is between clients and gateways on the one side to talk to
   monitors, managers, MDS nodes, OSD nodes. It is also used by monitors,
   managers, and MDS nodes to talk with OSD nodes.
  </p><div class="figure" id="network-overview-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/network-overview-diagram.png" target="_blank"><img src="images/network-overview-diagram.png" width="" alt="Network Overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.1: </span><span class="title-name">Network Overview </span><a title="Permalink" class="permalink" href="#network-overview-figure">#</a></h6></div></div><section class="sect2" id="ceph-install-ceph-deploy-network" data-id-title="Network Recommendations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.1 </span><span class="title-name">Network Recommendations</span> <a title="Permalink" class="permalink" href="#ceph-install-ceph-deploy-network">#</a></h3></div></div></div><p>
    For the Ceph network environment, we recommend two bonded 25 GbE (or
    faster) network interfaces bonded using 802.3ad (LACP). The use of two
    network interfaces provides aggregation and fault-tolerance. The bond
    should then be used to provide two VLAN interfaces, one for the public
    network, and the second for the cluster network. Details on bonding the
    interfaces can be found in
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-iface-bonding" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-iface-bonding</a>.
   </p><p>
    Fault tolerance can be enhanced through isolating the components into
    failure domains. To improve fault tolerance of the network, bonding one
    interface from two separate Network Interface Cards (NIC) offers protection
    against failure of a single NIC. Similarly, creating a bond across two
    switches protects against failure of a switch. We recommend consulting with
    the network equipment vendor in order to architect the level of fault
    tolerance required.
   </p><div id="id-1.4.3.3.5.6.4" data-id-title="Administration Network not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Administration Network not Supported</h6><p>
     Additional administration network setup—that enables for example
     separating SSH, Salt, or DNS networking—is neither tested nor
     supported.
    </p></div><div id="id-1.4.3.3.5.6.5" data-id-title="Nodes Configured via DHCP" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Nodes Configured via DHCP</h6><p>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various Ceph daemons start. If this happens, the Ceph MONs and OSDs
     will not start correctly (running <code class="command">systemctl status
     ceph\*</code> will result in "unable to bind" errors). To avoid this
     issue, we recommend increasing the DHCP client timeout to at least 30
     seconds on each node in your storage cluster. This can be done by changing
     the following settings on each node:
    </p><p>
     In <code class="filename">/etc/sysconfig/network/dhcp</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">DHCLIENT_WAIT_AT_BOOT="30"</pre></div><p>
     In <code class="filename">/etc/sysconfig/network/config</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">WAIT_FOR_INTERFACES="60"</pre></div></div><section class="sect3" id="storage-bp-net-private" data-id-title="Adding a Private Network to a Running Cluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.1 </span><span class="title-name">Adding a Private Network to a Running Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-net-private">#</a></h4></div></div></div><p>
     If you do not specify a cluster network during Ceph deployment, it
     assumes a single public network environment. While Ceph operates fine
     with a public network, its performance and security improves when you set
     a second private cluster network. To support two networks, each Ceph
     node needs to have at least two network cards.
    </p><p>
     You need to apply the following changes to each Ceph node. It is
     relatively quick to do for a small cluster, but can be very time consuming
     if you have a cluster consisting of hundreds or thousands of nodes.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Stop Ceph related services on each cluster node.
      </p><p>
       Add a line to <code class="filename">/etc/ceph/ceph.conf</code> to define the
       cluster network, for example:
      </p><div class="verbatim-wrap"><pre class="screen">cluster network = 10.0.0.0/24</pre></div><p>
       If you need to specifically assign static IP addresses or override
       <code class="option">cluster network</code> settings, you can do so with the
       optional <code class="option">cluster addr</code>.
      </p></li><li class="step"><p>
       Check that the private cluster network works as expected on the OS
       level.
      </p></li><li class="step"><p>
       Start Ceph related services on each cluster node.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph.target</pre></div></li></ol></div></div></section><section class="sect3" id="storage-bp-net-subnets" data-id-title="Monitor Nodes on Different Subnets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.2 </span><span class="title-name">Monitor Nodes on Different Subnets</span> <a title="Permalink" class="permalink" href="#storage-bp-net-subnets">#</a></h4></div></div></div><p>
     If the monitor nodes are on multiple subnets, for example they are located
     in different rooms and served by different switches, you need to adjust
     the <code class="filename">ceph.conf</code> file accordingly. For example, if the
     nodes have IP addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12, add the
     following lines to their <code class="literal">global</code> section:
    </p><div class="verbatim-wrap"><pre class="screen">[global]
 [...]
 mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
 mon initial members = MON1, MON2, MON3
 [...]</pre></div><p>
     Additionally, if you need to specify a per-monitor public address or
     network, you need to add a
     <code class="literal">[mon.<em class="replaceable">X</em>]</code> section for each
     monitor:
    </p><div class="verbatim-wrap"><pre class="screen">[mon.MON1]
 public network = 192.168.123.0/24

 [mon.MON2]
 public network = 1.2.3.0/24

 [mon.MON3]
 public network = 242.12.33.12/0</pre></div></section></section></section><section class="sect1" id="multi-architecture" data-id-title="Multiple Architecture Configurations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Multiple Architecture Configurations</span> <a title="Permalink" class="permalink" href="#multi-architecture">#</a></h2></div></div></div><p>
   SUSE Enterprise Storage supports both x86 and Arm architectures. When considering
   each architecture, it is important to note that from a cores per OSD,
   frequency, and RAM perspective, there is no real difference between CPU
   architectures for sizing.
  </p><p>
   As with smaller x86 processors (non-server), lower-performance Arm-based
   cores may not provide an optimal experience, especially when used for
   erasure coded pools.
  </p><div id="id-1.4.3.3.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Throughout the documentation, <em class="replaceable">SYSTEM-ARCH</em> is
    used in place of x86 or Arm.
   </p></div></section><section class="sect1" id="ses-hardware-config" data-id-title="Hardware Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Hardware Configuration</span> <a title="Permalink" class="permalink" href="#ses-hardware-config">#</a></h2></div></div></div><p>
   For the best product experience, we recommend to start with the recommended
   cluster configuration. For a test cluster or a cluster with less performance
   requirements, we document a minimal supported cluster configuration.
  </p><section class="sect2" id="ses-bp-minimum-cluster" data-id-title="Minimum Cluster Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.1 </span><span class="title-name">Minimum Cluster Configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-minimum-cluster">#</a></h3></div></div></div><p>
    A minimal product cluster configuration consists of:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least four physical nodes (OSD nodes) with co-location of services
     </p></li><li class="listitem"><p>
      Dual-10 Gb Ethernet as a bonded network
     </p></li><li class="listitem"><p>
      A separate Admin Node (can be virtualized on an external node)
     </p></li></ul></div><p>
    A detailed configuration is:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Separate Admin Node with 4 GB RAM, four cores, 1 TB storage capacity. This is
      typically the Salt master node. Ceph services and gateways, such as
      Ceph Monitor, Metadata Server, Ceph OSD, Object Gateway, or NFS Ganesha are not supported on the Admin Node
      as it needs to orchestrate the cluster update and upgrade processes
      independently.
     </p></li><li class="listitem"><p>
      At least four physical OSD nodes, with eight OSD disks each, see
      <a class="xref" href="#sysreq-osd" title="2.4.1. Minimum Requirements">Section 2.4.1, “Minimum Requirements”</a> for requirements.
     </p><p>
      The total capacity of the cluster should be sized so that even with one
      node unavailable, the total used capacity (including redundancy) does not
      exceed 80%.
     </p></li><li class="listitem"><p>
      Three Ceph Monitor instances. Monitors need to be run from SSD/NVMe storage, not
      HDDs, for latency reasons.
     </p></li><li class="listitem"><p>
      Monitors, Metadata Server, and gateways can be co-located on the OSD nodes, see
      <a class="xref" href="#ses-bp-diskshare" title="2.12. OSD and Monitor Sharing One Server">Section 2.12, “OSD and Monitor Sharing One Server”</a> for monitor co-location. If you
      co-locate services, the memory and CPU requirements need to be added up.
     </p></li><li class="listitem"><p>
      iSCSI Gateway, Object Gateway, and Metadata Server require at least incremental 4 GB RAM and four
      cores.
     </p></li><li class="listitem"><p>
      If you are using CephFS, S3/Swift, iSCSI, at least two instances of
      the respective roles (Metadata Server, Object Gateway, iSCSI) are required for redundancy
      and availability.
     </p></li><li class="listitem"><p>
      The nodes are to be dedicated to SUSE Enterprise Storage and must not be used for
      any other physical, containerized, or virtualized workload.
     </p></li><li class="listitem"><p>
      If any of the gateways (iSCSI, Object Gateway, NFS Ganesha, Metadata Server, ...) are
      deployed within VMs, these VMs must not be hosted on the physical
      machines serving other cluster roles. (This is unnecessary, as they are
      supported as collocated services.)
     </p></li><li class="listitem"><p>
      When deploying services as VMs on hypervisors outside the core physical
      cluster, failure domains must be respected to ensure redundancy.
     </p><p>
      For example, do not deploy multiple roles of the same type on the same
      hypervisor, such as multiple MONs or MDSs instances.
     </p></li><li class="listitem"><p>
      When deploying inside VMs, it is particularly crucial to ensure that the
      nodes have strong network connectivity and well working time
      synchronization.
     </p></li><li class="listitem"><p>
      The hypervisor nodes must be adequately sized to avoid interference by
      other workloads consuming CPU, RAM, network, and storage resources.
     </p></li></ul></div><div class="figure" id="id-1.4.3.3.7.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/minimal-ses.png" target="_blank"><img src="images/minimal-ses.png" width="" alt="Minimum Cluster Configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.2: </span><span class="title-name">Minimum Cluster Configuration </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.7.3.6">#</a></h6></div></div></section><section class="sect2" id="ses-bp-production-cluster" data-id-title="Recommended Production Cluster Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.2 </span><span class="title-name">Recommended Production Cluster Configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-production-cluster">#</a></h3></div></div></div><p>
    Once you grow your cluster, we recommend to relocate monitors, Metadata Server, and
    gateways on separate nodes to ensure better fault tolerance.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Seven Object Storage Nodes
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        No single node exceeds ~15% of total storage.
       </p></li><li class="listitem"><p>
        The total capacity of the cluster should be sized so that even with one
        node unavailable, the total used capacity (including redundancy) does
        not exceed 80%.
       </p></li><li class="listitem"><p>
        25 Gb Ethernet or better, bonded for internal cluster and external
        public network each.
       </p></li><li class="listitem"><p>
        56+ OSDs per storage cluster.
       </p></li><li class="listitem"><p>
        See <a class="xref" href="#sysreq-osd" title="2.4.1. Minimum Requirements">Section 2.4.1, “Minimum Requirements”</a> for further recommendation.
       </p></li></ul></div></li><li class="listitem"><p>
      Dedicated physical infrastructure nodes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Three Ceph Monitor nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-mon" title="2.5. Monitor Nodes">Section 2.5, “Monitor Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Object Gateway nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-rgw" title="2.6. Object Gateway Nodes">Section 2.6, “Object Gateway Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        iSCSI Gateway nodes: 16 GB RAM, 6-8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-iscsi" title="2.9. iSCSI Nodes">Section 2.9, “iSCSI Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Metadata Server nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
        RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-mds" title="2.7. Metadata Server Nodes">Section 2.7, “Metadata Server Nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        One SES Admin Node: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p></li></ul></div></li></ul></div></section></section><section class="sect1" id="deployment-osd-recommendation" data-id-title="Object Storage Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span> <a title="Permalink" class="permalink" href="#deployment-osd-recommendation">#</a></h2></div></div></div><section class="sect2" id="sysreq-osd" data-id-title="Minimum Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.1 </span><span class="title-name">Minimum Requirements</span> <a title="Permalink" class="permalink" href="#sysreq-osd">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The following CPU recommendations account for devices independent of
      usage by Ceph:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        1x 2GHz CPU Thread per spinner.
       </p></li><li class="listitem"><p>
        2x 2GHz CPU Thread per SSD.
       </p></li><li class="listitem"><p>
        4x 2GHz CPU Thread per NVMe.
       </p></li></ul></div></li><li class="listitem"><p>
      Separate 10 GbE networks (public/client and internal), required 4x 10
      GbE, recommended 2x 25 GbE.
     </p></li><li class="listitem"><p>
      Total RAM required = number of OSDs x (1 GB +
      <code class="option">osd_memory_target</code>) + 16 GB
     </p><p>
      The default for <code class="option">osd_memory_target</code> is 4 GB. Refer to
      <span class="intraxref">Book “Administration Guide”, Chapter 25 “Ceph Cluster Configuration”, Section 25.2.1 “Automatic Cache Sizing”</span> for more details on
      <code class="option">osd_memory_target</code>.
     </p></li><li class="listitem"><p>
      OSD disks in JBOD configurations or or individual RAID-0 configurations.
     </p></li><li class="listitem"><p>
      OSD journal can reside on OSD disk.
     </p></li><li class="listitem"><p>
      OSD disks should be exclusively used by SUSE Enterprise Storage.
     </p></li><li class="listitem"><p>
      Dedicated disk and SSD for the operating system, preferably in a RAID 1
      configuration.
     </p></li><li class="listitem"><p>
      Allocate at least an additional 4 GB of RAM if this OSD host will host
      part of a cache pool used for cache tiering.
     </p></li><li class="listitem"><p>
      Ceph Monitors, gateway and Metadata Servers can reside on Object Storage Nodes.
     </p></li><li class="listitem"><p>
      For disk performance reasons, OSD nodes are bare metal nodes. No other
      workloads should run on an OSD node unless it is a minimal setup of
      Ceph Monitors and Ceph Managers.
     </p></li><li class="listitem"><p>
      SSDs for Journal with 6:1 ratio SSD journal to OSD.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-mindisk" data-id-title="Minimum Disk Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.2 </span><span class="title-name">Minimum Disk Size</span> <a title="Permalink" class="permalink" href="#ses-bp-mindisk">#</a></h3></div></div></div><p>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal (for FileStore) or WAL/DB device (for BlueStore), and the
    primary space for the stored data. The minimum (and default) value for the
    journal/WAL/DB is 6 GB. The minimum space for data is 5 GB, as partitions
    smaller than 5 GB are automatically assigned the weight of 0.
   </p><p>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </p></section><section class="sect2" id="rec-waldb-size" data-id-title="Recommended Size for the BlueStores WAL and DB Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.3 </span><span class="title-name">Recommended Size for the BlueStore's WAL and DB Device</span> <a title="Permalink" class="permalink" href="#rec-waldb-size">#</a></h3></div></div></div><div id="id-1.4.3.3.8.4.2" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
     Refer to <a class="xref" href="#about-bluestore" title="1.4. BlueStore">Section 1.4, “BlueStore”</a> for more information on
     BlueStore.
    </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      We recommend reserving 4 GB for the WAL device. While the minimal DB
      size is 64 GB for RBD-only workloads, the recommended DB size for
      Object Gateway and CephFS workloads is 2% of the main device capacity (but at
      least 196 GB).
     </p></li><li class="listitem"><p>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows Ceph to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as Ceph uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not used fully then its
      space is not wasted but used for DB operation.
     </p><p>
      To share the DB device with the WAL, do <span class="emphasis"><em>not</em></span> specify
      the WAL device, and specify only the DB device.
     </p><p>
      Find more information about specifying an OSD layout in
      <a class="xref" href="#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a>.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-share-ssd-journal" data-id-title="Using SSD for OSD Journals"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.4 </span><span class="title-name">Using SSD for OSD Journals</span> <a title="Permalink" class="permalink" href="#ses-bp-share-ssd-journal">#</a></h3></div></div></div><p>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </p><p>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk.
   </p><div id="id-1.4.3.3.8.5.4" data-id-title="Sharing an SSD for Multiple Journals" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing an SSD for Multiple Journals</h6><p>
     As journal data occupies relatively little space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than six journals on the same SSD disk and 12 on
     NVMe disks.
    </p></div></section><section class="sect2" id="maximum-count-of-disks-osd" data-id-title="Maximum Recommended Number of Disks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.5 </span><span class="title-name">Maximum Recommended Number of Disks</span> <a title="Permalink" class="permalink" href="#maximum-count-of-disks-osd">#</a></h3></div></div></div><p>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Network bandwidth.</em></span> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Memory.</em></span> RAM above 2 GB is used for the
      BlueStore cache. With the default <code class="option">osd_memory_target</code> of
      4 GB, the system has a reasonable starting cache size for spinning
      media. If using SSD or NVME, consider increasing the cache size and RAM
      allocation per OSD to maximize performance.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Fault tolerance.</em></span> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </p></li></ul></div></section></section><section class="sect1" id="sysreq-mon" data-id-title="Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Monitor Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mon">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     At least three Ceph Monitor nodes are required. The number of monitors should
     always be odd (1+2n).
    </p></li><li class="listitem"><p>
     At least 4 GB of RAM. For large clusters, provide 5-10 GB of RAM.
    </p></li><li class="listitem"><p>
     Processor with four logical cores.
    </p></li><li class="listitem"><p>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <code class="filename">/var/lib/ceph</code> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </p></li><li class="listitem"><p>
     There must only be one monitor process per node.
    </p></li><li class="listitem"><p>
     Mixing OSD, monitor, or Object Gateway nodes is only supported if sufficient
     hardware resources are available. That means that the requirements for all
     services need to be added up.
    </p></li><li class="listitem"><p>
     Two network interfaces bonded to multiple switches.
    </p></li></ul></div></section><section class="sect1" id="sysreq-rgw" data-id-title="Object Gateway Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Object Gateway Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-rgw">#</a></h2></div></div></div><p>
   Object Gateway nodes should have six to eight CPU cores and 32 GB of RAM (64 GB
   recommended). When other processes are co-located on the same machine, their
   requirements need to be added up.
  </p></section><section class="sect1" id="sysreq-mds" data-id-title="Metadata Server Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Metadata Server Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mds">#</a></h2></div></div></div><p>
   Proper sizing of the Metadata Server nodes depends on the specific use case.
   Generally, the more open files the Metadata Server is to handle, the more CPU and RAM
   it needs. The following are the minimum requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     3 GB of RAM for each Metadata Server daemon.
    </p></li><li class="listitem"><p>
     Bonded network interface.
    </p></li><li class="listitem"><p>
     2.5 GHz CPU with at least 2 cores.
    </p></li></ul></div></section><section class="sect1" id="sysreq-admin-node" data-id-title="Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span> <a title="Permalink" class="permalink" href="#sysreq-admin-node">#</a></h2></div></div></div><p>
   At least 4 GB of RAM and a quad-core CPU are required. This includes running
   the Salt master on the Admin Node. For large clusters with hundreds of nodes, 6 GB
   of RAM is suggested.
  </p></section><section class="sect1" id="sysreq-iscsi" data-id-title="iSCSI Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">iSCSI Nodes</span> <a title="Permalink" class="permalink" href="#sysreq-iscsi">#</a></h2></div></div></div><p>
   iSCSI nodes should have six to eight CPU cores and 16 GB of RAM.
  </p></section><section class="sect1" id="req-ses-other" data-id-title="SUSE Enterprise Storage 6 and Other SUSE Products"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">SUSE Enterprise Storage 6 and Other SUSE Products</span> <a title="Permalink" class="permalink" href="#req-ses-other">#</a></h2></div></div></div><p>
   This section contains important information about integrating SUSE Enterprise Storage
   6 with other SUSE products.
  </p><section class="sect2" id="req-ses-suma" data-id-title="SUSE Manager"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.10.1 </span><span class="title-name">SUSE Manager</span> <a title="Permalink" class="permalink" href="#req-ses-suma">#</a></h3></div></div></div><p>
    SUSE Manager and SUSE Enterprise Storage are not integrated, therefore SUSE Manager cannot
    currently manage a SUSE Enterprise Storage cluster.
   </p></section></section><section class="sect1" id="sysreq-naming" data-id-title="Naming Limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Naming Limitations</span> <a title="Permalink" class="permalink" href="#sysreq-naming">#</a></h2></div></div></div><p>
   Ceph does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a Ceph
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all Ceph
   object/configuration names.
  </p></section><section class="sect1" id="ses-bp-diskshare" data-id-title="OSD and Monitor Sharing One Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">OSD and Monitor Sharing One Server</span> <a title="Permalink" class="permalink" href="#ses-bp-diskshare">#</a></h2></div></div></div><p>
   Although it is technically possible to run Ceph OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance—the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </p><p>
   Another consideration is whether to share disks between an OSD, a monitor
   node, and the operating system on the server. The answer is simple: if
   possible, dedicate a separate disk to OSD, and a separate server to a
   monitor node.
  </p><p>
   Although Ceph supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </p><div id="id-1.4.3.3.16.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If it is <span class="emphasis"><em>really</em></span> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <code class="filename">/var/lib/ceph/mon</code> directory for slightly better
    performance.
   </p></div></section></section><section class="chapter" id="cha-admin-ha" data-id-title="Admin Node HA Setup"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Admin Node HA Setup</span> <a title="Permalink" class="permalink" href="#cha-admin-ha">#</a></h2></div></div></div><p>
  The <span class="emphasis"><em>Admin Node</em></span> is a Ceph cluster node where the Salt master
  service runs. The Admin Node is a central point of the Ceph cluster because it
  manages the rest of the cluster nodes by querying and instructing their
  Salt minion services. It usually includes other services as well, for example
  the <span class="emphasis"><em>Grafana</em></span> dashboard backed by the
  <span class="emphasis"><em>Prometheus</em></span> monitoring toolkit.
 </p><p>
  In case of Admin Node failure, you usually need to provide new working hardware
  for the node and restore the complete cluster configuration stack from a
  recent backup. Such a method is time consuming and causes cluster outage.
 </p><p>
  To prevent the Ceph cluster performance downtime caused by the Admin Node
  failure, we recommend making use of a High Availability (HA) cluster for the Ceph Admin Node.
 </p><section class="sect1" id="admin-ha-architecture" data-id-title="Outline of the HA Cluster for Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA Cluster for Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-architecture">#</a></h2></div></div></div><p>
   The idea of an HA cluster is that in case of one cluster node failing, the
   other node automatically takes over its role, including the virtualized
   Admin Node. This way, other Ceph cluster nodes do not notice that the Admin Node
   failed.
  </p><p>
   The minimal HA solution for the Admin Node requires the following hardware:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Two bare metal servers able to run SUSE Linux Enterprise with the High Availability extension and
     virtualize the Admin Node.
    </p></li><li class="listitem"><p>
     Two or more redundant network communication paths, for example via Network
     Device Bonding.
    </p></li><li class="listitem"><p>
     Shared storage to host the disk image(s) of the Admin Node virtual machine. The
     shared storage needs to be accessible from both servers. It can be, for
     example, an NFS export, a Samba share, or iSCSI target.
    </p></li></ul></div><p>
   Find more details on the cluster requirements at
   <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html#sec-ha-inst-quick-req" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html#sec-ha-inst-quick-req</a>.
  </p><div class="figure" id="id-1.4.3.4.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_admin_ha1.png" target="_blank"><img src="images/ceph_admin_ha1.png" width="" alt="2-Node HA Cluster for Admin Node"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.1: </span><span class="title-name">2-Node HA Cluster for Admin Node </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.6.6">#</a></h6></div></div></section><section class="sect1" id="admin-ha-cluster" data-id-title="Building a HA Cluster with Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Building a HA Cluster with Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-cluster">#</a></h2></div></div></div><p>
   The following procedure summarizes the most important steps of building the
   HA cluster for virtualizing the Admin Node. For details, refer to the indicated
   links.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up a basic 2-node HA cluster with shared storage as described in
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html</a>.
    </p></li><li class="step"><p>
     On both cluster nodes, install all packages required for running the KVM
     hypervisor and the <code class="systemitem">libvirt</code> toolkit as described in
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-vt-installation.html#sec-vt-installation-kvm" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-vt-installation.html#sec-vt-installation-kvm</a>.
    </p></li><li class="step"><p>
     On the first cluster node, create a new KVM virtual machine (VM) making
     use of <code class="systemitem">libvirt</code> as described in
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-kvm-inst.html#sec-libvirt-inst-virt-install" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-kvm-inst.html#sec-libvirt-inst-virt-install</a>.
     Use the preconfigured shared storage to store the disk images of the VM.
    </p></li><li class="step"><p>
     After the VM setup is complete, export its configuration to an XML file on
     the shared storage. Use the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh dumpxml <em class="replaceable">VM_NAME</em> &gt; /path/to/shared/vm_name.xml</pre></div></li><li class="step"><p>
     Create a resource for the Admin Node VM. Refer to
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-conf-hawk2.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-conf-hawk2.html</a>
     for general info on creating HA resources. Detailed info on creating
     resources for a KVM virtual machine is described in
     <a class="link" href="http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29" target="_blank">http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29</a>.
    </p></li><li class="step"><p>
     On the newly-created VM guest, deploy the Admin Node including the additional
     services you need there. Follow the relevant steps in
     <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>. At the same time, deploy the
     remaining Ceph cluster nodes on the non-HA cluster servers.
    </p></li></ol></div></div></section></section><section class="chapter" id="id-1.4.3.5" data-id-title="User Privileges and Command Prompts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">User Privileges and Command Prompts</span> <a title="Permalink" class="permalink" href="#id-1.4.3.5">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect1" id="id-1.4.3.5.4" data-id-title="Salt/DeepSea Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Salt/DeepSea Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.4.3.5.4">#</a></h2></div></div></div><p>
   These commands help you to deploy or upgrade the Ceph cluster, run
   commands on several (or all) cluster nodes at the same time, or assist you
   when adding or removing cluster nodes. The most frequently used are
   <code class="command">salt</code>, <code class="command">salt-run</code>, and
   <code class="command">deepsea</code>. You need to run Salt commands on the Salt master
   node (refer to <a class="xref" href="#deepsea-description" title="5.2. Introduction to DeepSea">Section 5.2, “Introduction to DeepSea”</a> for details) as
   <code class="systemitem">root</code>. These commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div></section><section class="sect1" id="id-1.4.3.5.5" data-id-title="Ceph Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Ceph Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.4.3.5.5">#</a></h2></div></div></div><p>
   These are lower level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">rbd</code>,
   <code class="command">radosgw-admin</code>, or <code class="command">crushtool</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   Safer and recommended option is to create a more restrictive individual key
   for each administrator user and put it in a directory where the users can
   read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.4.3.5.5.6" data-id-title="Path to Ceph Keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph Keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephadm</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth list</pre></div><div id="id-1.4.3.5.5.11" data-id-title="Commands for Specific Nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for Specific Nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code></pre></div></div></section><section class="sect1" id="id-1.4.3.5.6" data-id-title="General Linux Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">General Linux Commands</span> <a title="Permalink" class="permalink" href="#id-1.4.3.5.6">#</a></h2></div></div></div><p>
   Linux commands not related to Ceph or DeepSea, such as
   <code class="command">mount</code>, <code class="command">cat</code>, or
   <code class="command">openssl</code>, are introduced either with the <code class="prompt user">cephadm@adm &gt; </code>
   or <code class="prompt user">root # </code> prompts, depending on which privileges the related command
   requires.
  </p></section><section class="sect1" id="id-1.4.3.5.7" data-id-title="Additional Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.4 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#id-1.4.3.5.7">#</a></h2></div></div></div><p>
   For more information on Ceph key management, refer to
   <span class="intraxref">Book “Administration Guide”, Chapter 19 “Authentication with <code class="systemitem">cephx</code>”, Section 19.2 “Key Management”</span>.
  </p></section></section></div><div class="part" id="ses-deployment" data-id-title="Cluster Deployment and Upgrade"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Cluster Deployment and Upgrade </span><a title="Permalink" class="permalink" href="#ses-deployment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#ceph-install-saltstack"><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span></a></span></li><dd class="toc-abstract"><p>
  Salt along with DeepSea is a <span class="emphasis"><em>stack</em></span> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with Salt:
 </p></dd><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">6 </span><span class="title-name">Upgrading from Previous Releases</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage 5.5 to
  version 6. Note that version 5.5 is basically 5
  with all latest patches applied.
 </p></dd><li><span class="chapter"><a href="#ceph-deploy-ds-custom"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span></a></span></li><dd class="toc-abstract"><p>You can change the default cluster configuration generated in Stage 2 (refer to DeepSea Stages Description). For example, you may need to change network settings, or software that is installed on the Admin Node by default. You can perform the former by modifying the pillar updated after Stage 2, whi…</p></dd></ul></div><section class="chapter" id="ceph-install-saltstack" data-id-title="Deploying with DeepSea/Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Deploying with DeepSea/Salt</span> <a title="Permalink" class="permalink" href="#ceph-install-saltstack">#</a></h2></div></div></div><p>
  Salt along with DeepSea is a <span class="emphasis"><em>stack</em></span> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with Salt:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Salt minions</em></span> are the nodes controlled by a dedicated
    node called Salt master. Salt minions have roles, for example Ceph OSD, Ceph Monitor,
    Ceph Manager, Object Gateway, iSCSI Gateway, or NFS Ganesha.
   </p></li><li class="listitem"><p>
    A Salt master runs its own Salt minion. It is required for running privileged
    tasks—for example creating, authorizing, and copying keys to
    minions—so that remote minions never need to run privileged tasks.
   </p><div id="id-1.4.4.2.4.2.2" data-id-title="Sharing Multiple Roles per Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing Multiple Roles per Server</h6><p>
     You will get the best performance from your Ceph cluster when each role
     is deployed on a separate node. But real deployments sometimes require
     sharing one node for multiple roles. To avoid trouble with performance and
     the upgrade procedure, do not deploy the Ceph OSD, Metadata Server, or Ceph Monitor role to
     the Admin Node.
    </p></div></li><li class="listitem"><p>
    Salt minions need to correctly resolve the Salt master's host name over the
    network. By default, they look for the <code class="systemitem">salt</code> host
    name, but you can specify any other network-reachable host name in the
    <code class="filename">/etc/salt/minion</code> file, see
    <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>.
   </p></li></ul></div><section class="sect1" id="cha-ceph-install-relnotes" data-id-title="Read the Release Notes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Read the Release Notes</span> <a title="Permalink" class="permalink" href="#cha-ceph-install-relnotes">#</a></h2></div></div></div><p>
   In the release notes you can find additional information on changes since
   the previous release of SUSE Enterprise Storage. Check the release notes to see
   whether:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     your hardware needs special considerations.
    </p></li><li class="listitem"><p>
     any used software packages have changed significantly.
    </p></li><li class="listitem"><p>
     special precautions are necessary for your installation.
    </p></li></ul></div><p>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </p><p>
   After having installed the package <span class="package">release-notes-ses</span>,
   find the release notes locally in the directory
   <code class="filename">/usr/share/doc/release-notes</code> or online at
   <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section><section class="sect1" id="deepsea-description" data-id-title="Introduction to DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Introduction to DeepSea</span> <a title="Permalink" class="permalink" href="#deepsea-description">#</a></h2></div></div></div><p>
   The goal of DeepSea is to save the administrator time and confidently
   perform complex operations on a Ceph cluster.
  </p><p>
   Ceph is a very configurable software solution. It increases both the
   freedom and responsibility of system administrators.
  </p><p>
   The minimal Ceph setup is good for demonstration purposes, but does not
   show interesting features of Ceph that you can see with a big number of
   nodes.
  </p><p>
   DeepSea collects and stores data about individual servers, such as
   addresses and device names. For a distributed storage system such as Ceph,
   there can be hundreds of such items to collect and store. Collecting the
   information and entering the data manually into a configuration management
   tool is exhausting and error prone.
  </p><p>
   The steps necessary to prepare the servers, collect the configuration, and
   configure and deploy Ceph are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </p><p>
   DeepSea addresses these observations with the following strategy:
   DeepSea consolidates the administrator's decisions in a single file. The
   decisions include cluster assignment, role assignment and profile
   assignment. And DeepSea collects each set of tasks into a simple goal.
   Each goal is a <span class="emphasis"><em>stage</em></span>:
  </p><div class="itemizedlist" id="deepsea-stage-description" data-id-title="DeepSea Stages Description"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea Stages Description </span><a title="Permalink" class="permalink" href="#deepsea-stage-description">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Stage 0</strong></span>—the
     <span class="bold"><strong>preparation</strong></span>— during this stage, all
     required updates are applied and your system may be rebooted.
    </p><div id="id-1.4.4.2.6.8.2.2" data-id-title="Re-run Stage 0 after the Admin Node Reboot" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Re-run Stage 0 after the Admin Node Reboot</h6><p>
      If the Admin Node reboots during stage 0 to load the new kernel version, you
      need to run stage 0 again, otherwise minions will not be targeted.
     </p></div></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 1</strong></span>—the
     <span class="bold"><strong>discovery</strong></span>—here all hardware in your
     cluster is being detected and necessary information for the Ceph
     configuration is being collected. For details about configuration, refer
     to <a class="xref" href="#deepsea-pillar-salt-configuration" title="5.5. Configuration and Customization">Section 5.5, “Configuration and Customization”</a>.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 2</strong></span>—the
     <span class="bold"><strong>configuration</strong></span>—you need to prepare
     configuration data in a particular format.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 3</strong></span>—the
     <span class="bold"><strong>deployment</strong></span>—creates a basic Ceph
     cluster with mandatory Ceph services. See
     <a class="xref" href="#storage-intro-core-nodes" title="1.2.3. Ceph Nodes and Daemons">Section 1.2.3, “Ceph Nodes and Daemons”</a> for their list.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 4</strong></span>—the
     <span class="bold"><strong>services</strong></span>—additional features of
     Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each
     is optional.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stage 5</strong></span>—the removal stage. This
     stage is not mandatory and during the initial setup it is usually not
     needed. In this stage the roles of minions and also the cluster
     configuration are removed. You need to run this stage when you need to
     remove a storage node from your cluster. For details refer to
     <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.3 “Removing and Reinstalling Cluster Nodes”</span>.
    </p></li></ul></div><section class="sect2" id="deepsea-organisation-locations" data-id-title="Organization and Important Locations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.1 </span><span class="title-name">Organization and Important Locations</span> <a title="Permalink" class="permalink" href="#deepsea-organisation-locations">#</a></h3></div></div></div><p>
    Salt has several standard locations and several naming conventions used
    on your master node:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.6.9.3.1"><span class="term"><code class="filename">/srv/pillar</code></span></dt><dd><p>
       The directory stores configuration data for your cluster minions.
       <span class="emphasis"><em>Pillar</em></span> is an interface for providing global
       configuration values to all your cluster minions.
      </p></dd><dt id="id-1.4.4.2.6.9.3.2"><span class="term"><code class="filename">/srv/salt/</code></span></dt><dd><p>
       The directory stores Salt state files (also called
       <span class="emphasis"><em>sls</em></span> files). State files are formatted descriptions
       of states in which the cluster should be.
       
      </p></dd><dt id="id-1.4.4.2.6.9.3.3"><span class="term"><code class="filename">/srv/module/runners</code></span></dt><dd><p>
       The directory stores Python scripts known as runners. Runners are
       executed on the master node.
      </p></dd><dt id="id-1.4.4.2.6.9.3.4"><span class="term"><code class="filename">/srv/salt/_modules</code></span></dt><dd><p>
       The directory stores Python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </p></dd><dt id="id-1.4.4.2.6.9.3.5"><span class="term"><code class="filename">/srv/pillar/ceph</code></span></dt><dd><p>
       The directory is used by DeepSea. Collected configuration data are
       stored here.
      </p></dd><dt id="id-1.4.4.2.6.9.3.6"><span class="term"><code class="filename">/srv/salt/ceph</code></span></dt><dd><p>
       A directory used by DeepSea. It stores sls files that can be in
       different formats, but each subdirectory contains sls files. Each
       subdirectory contains only one type of sls file. For example,
       <code class="filename">/srv/salt/ceph/stage</code> contains orchestration files
       that are executed by <code class="command">salt-run state.orchestrate</code>.
      </p></dd></dl></div></section><section class="sect2" id="ds-minion-targeting" data-id-title="Targeting the Minions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.2 </span><span class="title-name">Targeting the Minions</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting">#</a></h3></div></div></div><p>
    DeepSea commands are executed via the Salt infrastructure. When using
    the <code class="command">salt</code> command, you need to specify a set of
    Salt minions that the command will affect. We describe the set of the minions
    as a <span class="emphasis"><em>target</em></span> for the <code class="command">salt</code> command.
    The following sections describe possible methods to target the minions.
   </p><section class="sect3" id="ds-minion-targeting-name" data-id-title="Matching the Minion Name"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.1 </span><span class="title-name">Matching the Minion Name</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-name">#</a></h4></div></div></div><p>
     You can target a minion or a group of minions by matching their names. A
     minion's name is usually the short host name of the node where the minion
     runs. This is a general Salt targeting method, not related to DeepSea.
     You can use globbing, regular expressions, or lists to limit the range of
     minion names. The general syntax follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> example.module</pre></div><div id="id-1.4.4.2.6.10.3.4" data-id-title="Ceph-only Cluster" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Ceph-only Cluster</h6><p>
      If all Salt minions in your environment belong to your Ceph cluster, you
      can safely substitute <em class="replaceable">target</em> with
      <code class="literal">'*'</code> to include <span class="emphasis"><em>all</em></span> registered
      minions.
     </p></div><p>
     Match all minions in the example.net domain (assuming the minion names are
     identical to their "full" host names):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div><p>
     Match the 'web1' to 'web5' minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'web[1-5]' test.ping</pre></div><p>
     Match both 'web1-prod' and 'web1-devel' minions using a regular
     expression:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -E 'web1-(prod|devel)' test.ping</pre></div><p>
     Match a simple list of minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -L 'web1,web2,web3' test.ping</pre></div><p>
     Match all minions in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-grain" data-id-title="Targeting with a DeepSea Grain"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.2 </span><span class="title-name">Targeting with a DeepSea Grain</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-grain">#</a></h4></div></div></div><p>
     In a heterogeneous Salt-managed environment where SUSE Enterprise Storage
     6 is deployed on a subset of nodes alongside other cluster
     solutions, you need to mark the relevant minions by applying a 'deepsea'
     grain to them before running DeepSea stage 0. This way, you can easily
     target DeepSea minions in environments where matching by the minion name
     is problematic.
    </p><p>
     To apply the 'deepsea' grain to a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.append deepsea default</pre></div><p>
     To remove the 'deepsea' grain from a group of minions, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> grains.delval deepsea destructive=True</pre></div><p>
     After applying the 'deepsea' grain to the relevant minions, you can target
     them as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -G 'deepsea:*' test.ping</pre></div><p>
     The following command is an equivalent:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'G@deepsea:*' test.ping</pre></div></section><section class="sect3" id="ds-minion-targeting-dsminions" data-id-title="Set the deepsea_minions Option"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.3 </span><span class="title-name">Set the <code class="option">deepsea_minions</code> Option</span> <a title="Permalink" class="permalink" href="#ds-minion-targeting-dsminions">#</a></h4></div></div></div><p>
     Setting the <code class="option">deepsea_minions</code> option's target is a
     requirement for DeepSea deployments. DeepSea uses it to instruct
     minions during the execution of stages (refer to
     <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> for details.
    </p><p>
     To set or change the <code class="option">deepsea_minions</code> option, edit the
     <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code> file on the
     Salt master and add or replace the following line:
    </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: <em class="replaceable">target</em></pre></div><div id="id-1.4.4.2.6.10.5.5" data-id-title="deepsea_minions Target" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="option">deepsea_minions</code> Target</h6><p>
      As the <em class="replaceable">target</em> for the
      <code class="option">deepsea_minions</code> option, you can use any targeting
      method: both
      <a class="xref" href="#ds-minion-targeting-name" title="5.2.2.1. Matching the Minion Name">Matching the Minion Name</a> and
      <a class="xref" href="#ds-minion-targeting-grain" title="5.2.2.2. Targeting with a DeepSea Grain">Targeting with a DeepSea Grain</a>.
     </p><p>
      Match all Salt minions in the cluster:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: '*'</pre></div><p>
      Match all minions with the 'deepsea' grain:
     </p><div class="verbatim-wrap"><pre class="screen">deepsea_minions: 'G@deepsea:*'</pre></div></div></section><section class="sect3" id="id-1.4.4.2.6.10.6" data-id-title="For More Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.2.4 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.4.4.2.6.10.6">#</a></h4></div></div></div><p>
     You can use more advanced ways to target minions using the Salt
     infrastructure. The 'deepsea-minions' manual page gives you more details
     about DeepSea targeting (<code class="command">man 7 deepsea_minions</code>).
    </p></section></section></section><section class="sect1" id="ceph-install-stack" data-id-title="Cluster Deployment"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.3 </span><span class="title-name">Cluster Deployment</span> <a title="Permalink" class="permalink" href="#ceph-install-stack">#</a></h2></div></div></div><p>
   The cluster deployment process has several phases. First, you need to
   prepare all nodes of the cluster by configuring Salt and then deploy and
   configure Ceph.
  </p><div id="dev-env" data-id-title="Deploying Monitor Nodes without Defining OSD Profiles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Deploying Monitor Nodes without Defining OSD Profiles</h6><p>
    If you need to skip defining storage roles for OSD as described in
    <a class="xref" href="#policy-role-assignment" title="5.5.1.2. Role Assignment">Section 5.5.1.2, “Role Assignment”</a> and deploy Ceph Monitor nodes first, you
    can do so by setting the <code class="option">DEV_ENV</code> variable.
   </p><p>
    This allows deploying monitors without the presence of the
    <code class="filename">role-storage/</code> directory, as well as deploying a Ceph
    cluster with at least <span class="emphasis"><em>one</em></span> storage, monitor, and
    manager role.
   </p><p>
    To set the environment variable, either enable it globally by setting it in
    the <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file, or set it
    for the current shell session only:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>export DEV_ENV=true</pre></div><p>
    As an example, <code class="filename">/srv/pillar/ceph/stack/global.yml</code> can
    be created with the following contents:
   </p><div class="verbatim-wrap"><pre class="screen">DEV_ENV: <em class="replaceable">True</em></pre></div></div><p>
   The following procedure describes the cluster preparation in detail.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install and register SUSE Linux Enterprise Server 15 SP1 together with the SUSE Enterprise Storage
     6 extension on each node of the cluster.
    </p></li><li class="step"><p>
     Verify that proper products are installed and registered by listing
     existing software repositories. Run <code class="command">zypper lr -E</code> and
     compare the output with the following list:
    </p><div class="verbatim-wrap"><pre class="screen"> SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates</pre></div></li><li class="step"><p>
     Configure network settings including proper DNS name resolution on each
     node. The Salt master and all the Salt minions need to resolve each other by
     their host names. For more information on configuring a network, see
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-yast" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-yast</a>
     For more information on configuring a DNS server, see
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-dns.html</a>.
    </p><div id="id-1.4.4.2.7.5.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If cluster nodes are configured for multiple networks, DeepSea will use
      the network to which their host names (or FQDN) resolves. Consider the
      following example <code class="filename">/etc/hosts</code>:
     </p><div class="verbatim-wrap"><pre class="screen">192.168.100.1 ses1.example.com       ses1
172.16.100.1  ses1clus.cluster.lan   ses1clus</pre></div><p>
      In the above example, the <code class="literal">ses1</code> minion will resolve to
      the <code class="literal">192.168.100.x</code> network and DeepSea will use this
      network as the <span class="emphasis"><em>public network</em></span>. If the desired public
      network is <code class="literal">172.16.100.x</code>, then host name should be
      changed to <code class="literal">ses1clus</code>.
     </p></div></li><li class="step"><p>
     Install the <code class="literal">salt-master</code> and
     <code class="literal">salt-minion</code> packages on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in salt-master salt-minion</pre></div><p>
     Check that the <code class="systemitem">salt-master</code> service is enabled and
     started, and enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl enable salt-master.service
<code class="prompt user">root@master # </code>systemctl start salt-master.service</pre></div></li><li class="step"><p>
     If you intend to use firewall, verify that the Salt master node has ports
     4505 and 4506 open to all Salt minion nodes. If the ports are closed, you
     can open them using the <code class="command">yast2 firewall</code> command by
     allowing the <span class="guimenu">SaltStack</span> service.
    </p><div id="id-1.4.4.2.7.5.5.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
      DeepSea deployment stages fail when firewall is active (and even
      configured). To pass the stages correctly, you need to either turn the
      firewall off by running
     </p><div class="verbatim-wrap"><pre class="screen">    <code class="prompt user">root # </code>systemctl stop firewalld.service</pre></div><p>
      or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div></li><li class="step"><p>
     Install the package <code class="literal">salt-minion</code> on all minion nodes.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div><p>
     Make sure that the <span class="emphasis"><em>fully qualified domain name</em></span> of
     each node can be resolved to the public network IP address by all other
     nodes.
    </p></li><li class="step"><p>
     Configure all minions (including the master minion) to connect to the
     master. If your Salt master is not reachable by the host name
     <code class="literal">salt</code>, edit the file
     <code class="filename">/etc/salt/minion</code> or create a new file
     <code class="filename">/etc/salt/minion.d/master.conf</code> with the following
     content:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">host_name_of_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the Salt service on all Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     Check that the <code class="systemitem">salt-minion</code> service is enabled and
     started on all nodes. Enable and start it if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable salt-minion.service
<code class="prompt user">root # </code>systemctl start salt-minion.service</pre></div></li><li class="step"><p>
     Verify each Salt minion's fingerprint and accept all salt keys on the
     Salt master if the fingerprints match.
    </p><div id="id-1.4.4.2.7.5.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If the Salt minion fingerprint comes back empty, make sure the Salt minion
      has a Salt master configuration and it can communicate with the Salt master.
     </p></div><p>
     View each minion's fingerprint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     After gathering fingerprints of all the Salt minions, list fingerprints of
     all unaccepted minion keys on the Salt master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
     If the minions' fingerprints match, accept them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept-all</pre></div></li><li class="step"><p>
     Verify that the keys have been accepted:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --list-all</pre></div></li><li class="step"><p>
     By default, DeepSea uses the Admin Node as the time server for other cluster
     nodes. Therefore, if the Admin Node is not virtualized, select one or more time
     servers or pools, and synchronize the local time against them. Verify that
     the time synchronization service is enabled on each system start-up. Find
     more information on setting up time synchronization in
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html#sec-ntp-yast" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html#sec-ntp-yast</a>.
    </p><p>
     If the Admin Node is a virtual machine, provide better time sources for the
     cluster nodes by overriding the default NTP client configuration:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> on the
       Salt master node and add the following line:
      </p><div class="verbatim-wrap"><pre class="screen">time_server: <em class="replaceable">CUSTOM_NTP_SERVER</em></pre></div><p>
       To add multiple time servers, the format is as follows:
      </p><div class="verbatim-wrap"><pre class="screen">time_server:
  - <em class="replaceable">CUSTOM_NTP_SERVER1</em>
  - <em class="replaceable">CUSTOM_NTP_SERVER2</em>
  - <em class="replaceable">CUSTOM_NTP_SERVER3</em>
[...]</pre></div></li><li class="listitem"><p>
       Refresh the Salt pillar:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="listitem"><p>
       Verify the changed value:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' pillar.items</pre></div></li><li class="listitem"><p>
       Apply the new setting:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' state.apply ceph.time</pre></div></li></ol></div></li><li class="step" id="deploy-wiping-disk"><p>
     Prior to deploying SUSE Enterprise Storage 6, manually zap all the
     disks. Remember to replace 'X' with the correct disk letter:
    </p><ol type="a" class="substeps"><li class="step"><p>
       Stop all processes that are using the specific disk.
      </p></li><li class="step"><p>
       Verify whether any partition on the disk is mounted, and unmount if
       needed.
      </p></li><li class="step"><p>
       If the disk is managed by LVM, deactivate and delete the whole LVM
       infrastructure. Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-lvm.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-lvm.html</a>
       for more details.
      </p></li><li class="step"><p>
       If the disk is part of MD RAID, deactivate the RAID. Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-software-raid.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-software-raid.html</a>
       for more details.
      </p></li><li class="step"><div id="id-1.4.4.2.7.5.12.2.5.1" data-id-title="Rebooting the Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Rebooting the Server</h6><p>
        If you get error messages such as 'partition in use' or 'kernel cannot
        be updated with the new partition table' during the following steps,
        reboot the server.
       </p></div><p>
       Wipe data and partitions on the disk:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-volume lvm zap /dev/sdX --destroy</pre></div></li><li class="step"><p>
       Verify that the drive is empty (with no GPT structures) using:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>parted -s /dev/sdX print free</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<code class="prompt user">root # </code>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C</pre></div></li></ol></li><li class="step"><p>
     Optionally, if you need to preconfigure the cluster's network settings
     before the <span class="package">deepsea</span> package is installed, create
     <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> manually and
     set the <code class="option">cluster_network:</code> and
     <code class="option">public_network:</code> options. Note that the file will not be
     overwritten after you install <span class="package">deepsea</span>. Then, run:
    </p><div class="verbatim-wrap"><pre class="screen">chown -R salt:salt /srv/pillar/ceph/stack</pre></div><div id="id-1.4.4.2.7.5.13.3" data-id-title="Enabling IPv6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Enabling IPv6</h6><p>
      If you need to enable IPv6 network addressing, refer to
      <a class="xref" href="#ds-modify-ipv6" title="7.2.1. Enabling IPv6 for Ceph Cluster Deployment">Section 7.2.1, “Enabling IPv6 for Ceph Cluster Deployment”</a>
     </p></div></li><li class="step"><p>
     Install DeepSea on the Salt master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in deepsea</pre></div></li><li class="step"><p>
     The value of the <code class="option">master_minion</code> parameter is dynamically
     derived from the <code class="filename">/etc/salt/minion_id</code> file on the
     Salt master. If you need to override the discovered value, edit the file
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and set a relevant
     value:
    </p><div class="verbatim-wrap"><pre class="screen">master_minion: <em class="replaceable">MASTER_MINION_NAME</em></pre></div><p>
     If your Salt master is reachable via more host names, use the Salt minion name
     for the storage cluster as returned by the <code class="command">salt-key -L</code>
     command. If you used the default host name for your
     Salt master—<span class="emphasis"><em>salt</em></span>—in the
     <span class="emphasis"><em>ses</em></span> domain, then the file looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">master_minion: salt.ses</pre></div></li></ol></div></div><p>
   Now you deploy and configure Ceph. Unless specified otherwise, all steps
   are mandatory.
  </p><div id="id-1.4.4.2.7.7" data-id-title="Salt Command Conventions" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Salt Command Conventions</h6><p>
    There are two possible ways to run <code class="command">salt-run
    state.orch</code>—one is with
    'stage.<em class="replaceable">STAGE_NUMBER</em>', the other is with the name
    of the stage. Both notations have the same impact and it is fully your
    preference which command you use.
   </p></div><div class="procedure" id="ds-depl-stages" data-id-title="Running Deployment Stages"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 5.1: </span><span class="title-name">Running Deployment Stages </span><a title="Permalink" class="permalink" href="#ds-depl-stages">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Ensure the Salt minions belonging to the Ceph cluster are correctly
     targeted through the <code class="option">deepsea_minions</code> option in
     <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code>. Refer to
     <a class="xref" href="#ds-minion-targeting-dsminions" title="5.2.2.3. Set the deepsea_minions Option">Section 5.2.2.3, “Set the <code class="option">deepsea_minions</code> Option”</a> for more information.
    </p></li><li class="step"><p>
     By default, DeepSea deploys Ceph clusters with tuned profiles active
     on Ceph Monitor, Ceph Manager, and Ceph OSD nodes. In some cases, you may need to deploy
     without tuned profiles. To do so, put the following lines in
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> before running
     DeepSea stages:
    </p><div class="verbatim-wrap"><pre class="screen">alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off</pre></div></li><li class="step"><p>
     <span class="emphasis"><em>Optional</em></span>: create Btrfs sub-volumes for
     <code class="filename">/var/lib/ceph/</code>. This step needs to be executed before
     DeepSea stage.0. To migrate existing directories or for more details,
     see <span class="intraxref">Book “Administration Guide”, Chapter 33 “Hints and Tips”, Section 33.6 “Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes”</span>.
    </p><p>
     Apply the following commands to each of the Salt minions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'MONITOR_NODES' saltutil.sync_all
<code class="prompt user">root@master # </code>salt 'MONITOR_NODES' state.apply ceph.subvolume</pre></div><div id="id-1.4.4.2.7.8.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The Ceph.subvolume command creates <code class="filename">/var/lib/ceph</code>
      as a <code class="filename">@/var/lib/ceph</code> Btrfs subvolume.
     </p></div><p>
     The new subvolume is now mounted and <code class="literal">/etc/fstab</code> is
     updated.
    </p></li><li class="step"><p>
     Prepare your cluster. Refer to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>
     for more details.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.prep</pre></div><div id="id-1.4.4.2.7.8.5.5" data-id-title="Run or Monitor Stages using DeepSea CLI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Run or Monitor Stages using DeepSea CLI</h6><p>
      Using the DeepSea CLI, you can follow the stage execution progress in
      real-time, either by running the DeepSea CLI in the monitoring mode, or
      by running the stage directly through DeepSea CLI. For details refer to
      <a class="xref" href="#deepsea-cli" title="5.4. DeepSea CLI">Section 5.4, “DeepSea CLI”</a>.
     </p></div></li><li class="step"><p>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <code class="filename">/srv/pillar/ceph/proposals</code>. The data are stored in
     the YAML format in *.sls or *.yml files.
    </p><p>
     Run the following command to trigger the discovery stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.discovery</pre></div></li><li class="step"><p>
     After the previous command finishes successfully, create a
     <code class="filename">policy.cfg</code> file in
     <code class="filename">/srv/pillar/ceph/proposals</code>. For details refer to
     <a class="xref" href="#policy-configuration" title="5.5.1. The policy.cfg File">Section 5.5.1, “The <code class="filename">policy.cfg</code> File”</a>.
    </p><div id="id-1.4.4.2.7.8.7.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you need to change the cluster's network setting, edit
      <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code> and adjust
      the lines starting with <code class="literal">cluster_network:</code> and
      <code class="literal">public_network:</code>.
     </p></div></li><li class="step"><p>
     The configuration stage parses the <code class="filename">policy.cfg</code> file
     and merges the included files into their final form. Cluster and role
     related content are placed in
     <code class="filename">/srv/pillar/ceph/cluster</code>, while Ceph specific
     content is placed in <code class="filename">/srv/pillar/ceph/stack/default</code>.
    </p><p>
     Run the following command to trigger the configuration stage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.configure</pre></div><p>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for the specified minions (for
     example, named <code class="literal">ceph_minion1</code>,
     <code class="literal">ceph_minion2</code>, etc.) by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'ceph_minion*' pillar.items</pre></div><div id="id-1.4.4.2.7.8.8.8" data-id-title="Modifying OSDs Layout" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Modifying OSD's Layout</h6><p>
      If you want to modify the default OSD's layout and change the drive
      groups configuration, follow the procedure described in
      <a class="xref" href="#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a>.
     </p></div><div id="id-1.4.4.2.7.8.8.9" data-id-title="Overwriting Defaults" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Overwriting Defaults</h6><p>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <a class="xref" href="#ceph-deploy-ds-custom" title="Chapter 7. Customizing the Default Configuration">Chapter 7, <em>Customizing the Default Configuration</em></a>.
     </p></div></li><li class="step"><p>
     Now you run the deployment stage. In this stage, the pillar is validated,
     and the Ceph Monitor and Ceph OSD daemons are started:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.deploy</pre></div><p>
     The command may take several minutes. If it fails, you need to fix the
     issue and run the previous stages again. After the command succeeds, run
     the following to check the status:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -s</pre></div></li><li class="step"><p>
     The last step of the Ceph cluster deployment is the
     <span class="emphasis"><em>services</em></span> stage. Here you instantiate any of the
     currently supported services: iSCSI Gateway, CephFS, Object Gateway, and NFS Ganesha. In
     this stage, the necessary pools, authorizing keyrings, and starting
     services are created. To start the stage, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.services</pre></div><p>
     Depending on the setup, the command may run for several minutes.
    </p></li><li class="step"><p>
     <span class="emphasis"><em>Disable insecure clients</em></span>. Since Nautilus v14.2.20,
     a new health warning was introduced that informs you that insecure clients
     are allowed to join the cluster. This warning is <span class="emphasis"><em>on</em></span>
     by default. The Ceph Dashboard will show the cluster in the
     <code class="literal">HEALTH_WARN</code> status and verifying the cluster status on
     the command line informs you as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]</pre></div><p>
     This warning means that the Ceph Monitors are still allowing old, unpatched
     clients to connect to the cluster. This ensures existing clients can still
     connect while the cluster is being upgraded, but warns you that there is a
     problem that needs to be addressed. When the cluster and all clients are
     upgraded to the latest version of Ceph, disallow unpatched clients by
     running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></li><li class="step"><p>
     Before you continue, we strongly recommend enabling the Ceph telemetry
     module. For more information, see <span class="intraxref">Book “Administration Guide”, Chapter 21 “Ceph Manager Modules”, Section 21.2 “Telemetry Module”</span>
     for information and instructions.
    </p></li></ol></div></div></section><section class="sect1" id="deepsea-cli" data-id-title="DeepSea CLI"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.4 </span><span class="title-name">DeepSea CLI</span> <a title="Permalink" class="permalink" href="#deepsea-cli">#</a></h2></div></div></div><p>
   DeepSea also provides a command line interface (CLI) tool that allows the
   user to monitor or run stages while visualizing the execution progress in
   real-time. Verify that the <span class="package">deepsea-cli</span> package is
   installed before you run the <code class="command">deepsea</code> executable.
  </p><p>
   Two modes are supported for visualizing a stage's execution progress:
  </p><div class="itemizedlist" id="deepsea-cli-modes" data-id-title="DeepSea CLI Modes"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">DeepSea CLI Modes </span><a title="Permalink" class="permalink" href="#deepsea-cli-modes">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Monitoring mode</strong></span>: visualizes the execution
     progress of a DeepSea stage triggered by the <code class="command">salt-run</code>
     command issued in another terminal session.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Stand-alone mode</strong></span>: runs a DeepSea stage
     while providing real-time visualization of its component steps as they are
     executed.
    </p></li></ul></div><div id="id-1.4.4.2.8.5" data-id-title="DeepSea CLI Commands" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: DeepSea CLI Commands</h6><p>
    The DeepSea CLI commands can only be run on the Salt master node with the
    <code class="systemitem">root</code> privileges.
   </p></div><section class="sect2" id="deepsea-cli-monitor" data-id-title="DeepSea CLI: Monitor Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.1 </span><span class="title-name">DeepSea CLI: Monitor Mode</span> <a title="Permalink" class="permalink" href="#deepsea-cli-monitor">#</a></h3></div></div></div><p>
    The progress monitor provides a detailed, real-time visualization of what
    is happening during execution of stages using <code class="command">salt-run
    state.orch</code> commands in other terminal sessions.
   </p><div id="id-1.4.4.2.8.6.3" data-id-title="Start Monitor in a New Terminal Session" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Start Monitor in a New Terminal Session</h6><p>
     You need to start the monitor in a new terminal window
     <span class="emphasis"><em>before</em></span> running any <code class="command">salt-run
     state.orch</code> so that the monitor can detect the start of the
     stage's execution.
    </p></div><p>
    If you start the monitor after issuing the <code class="command">salt-run
    state.orch</code> command, then no execution progress will be shown.
   </p><p>
    You can start the monitor mode by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea monitor</pre></div><p>
    For more information about the available command line options of the
    <code class="command">deepsea monitor</code> command, check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>man deepsea-monitor</pre></div></section><section class="sect2" id="deepsea-cli-standalone" data-id-title="DeepSea CLI: Stand-alone Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.4.2 </span><span class="title-name">DeepSea CLI: Stand-alone Mode</span> <a title="Permalink" class="permalink" href="#deepsea-cli-standalone">#</a></h3></div></div></div><p>
    In the stand-alone mode, DeepSea CLI can be used to run a DeepSea
    stage, showing its execution in real-time.
   </p><p>
    The command to run a DeepSea stage from the DeepSea CLI has the
    following form:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea stage run <em class="replaceable">stage-name</em></pre></div><p>
    where <em class="replaceable">stage-name</em> corresponds to the way Salt
    orchestration state files are referenced. For example, stage
    <span class="bold"><strong>deploy</strong></span>, which corresponds to the directory
    located in <code class="filename">/srv/salt/ceph/stage/deploy</code>, is referenced
    as <span class="bold"><strong>ceph.stage.deploy</strong></span>.
   </p><p>
    This command is an alternative to the Salt-based commands for running
    DeepSea stages (or any DeepSea orchestration state file).
   </p><p>
    The command <code class="command">deepsea stage run ceph.stage.0</code> is equivalent
    to <code class="command">salt-run state.orch ceph.stage.0</code>.
   </p><p>
    For more information about the available command line options accepted by
    the <code class="command">deepsea stage run</code> command, check its manual page:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>man deepsea-stage run</pre></div><p>
    In the following figure shows an example of the output of the DeepSea CLI
    when running <span class="underline">Stage 2</span>:
   </p><div class="figure" id="id-1.4.4.2.8.7.11"><div class="figure-contents"><div class="mediaobject"><a href="images/deepsea-cli-stage2-screenshot.png" target="_blank"><img src="images/deepsea-cli-stage2-screenshot.png" width="" alt="DeepSea CLI stage execution progress output"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">DeepSea CLI stage execution progress output </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.8.7.11">#</a></h6></div></div><section class="sect3" id="deepsea-cli-run-alias" data-id-title="DeepSea CLI stage run Alias"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.4.2.1 </span><span class="title-name">DeepSea CLI <code class="command">stage run</code> Alias</span> <a title="Permalink" class="permalink" href="#deepsea-cli-run-alias">#</a></h4></div></div></div><p>
     For advanced users of Salt, we also support an alias for running a
     DeepSea stage that takes the Salt command used to run a stage, for
     example, <code class="command">salt-run state.orch
     <em class="replaceable">stage-name</em></code>, as a command of the
     DeepSea CLI.
    </p><p>
     Example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>deepsea salt-run state.orch <em class="replaceable">stage-name</em></pre></div></section></section></section><section class="sect1" id="deepsea-pillar-salt-configuration" data-id-title="Configuration and Customization"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.5 </span><span class="title-name">Configuration and Customization</span> <a title="Permalink" class="permalink" href="#deepsea-pillar-salt-configuration">#</a></h2></div></div></div><section class="sect2" id="policy-configuration" data-id-title="The policy.cfg File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.1 </span><span class="title-name">The <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="#policy-configuration">#</a></h3></div></div></div><p>
    The <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>
    configuration file is used to determine roles of individual cluster nodes.
    For example, which nodes act as Ceph OSDs or Ceph Monitors. Edit
    <code class="filename">policy.cfg</code> in order to reflect your desired cluster
    setup. The order of the sections is arbitrary, but the content of included
    lines overwrites matching keys from the content of previous lines.
   </p><div id="id-1.4.4.2.9.2.3" data-id-title="Examples of policy.cfg" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Examples of <code class="filename">policy.cfg</code></h6><p>
     You can find several examples of complete policy files in the
     <code class="filename">/usr/share/doc/packages/deepsea/examples/</code> directory.
    </p></div><section class="sect3" id="policy-cluster-assignment" data-id-title="Cluster Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.1 </span><span class="title-name">Cluster Assignment</span> <a title="Permalink" class="permalink" href="#policy-cluster-assignment">#</a></h4></div></div></div><p>
     In the <span class="bold"><strong>cluster</strong></span> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <span class="bold"><strong>ceph</strong></span> follow.
    </p><p>
     To include <span class="bold"><strong>all</strong></span> minions, add the following
     lines:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/*.sls</pre></div><p>
     To <span class="bold"><strong>whitelist</strong></span> a particular minion:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/abc.domain.sls</pre></div><p>
     or a group of minions—you can shell glob matching:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-ceph/cluster/mon*.sls</pre></div><p>
     To <span class="bold"><strong>blacklist</strong></span> minions, set the them to
     <code class="literal">unassigned</code>:
    </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/client*.sls</pre></div></section><section class="sect3" id="policy-role-assignment" data-id-title="Role Assignment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.2 </span><span class="title-name">Role Assignment</span> <a title="Permalink" class="permalink" href="#policy-role-assignment">#</a></h4></div></div></div><p>
     This section provides you with details on assigning 'roles' to your
     cluster nodes. A 'role' in this context means the service you need to run
     on the node, such as Ceph Monitor, Object Gateway, or iSCSI Gateway. No role is assigned
     automatically, only roles added to <code class="command">policy.cfg</code> will be
     deployed.
    </p><p>
     The assignment follows this pattern:
    </p><div class="verbatim-wrap"><pre class="screen">role-<em class="replaceable">ROLE_NAME</em>/<em class="replaceable">PATH</em>/<em class="replaceable">FILES_TO_INCLUDE</em></pre></div><p>
     Where the items have the following meaning and values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <em class="replaceable">ROLE_NAME</em> is any of the following: 'master',
       'admin', 'mon', 'mgr', 'storage', 'mds', 'igw', 'rgw', 'ganesha',
       'grafana', or 'prometheus'.
      </p></li><li class="listitem"><p>
       <em class="replaceable">PATH</em> is a relative directory path to .sls or
       .yml files. In case of .sls files, it usually is
       <code class="filename">cluster</code>, while .yml files are located at
       <code class="filename">stack/default/ceph/minions</code>.
      </p></li><li class="listitem"><p>
       <em class="replaceable">FILES_TO_INCLUDE</em> are the Salt state files
       or YAML configuration files. They normally consist of Salt minions' host
       names, for example <code class="filename">ses5min2.yml</code>. Shell globbing can
       be used for more specific matching.
      </p></li></ul></div><p>
     An example for each role follows:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>master</em></span> - the node has admin keyrings to all Ceph
       clusters. Currently, only a single Ceph cluster is supported. As the
       <span class="emphasis"><em>master</em></span> role is mandatory, always add a similar line
       to the following:
      </p><div class="verbatim-wrap"><pre class="screen">role-master/cluster/master*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>admin</em></span> - the minion will have an admin keyring. You
       define the role as follows:
      </p><div class="verbatim-wrap"><pre class="screen">role-admin/cluster/abc*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mon</em></span> - the minion will provide the monitor service
       to the Ceph cluster. This role requires addresses of the assigned
       minions. From SUSE Enterprise Storage 5, the public addresses are calculated
       dynamically and are no longer needed in the Salt pillar.
      </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls</pre></div><p>
       The example assigns the monitor role to a group of minions.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>mgr</em></span> - the Ceph manager daemon which collects all
       the state information from the whole cluster. Deploy it on all minions
       where you plan to deploy the Ceph monitor role.
      </p><div class="verbatim-wrap"><pre class="screen">role-mgr/cluster/mgr*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>storage</em></span> - use this role to specify storage nodes.
      </p><div class="verbatim-wrap"><pre class="screen">role-storage/cluster/data*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>mds</em></span> - the minion will provide the metadata service
       to support CephFS.
      </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mds*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>igw</em></span> - the minion will act as an iSCSI Gateway. This role
       requires addresses of the assigned minions, thus you need to also
       include the files from the <code class="filename">stack</code> directory:
      </p><div class="verbatim-wrap"><pre class="screen">role-igw/cluster/*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>rgw</em></span> - the minion will act as an Object Gateway:
      </p><div class="verbatim-wrap"><pre class="screen">role-rgw/cluster/rgw*.sls</pre></div></li><li class="listitem"><p>
       <span class="emphasis"><em>ganesha</em></span> - the minion will act as an NFS Ganesha
       server. The 'ganesha' role requires either an 'rgw' or 'mds' role in
       cluster, otherwise the validation will fail in Stage 3.
      </p><div class="verbatim-wrap"><pre class="screen">role-ganesha/cluster/ganesha*.sls</pre></div><p>
       To successfully install NFS Ganesha, additional configuration is required.
       If you want to use NFS Ganesha, read <a class="xref" href="#cha-as-ganesha" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a>
       before executing stages 2 and 4. However, it is possible to install
       NFS Ganesha later.
      </p><p>
       In some cases it can be useful to define custom roles for NFS Ganesha
       nodes. For details, see <span class="intraxref">Book “Administration Guide”, Chapter 30 “NFS Ganesha: Export Ceph Data via NFS”, Section 30.3 “Custom NFS Ganesha Roles”</span>.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>grafana</em></span>, <span class="emphasis"><em>prometheus</em></span> - this
       node adds Grafana charts based on Prometheus alerting to the
       Ceph Dashboard. Refer to <span class="intraxref">Book “Administration Guide”</span> for its detailed
       description.
      </p><div class="verbatim-wrap"><pre class="screen">role-grafana/cluster/grafana*.sls</pre></div><div class="verbatim-wrap"><pre class="screen">role-prometheus/cluster/prometheus*.sls</pre></div></li></ul></div><div id="id-1.4.4.2.9.2.5.9" data-id-title="Multiple Roles of Cluster Nodes" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Multiple Roles of Cluster Nodes</h6><p>
      You can assign several roles to a single node. For example, you can
      assign the 'mds' roles to the monitor nodes:
     </p><div class="verbatim-wrap"><pre class="screen">role-mds/cluster/mon[1,2]*.sls</pre></div></div></section><section class="sect3" id="policy-common-configuration" data-id-title="Common Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.3 </span><span class="title-name">Common Configuration</span> <a title="Permalink" class="permalink" href="#policy-common-configuration">#</a></h4></div></div></div><p>
     The common configuration section includes configuration files generated
     during the <span class="emphasis"><em>discovery (Stage 1)</em></span>. These configuration
     files store parameters like <code class="literal">fsid</code> or
     <code class="literal">public_network</code>. To include the required Ceph common
     configuration, add the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</pre></div></section><section class="sect3" id="deepsea-policy-filtering" data-id-title="Item Filtering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.4 </span><span class="title-name">Item Filtering</span> <a title="Permalink" class="permalink" href="#deepsea-policy-filtering">#</a></h4></div></div></div><p>
     Sometimes it is not practical to include all files from a given directory
     with *.sls globbing. The <code class="filename">policy.cfg</code> file parser
     understands the following filters:
    </p><div id="id-1.4.4.2.9.2.7.3" data-id-title="Advanced Techniques" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Advanced Techniques</h6><p>
      This section describes filtering techniques for advanced users. When not
      used correctly, filtering can cause problems for example in case your
      node numbering changes.
     </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.9.2.7.4.1"><span class="term">slice=[start:end]</span></dt><dd><p>
        Use the slice filter to include only items <span class="emphasis"><em>start</em></span>
        through <span class="emphasis"><em>end-1</em></span>. Note that items in the given
        directory are sorted alphanumerically. The following line includes the
        third to fifth files from the <code class="filename">role-mon/cluster/</code>
        subdirectory:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/*.sls slice[3:6]</pre></div></dd><dt id="id-1.4.4.2.9.2.7.4.2"><span class="term">re=regexp</span></dt><dd><p>
        Use the regular expression filter to include only items matching the
        given expressions. For example:
       </p><div class="verbatim-wrap"><pre class="screen">role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</pre></div></dd></dl></div></section><section class="sect3" id="deepsea-example-policy-cfg" data-id-title="Example policy.cfg File"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.1.5 </span><span class="title-name">Example <code class="filename">policy.cfg</code> File</span> <a title="Permalink" class="permalink" href="#deepsea-example-policy-cfg">#</a></h4></div></div></div><p>
     Following is an example of a basic <code class="filename">policy.cfg</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">## Cluster Assignment
cluster-ceph/cluster/*.sls <span class="callout" id="co-policy-1">1</span>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <span class="callout" id="co-policy-2">2</span>
role-admin/cluster/sesclient*.sls <span class="callout" id="co-policy-3">3</span>

# MON
role-mon/cluster/ses-example-[123].sls <span class="callout" id="co-policy-5">4</span>

# MGR
role-mgr/cluster/ses-example-[123].sls <span class="callout" id="co-policy-mgr">5</span>

# STORAGE
role-storage/cluster/ses-example-[5678].sls <span class="callout" id="co-policy-storage">6</span>

# MDS
role-mds/cluster/ses-example-4.sls <span class="callout" id="co-policy-6">7</span>

# IGW
role-igw/cluster/ses-example-4.sls <span class="callout" id="co-policy-10">8</span>

# RGW
role-rgw/cluster/ses-example-4.sls <span class="callout" id="co-policy-11">9</span>

# COMMON
config/stack/default/global.yml <span class="callout" id="co-policy-8">10</span>
config/stack/default/ceph/cluster.yml <span class="callout" id="co-policy-13">11</span></pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-1"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Indicates that all minions are included in the Ceph cluster. If you
       have minions you do not want to include in the Ceph cluster, use:
      </p><div class="verbatim-wrap"><pre class="screen">cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</pre></div><p>
       The first line marks all minions as unassigned. The second line
       overrides minions matching 'ses-example-*.sls', and assigns them to the
       Ceph cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-2"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       The minion called 'examplesesadmin' has the 'master' role. This, by the
       way, means it will get admin keys to the cluster.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-3"><span class="callout">3</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'sesclient*' will get admin keys as well.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-5"><span class="callout">4</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (presumably three minions:
       ses-example-1, ses-example-2, and ses-example-3) will be set up as MON
       nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-mgr"><span class="callout">5</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[123]' (all MON nodes in the example)
       will be set up as MGR nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-storage"><span class="callout">6</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       All minions matching 'ses-example-[5678]' will be set up as storage
       nodes.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-6"><span class="callout">7</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the MDS role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-10"><span class="callout">8</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the IGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-11"><span class="callout">9</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Minion 'ses-example-4' will have the RGW role.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-8"><span class="callout">10</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-policy-13"><span class="callout">11</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
       Means that we accept the default values for common configuration
       parameters such as <code class="option">fsid</code> and
       <code class="option">public_network</code>.
      </p></td></tr></table></div></section></section><section class="sect2" id="ds-drive-groups" data-id-title="DriveGroups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.2 </span><span class="title-name">DriveGroups</span> <a title="Permalink" class="permalink" href="#ds-drive-groups">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>DriveGroups</em></span> specify the layouts of OSDs in the Ceph
    cluster. They are defined in a single file
    <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>.
   </p><p>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on solid state and spinners) or
    share the same deployment options (identical, for example same object
    store, same encryption option, stand-alone OSDs). To avoid explicitly
    listing devices, DriveGroups use a list of filter items that correspond to a
    few selected fields of <code class="command">ceph-volume</code>'s inventory reports.
    In the simplest case this could be the 'rotational' flag (all solid-state
    drives are to be db_devices, all rotating ones data devices) or something
    more involved such as 'model' strings, or sizes. DeepSea will provide
    code that translates these DriveGroups into actual device lists for
    inspection by the user.
   </p><div id="id-1.4.4.2.9.3.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Note that the filters use an <code class="literal">OR</code> gate to match against
     the drives.
    </p></div><p>
    Following is a simple procedure that demonstrates the basic workflow when
    configuring DriveGroups:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Inspect your disks' properties as seen by the
      <code class="command">ceph-volume</code> command. Only these properties are
      accepted by DriveGroups:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.details</pre></div></li><li class="step"><p>
      Open the
      <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
      YAML file and adjust to your needs. Refer to
      <a class="xref" href="#ds-drive-groups-specs" title="5.5.2.1. Specification">Section 5.5.2.1, “Specification”</a>. Remember to use spaces instead
      of tabs. Find more advanced examples in
      <a class="xref" href="#ds-drive-groups-examples" title="5.5.2.4. Examples">Section 5.5.2.4, “Examples”</a>. The following example
      includes all drives available to Ceph as OSDs:
     </p><div class="verbatim-wrap"><pre class="screen">default_drive_group_name:
  target: '*'
  data_devices:
    all: true</pre></div></li><li class="step"><p>
      Verify new layouts:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.list</pre></div><p>
      This runner returns you a structure of matching disks based on your
      DriveGroups. If you are not happy with the result, repeat the previous
      step.
     </p><div id="id-1.4.4.2.9.3.6.3.4" data-id-title="Detailed Report" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Detailed Report</h6><p>
       In addition to the <code class="command">disks.list</code> runner, there is a
       <code class="command">disks.report</code> runner that prints out a detailed report
       of what will happen in the next DeepSea stage 3 invocation.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.report</pre></div></div></li><li class="step"><p>
      Deploy OSDs. On the next DeepSea stage 3 invocation, the OSD disks will
      be deployed according to your DriveGroups specification.
     </p></li></ol></div></div><section class="sect3" id="ds-drive-groups-specs" data-id-title="Specification"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.1 </span><span class="title-name">Specification</span> <a title="Permalink" class="permalink" href="#ds-drive-groups-specs">#</a></h4></div></div></div><p>
     <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
     can take one of two basic forms, depending on whether BlueStore or
     FileStore is to be used. For BlueStore setups,
     <code class="filename">drive_groups.yml</code> can be as follows:
    </p><div class="verbatim-wrap"><pre class="screen">drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  db_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  wal_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')</pre></div><p>
     For FileStore setups, <code class="filename">drive_groups.yml</code> can be as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  journal_devices:
    drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
  format: filestore
  encryption: True</pre></div><div id="id-1.4.4.2.9.3.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If you are unsure if your OSD is encrypted, see
      <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.5 “Verify an Encrypted OSD”</span>.
     </p></div></section><section class="sect3" id="id-1.4.4.2.9.3.8" data-id-title="Matching Disk Devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.2 </span><span class="title-name">Matching Disk Devices</span> <a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.8">#</a></h4></div></div></div><p>
     You can describe the specification using the following filters:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       By a disk model:
      </p><div class="verbatim-wrap"><pre class="screen">model: <em class="replaceable">DISK_MODEL_STRING</em></pre></div></li><li class="listitem"><p>
       By a disk vendor:
      </p><div class="verbatim-wrap"><pre class="screen">vendor: <em class="replaceable">DISK_VENDOR_STRING</em></pre></div><div id="id-1.4.4.2.9.3.8.3.2.3" data-id-title="Lowercase Vendor String" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Lowercase Vendor String</h6><p>
        Always lowercase the <em class="replaceable">DISK_VENDOR_STRING</em>.
       </p></div></li><li class="listitem"><p>
       Whether a disk is rotational or not. SSDs and NVME drives are not
       rotational.
      </p><div class="verbatim-wrap"><pre class="screen">rotational: 0</pre></div></li><li class="listitem"><p>
       Deploy a node using <span class="emphasis"><em>all</em></span> available drives for OSDs:
      </p><div class="verbatim-wrap"><pre class="screen">data_devices:
  all: true</pre></div></li><li class="listitem"><p>
       Additionally, by limiting the number of matching disks:
      </p><div class="verbatim-wrap"><pre class="screen">limit: 10</pre></div></li></ul></div></section><section class="sect3" id="id-1.4.4.2.9.3.9" data-id-title="Filtering Devices by Size"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.3 </span><span class="title-name">Filtering Devices by Size</span> <a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.9">#</a></h4></div></div></div><p>
     You can filter disk devices by their size—either by an exact size,
     or a size range. The <code class="option">size:</code> parameter accepts arguments in
     the following form:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       '10G' - Includes disks of an exact size.
      </p></li><li class="listitem"><p>
       '10G:40G' - Includes disks whose size is within the range.
      </p></li><li class="listitem"><p>
       ':10G' - Includes disks less than or equal to 10 GB in size.
      </p></li><li class="listitem"><p>
       '40G:' - Includes disks equal to or greater than 40 GB in size.
      </p></li></ul></div><div class="example" id="id-1.4.4.2.9.3.9.4" data-id-title="Matching by Disk Size"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.1: </span><span class="title-name">Matching by Disk Size </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.9.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'</pre></div></div></div><div id="id-1.4.4.2.9.3.9.5" data-id-title="Quotes Required" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quotes Required</h6><p>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </p></div><div id="id-1.4.4.2.9.3.9.6" data-id-title="Unit Shortcuts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Unit Shortcuts</h6><p>
      Instead of (G)igabytes, you can specify the sizes in (M)egabytes or
      (T)erabytes as well.
     </p></div></section><section class="sect3" id="ds-drive-groups-examples" data-id-title="Examples"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.5.2.4 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="#ds-drive-groups-examples">#</a></h4></div></div></div><p>
     This section includes examples of different OSD setups.
    </p><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.3" data-id-title="Simple Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.2: </span><span class="title-name">Simple Setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.10.3">#</a></h6></div><div class="example-contents"><p>
      This example describes two nodes with the same setup:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      The corresponding <code class="filename">drive_groups.yml</code> file will be as
      follows:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ</pre></div><p>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0</pre></div><p>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </p><p>
      If you know that drives with more than 2 TB will always be the
      slower data devices, you can filter by size:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.4" data-id-title="Advanced Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.3: </span><span class="title-name">Advanced Setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.10.4">#</a></h6></div><div class="example-contents"><p>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        12 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      Such a setup can be defined with two layouts as follows:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB</pre></div><p>
      Note that any drive of the size 256 GB and any drive from Samsung
      will match as a DB device with this example.
     </p></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.5" data-id-title="Advanced Setup with Non-uniform Nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.4: </span><span class="title-name">Advanced Setup with Non-uniform Nodes </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.10.5">#</a></h6></div><div class="example-contents"><p>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </p><p>
      Nodes 1-5:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      Nodes 6-10:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        5 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        20 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      You can use the 'target' key in the layout to target specific nodes.
      Salt target notation helps to keep things simple:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0</pre></div><p>
      followed by
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.6" data-id-title="Expert Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.5: </span><span class="title-name">Expert Setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.10.6">#</a></h6></div><div class="example-contents"><p>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><div class="verbatim-wrap"><pre class="screen">drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.2.9.3.10.7" data-id-title="Complex (and Unlikely) Setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 5.6: </span><span class="title-name">Complex (and Unlikely) Setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.9.3.10.7">#</a></h6></div><div class="example-contents"><p>
      In the following setup, we are trying to define:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 HDDs backed by 1 SSD(db) and 1 NVMe(wal)
       </p></li><li class="listitem"><p>
        8 SSDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 SSDs stand-alone (encrypted)
       </p></li><li class="listitem"><p>
        1 HDD is spare and should not be deployed
       </p></li></ul></div><p>
      The summary of used drives follows:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        23 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        10 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        1 NVMe
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      The DriveGroups definition will be the following:
     </p><div class="verbatim-wrap"><pre class="screen">drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True</pre></div><p>
      One HDD will remain as the file is being parsed from top to bottom.
     </p></div></div></div></section></section><section class="sect2" id="adjusting-ceph-conf" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.5.3 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="#adjusting-ceph-conf">#</a></h3></div></div></div><p>
    If you need to put custom settings into the <code class="filename">ceph.conf</code>
    configuration file, see <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.14 “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</span> for more
    details.
   </p></section></section></section><section class="chapter" id="cha-ceph-upgrade" data-id-title="Upgrading from Previous Releases"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Upgrading from Previous Releases</span> <a title="Permalink" class="permalink" href="#cha-ceph-upgrade">#</a></h2></div></div></div><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage 5.5 to
  version 6. Note that version 5.5 is basically 5
  with all latest patches applied.
 </p><div id="id-1.4.4.3.4" data-id-title="Upgrade from Older Releases Not Supported" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Upgrade from Older Releases Not Supported</h6><p>
   Upgrading from SUSE Enterprise Storage versions older than 5.5 is not
   supported. You first need to upgrade to the latest version of SUSE Enterprise Storage
   5.5 and then follow the steps in this chapter.
  </p></div><section class="sect1" id="upgrade-general-considerations" data-id-title="General Considerations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">General Considerations</span> <a title="Permalink" class="permalink" href="#upgrade-general-considerations">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If openATTIC is located on the Admin Node, it will be unavailable after you upgrade
     the node. The new Ceph Dashboard will not be available until you deploy it by
     using DeepSea.
    </p></li><li class="listitem"><p>
     The cluster upgrade may take a long time—approximately the time it
     takes to upgrade one machine multiplied by the number of cluster nodes.
    </p></li><li class="listitem"><p>
     A single node cannot be upgraded while running the previous SUSE Linux Enterprise Server release,
     but needs to be rebooted into the new version's installer. Therefore the
     services that the node provides will be unavailable for some time. The
     core cluster services will still be available—for example if one MON
     is down during upgrade, there are still at least two active MONs.
     Unfortunately, single instance services, such as a single iSCSI Gateway, will be
     unavailable.
    </p></li></ul></div></section><section class="sect1" id="before-upgrade" data-id-title="Steps to Take before Upgrading the First Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Steps to Take before Upgrading the First Node</span> <a title="Permalink" class="permalink" href="#before-upgrade">#</a></h2></div></div></div><section class="sect2" id="before-upgrade-release-notes" data-id-title="Read the Release Notes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Read the Release Notes</span> <a title="Permalink" class="permalink" href="#before-upgrade-release-notes">#</a></h3></div></div></div><p>
    In the SES 6 release notes, you can find additional
    information on changes since the previous release of SUSE Enterprise Storage. Check
    the SES 6 release notes online to see whether:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Your hardware needs special considerations.
     </p></li><li class="listitem"><p>
      Any used software packages have changed significantly.
     </p></li><li class="listitem"><p>
      Special precautions are necessary for your installation.
     </p></li></ul></div><p>
    You can find SES 6 release notes online at
    <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
   </p></section><section class="sect2" id="before-upgrade-password" data-id-title="Verify Your Password"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.2 </span><span class="title-name">Verify Your Password</span> <a title="Permalink" class="permalink" href="#before-upgrade-password">#</a></h3></div></div></div><p>
    Your password must be changed to meet SUSE Enterprise Storage 6
    requirements. Ensure you change the username and password on
    <span class="emphasis"><em>all</em></span> initiators as well. For more information on
    changing your password, see <a class="xref" href="#chap-auth-password" title="10.4.4.3. CHAP Authentication">Section 10.4.4.3, “CHAP Authentication”</a>.
   </p></section><section class="sect2" id="before-upgrade-verify-upgrade" data-id-title="Verify the Previous Upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.3 </span><span class="title-name">Verify the Previous Upgrade</span> <a title="Permalink" class="permalink" href="#before-upgrade-verify-upgrade">#</a></h3></div></div></div><p>
    In case you previously upgraded from version 4, verify that the upgrade to
    version 5 was completed successfully:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Check for the existence of the file
     </p><div class="verbatim-wrap"><pre class="screen">/srv/salt/ceph/configuration/files/ceph.conf.import</pre></div><p>
      It is created by the import process during the upgrade from SES 4 to 5.
      Also, the <code class="option">configuration_init: default-import</code> option is
      set in the file
      <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>
     </p><p>
      If <code class="option">configuration_init</code> is still set to
      <code class="option">default-import</code>, the cluster is using
      <code class="filename">ceph.conf.import</code> as its configuration file and not
      DeepSea's default <code class="filename">ceph.conf</code> which is compiled from
      files in
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>
     </p><p>
      Therefore you need to inspect <code class="filename">ceph.conf.import</code> for
      any custom configuration, and possibly move the configuration to one of
      the files in
     </p><div class="verbatim-wrap"><pre class="screen">/srv/salt/ceph/configuration/files/ceph.conf.d/</pre></div><p>
      Then remove the <code class="option">configuration_init: default-import</code> line
      from
      <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>
     </p><div id="id-1.4.4.3.6.4.3.1.8" data-id-title="Default DeepSea Configuration" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Default DeepSea Configuration</h6><p>
       If you <span class="bold"><strong>do not</strong></span> merge the configuration
       from <code class="filename">ceph.conf.import</code> and remove the
       <code class="option">configuration_init: default-import</code> option, any default
       configuration settings we ship as part of DeepSea (stored in
       <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>)
       will not be applied to the cluster.
      </p></div></li><li class="listitem"><p>
      Run the <code class="command">salt-run upgrade.check</code> command to verify that
      the cluster uses the new bucket type <code class="literal">straw2</code>, and that
      the Admin Node is not a storage node. The default is <code class="literal">straw2</code>
      for any newly created buckets.
     </p><div id="id-1.4.4.3.6.4.3.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       The new <code class="literal">straw2</code> bucket type fixes several limitations
       in the original <code class="literal">straw</code> bucket type. The previous
       <code class="literal">straw</code> buckets would change some mappings that should
       have changed when a weight was adjusted. <code class="literal">straw2</code>
       achieves the original goal of only changing mappings to or from the
       bucket item whose weight has changed.
      </p><p>
       Changing a bucket type from <code class="literal">straw</code> to
       <code class="literal">straw2</code> results in a small amount of data movement,
       depending on how much the bucket item weights vary from each other. When
       the weights are all the same, no data will move. When an item's weight
       varies significantly there will be more movement. To migrate, execute:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap -o backup-crushmap
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush set-all-straw-buckets-to-straw2</pre></div><p>
       If there are problems, you can revert this change with:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd setcrushmap -i backup-crushmap</pre></div><p>
       Moving to <code class="literal">straw2</code> buckets unlocks a few recent
       features, such as the <code class="literal">crush-compat</code> balancer mode that
       was added in Ceph Luminous (SES 4).
      </p></div></li><li class="listitem"><p>
      Check that Ceph 'jewel' profile is used:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush dump | grep profile</pre></div></li></ul></div></section><section class="sect2" id="before-upgrade-rbd-clients" data-id-title="Upgrade Old RBD Kernel Clients"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.4 </span><span class="title-name">Upgrade Old RBD Kernel Clients</span> <a title="Permalink" class="permalink" href="#before-upgrade-rbd-clients">#</a></h3></div></div></div><p>
    In case old RBD kernel clients (older than SUSE Linux Enterprise Server 12 SP3) are being used,
    refer to <span class="intraxref">Book “Administration Guide”, Chapter 23 “RADOS Block Device”, Section 23.9 “Mapping RBD Using Old Kernel Clients”</span>. We recommend upgrading old
    RBD kernel clients if possible.
   </p></section><section class="sect2" id="before-upgrade-apparmor" data-id-title="Adjust AppArmor"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.5 </span><span class="title-name">Adjust AppArmor</span> <a title="Permalink" class="permalink" href="#before-upgrade-apparmor">#</a></h3></div></div></div><p>
    If you used AppArmor in either 'complain' or 'enforce' mode, you need to set a
    Salt pillar variable before upgrading. Because SUSE Linux Enterprise Server 15 SP1 ships with AppArmor by
    default, AppArmor management was integrated into DeepSea stage 0. The default
    behavior in SUSE Enterprise Storage 6 is to remove AppArmor and related
    profiles. If you want to retain the behavior configured in SUSE Enterprise Storage
    5.5, verify that one of the following lines is present in
    the <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file before
    starting the upgrade:
   </p><div class="verbatim-wrap"><pre class="screen">apparmor_init: default-enforce</pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen">apparmor_init: default-complain</pre></div></section><section class="sect2" id="before-upgrade-mds-names" data-id-title="Verify MDS Names"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.6 </span><span class="title-name">Verify MDS Names</span> <a title="Permalink" class="permalink" href="#before-upgrade-mds-names">#</a></h3></div></div></div><p>
    From SUSE Enterprise Storage 6, MDS names are no longer allowed to
    begin with a digit, and such names will cause MDS daemons to refuse to
    start. You can check whether your daemons have such names either by running
    the <code class="command">ceph fs status</code> command, or by restarting an MDS and
    checking its logs for the following message:
   </p><div class="verbatim-wrap"><pre class="screen">deprecation warning: MDS id '1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.</pre></div><p>
    If you see the above message, the MDS names must be migrated before
    attempting to upgrade to SUSE Enterprise Storage 6. DeepSea provides
    an orchestration to automate such a migration. MDS names starting with a
    digit will be prepended with 'mds.':
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.mds.migrate-numerical-names</pre></div><div id="id-1.4.4.3.6.7.6" data-id-title="Custom Configuration Bound to MDS Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom Configuration Bound to MDS Names</h6><p>
     If you have configuration settings that are bound to MDS names and your
     MDS daemons have names starting with a digit, verify that your
     configuration settings apply to the new names as well (with the 'mds.'
     prefix). Consider the following example section in the
     <code class="filename">/etc/ceph/ceph.conf</code> file:
    </p><div class="verbatim-wrap"><pre class="screen">[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
mds cache memory limit = 1073741824
mds standby for name = 456-another-mds</pre></div><p>
     The <code class="command">ceph.mds.migrate-numerical-names</code> orchestrator will
     change the MDS daemon name '123-my-mds' to 'mds.123-my-mds'. You need to
     adjust the configuration to reflect the new name:
    </p><div class="verbatim-wrap"><pre class="screen">[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds</pre></div></div><p>
    This will add MDS daemons with the new names before removing the old MDS
    daemons. The number of MDS daemons will double for a short time. Clients
    will be able to access CephFS only after a short pause for failover to
    happen. Therefore plan the migration for a time when you expect little or
    no CephFS load.
   </p></section><section class="sect2" id="before-upgrade-scrub-settings" data-id-title="Consolidate Scrub-related Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.7 </span><span class="title-name">Consolidate Scrub-related Configuration</span> <a title="Permalink" class="permalink" href="#before-upgrade-scrub-settings">#</a></h3></div></div></div><p>
    The <code class="literal">osd_scrub_max_interval</code> and
    <code class="literal">osd_scrub_max_interval</code> settings are used by both OSD and
    MON daemons. OSDs use these settings to decide when to run scrub, and MONs
    use them to decide if a warning about scrub not running in time (running
    too long) should be shown. Therefore, if non-default settings are used,
    they should be visible by both OSD and MON daemons (that is, defined either
    in both <code class="literal">[osd]</code> and <code class="literal">[mon]</code> sections, or
    in the <code class="literal">[global]</code> section), otherwise the monitor may give
    a false alarm.
   </p><p>
    In SES 5.5 the monitor warning are disabled by default and the issue may
    not be noticed if the settings are overridden in the
    <code class="literal">[osd]</code> section only. But when the monitors are upgraded
    to SES 6, it will start to complain, because in this version the warnings
    are enabled by default. So if you define non-default scrub settings in your
    configuration only in the <code class="literal">[osd]</code> section, it is desirable
    to move them to the <code class="literal">[global]</code> section before upgrading to
    SES 6 to avoid false alarms about scrub not running in time.
   </p></section><section class="sect2" id="upgrade-backup" data-id-title="Back Up Cluster Data"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.8 </span><span class="title-name">Back Up Cluster Data</span> <a title="Permalink" class="permalink" href="#upgrade-backup">#</a></h3></div></div></div><p>
    Although creating backups of a cluster's configuration and data is not
    mandatory, we strongly recommend backing up important configuration files
    and cluster data. Refer to <span class="intraxref">Book “Administration Guide”, Chapter 3 “Backing Up Cluster Configuration and Data”</span> for more
    details.
   </p></section><section class="sect2" id="upgrade-ntp" data-id-title="Migrate from ntpd to chronyd"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.9 </span><span class="title-name">Migrate from <code class="systemitem">ntpd</code> to <code class="systemitem">chronyd</code></span> <a title="Permalink" class="permalink" href="#upgrade-ntp">#</a></h3></div></div></div><p>
    SUSE Linux Enterprise Server 15 SP1 no longer uses <code class="systemitem">ntpd</code> to
    synchronize the local host time. Instead,
    <code class="systemitem">chronyd</code> is used. You need to
    migrate the time synchronization daemon on each cluster node. You can
    migrate to <code class="systemitem">chronyd</code> either
    <span class="bold"><strong>before</strong></span> migrating the cluster, or upgrade
    the cluster and migrate to <code class="systemitem">chronyd</code>
    <span class="bold"><strong>afterward</strong></span>.
   </p><div id="id-1.4.4.3.6.10.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Before you continue, review your current
     <code class="systemitem">ntpd</code> settings and determine if you
     want to keep using the same time server. Keep in mind that the default
     behaviour <span class="emphasis"><em>will</em></span> convert to using
     <code class="systemitem">chronyd</code>.
    </p><p>
     If you want to manually maintain the
     <code class="systemitem">chronyd</code> configuration, follow the
     instructions below and ensure you disable
     <code class="systemitem">ntpd</code> time configuration. See
     <a class="xref" href="#disable-time-sync" title="Disabling Time Synchronization">Procedure 7.1, “Disabling Time Synchronization”</a> for more information.
    </p></div><div class="procedure" id="id-1.4.4.3.6.10.4" data-id-title="Migrate to chronyd before the Cluster Upgrade"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 6.1: </span><span class="title-name">Migrate to <code class="systemitem">chronyd</code> <span class="emphasis"><em>before</em></span> the Cluster Upgrade </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.10.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Install the <span class="package">chrony</span> package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper install chrony</pre></div></li><li class="step"><p>
      Edit the <code class="systemitem">chronyd</code> configuration
      file <code class="filename">/etc/chrony.conf</code> and add NTP sources from the
      current <code class="systemitem">ntpd</code> configuration in
      <code class="filename">/etc/ntp.conf</code>.
     </p><div id="id-1.4.4.3.6.10.4.3.2" data-id-title="More Details on chronyd Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Details on <code class="systemitem">chronyd</code> Configuration</h6><p>
       Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html</a>
       to find more details about how to include time sources in
       <code class="systemitem">chronyd</code> configuration.
      </p></div></li><li class="step"><p>
      Edit the file <code class="filename">/srv/salt/ceph/rescind/time/ntp</code> and
      remove the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/ntp.conf:
file.absent</pre></div></li><li class="step"><p>
      Remove the directory <code class="filename">/srv/salt/ceph/time/ntp</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rm -rf /srv/salt/ceph/time/ntp</pre></div></li><li class="step"><p>
      Edit the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
      add or update the <code class="option">time_init</code> option to use
      <code class="literal">chrony</code>:
     </p><div class="verbatim-wrap"><pre class="screen">time_init: chrony</pre></div></li><li class="step"><p>
      Disable and stop the <code class="systemitem">ntpd</code>
      service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</pre></div></li><li class="step"><p>
      Start and enable the <code class="systemitem">chronyd</code>
      service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</pre></div></li><li class="step"><p>
      Verify the status of <code class="systemitem">chronyd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>chronyc tracking</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.4.3.6.10.5" data-id-title="Migrate to chronyd after the Cluster Upgrade"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 6.2: </span><span class="title-name">Migrate to <code class="systemitem">chronyd</code> <span class="emphasis"><em>after</em></span> the Cluster Upgrade </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.10.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      During cluster upgrade, add the following software repositories:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        SLE-Module-Legacy15-SP1-Pool
       </p></li><li class="listitem"><p>
        SLE-Module-Legacy15-SP1-Updates
       </p></li></ul></div></li><li class="step"><p>
      Upgrade the cluster to version 6.
     </p></li><li class="step"><p>
      Edit the <code class="systemitem">chronyd</code> configuration
      file <code class="filename">/etc/chrony.conf</code> and add NTP sources from the
      current <code class="systemitem">ntpd</code> configuration in
      <code class="filename">/etc/ntp.conf</code>.
     </p><div id="id-1.4.4.3.6.10.5.4.2" data-id-title="More Details on chronyd Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Details on <code class="systemitem">chronyd</code> Configuration</h6><p>
       Refer to
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html</a>
       to find more details about how to include time sources in
       <code class="systemitem">chronyd</code> configuration.
      </p></div></li><li class="step"><p>
      Edit the file <code class="filename">/srv/salt/ceph/rescind/time/ntp</code> and
      remove the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">/etc/ntp.conf:
file.absent</pre></div></li><li class="step"><p>
      Remove the directory <code class="filename">/srv/salt/ceph/time/ntp</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rm -rf /srv/salt/ceph/time/ntp</pre></div></li><li class="step"><p>
      Edit the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
      add or update the <code class="option">time_init</code> option to use
      <code class="literal">chrony</code>:
     </p><div class="verbatim-wrap"><pre class="screen">time_init: chrony</pre></div></li><li class="step"><p>
      Disable and stop the <code class="systemitem">ntpd</code>
      service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</pre></div></li><li class="step"><p>
      Start and enable the <code class="systemitem">chronyd</code>
      service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</pre></div></li><li class="step"><p>
      Migrate from <code class="systemitem">ntpd</code> to
      <code class="systemitem">chronyd</code>.
     </p></li><li class="step"><p>
      Verify the status of <code class="systemitem">chronyd</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>chronyc tracking</pre></div></li><li class="step"><p>
      Remove the legacy software repositories that you added to keep
      <code class="systemitem">ntpd</code> in the system during the
      upgrade process.
     </p></li></ol></div></div></section><section class="sect2" id="upgrade-prepare" data-id-title="Patch Cluster Prior to Upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.10 </span><span class="title-name">Patch Cluster Prior to Upgrade</span> <a title="Permalink" class="permalink" href="#upgrade-prepare">#</a></h3></div></div></div><p>
    Apply the latest patches to all cluster nodes prior to upgrade.
   </p><section class="sect3" id="upgrade-prepare-repos" data-id-title="Required Software Repositories"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.10.1 </span><span class="title-name">Required Software Repositories</span> <a title="Permalink" class="permalink" href="#upgrade-prepare-repos">#</a></h4></div></div></div><p>
     Check that required repositories are configured on each host of the
     cluster. To list all available repositories, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper lr</pre></div><div id="id-1.4.4.3.6.11.3.4" data-id-title="Remove SUSE Enterprise Storage 5.5 LTSS Repositories" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Remove SUSE Enterprise Storage 5.5 LTSS Repositories</h6><p>
      Upgrades will fail if LTSS repositories are configured in SUSE Enterprise Storage
      5.5. Find their IDs and remove them from the system. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper lr
[...]
12 | SUSE_Linux_Enterprise_Server_LTSS_12_SP3_x86_64:SLES12-SP3-LTSS-Debuginfo-Updates
13 | SUSE_Linux_Enterprise_Server_LTSS_12_SP3_x86_64:SLES12-SP3-LTSS-Updates
[...]
<code class="prompt user">root # </code>zypper rr 12 13</pre></div></div><div id="id-1.4.4.3.6.11.3.5" data-id-title="Upgrade Without Using SCC, SMT, or RMT" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Upgrade Without Using SCC, SMT, or RMT</h6><p>
      If your nodes are not subscribed to one of the supported software channel
      providers that handle automatic channel adjustment—such as SMT,
      RMT, or SCC—you may need to enable additional software modules and
      channels.
     </p></div><p>
     SUSE Enterprise Storage 5.5 requires:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       SLES12-SP3-Installer-Updates
      </p></li><li class="listitem"><p>
       SLES12-SP3-Pool
      </p></li><li class="listitem"><p>
       SLES12-SP3-Updates
      </p></li><li class="listitem"><p>
       SUSE-Enterprise-Storage-5-Pool
      </p></li><li class="listitem"><p>
       SUSE-Enterprise-Storage-5-Updates
      </p></li></ul></div><p>
     NFS/SMB Gateway on SLE-HA on SUSE Linux Enterprise Server 12 SP3 requires:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       SLE-HA12-SP3-Pool
      </p></li><li class="listitem"><p>
       SLE-HA12-SP3-Updates
      </p></li></ul></div></section><section class="sect3" id="upgrade-prepare-staging" data-id-title="Repository Staging Systems"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.10.2 </span><span class="title-name">Repository Staging Systems</span> <a title="Permalink" class="permalink" href="#upgrade-prepare-staging">#</a></h4></div></div></div><p>
     If you are using one of the repository staging systems—SMT, or
     RMT—create a new frozen patch level for the current and the new
     SUSE Enterprise Storage version.
    </p><p>
     Find more information in:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <a class="link" href="https://documentation.suse.com/sles/12-SP5/html/SLES-all/book-smt.html" target="_blank">https://documentation.suse.com/sles/12-SP5/html/SLES-all/book-smt.html</a>,
      </p></li><li class="listitem"><p>
       <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-rmt.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-rmt.html</a>.
      </p></li><li class="listitem"><p>
       <a class="link" href="https://documentation.suse.com/suma/3.2/" target="_blank">https://documentation.suse.com/suma/3.2/</a>,
      </p></li></ul></div></section><section class="sect3" id="upgrade-prepare-patch" data-id-title="Patch the Whole Cluster to the Latest Patches"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.10.3 </span><span class="title-name">Patch the Whole Cluster to the Latest Patches</span> <a title="Permalink" class="permalink" href="#upgrade-prepare-patch">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Apply the latest patches of SUSE Enterprise Storage 5.5 and
       SUSE Linux Enterprise Server 12 SP3 to each Ceph cluster node. Verify that correct software
       repositories are connected to each cluster node (see
       <a class="xref" href="#upgrade-prepare-repos" title="6.2.10.1. Required Software Repositories">Section 6.2.10.1, “Required Software Repositories”</a>) and run DeepSea stage 0:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
       After stage 0 completes, verify that each cluster node's status includes
       'HEALTH_OK'. If not, resolve the problem before any possible reboots in
       the next steps.
      </p></li><li class="step"><p>
       Run <code class="command">zypper ps</code> to check for processes that may still
       be running with outdated libraries or binaries, and reboot if there are
       any.
      </p></li><li class="step"><p>
       Verify that the running kernel is the latest available, and reboot if
       not. Check outputs of the following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>uname -a
<code class="prompt user">cephadm@adm &gt; </code>rpm -qa kernel-default</pre></div></li><li class="step"><p>
       Verify that the <span class="package">ceph</span> package is version 12.2.12 or
       newer. Verify that the <span class="package">deepsea</span> package is version
       0.8.9 or newer.
      </p></li><li class="step"><p>
       If you previously used any of the <code class="option">bluestore_cache</code>
       settings, they are no longer effective from <span class="package">ceph</span>
       version 12.2.10. The new setting
       <code class="option">bluestore_cache_autotune</code> which is set to 'true' by
       default disables manual cache sizing. To turn on the old behavior, you
       need to set <code class="option">bluestore_cache_autotune=false</code>. Refer to
       <span class="intraxref">Book “Administration Guide”, Chapter 25 “Ceph Cluster Configuration”, Section 25.2.1 “Automatic Cache Sizing”</span> for details.
      </p></li></ol></div></div></section></section><section class="sect2" id="upgrade-verify-current" data-id-title="Verify the Current Environment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.11 </span><span class="title-name">Verify the Current Environment</span> <a title="Permalink" class="permalink" href="#upgrade-verify-current">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If the system has obvious problems, fix them before starting the upgrade.
      Upgrading never fixes existing system problems.
     </p></li><li class="listitem"><p>
      Check cluster performance. You can use commands such as <code class="command">rados
      bench</code>, <code class="command">ceph tell osd.* bench</code>, or
      <code class="command">iperf3</code>.
     </p></li><li class="listitem"><p>
      Verify access to gateways (such as iSCSI Gateway or Object Gateway) and RADOS Block Device.
     </p></li><li class="listitem"><p>
      Document specific parts of the system setup, such as network setup,
      partitioning, or installation details.
     </p></li><li class="listitem"><p>
      Use <code class="command">supportconfig</code> to collect important system
      information and save it outside cluster nodes. Find more information in
      <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-adm-support.html#sec-admsupport-submit" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-adm-support.html#sec-admsupport-submit</a>.
     </p></li><li class="listitem"><p>
      Ensure there is enough free disk space on each cluster node. Check free
      disk space with <code class="command">df -h</code>. When needed, free up additional
      disk space by removing unneeded files/directories or removing obsolete OS
      snapshots. If there is not enough free disk space, do not continue with
      the upgrade until you have freed enough disk space.
     </p></li></ul></div></section><section class="sect2" id="upgrade-verify-state" data-id-title="Check the Clusters State"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.12 </span><span class="title-name">Check the Cluster's State</span> <a title="Permalink" class="permalink" href="#upgrade-verify-state">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Check the <code class="command">cluster health</code> command before starting the
      upgrade procedure. Do not start the upgrade unless each cluster node
      reports 'HEALTH_OK'.
     </p></li><li class="listitem"><p>
      Verify that all services are running:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Salt master and Salt master daemons.
       </p></li><li class="listitem"><p>
        Ceph Monitor and Ceph Manager daemons.
       </p></li><li class="listitem"><p>
        Metadata Server daemons.
       </p></li><li class="listitem"><p>
        Ceph OSD daemons.
       </p></li><li class="listitem"><p>
        Object Gateway daemons.
       </p></li><li class="listitem"><p>
        iSCSI Gateway daemons.
       </p></li></ul></div></li></ul></div><p>
    The following commands provide details of the cluster state and specific
    configuration:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.3.6.13.4.1"><span class="term"><code class="command">ceph -s</code></span></dt><dd><p>
       Prints a brief summary of Ceph cluster health, running services, data
       usage, and I/O statistics. Verify that it reports 'HEALTH_OK' before
       starting the upgrade.
      </p></dd><dt id="id-1.4.4.3.6.13.4.2"><span class="term"><code class="command">ceph health detail</code></span></dt><dd><p>
       Prints details if Ceph cluster health is not OK.
      </p></dd><dt id="id-1.4.4.3.6.13.4.3"><span class="term"><code class="command">ceph versions</code></span></dt><dd><p>
       Prints versions of running Ceph daemons.
      </p></dd><dt id="id-1.4.4.3.6.13.4.4"><span class="term"><code class="command">ceph df</code></span></dt><dd><p>
       Prints total and free disk space on the cluster. Do not start the
       upgrade if the cluster's free disk space is less than 25% of the total
       disk space.
      </p></dd><dt id="id-1.4.4.3.6.13.4.5"><span class="term"><code class="command">salt '*' cephprocesses.check results=true</code></span></dt><dd><p>
       Prints running Ceph processes and their PIDs sorted by Salt minions.
      </p></dd><dt id="id-1.4.4.3.6.13.4.6"><span class="term"><code class="command">ceph osd dump | grep ^flags</code></span></dt><dd><p>
       Verify that 'recovery_deletes' and 'purged_snapdirs' flags are present.
       If not, you can force a scrub on all placement groups by running the
       following command. Be aware that this forced scrub may possibly have a
       negative impact on your Ceph clients’ performance.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub</pre></div></dd></dl></div></section><section class="sect2" id="filestore2bluestore" data-id-title="Migrate OSDs to BlueStore"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.13 </span><span class="title-name">Migrate OSDs to BlueStore</span> <a title="Permalink" class="permalink" href="#filestore2bluestore">#</a></h3></div></div></div><p>
    OSD BlueStore is a new back-end for the OSD daemons. It is the default
    option since SUSE Enterprise Storage 5. Compared to FileStore, which stores objects
    as files in an XFS file system, BlueStore can deliver increased
    performance because it stores objects directly on the underlying block
    device. BlueStore also enables other features, such as built-in
    compression and EC overwrites, that are unavailable with FileStore.
   </p><p>
    Specifically for BlueStore, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a BlueStore OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on different, for
    example faster, media.
   </p><p>
    In SUSE Enterprise Storage 5, both FileStore and BlueStore are supported and it
    is possible for FileStore and BlueStore OSDs to co-exist in a single
    cluster. During the SUSE Enterprise Storage upgrade procedure, FileStore OSDs are
    not automatically converted to BlueStore.
   </p><div id="id-1.4.4.3.6.14.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Migration to BlueStore needs to be completed on all OSD nodes
     <span class="bold"><strong>before</strong></span> the cluster upgrade because
     FileStore OSDs are not supported in SES 6.
    </p></div><p>
    Before converting to BlueStore, the OSDs need to be running SUSE Enterprise Storage
    5. The conversion is a slow process as all data gets re-written twice.
    Though the migration process can take a long time to complete, there is no
    cluster outage and all clients can continue accessing the cluster during
    this period. However, do expect lower performance for the duration of the
    migration. This is caused by rebalancing and backfilling of cluster data.
   </p><p>
    Use the following procedure to migrate FileStore OSDs to BlueStore:
   </p><div id="id-1.4.4.3.6.14.8" data-id-title="Turn Off Safety Measures" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Turn Off Safety Measures</h6><p>
     Salt commands needed for running the migration are blocked by safety
     measures. In order to turn these precautions off, run the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety</pre></div><p>
     Rebuild the nodes before continuing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code> salt-run rebuild.node <em class="replaceable">TARGET</em></pre></div><p>
     You can also choose to rebuild each node individually. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code> salt-run rebuild.node data1.ceph</pre></div><p>
     The <code class="literal">rebuild.node</code> always removes and recreates all OSDs
     on the node.
    </p></div><div id="id-1.4.4.3.6.14.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     If one OSD fails to convert, re-running the rebuild destroys the
     already-converted BlueStore OSDs. Instead of re-running the rebuild, you
     can run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.deploy target=<em class="replaceable">NODE-ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.deploy target=data1.ceph</pre></div></div><p>
    After the migration to BlueStore, the object count will remain the same
    and disk usage will be nearly the same.
   </p></section></section><section class="sect1" id="upgrade-backup-order-of-nodes" data-id-title="Order in Which Nodes Must Be Upgraded"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Order in Which Nodes Must Be Upgraded</span> <a title="Permalink" class="permalink" href="#upgrade-backup-order-of-nodes">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Certain types of daemons depend upon others. For example, Ceph Object Gateways
     depend upon Ceph MON and OSD daemons. We recommend upgrading in this
     order:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Admin Node
      </p></li><li class="listitem"><p>
       Ceph Monitors/Ceph Managers
      </p></li><li class="listitem"><p>
       Metadata Servers
      </p></li><li class="listitem"><p>
       Ceph OSDs
      </p></li><li class="listitem"><p>
       Object Gateways
      </p></li><li class="listitem"><p>
       iSCSI Gateways
      </p></li><li class="listitem"><p>
       NFS Ganesha
      </p></li><li class="listitem"><p>
       Samba Gateways
      </p></li></ol></div></li></ul></div></section><section class="sect1" id="upgrade-offline-ctdb-cluster" data-id-title="Offline Upgrade of CTDB Clusters"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.4 </span><span class="title-name">Offline Upgrade of CTDB Clusters</span> <a title="Permalink" class="permalink" href="#upgrade-offline-ctdb-cluster">#</a></h2></div></div></div><p>
   CTDB provides a clustered database used by Samba Gateways. The CTDB protocol does
   not support clusters of nodes communicating with different protocol
   versions. Therefore, CTDB nodes need to be taken offline prior to performing
   a SUSE Enterprise Storage upgrade.
  </p><p>
   CTDB refuses to start if it is running alongside an incompatible version.
   For example, if you start a SUSE Enterprise Storage 6 CTDB version while
   SUSE Enterprise Storage 5.5 CTDB versions are running, then it will
   fail.
  </p><p>
   To take the CTDB offline, stop the SLE-HA cloned CTDB resource. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>crm resource stop <em class="replaceable">cl-ctdb</em></pre></div><p>
   This will stop the resource across all gateway nodes (assigned to the cloned
   resource). Verify all the services are stopped by running the following
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>crm status</pre></div><div id="id-1.4.4.3.8.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Ensure CTDB is taken offline prior to the SUSE Enterprise Storage 5.5
    to SUSE Enterprise Storage 6 upgrade of the CTDB and Samba Gateway packages.
    SLE-HA may also specify requirements for the upgrade of the underlying
    pacemaker/Linux-HA cluster; these should be tracked separately.
   </p></div><p>
   The SLE-HA cloned CTDB resource can be restarted once the new packages have
   been installed on all Samba Gateway nodes and the underlying pacemaker/Linux-HA
   cluster is up. To restart the CTDB resource run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>crm resource start cl-ctdb</pre></div></section><section class="sect1" id="upgrade-one-node" data-id-title="Per-Node Upgrade Instructions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.5 </span><span class="title-name">Per-Node Upgrade Instructions</span> <a title="Permalink" class="permalink" href="#upgrade-one-node">#</a></h2></div></div></div><p>
   To ensure the core cluster services are available during the upgrade, you
   need to upgrade the cluster nodes sequentially one by one. There are two
   ways you can perform the upgrade of a node: either using the <span class="emphasis"><em>
   installer DVD</em></span> or using the <span class="emphasis"><em>distribution migration
   system</em></span>.
  </p><p>
   After upgrading each node, we recommend running
   <code class="command">rpmconfigcheck</code> to check for any updated configuration
   files that have been edited locally. If the command returns a list of file
   names with a suffix <code class="filename">.rpmnew</code>,
   <code class="filename">.rpmorig</code>, or <code class="filename">.rpmsave</code>, compare
   these files against the current configuration files to ensure that no local
   changes have been lost. If necessary, update the affected files. For more
   information on working with <code class="filename">.rpmnew</code>,
   <code class="filename">.rpmorig</code>, and <code class="filename">.rpmsave</code> files,
   refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-sw-cl.html#sec-rpm-packages-manage" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-sw-cl.html#sec-rpm-packages-manage</a>.
  </p><div id="id-1.4.4.3.9.4" data-id-title="Orphaned Packages" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Orphaned Packages</h6><p>
    After a node is upgraded, a number of packages will be in an 'orphaned'
    state without a parent repository. This happens because python3 related
    packages do not make python2 packages obsolete.
   </p><p>
    Find more information about listing orphaned packages in
    <a class="link" href="https://documentation.suse.com/sles/12-SP5/html/SLES-all/cha-sw-cl.html#sec-zypper-softup-orphaned" target="_blank">https://documentation.suse.com/sles/12-SP5/html/SLES-all/cha-sw-cl.html#sec-zypper-softup-orphaned</a>.
   </p></div><section class="sect2" id="upgrade-one-node-manual" data-id-title="Manual Node Upgrade Using the Installer DVD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.1 </span><span class="title-name">Manual Node Upgrade Using the Installer DVD</span> <a title="Permalink" class="permalink" href="#upgrade-one-node-manual">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Reboot the node from the SUSE Linux Enterprise Server 15 SP1 installer DVD/image.
     </p></li><li class="step"><p>
      On the YaST command line, add the option
      <code class="option">YAST_ACTIVATE_LUKS=0</code>. This option ensures that the
      system does not ask for a password for encrypted disks.
     </p><div id="id-1.4.4.3.9.5.2.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       This must not be enabled by default as it would break full-disk
       encryption on the system disk or part of the system disk. This parameter
       only works if it is provided by the installer. If not provided, you will
       be prompted for an encryption password for each individual disk
       partition.
      </p><p>
       This is only supported since the 3rd Quarterly Update of SLES 15 SP1.
       You need SLE-15-SP1-Installer-DVD-*-QU3-DVD1.iso media or newer.
      </p></div></li><li class="step"><p>
      Select <span class="guimenu">Upgrade</span> from the boot menu.
     </p></li><li class="step"><p>
      On the <span class="guimenu">Select the Migration Target</span> screen, verify that
      'SUSE Linux Enterprise Server 15 SP1' is selected and activate the <span class="guimenu">Manually Adjust the
      Repositories for Migration</span> check box.
     </p><div class="figure" id="id-1.4.4.3.9.5.2.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/migration-target.png" target="_blank"><img src="images/migration-target.png" width="" alt="Select the Migration Target"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.1: </span><span class="title-name">Select the Migration Target </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.9.5.2.4.2">#</a></h6></div></div></li><li class="step"><p>
      Select the following modules to install:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        SUSE Enterprise Storage 6 x86_64
       </p></li><li class="listitem"><p>
        Basesystem Module 15 SP1 x86_64
       </p></li><li class="listitem"><p>
        Desktop Applications Module 15 SP1 x86_64
       </p></li><li class="listitem"><p>
        Legacy Module 15 SP1 x86_64
       </p></li><li class="listitem"><p>
        Server Applications Module 15 SP1 x86_64
       </p></li></ul></div></li><li class="step"><p>
      On the <span class="guimenu">Previously Used Repositories</span> screen, verify
      that the correct repositories are selected. If the system is not
      registered with SCC/SMT, you need to add the repositories manually.
     </p><p>
      SUSE Enterprise Storage 6 requires:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        SLE-Module-Basesystem15-SP1-Pool
       </p></li><li class="listitem"><p>
         SLE-Module-Basesystem15-SP1-Updates
       </p></li><li class="listitem"><p>
         SLE-Module-Server-Applications15-SP1-Pool
       </p></li><li class="listitem"><p>
         SLE-Module-Server-Applications15-SP1-Updates
       </p></li><li class="listitem"><p>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </p></li><li class="listitem"><p>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </p></li><li class="listitem"><p>
         SLE-Product-SLES15-SP1-Pool
       </p></li><li class="listitem"><p>
         SLE-Product-SLES15-SP1-Updates
       </p></li><li class="listitem"><p>
         SLE15-SP1-Installer-Updates
       </p></li><li class="listitem"><p>
         SUSE-Enterprise-Storage-6-Pool
       </p></li><li class="listitem"><p>
         SUSE-Enterprise-Storage-6-Updates
       </p></li></ul></div><p>
      If you intend to migrate <code class="systemitem">ntpd</code> to
      <code class="systemitem">chronyd</code> after SES migration
      (refer to <a class="xref" href="#upgrade-ntp" title="6.2.9. Migrate from ntpd to chronyd">Section 6.2.9, “Migrate from <code class="systemitem">ntpd</code> to <code class="systemitem">chronyd</code>”</a>), include the following
      repositories:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        SLE-Module-Legacy15-SP1-Pool
       </p></li><li class="listitem"><p>
        SLE-Module-Legacy15-SP1-Updates
       </p></li></ul></div><p>
      NFS/SMB Gateway on SLE-HA on SUSE Linux Enterprise Server 15 SP1 requires:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        SLE-Product-HA15-SP1-Pool
       </p></li><li class="listitem"><p>
        SLE-Product-HA15-SP1-Updates
       </p></li></ul></div></li><li class="step"><p>
      Review the <span class="guimenu">Installation Settings</span> and start the
      installation procedure by clicking <span class="guimenu">Update</span>.
     </p></li></ol></div></div></section><section class="sect2" id="upgrade-one-node-auto" data-id-title="Node Upgrade Using the SUSE Distribution Migration System"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.2 </span><span class="title-name">Node Upgrade Using the SUSE Distribution Migration System</span> <a title="Permalink" class="permalink" href="#upgrade-one-node-auto">#</a></h3></div></div></div><p>
    The <span class="emphasis"><em>Distribution Migration System</em></span> (DMS) provides an
    upgrade path for an installed SUSE Linux Enterprise system from one major version to
    another. The following procedure utilizes DMS to upgrade SUSE Enterprise Storage
    5.5 to version 6, including the underlying
    SUSE Linux Enterprise Server 12 SP3 to SUSE Linux Enterprise Server 15 SP1 migration.
   </p><p>
    Refer to
    <a class="link" href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/" target="_blank">https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/</a>
    to find both general and detailed information about DMS.
   </p><section class="sect3" id="upgrade-node-before-you-begin" data-id-title="Before You Begin"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.5.2.1 </span><span class="title-name">Before You Begin</span> <a title="Permalink" class="permalink" href="#upgrade-node-before-you-begin">#</a></h4></div></div></div><p>
     Before the starting the upgrade process, check whether the
     <span class="package">sles-ltss-release</span> or
     <span class="package">sles-ltss-release-POOL</span> packages are installed on any
     node of the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>rpm -q sles-ltss-release
 <code class="prompt user">root@minion &gt; </code>rpm -q sles-ltss-release-POOL</pre></div><p>
     If either or both are installed, remove them:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper rm -y sles-ltss-release sles-ltss-release-POOL</pre></div><div id="id-1.4.4.3.9.6.4.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      This must be done on all nodes of the cluster before proceeding.
     </p></div><div id="id-1.4.4.3.9.6.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ensure you also follow the <a class="xref" href="#upgrade-verify-state" title="6.2.12. Check the Cluster's State">Section 6.2.12, “Check the Cluster's State”</a>
      guidelines. The upgrade must not be started until all nodes are fully
      patched. See <a class="xref" href="#upgrade-prepare-patch" title="6.2.10.3. Patch the Whole Cluster to the Latest Patches">Section 6.2.10.3, “Patch the Whole Cluster to the Latest Patches”</a> for more
      information.
     </p></div></section><section class="sect3" id="upgrade-nodes-dms" data-id-title="Upgrading Nodes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.5.2.2 </span><span class="title-name">Upgrading Nodes</span> <a title="Permalink" class="permalink" href="#upgrade-nodes-dms">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install the migration RPM packages. They adjust the GRUB boot loader
       to automatically trigger the upgrade on next reboot. Install the
       <span class="package">SLES15-SES-Migration</span> and
       <span class="package">suse-migration-sle15-activation</span> packages:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper install SLES15-SES-Migration suse-migration-sle15-activation</pre></div></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
         If the node being upgraded <span class="bold"><strong>is</strong></span>
         registered with a repository staging system such as SCC, SMT, RMT, or
         SUSE Manager, create the
         <code class="filename">/etc/sle-migration-service.yml</code> with the following
         content:
        </p><div class="verbatim-wrap"><pre class="screen">  use_zypper_migration: true
  preserve:
    rules:
      - /etc/udev/rules.d/70-persistent-net.rules</pre></div></li><li class="step"><p>
         If the node being upgraded is <span class="bold"><strong>not</strong></span>
         registered with a repository staging system such as SCC, SMT, RMT, or
         SUSE Manager, perform the following changes:
        </p><ol type="i" class="substeps"><li class="step"><p>
           Create the <code class="filename">/etc/sle-migration-service.yml</code> with
           the following content:
          </p><div class="verbatim-wrap"><pre class="screen">  use_zypper_migration: false
  preserve:
    rules:
      - /etc/udev/rules.d/70-persistent-net.rules</pre></div></li><li class="step"><p>
           Disable or remove the SLE 12 SP3 and SES 5 repos, and add the SLE 15
           SP1 and SES6 repos. Find the list of related repositories in
           <a class="xref" href="#upgrade-prepare-repos" title="6.2.10.1. Required Software Repositories">Section 6.2.10.1, “Required Software Repositories”</a>.
          </p></li></ol></li></ol></li><li class="step"><p>
       Reboot to start the upgrade. While the upgrade is running, you can log
       in to the upgraded node via <code class="command">ssh</code> as the migration user
       using the existing SSH key from the host system as described in
       <a class="link" href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/" target="_blank">https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/</a>.
       For SUSE Enterprise Storage, if you have physical access or direct console access
       to the machine, you can also log in as <code class="systemitem">root</code> on the system console
       using the password <code class="literal">sesupgrade</code>. The node will reboot
       automatically after the upgrade.
      </p><div id="id-1.4.4.3.9.6.5.2.3.2" data-id-title="Upgrade Failure" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Upgrade Failure</h6><p>
        If the upgrade fails, inspect
        <code class="filename">/var/log/distro_migration.log</code>. Fix the problem,
        re-install the migration RPM packages, and reboot the node.
       </p></div></li></ol></div></div></section></section></section><section class="sect1" id="upgrade-adm" data-id-title="Upgrade the Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.6 </span><span class="title-name">Upgrade the Admin Node</span> <a title="Permalink" class="permalink" href="#upgrade-adm">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The following commands will still work, although Salt minions are running
     old versions of Ceph and Salt: <code class="command">salt '*' test.ping</code>
     and <code class="command">ceph status</code>
    </p></li><li class="listitem"><p>
     After the upgrade of the Admin Node, openATTIC will no longer be installed.
    </p></li><li class="listitem"><p>
     If the Admin Node hosted SMT, complete its migration to RMT (refer to
     <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-rmt-migrate.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-rmt-migrate.html</a>).
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>Use the procedure described in
     <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
    </p></li></ul></div><div id="id-1.4.4.3.10.3" data-id-title="Status of Cluster Nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Status of Cluster Nodes</h6><p>
    After the Admin Node is upgraded, you can run the <code class="command">salt-run
    upgrade.status</code> command to view useful information about cluster
    nodes. The command lists the Ceph and OS versions of all nodes, and
    recommends the order in which to upgrade any nodes that are still running
    old versions.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run upgrade.status
The newest installed software versions are:
  ceph: ceph version 14.2.1-468-g994fd9e0cc (994fd9e0ccc50c2f3a55a3b7a3d4e0ba74786d50) nautilus (stable)
  os: SUSE Linux Enterprise Server 15 SP1

Nodes running these software versions:
  admin.ceph (assigned roles: master)
  mon2.ceph (assigned roles: admin, mon, mgr)

Nodes running older software versions must be upgraded in the following order:
   1: mon1.ceph (assigned roles: admin, mon, mgr)
   2: mon3.ceph (assigned roles: admin, mon, mgr)
   3: data1.ceph (assigned roles: storage)
[...]</pre></div></div></section><section class="sect1" id="upgrade-mons" data-id-title="Upgrade Ceph Monitor/Ceph Manager Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.7 </span><span class="title-name">Upgrade Ceph Monitor/Ceph Manager Nodes</span> <a title="Permalink" class="permalink" href="#upgrade-mons">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If your cluster <span class="bold"><strong>does not use</strong></span> MDS roles,
     upgrade MON/MGR nodes one by one.
    </p></li><li class="listitem"><p>
     If your cluster <span class="bold"><strong>uses</strong></span> MDS roles, and
     MON/MGR and MDS roles are co-located, you need to shrink the MDS cluster
     and then upgrade the co-located nodes. Refer to
     <a class="xref" href="#upgrade-mds" title="6.8. Upgrade Metadata Servers">Section 6.8, “Upgrade Metadata Servers”</a> for more details.
    </p></li><li class="listitem"><p>
     If your cluster <span class="bold"><strong>uses</strong></span> MDS roles and they
     run on <span class="bold"><strong>dedicated</strong></span> servers, upgrade all
     MON/MGR nodes one by one, then shrink the MDS cluster and upgrade it.
     Refer to <a class="xref" href="#upgrade-mds" title="6.8. Upgrade Metadata Servers">Section 6.8, “Upgrade Metadata Servers”</a> for more details.
    </p></li></ul></div><div id="id-1.4.4.3.11.3" data-id-title="Ceph Monitor Upgrade" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Ceph Monitor Upgrade</h6><p>
    Due to a limitation in the Ceph Monitor design, once two MONs have been upgraded
    to SUSE Enterprise Storage 6 and have formed a quorum, the third MON
    (while still on SUSE Enterprise Storage 5.5) will not rejoin the MON
    cluster if it restarted for any reason, including a node reboot. Therefore,
    when two MONs have been upgraded it is best to upgrade the rest as soon as
    possible.
   </p></div><p>
   <span class="bold"><strong>Use the procedure described in
   <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
  </p></section><section class="sect1" id="upgrade-mds" data-id-title="Upgrade Metadata Servers"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.8 </span><span class="title-name">Upgrade Metadata Servers</span> <a title="Permalink" class="permalink" href="#upgrade-mds">#</a></h2></div></div></div><p>
   You need to shrink the Metadata Server (MDS) cluster. Because of incompatible features
   between the SUSE Enterprise Storage 5.5 and 6 versions,
   the older MDS daemons will shut down as soon as they see a single SES
   6 level MDS join the cluster. Therefore it is necessary to
   shrink the MDS cluster to a single active MDS (and no standbys) for the
   duration of the MDS node upgrades. As soon as the second node is upgraded,
   you can extend the MDS cluster again.
  </p><div id="id-1.4.4.3.12.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    On a heavily loaded MDS cluster, you may need to reduce the load (for
    example by stopping clients) so that a single active MDS is able to handle
    the workload.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Note the current value of the <code class="option">max_mds</code> option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs get cephfs | grep max_mds</pre></div></li><li class="step"><p>
     Shrink the MDS cluster if you have more then 1 active MDS daemon, i.e.
     <code class="option">max_mds</code> is &gt; 1. To shrink the MDS cluster, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">FS_NAME</em> max_mds 1</pre></div><p>
     where <em class="replaceable">FS_NAME</em> is the name of your CephFS
     instance ('cephfs' by default).
    </p></li><li class="step"><p>
     Find the node hosting one of the standby MDS daemons. Consult the output
     of the <code class="command">ceph fs status</code> command and start the upgrade of
     the MDS cluster on this node.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+</pre></div><p>
     In this example, you need to start the upgrade procedure either on node
     'mon3-6' or 'mon2-6'.
    </p></li><li class="step"><p>
     Upgrade the node with the standby MDS daemon. After the upgraded MDS node
     starts, the outdated MDS daemons will shut down automatically. At this
     point, clients may experience a short downtime of the CephFS service.
    </p><p>
     <span class="bold"><strong>Use the procedure described in
     <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
    </p></li><li class="step"><p>
     Upgrade the remaining MDS nodes.
    </p></li><li class="step"><p>
     Reset <code class="option">max_mds</code> to the desired configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">FS_NAME</em> max_mds <em class="replaceable">ACTIVE_MDS_COUNT</em></pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-main-osd" data-id-title="Upgrade Ceph OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.9 </span><span class="title-name">Upgrade Ceph OSDs</span> <a title="Permalink" class="permalink" href="#upgrade-main-osd">#</a></h2></div></div></div><p>
   For each storage node, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Identify which OSD daemons are running on a particular node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tree</pre></div></li><li class="step"><p>
     Set the <code class="option">noout</code> flag for each OSD daemon on the node that
     is being upgraded:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd add-noout osd.<em class="replaceable">OSD_ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>for i in $(ceph osd ls-tree <em class="replaceable">OSD_NODE_NAME</em>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</pre></div><p>
     Verify with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail | grep noout</pre></div><p>
     or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</pre></div></li><li class="step"><p>
     Create <code class="filename">/etc/ceph/osd/*.json</code> files for all existing
     OSDs by running the following command on the node that is going to be
     upgraded:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume simple scan --force</pre></div></li><li class="step"><p>
     Upgrade the OSD node. <span class="bold"><strong>Use the procedure described in
     <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
    </p></li><li class="step"><p>
     Activate all OSDs found in the system:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume simple activate --all</pre></div><div id="id-1.4.4.3.13.3.5.3" data-id-title="Activating Data Partitions Individually" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Activating Data Partitions Individually</h6><p>
      If you want to activate data partitions individually, you need to find
      the correct <code class="command">ceph-volume</code> command for each partition to
      activate it. Replace <em class="replaceable">X1</em> with the partition's
      correct letter/number:
     </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm@osd &gt; </code>ceph-volume simple scan /dev/sd<em class="replaceable">X1</em></pre></div><p>
      For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290</pre></div><p>
      The last line of the output contains the command to activate the
      partition:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--&gt; All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--&gt; Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290</pre></div></div></li><li class="step"><p>
     Verify that the OSD node will start properly after the reboot.
    </p></li><li class="step"><p>
     Address the 'Legacy BlueStore stats reporting detected on XX OSD(s)'
     message:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <span class="bold"><strong>Legacy BlueStore stats reporting detected on 6 OSD(s)</strong></span></pre></div><p>
     The warning is normal when upgrading Ceph to 14.2.2. You can disable it
     by setting:
    </p><div class="verbatim-wrap"><pre class="screen">bluestore_warn_on_legacy_statfs = false</pre></div><p>
     The proper fix is to run the following command on all OSDs while they are
     stopped:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</pre></div><p>
     Following is a helper script that runs the <code class="command">ceph-bluestore-tool
     repair</code> for all OSDs on the <em class="replaceable">NODE_NAME</em>
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>OSDNODE=<em class="replaceable">OSD_NODE_NAME</em>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE* cmd.run "systemctl stop ceph-osd@$OSD";\
 salt $OSDNODE* cmd.run "ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD";\
 salt $OSDNODE* cmd.run "systemctl start ceph-osd@$OSD";\
 done</pre></div></li><li class="step"><p>
     Unset the 'noout' flag for each OSD daemon on the node that is upgraded:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd rm-noout osd.<em class="replaceable">OSD_ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>for i in $(ceph osd ls-tree <em class="replaceable">OSD_NODE_NAME</em>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</pre></div><p>
     Verify with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail | grep noout</pre></div><p>
     Note:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <span class="bold"><strong>Legacy BlueStore stats reporting detected on 6 OSD(s)</strong></span></pre></div></li><li class="step"><p>
     Verify the cluster status. It will be similar to the following output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean</pre></div></li><li class="step"><p>
     Once the last OSD node has been upgraded, issue the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd require-osd-release nautilus</pre></div><p>
     This disallows pre-SUSE Enterprise Storage 6 and Nautilus OSDs and
     enables all new SUSE Enterprise Storage 6 and Nautilus-only OSD
     functionality.
    </p></li><li class="step"><p>
     Enable the new <code class="literal">v2</code> network protocol by issuing the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon enable-msgr2</pre></div><p>
     This instructs all monitors that bind to the old default port for the
     legacy <code class="literal">v1</code> Messenger protocol (6789) to also bind to the
     new <code class="literal">v2</code> protocol port (3300). To see if all monitors
     have been updated, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon dump</pre></div><p>
     Verify that each monitor has both a <code class="literal">v2:</code> and
     <code class="literal">v1:</code> address listed.
    </p></li><li class="step"><p>
     Verify that all OSD nodes were rebooted and that OSDs started
     automatically after the reboot.
    </p></li></ol></div></div></section><section class="sect1" id="upgrade-appnodes-order" data-id-title="Upgrade Gateway Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.10 </span><span class="title-name">Upgrade Gateway Nodes</span> <a title="Permalink" class="permalink" href="#upgrade-appnodes-order">#</a></h2></div></div></div><p>
   Upgrade gateway nodes in the following order:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Object Gateways
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If the Object Gateways are fronted by a load balancer, then a rolling upgrade of
       the Object Gateways should be possible without an outage.
      </p></li><li class="listitem"><p>
       Validate that the Object Gateway daemons are running after each upgrade, and test
       with S3/Swift client.
      </p></li><li class="listitem"><p>
       <span class="bold"><strong>Use the procedure described in
       <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
      </p></li></ul></div></li><li class="listitem"><p>
     iSCSI Gateways
    </p><div id="id-1.4.4.3.14.3.2.2" data-id-title="Package Dependency Conflict" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Package Dependency Conflict</h6><p>
      After a package dependency is calculated, you need to resolve a package
      dependency conflict. It applies to the
      <code class="literal">patterns-ses-ceph_iscsi</code> version mismatch.
     </p><div class="figure" id="id-1.4.4.3.14.3.2.2.3"><div class="figure-contents"><div class="mediaobject"><a href="images/igw_pkg_conflict.png" target="_blank"><img src="images/igw_pkg_conflict.png" width="" alt="Dependency Conflict Resolution"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.2: </span><span class="title-name">Dependency Conflict Resolution </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.14.3.2.2.3">#</a></h6></div></div><p>
      From the four presented solutions, choose deinstalling the
      <code class="literal">patterns-ses-ceph_iscsi</code> pattern. This way you will
      keep the required <span class="package">lrbd</span> package installed.
     </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If iSCSI initiators are configured with multipath, then a rolling
       upgrade of the iSCSI Gateways should be possible without an outage.
      </p></li><li class="listitem"><p>
       Validate that the <code class="systemitem">lrbd</code> daemon is
       running after each upgrade, and test with initiator.
      </p></li><li class="listitem"><p>
       <span class="bold"><strong>Use the procedure described in
       <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
      </p></li></ul></div></li><li class="listitem"><p>
     NFS Ganesha. <span class="bold"><strong>Use the procedure described in
     <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
    </p></li><li class="listitem"><p>
     Samba Gateways. <span class="bold"><strong>Use the procedure described in
     <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>.</strong></span>
    </p></li></ol></div></section><section class="sect1" id="final-steps" data-id-title="Steps to Take after the Last Node Has Been Upgraded"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.11 </span><span class="title-name">Steps to Take after the Last Node Has Been Upgraded</span> <a title="Permalink" class="permalink" href="#final-steps">#</a></h2></div></div></div><section class="sect2" id="final-steps-update-conf" data-id-title="Update Ceph Monitor Setting"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.11.1 </span><span class="title-name">Update Ceph Monitor Setting</span> <a title="Permalink" class="permalink" href="#final-steps-update-conf">#</a></h3></div></div></div><p>
    For each host that has been upgraded — OSD, MON, MGR, MDS, and
    Gateway nodes, as well as client hosts — update your
    <code class="filename">ceph.conf</code> file so that it either specifies no monitor
    port (if you are running the monitors on the default ports) or references
    both the <code class="literal">v2</code> and <code class="literal">v1</code> addresses and
    ports explicitly.
   </p><div id="id-1.4.4.3.15.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Things will still work if only the <code class="literal">v1</code> IP and port are
     listed, but each CLI instantiation or daemon will need to reconnect after
     learning that the monitors also speak the <code class="literal">v2</code> protocol.
     This slows things down and prevents a full transition to the
     <code class="literal">v2</code> protocol.
    </p></div></section><section class="sect2" id="final-steps-disable-insecure" data-id-title="Disable Insecure Clients"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.11.2 </span><span class="title-name">Disable Insecure Clients</span> <a title="Permalink" class="permalink" href="#final-steps-disable-insecure">#</a></h3></div></div></div><p>
    Since Nautilus v14.2.20, a new health warning was introduced that informs
    you that insecure clients are allowed to join the cluster. This warning is
    <span class="emphasis"><em>on</em></span> by default. The Ceph Dashboard will show the cluster
    in the <code class="literal">HEALTH_WARN</code> status and verifying the cluster
    status on the command line informs you as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]</pre></div><p>
    This warning means that the Ceph Monitors are still allowing old, unpatched
    clients to connect to the cluster. This ensures existing clients can still
    connect while the cluster is being upgraded, but warns you that there is a
    problem that needs to be addressed. When the cluster and all clients are
    upgraded to the latest version of Ceph, disallow unpatched clients by
    running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></section><section class="sect2" id="final-steps-meter" data-id-title="Enable the Telemetry Module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.11.3 </span><span class="title-name">Enable the Telemetry Module</span> <a title="Permalink" class="permalink" href="#final-steps-meter">#</a></h3></div></div></div><p>
    Finally, consider enabling the Telemetry module to send anonymized usage
    statistics and crash information to the upstream Ceph developers. To see
    what would be reported (without actually sending any information to
    anyone):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module enable telemetry
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show</pre></div><p>
    If you are comfortable with the high-level cluster metadata that will be
    reported, you can opt-in to automatically report it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div></section></section><section class="sect1" id="upgrade-main-policy" data-id-title="Update policy.cfg and Deploy Ceph Dashboard Using DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.12 </span><span class="title-name">Update <code class="filename">policy.cfg</code> and Deploy Ceph Dashboard Using DeepSea</span> <a title="Permalink" class="permalink" href="#upgrade-main-policy">#</a></h2></div></div></div><p>
   On the Admin Node, edit
   <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and apply the
   following changes:
  </p><div id="id-1.4.4.3.16.3" data-id-title="No New Services" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: No New Services</h6><p>
    During cluster upgrade, do not add new services to the
    <code class="filename">policy.cfg</code> file. Change the cluster architecture only
    after the upgrade is completed.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Remove <code class="literal">role-openattic</code>.
    </p></li><li class="step"><p>
     Add <code class="literal">role-prometheus</code> and <code class="literal">role-grafana</code>
     to the node that had Prometheus and Grafana installed, usually the
     Admin Node.
    </p></li><li class="step"><p>
     Role <code class="literal">profile-<em class="replaceable">PROFILE_NAME</em></code> is
     now ignored. Add new corresponding role, <code class="literal">role-storage</code>
     line. For example, for existing
    </p><div class="verbatim-wrap"><pre class="screen">profile-default/cluster/*.sls</pre></div><p>
     add
    </p><div class="verbatim-wrap"><pre class="screen">role-storage/cluster/*.sls</pre></div></li><li class="step"><p>
     Synchronize all Salt modules:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.sync_all</pre></div></li><li class="step"><p>
     Update the Salt pillar by running DeepSea stage 1 and stage 2:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div></li><li class="step"><p>
     Clean up openATTIC:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">OA_MINION</em> state.apply ceph.rescind.openattic
<code class="prompt user">root@master # </code>salt <em class="replaceable">OA_MINION</em> state.apply ceph.remove.openattic</pre></div></li><li class="step"><p>
     Unset the <code class="option">restart_igw</code> grain to prevent stage 0 from
     restarting iSCSI Gateway, which is not installed yet:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' grains.delkey restart_igw</pre></div></li><li class="step"><p>
     Finally, run through DeepSea stages 0-4:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div><div id="id-1.4.4.3.16.4.8.3" data-id-title="subvolume missing Errors during Stage 3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: 'subvolume missing' Errors during Stage 3</h6><p>
      DeepSea stage 3 may fail with an error similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen">subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</pre></div><p>
      In this case, you need to edit
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
      add the following line:
     </p><div class="verbatim-wrap"><pre class="screen">subvolume_init: disabled</pre></div><p>
      Then refresh the Salt pillar and re-run DeepSea stage.3:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.refresh_pillar
 <code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div><p>
      After DeepSea successfully finished stage.3, the Ceph Dashboard will be
      running. Refer to <span class="intraxref">Book “Administration Guide”</span> for a detailed
      overview of Ceph Dashboard features.
     </p><p>
      To list nodes running dashboard, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr services | grep dashboard</pre></div><p>
      To list admin credentials, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call grains.get dashboard_creds</pre></div></div></li><li class="step"><p>
     Sequentially restart the Object Gateway services to use 'beast' Web server instead
     of the outdated 'civetweb':
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.rgw.force</pre></div></li><li class="step"><p>
     Before you continue, we strongly recommend enabling the Ceph telemetry
     module. For more information, see <span class="intraxref">Book “Administration Guide”, Chapter 21 “Ceph Manager Modules”, Section 21.2 “Telemetry Module”</span>
     for information and instructions.
    </p></li></ol></div></div></section><section class="sect1" id="upgrade-drive-groups" data-id-title="Migration from Profile-based Deployments to DriveGroups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.13 </span><span class="title-name">Migration from Profile-based Deployments to DriveGroups</span> <a title="Permalink" class="permalink" href="#upgrade-drive-groups">#</a></h2></div></div></div><p>
   In SUSE Enterprise Storage 5.5, DeepSea offered so called 'profiles'
   to describe the layout of your OSDs. Starting with SUSE Enterprise Storage
   6, we moved to a different approach called
   <span class="emphasis"><em>DriveGroups</em></span> (find more details in
   <a class="xref" href="#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a>).
  </p><div id="id-1.4.4.3.17.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Migrating to the new approach is not immediately mandatory. Destructive
    operations, such as <code class="command">salt-run osd.remove</code>,
    <code class="command">salt-run osd.replace</code>, or <code class="command">salt-run
    osd.purge</code> are still available. However, adding new OSDs will
    require your action.
   </p></div><p>
   Because of the different approach of these implementations, we do not offer
   an automated migration path. However, we offer a variety of
   tools—Salt runners—to make the migration as simple as
   possible.
  </p><section class="sect2" id="id-1.4.4.3.17.5" data-id-title="Analyze the Current Layout"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.13.1 </span><span class="title-name">Analyze the Current Layout</span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.17.5">#</a></h3></div></div></div><p>
    To view information about the currently deployed OSDs, synchronize all
    Salt modules and run the <code class="command">disks.discover</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.sync_all
<code class="prompt user">root@master # </code>salt-run disks.discover</pre></div><p>
    Alternatively, you can inspect the content of the files in the
    <code class="filename">/srv/pillar/ceph/proposals/profile-*/</code> directories.
    They have a similar structure to the following:
   </p><div class="verbatim-wrap"><pre class="screen">ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore</pre></div></section><section class="sect2" id="id-1.4.4.3.17.6" data-id-title="Create DriveGroups Matching the Current Layout"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.13.2 </span><span class="title-name">Create DriveGroups Matching the Current Layout</span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.17.6">#</a></h3></div></div></div><p>
    Refer to <a class="xref" href="#ds-drive-groups-specs" title="5.5.2.1. Specification">Section 5.5.2.1, “Specification”</a> for more details on
    DriveGroups specification.
   </p><p>
    The difference between a fresh deployment and upgrade scenario is that the
    drives to be migrated are already 'used'. Because
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.list</pre></div><p>
    looks for unused disks only, use
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.list include_unavailable=True</pre></div><p>
    Adjust DriveGroups until you match your current setup. For a more visual
    representation of what will be happening, use the following command. Note
    that it has no output if there are no free disks:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.report bypass_pillar=True</pre></div><p>
    If you verified that your DriveGroups are properly configured and want to
    apply the new approach, remove the files from the
    <code class="filename">/srv/pillar/ceph/proposals/profile-<em class="replaceable">PROFILE_NAME</em>/</code>
    directory, remove the corresponding
    <code class="literal">profile-<em class="replaceable">PROFILE_NAME</em>/cluster/*.sls</code>
    lines from the <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>
    file, and run DeepSea stage 2 to refresh the Salt pillar.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
    Verify the result by running the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt target_node pillar.get ceph:storage
<code class="prompt user">root@master # </code>salt-run disks.report</pre></div><div id="id-1.4.4.3.17.6.13" data-id-title="Incorrect DriveGroups Configuration" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Incorrect DriveGroups Configuration</h6><p>
     If your DriveGroups are not properly configured and there are spare disks in
     your setup, they will be deployed in the way you specified them. We
     recommend running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disks.report</pre></div></div></section><section class="sect2" id="upgrade-osd-deployment" data-id-title="OSD Deployment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.13.3 </span><span class="title-name">OSD Deployment</span> <a title="Permalink" class="permalink" href="#upgrade-osd-deployment">#</a></h3></div></div></div><p>
    As of the Ceph Mimic release (SES 5), the <code class="literal">ceph-disk</code>
    tool is deprecated, and as of the Ceph Nautilus release (SES 6) it is no
    longer shipped upstream.
   </p><p>
    <code class="literal">ceph-disk</code> is still supported in SUSE Enterprise Storage 6. Any
    pre-deployed <code class="literal">ceph-disk</code> OSDs will continue to function
    normally. However, when a disk breaks there is <span class="emphasis"><em>no</em></span>
    migration path: a disk will need to be re-deployed.
   </p><p>
    For completeness, consider migrating OSDs on the whole node. There are two
    paths for SUSE Enterprise Storage 6 users:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Keep OSDs deployed with <code class="literal">ceph-disk</code>: The
      <code class="command">simple</code> command provides a way to take over the
      management while disabling <code class="literal">ceph-disk</code> triggers.
     </p></li><li class="listitem"><p>
      Re-deploy existings OSDs with <code class="command">ceph-volume</code>. For more
      information on replacing your OSDs, see <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.8 “Replacing an OSD Disk”</span>.
     </p></li></ul></div><div id="id-1.4.4.3.17.7.6" data-id-title="Migrate to LVM Format" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Migrate to LVM Format</h6><p>
     Whenever a single legacy OSD needs to be replaced on a node, all OSDs that
     share devices with it need to be migrated to the LVM-based format.
    </p></div></section><section class="sect2" id="id-1.4.4.3.17.8" data-id-title="More Complex Setups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.13.4 </span><span class="title-name">More Complex Setups</span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.17.8">#</a></h3></div></div></div><p>
    If you have a more sophisticated setup than just stand-alone OSDs, for
    example dedicated WAL/DBs or encrypted OSDs, the migration can only happen
    when all OSDs assigned to that WAL/DB device are removed. This is due to
    the <code class="command">ceph-volume</code> command that creates Logical Volumes on
    disks before deployment. This prevents the user from mixing partition based
    deployments with LV based deployments. In such cases it is best to manually
    remove all OSDs that are assigned to a WAL/DB device and re-deploy them
    using the DriveGroups approach.
   </p></section></section></section><section class="chapter" id="ceph-deploy-ds-custom" data-id-title="Customizing the Default Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Customizing the Default Configuration</span> <a title="Permalink" class="permalink" href="#ceph-deploy-ds-custom">#</a></h2></div></div></div><p>
  You can change the default cluster configuration generated in Stage 2 (refer
  to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a>). For example, you may need to
  change network settings, or software that is installed on the Admin Node by
  default. You can perform the former by modifying the pillar updated after
  Stage 2, while the latter is usually done by creating a custom
  <code class="literal">sls</code> file and adding it to the pillar. Details are
  described in following sections.
 </p><section class="sect1" id="using-customized-files" data-id-title="Using Customized Configuration Files"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Using Customized Configuration Files</span> <a title="Permalink" class="permalink" href="#using-customized-files">#</a></h2></div></div></div><p>
   This section lists several tasks that require adding/changing your own
   <code class="literal">sls</code> files. Such a procedure is typically used when you
   need to change the default deployment process.
  </p><div id="id-1.4.4.4.4.3" data-id-title="Prefix Custom .sls Files" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Prefix Custom .sls Files</h6><p>
    Your custom .sls files belong to the same subdirectory as DeepSea's .sls
    files. To prevent overwriting your .sls files with the possibly newly added
    ones from the DeepSea package, prefix their name with the
    <code class="filename">custom-</code> string.
   </p></div><section class="sect2" id="id-1.4.4.4.4.4" data-id-title="Disabling a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.1 </span><span class="title-name">Disabling a Deployment Step</span> <a title="Permalink" class="permalink" href="#id-1.4.4.4.4.4">#</a></h3></div></div></div><p>
    If you address a specific task outside of the DeepSea deployment process
    and therefore need to skip it, create a 'no-operation' file following this
    example:
   </p><div class="procedure" id="disable-time-sync" data-id-title="Disabling Time Synchronization"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.1: </span><span class="title-name">Disabling Time Synchronization </span><a title="Permalink" class="permalink" href="#disable-time-sync">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/time/disabled.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">disable time setting:
test.nop</pre></div></li><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code>, add the
      following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">time_init: disabled</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.time
admin.ceph:
  Name: disable time setting - Function: test.nop - Result: Clean

Summary for admin.ceph
------------
Succeeded: 1
Failed:    0
------------
Total states run:     1</pre></div><div id="id-1.4.4.4.4.4.3.4.3" data-id-title="Unique ID" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unique ID</h6><p>
       The task ID 'disable time setting' may be any message unique within an
       <code class="literal">sls</code> file. Prevent ID collisions by specifying unique
       descriptions.
      </p></div></li></ol></div></div></section><section class="sect2" id="deepsea-replacing-step" data-id-title="Replacing a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.2 </span><span class="title-name">Replacing a Deployment Step</span> <a title="Permalink" class="permalink" href="#deepsea-replacing-step">#</a></h3></div></div></div><p>
    If you need to replace the default behavior of a specific step with a
    custom one, create a custom <code class="literal">sls</code> file with replacement
    content.
   </p><p>
    By default <code class="filename">/srv/salt/ceph/pool/default.sls</code> creates an
    rbd image called 'demo'. In our example, we do not want this image to be
    created, but we need two images: 'archive1' and 'archive2'.
   </p><div class="procedure" id="id-1.4.4.4.4.5.4" data-id-title="Replacing the demo rbd Image with Two Custom rbd Images"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.2: </span><span class="title-name">Replacing the <span class="emphasis"><em>demo</em></span> rbd Image with Two Custom rbd Images </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.4.5.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/pool/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">wait:
  module.run:
    - name: wait.out
    - kwargs:
        'status': "HEALTH_ERR"<span class="callout" id="co-deepsea-replace-wait">1</span>
    - fire_event: True

archive1:
  cmd.run:
    - name: "rbd -p rbd create archive1 --size=1024"<span class="callout" id="co-deepsea-replace-rbd">2</span>
    - unless: "rbd -p rbd ls | grep -q archive1$"
    - fire_event: True

archive2:
  cmd.run:
    - name: "rbd -p rbd create archive2 --size=768"
    - unless: "rbd -p rbd ls | grep -q archive2$"
    - fire_event: True</pre></div><div class="calloutlist"><table style="border: 0; "><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-deepsea-replace-wait"><span class="callout">1</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The <span class="bold"><strong>wait</strong></span> module will pause until the
        Ceph cluster does not have a status of <code class="literal">HEALTH_ERR</code>.
        In fresh installations, a Ceph cluster may have this status until a
        sufficient number of OSDs become available and the creation of pools
        has completed.
       </p></td></tr><tr><td style="width: 5%; vertical-align: top; text-align: left; "><p><a href="#co-deepsea-replace-rbd"><span class="callout">2</span></a> </p></td><td style="vertical-align: top; text-align: left; "><p>
        The <code class="command">rbd</code> command is not idempotent. If the same
        creation command is re-run after the image exists, the Salt state
        will fail. The <span class="bold"><strong>unless</strong></span> statement
        prevents this.
       </p></td></tr></table></div></li><li class="step"><p>
      To call the newly created custom file instead of the default, you need to
      edit <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>, add
      the following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">pool_init: custom</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.pool</pre></div></li></ol></div></div><div id="id-1.4.4.4.4.5.5" data-id-title="Authorization" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Authorization</h6><p>
     The creation of pools or images requires sufficient authorization. The
     <code class="literal">admin.ceph</code> minion has an admin keyring.
    </p></div><div id="id-1.4.4.4.4.5.6" data-id-title="Alternative Way" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Alternative Way</h6><p>
     Another option is to change the variable in
     <code class="filename">/srv/pillar/ceph/stack/ceph/roles/master.yml</code> instead.
     Using this file will reduce the clutter of pillar data for other minions.
    </p></div></section><section class="sect2" id="id-1.4.4.4.4.6" data-id-title="Modifying a Deployment Step"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.3 </span><span class="title-name">Modifying a Deployment Step</span> <a title="Permalink" class="permalink" href="#id-1.4.4.4.4.6">#</a></h3></div></div></div><p>
    Sometimes you may need a specific step to do some additional tasks. We do
    not recommend modifying the related state file as it may complicate a
    future upgrade. Instead, create a separate file to carry out the additional
    tasks identical to what was described in
    <a class="xref" href="#deepsea-replacing-step" title="7.1.2. Replacing a Deployment Step">Section 7.1.2, “Replacing a Deployment Step”</a>.
   </p><p>
    Name the new <code class="literal">sls</code> file descriptively. For example, if you
    need to create two rbd images in addition to the demo image, name the file
    <code class="filename">archive.sls</code>.
   </p><div class="procedure" id="id-1.4.4.4.4.6.4" data-id-title="Creating Two Additional rbd Images"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.3: </span><span class="title-name">Creating Two Additional rbd Images </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.4.6.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/pool/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">include:
 - .archive
 - .default</pre></div><div id="id-1.4.4.4.4.6.4.2.3" data-id-title="Include Precedence" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Include Precedence</h6><p>
       In this example, Salt will create the <span class="emphasis"><em>archive</em></span>
       images and then create the <span class="emphasis"><em>demo</em></span> image. The order
       does not matter in this example. To change the order, reverse the lines
       after the <code class="literal">include:</code> directive.
      </p><p>
       You can add the include line directly to
       <code class="filename">archive.sls</code> and all the images will get created as
       well. However, regardless of where the include line is placed, Salt
       processes the steps in the included file first. Although this behavior
       can be overridden with <span class="emphasis"><em>requires</em></span> and
       <span class="emphasis"><em>order</em></span> statements, a separate file that includes the
       others guarantees the order and reduces the chances of confusion.
      </p></div></li><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>, add
      the following line, and save it:
     </p><div class="verbatim-wrap"><pre class="screen">pool_init: custom</pre></div></li><li class="step"><p>
      Verify by refreshing the pillar and running the step:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh
<code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.pool</pre></div></li></ol></div></div></section><section class="sect2" id="id-1.4.4.4.4.7" data-id-title="Modifying a Deployment Stage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.4 </span><span class="title-name">Modifying a Deployment Stage</span> <a title="Permalink" class="permalink" href="#id-1.4.4.4.4.7">#</a></h3></div></div></div><p>
    If you need to add a completely separate deployment step, create three new
    files—an <code class="literal">sls</code> file that performs the command, an
    orchestration file, and a custom file which aligns the new step with the
    original deployment steps.
   </p><p>
    For example, if you need to run <code class="command">logrotate</code> on all minions
    as part of the preparation stage:
   </p><p>
    First create an <code class="literal">sls</code> file and include the
    <code class="command">logrotate</code> command.
   </p><div class="procedure" id="id-1.4.4.4.4.7.5" data-id-title="Running logrotate on all Salt minions"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.4: </span><span class="title-name">Running <code class="command">logrotate</code> on all Salt minions </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.4.7.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a directory such as <code class="filename">/srv/salt/ceph/logrotate</code>.
     </p></li><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/logrotate/init.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">rotate logs:
  cmd.run:
    - name: "/usr/sbin/logrotate /etc/logrotate.conf"</pre></div></li><li class="step"><p>
      Verify that the command works on a minion:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'admin.ceph' state.apply ceph.logrotate</pre></div></li></ol></div></div><p>
    Because the orchestration file needs to run before all other preparation
    steps, add it to the <span class="emphasis"><em>Prep</em></span> stage 0:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/stage/prep/logrotate.sls</code> with
      the following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">logrotate:
  salt.state:
    - tgt: '*'
    - sls: ceph.logrotate</pre></div></li><li class="step"><p>
      Verify that the orchestration file works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.prep.logrotate</pre></div></li></ol></div></div><p>
    The last file is the custom one which includes the additional step with the
    original steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create <code class="filename">/srv/salt/ceph/stage/prep/custom.sls</code> with the
      following content and save it:
     </p><div class="verbatim-wrap"><pre class="screen">include:
  - .logrotate
  - .master
  - .minion</pre></div></li><li class="step"><p>
      Override the default behavior. Edit
      <code class="filename">/srv/pillar/ceph/stack/global.yml</code>, add the following
      line, and save the file:
     </p><div class="verbatim-wrap"><pre class="screen">stage_prep: custom</pre></div></li><li class="step"><p>
      Verify that Stage 0 works:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li></ol></div></div><div id="id-1.4.4.4.4.7.10" data-id-title="Why global.yml?" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Why <code class="filename">global.yml</code>?</h6><p>
     The <code class="filename">global.yml</code> file is chosen over the
     <code class="filename">cluster.yml</code> because during the
     <span class="emphasis"><em>prep</em></span> stage, no minion belongs to the Ceph cluster
     and has no access to any settings in <code class="filename">cluster.yml</code>.
    </p></div></section><section class="sect2" id="ds-disable-reboots" data-id-title="Updates and Reboots during Stage 0"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.5 </span><span class="title-name">Updates and Reboots during Stage 0</span> <a title="Permalink" class="permalink" href="#ds-disable-reboots">#</a></h3></div></div></div><p>
    During stage 0 (refer to <a class="xref" href="#deepsea-stage-description" title="DeepSea Stages Description">DeepSea Stages Description</a> for
    more information on DeepSea stages), the Salt master and Salt minions may
    optionally reboot because newly updated packages, for example
    <span class="package">kernel</span>, require rebooting the system.
   </p><p>
    The default behavior is to install available new updates and
    <span class="emphasis"><em>not</em></span> reboot the nodes even in case of kernel updates.
   </p><p>
    You can change the default update/reboot behavior of DeepSea stage 0 by
    adding/changing the <code class="option">stage_prep_master</code> and
    <code class="option">stage_prep_minion</code> options in the
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file.
    <code class="option">stage_prep_master</code> sets the behavior of the Salt master, and
    <code class="option">stage_prep_minion</code> sets the behavior of all minions. All
    available parameters are:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.4.8.5.1"><span class="term">default</span></dt><dd><p>
       Install updates without rebooting.
      </p></dd><dt id="id-1.4.4.4.4.8.5.2"><span class="term">default-update-reboot</span></dt><dd><p>
       Install updates and reboot after updating.
      </p></dd><dt id="id-1.4.4.4.4.8.5.3"><span class="term">default-no-update-reboot</span></dt><dd><p>
       Reboot without installing updates.
      </p></dd><dt id="id-1.4.4.4.4.8.5.4"><span class="term">default-no-update-no-reboot</span></dt><dd><p>
       Do not install updates or reboot.
      </p></dd></dl></div><p>
    For example, to prevent the cluster nodes from installing updates and
    rebooting, edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and
    add the following lines:
   </p><div class="verbatim-wrap"><pre class="screen">stage_prep_master: default-no-update-no-reboot
stage_prep_minion: default-no-update-no-reboot</pre></div><div id="id-1.4.4.4.4.8.8" data-id-title="Values and Corresponding Files" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Values and Corresponding Files</h6><p>
     The values of <code class="option">stage_prep_master</code> correspond to file names
     located in <code class="filename">/srv/salt/ceph/stage/0/master</code>, while
     values of <code class="option">stage_prep_minion</code> correspond to files in
     <code class="filename">/srv/salt/ceph/stage/0/minion</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ls -l /srv/salt/ceph/stage/0/master
default-no-update-no-reboot.sls
default-no-update-reboot.sls
default-update-reboot.sls
[...]

<code class="prompt user">root@master # </code>ls -l /srv/salt/ceph/stage/0/minion
default-no-update-no-reboot.sls
default-no-update-reboot.sls
default-update-reboot.sls
[...]</pre></div></div></section></section><section class="sect1" id="discovered-configuration-modification" data-id-title="Modifying Discovered Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Modifying Discovered Configuration</span> <a title="Permalink" class="permalink" href="#discovered-configuration-modification">#</a></h2></div></div></div><p>
   After you completed Stage 2, you may want to change the discovered
   configuration. To view the current settings, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div><p>
   The output of the default configuration for a single minion is usually
   similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</pre></div><p>
   The above mentioned settings are distributed across several configuration
   files. The directory structure with these files is defined in the
   <code class="filename">/srv/pillar/ceph/stack/stack.cfg</code> directory. The
   following files usually describe your cluster:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> - the file affects
     all minions in the Salt cluster.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>/cluster.yml</code>
     - the file affects all minions in the Ceph cluster called
     <code class="literal">ceph</code>.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>/roles/<em class="replaceable">role</em>.yml</code>
     - affects all minions that are assigned the specific role in the
     <code class="literal">ceph</code> cluster.
    </p></li><li class="listitem"><p>
     <code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">ceph</em>/minions/<em class="replaceable">MINION_ID</em>/yml</code>
     - affects the individual minion.
    </p></li></ul></div><div id="id-1.4.4.4.5.8" data-id-title="Overwriting Directories with Default Values" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Overwriting Directories with Default Values</h6><p>
    There is a parallel directory tree that stores the default configuration
    setup in <code class="filename">/srv/pillar/ceph/stack/default</code>. Do not change
    values here, as they are overwritten.
   </p></div><p>
   The typical procedure for changing the collected configuration is the
   following:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Find the location of the configuration item you need to change. For
     example, if you need to change cluster related setting such as cluster
     network, edit the file
     <code class="filename">/srv/pillar/ceph/stack/ceph/cluster.yml</code>.
    </p></li><li class="step"><p>
     Save the file.
    </p></li><li class="step"><p>
     Verify the changes by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh</pre></div><p>
     and then
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div></li></ol></div></div><section class="sect2" id="ds-modify-ipv6" data-id-title="Enabling IPv6 for Ceph Cluster Deployment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.1 </span><span class="title-name">Enabling IPv6 for Ceph Cluster Deployment</span> <a title="Permalink" class="permalink" href="#ds-modify-ipv6">#</a></h3></div></div></div><p>
    Since IPv4 network addressing is prevalent, you need to enable IPv6 as a
    customization. DeepSea has no auto-discovery of IPv6 addressing.
   </p><p>
    To configure IPv6, set the <code class="option">public_network</code> and
    <code class="option">cluster_network</code> variables in the
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file to valid IPv6
    subnets. For example:
   </p><div class="verbatim-wrap"><pre class="screen">public_network: fd00:10::/64
cluster_network: fd00:11::/64</pre></div><p>
    Then run DeepSea stage 2 and verify that the network information matches
    the setting. Stage 3 will generate the <code class="filename">ceph.conf</code> with
    the necessary flags.
   </p><div id="id-1.4.4.4.5.11.6" data-id-title="No Support for Dual Stack" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: No Support for Dual Stack</h6><p>
     Ceph does not support dual stack—running Ceph simultaneously on
     IPv4 and IPv6 is not possible. DeepSea validation will reject a mismatch
     between <code class="option">public_network</code> and
     <code class="option">cluster_network</code> or within either variable. The following
     example will fail the validation.
    </p><div class="verbatim-wrap"><pre class="screen">public_network: "192.168.10.0/24 fd00:10::/64"</pre></div></div><div id="id-1.4.4.4.5.11.7" data-id-title="Avoid Using fe80::/10 link-local Addresses" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Avoid Using <code class="literal">fe80::/10 link-local</code> Addresses</h6><p>
     Avoid using <code class="literal">fe80::/10 link-local</code> addresses. All network
     interfaces have an assigned <code class="literal">fe80</code> address and require an
     interface qualifier for proper routing. Either assign IPv6 addresses
     allocated to your site or consider using <code class="literal">fd00::/8</code>.
     These are part of ULA and not globally routable.
    </p></div></section></section></section></div><div class="part" id="additional-software" data-id-title="Installation of Additional Services"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Installation of Additional Services </span><a title="Permalink" class="permalink" href="#additional-software">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-as-intro"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span></a></span></li><dd class="toc-abstract"><p>After you deploy your SUSE Enterprise Storage 6 cluster you may need to install additional software for accessing your data, such as the Object Gateway or the iSCSI Gateway, or you can deploy a clustered file system on top of the Ceph cluster. This chapter mainly focuses on manual installation. If y…</p></dd><li><span class="chapter"><a href="#cha-ceph-additional-software-installation"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
  Ceph Object Gateway is an object storage interface built on top of
  <code class="literal">librgw</code> to provide applications with a RESTful gateway to
  Ceph clusters. It supports two interfaces:
 </p></dd><li><span class="chapter"><a href="#cha-ceph-as-iscsi"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span></a></span></li><dd class="toc-abstract"><p>iSCSI is a storage area network (SAN) protocol that allows clients (called initiators) to send SCSI commands to SCSI storage devices (targets) on remote servers. SUSE Enterprise Storage 6 includes a facility that opens Ceph storage management to heterogeneous clients, such as Microsoft Windows* and …</p></dd><li><span class="chapter"><a href="#cha-ceph-as-cephfs"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span></a></span></li><dd class="toc-abstract"><p>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<code class="systemitem">librados</code>).
 </p></dd><li><span class="chapter"><a href="#cha-as-ganesha"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span></a></span></li><dd class="toc-abstract"><p>
  NFS Ganesha provides NFS access to either the Object Gateway or the CephFS. In
  SUSE Enterprise Storage 6, NFS versions 3 and 4 are supported. NFS Ganesha
  runs in the user space instead of the kernel space and directly interacts
  with the Object Gateway or CephFS.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-as-intro" data-id-title="Installation of Services to Access your Data"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Installation of Services to Access your Data</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-intro">#</a></h2></div></div></div><p>
  After you deploy your SUSE Enterprise Storage 6 cluster you may need to
  install additional software for accessing your data, such as the Object Gateway or the
  iSCSI Gateway, or you can deploy a clustered file system on top of the Ceph
  cluster. This chapter mainly focuses on manual installation. If you have a
  cluster deployed using Salt, refer to
  <a class="xref" href="#ceph-install-saltstack" title="Chapter 5. Deploying with DeepSea/Salt">Chapter 5, <em>Deploying with DeepSea/Salt</em></a> for a procedure on installing
  particular gateways or the CephFS.
 </p></section><section class="chapter" id="cha-ceph-additional-software-installation" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-additional-software-installation">#</a></h2></div></div></div><p>
  Ceph Object Gateway is an object storage interface built on top of
  <code class="literal">librgw</code> to provide applications with a RESTful gateway to
  Ceph clusters. It supports two interfaces:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>S3-compatible</em></span>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Swift-compatible</em></span>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the OpenStack Swift API.
   </p></li></ul></div><p>
  The Object Gateway daemon uses 'Beast' HTTP front-end by default. It uses the
  Boost.Beast library for HTTP parsing and the Boost.Asio library for
  asynchronous network I/O operations.
 </p><p>
  Because Object Gateway provides interfaces compatible with OpenStack Swift and Amazon
  S3, the Object Gateway has its own user management. Object Gateway can store data in the same
  cluster that is used to store data from CephFS clients or RADOS Block Device clients.
  The S3 and Swift APIs share a common name space, so you may write data with
  one API and retrieve it with the other.
 </p><div id="id-1.4.5.3.7" data-id-title="Object Gateway Deployed by DeepSea" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Object Gateway Deployed by DeepSea</h6><p>
   Object Gateway is installed as a DeepSea role, therefore you do not need to install
   it manually.
  </p><p>
   To install the Object Gateway during the cluster deployment, see
   <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>.
  </p><p>
   To add a new node with Object Gateway to the cluster, see
   <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.2 “Adding New Roles to Nodes”</span>.
  </p></div><section class="sect1" id="rgw-installation" data-id-title="Object Gateway Manual Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Object Gateway Manual Installation</span> <a title="Permalink" class="permalink" href="#rgw-installation">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install Object Gateway on a node that is not using port 80. The following command
     installs all required components:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>sudo zypper ref &amp;&amp; zypper in ceph-radosgw</pre></div></li><li class="step"><p>
     If the Apache server from the previous Object Gateway instance is running, stop it
     and disable the relevant service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code> sudo systemctl stop disable apache2.service</pre></div></li><li class="step"><p>
     Edit <code class="filename">/etc/ceph/ceph.conf</code> and add the following lines:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway_host]
 rgw frontends = "beast port=80"</pre></div><div id="id-1.4.5.3.8.2.3.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you want to configure Object Gateway/Beast for use with SSL encryption, modify
      the line accordingly:
     </p><div class="verbatim-wrap"><pre class="screen">rgw frontends = beast ssl_port=7480 ssl_certificate=<em class="replaceable">PATH_TO_CERTIFICATE.PEM</em></pre></div></div></li><li class="step"><p>
     Restart the Object Gateway service.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>sudo systemctl restart ceph-radosgw@rgw.gateway_host</pre></div></li></ol></div></div><section class="sect2" id="ses-rgw-config" data-id-title="Object Gateway Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.1 </span><span class="title-name">Object Gateway Configuration</span> <a title="Permalink" class="permalink" href="#ses-rgw-config">#</a></h3></div></div></div><p>
    Several steps are required to configure an Object Gateway.
   </p><section class="sect3" id="id-1.4.5.3.8.3.3" data-id-title="Basic Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.1 </span><span class="title-name">Basic Configuration</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.3">#</a></h4></div></div></div><p>
     Configuring a Ceph Object Gateway requires a running Ceph Storage Cluster. The
     Ceph Object Gateway is a client of the Ceph Storage Cluster. As a Ceph
     Storage Cluster client, it requires:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A host name for the gateway instance, for example
       <code class="systemitem">gateway</code>.
      </p></li><li class="listitem"><p>
       A storage cluster user name with appropriate permissions and a keyring.
      </p></li><li class="listitem"><p>
       Pools to store its data.
      </p></li><li class="listitem"><p>
       A data directory for the gateway instance.
      </p></li><li class="listitem"><p>
       An instance entry in the Ceph configuration file.
      </p></li></ul></div><p>
     Each instance must have a user name and key to communicate with a Ceph
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the Object Gateway instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the Ceph Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a keyring for the gateway:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
<code class="prompt user">cephadm@adm &gt; </code>sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Generate a Ceph Object Gateway user name and key for each instance. As an
       example, we will use the name <code class="systemitem">gateway</code> after
       <code class="systemitem">client.radosgw</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</pre></div></li><li class="step"><p>
       Add capabilities to the key:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Once you have created a keyring and key to enable the Ceph Object
       Gateway with access to the Ceph Storage Cluster, add the key to your
       Ceph Storage Cluster. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</pre></div></li><li class="step"><p>
       Distribute the keyring to the node with the gateway instance:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>scp /etc/ceph/ceph.client.rgw.keyring  ceph@<em class="replaceable">HOST_NAME</em>:/home/ceph
<code class="prompt user">cephadm@adm &gt; </code>ssh ceph@<em class="replaceable">HOST_NAME</em>
<code class="prompt user">cephadm@ogw &gt; </code>mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</pre></div></li></ol></div></div><div id="id-1.4.5.3.8.3.3.6" data-id-title="Use Bootstrap Keyring" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use Bootstrap Keyring</h6><p>
      An alternative way is to create the Object Gateway bootstrap keyring, and then
      create the Object Gateway keyring from it:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Create an Object Gateway bootstrap keyring on one of the monitor nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<em class="replaceable">NODE_HOST</em>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</pre></div></li><li class="step"><p>
        Create the
        <code class="filename">/var/lib/ceph/radosgw/ceph-<em class="replaceable">RGW_NAME</em></code>
        directory for storing the bootstrap keyring:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>mkdir \
/var/lib/ceph/radosgw/ceph-<em class="replaceable">RGW_NAME</em></pre></div></li><li class="step"><p>
        Create an Object Gateway keyring from the newly created bootstrap keyring:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>ceph \
 auth get-or-create client.rgw.<em class="replaceable">RGW_NAME</em> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<em class="replaceable">RGW_NAME</em>/keyring</pre></div></li><li class="step"><p>
        Copy the Object Gateway keyring to the Object Gateway host:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>scp \
/var/lib/ceph/radosgw/ceph-<em class="replaceable">RGW_NAME</em>/keyring \
<em class="replaceable">RGW_HOST</em>:/var/lib/ceph/radosgw/ceph-<em class="replaceable">RGW_NAME</em>/keyring</pre></div></li></ol></div></div></div></section><section class="sect3" id="ogw-pool-create" data-id-title="Create Pools (Optional)"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.2 </span><span class="title-name">Create Pools (Optional)</span> <a title="Permalink" class="permalink" href="#ogw-pool-create">#</a></h4></div></div></div><p>
     Ceph Object Gateways require Ceph Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the Ceph
     configuration file.
    </p><p>
     The pool names follow the
     <code class="literal"><em class="replaceable">ZONE_NAME</em>.<em class="replaceable">POOL_NAME</em></code>
     syntax. When configuring a gateway with the default region and zone, the
     default zone name is 'default' as in our example:
    </p><div class="verbatim-wrap"><pre class="screen">.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
default.rgw.buckets.index
default.rgw.buckets.data</pre></div><p>
     To create the pools manually, see
     <span class="intraxref">Book “Administration Guide”, Chapter 22 “Managing Storage Pools”, Section 22.2.2 “Create a Pool”</span>.
    </p><div id="id-1.4.5.3.8.3.4.6" data-id-title="Object Gateway and Erasure-Coded Pools" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Object Gateway and Erasure-Coded Pools</h6><p>
      Only the <code class="literal">default.rgw.buckets.data</code> pool can be erasure
      coded. All other pools need to be replicated, otherwise the gateway is
      not accessible.
     </p></div></section><section class="sect3" id="id-1.4.5.3.8.3.5" data-id-title="Adding Gateway Configuration to Ceph"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.3 </span><span class="title-name">Adding Gateway Configuration to Ceph</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.5">#</a></h4></div></div></div><p>
     Add the Ceph Object Gateway configuration to the Ceph Configuration file. The
     Ceph Object Gateway configuration requires you to identify the Ceph Object Gateway
     instance. Then, specify the host name where you installed the Ceph Object Gateway
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable">INSTANCE_NAME</em>]
host = <em class="replaceable">HOST_NAME</em>
keyring = /etc/ceph/ceph.client.rgw.keyring</pre></div><div id="id-1.4.5.3.8.3.5.4" data-id-title="Object Gateway Log File" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Object Gateway Log File</h6><p>
      To override the default Object Gateway log file, include the following:
     </p><div class="verbatim-wrap"><pre class="screen">log file = /var/log/radosgw/client.rgw.<em class="replaceable">INSTANCE_NAME</em>.log</pre></div></div><p>
     The <code class="literal">[client.rgw.*]</code> portion of the gateway instance
     identifies this portion of the Ceph configuration file as configuring a
     Ceph Storage Cluster client where the client type is a Ceph Object Gateway
     (radosgw). The instance name follows. For example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</pre></div><div id="id-1.4.5.3.8.3.5.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The <em class="replaceable">HOST_NAME</em> must be your machine host name,
      excluding the domain name.
     </p></div><p>
     Then turn off <code class="literal">print continue</code>. If you have it set to
     true, you may encounter problems with PUT operations:
    </p><div class="verbatim-wrap"><pre class="screen">rgw print continue = false</pre></div><p>
     To use a Ceph Object Gateway with subdomain S3 calls (for example
     <code class="literal">http://bucketname.hostname</code>), you must add the Ceph
     Object Gateway DNS name under the <code class="literal">[client.rgw.gateway]</code> section
     of the Ceph configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.gateway]
...
rgw dns name = <em class="replaceable">HOST_NAME</em></pre></div><p>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <code class="literal">http://<em class="replaceable">BUCKET_NAME</em>.<em class="replaceable">HOST_NAME</em></code>
     syntax. The <code class="filename">dnsmasq.conf</code> file should include the
     following settings:
    </p><div class="verbatim-wrap"><pre class="screen">address=/<em class="replaceable">HOST_NAME</em>/<em class="replaceable">HOST_IP_ADDRESS</em>
listen-address=<em class="replaceable">CLIENT_LOOPBACK_IP</em></pre></div><p>
     Then, add the <em class="replaceable">CLIENT_LOOPBACK_IP</em> IP address as
     the first DNS server on the client machine(s).
    </p></section><section class="sect3" id="id-1.4.5.3.8.3.6" data-id-title="Create Data Directory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.4 </span><span class="title-name">Create Data Directory</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.6">#</a></h4></div></div></div><p>
     Deployment scripts may not create the default Ceph Object Gateway data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <code class="literal">host</code> variables in the Ceph
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name,
     and the daemon ID.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /var/lib/ceph/radosgw/<em class="replaceable">CLUSTER_ID</em></pre></div><p>
     Using the example <code class="filename">ceph.conf</code> settings above, you would
     execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</pre></div></section><section class="sect3" id="id-1.4.5.3.8.3.7" data-id-title="Restart Services and Start the Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.5 </span><span class="title-name">Restart Services and Start the Gateway</span> <a title="Permalink" class="permalink" href="#id-1.4.5.3.8.3.7">#</a></h4></div></div></div><p>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your Ceph Storage Cluster service. Then, start up
     the <code class="systemitem">radosgw</code> service. For more information, see
     <span class="intraxref">Book “Administration Guide”, Chapter 15 “Introduction”</span> and
     <span class="intraxref">Book “Administration Guide”, Chapter 26 “Ceph Object Gateway”, Section 26.3 “Operating the Object Gateway Service”</span>.
    </p><p>
     When the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-as-iscsi" data-id-title="Installation of iSCSI Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Installation of iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-iscsi">#</a></h2></div></div></div><p>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <span class="emphasis"><em>initiators</em></span>) to send SCSI commands to SCSI storage
  devices (<span class="emphasis"><em>targets</em></span>) on remote servers. SUSE Enterprise Storage
  6 includes a facility that opens Ceph storage management to
  heterogeneous clients, such as Microsoft Windows* and VMware* vSphere, through the
  iSCSI protocol. Multipath iSCSI access enables availability and scalability
  for these clients, and the standardized iSCSI protocol also provides an
  additional layer of security isolation between clients and the SUSE Enterprise Storage
  6 cluster. The configuration facility is named <code class="systemitem">ceph-iscsi</code>. Using
  <code class="systemitem">ceph-iscsi</code>, Ceph storage administrators can define thin-provisioned,
  replicated, highly-available volumes supporting read-only snapshots,
  read-write clones, and automatic resizing with Ceph RADOS Block Device
  (RBD). Administrators can then export volumes either via a single <code class="systemitem">ceph-iscsi</code>
  gateway host, or via multiple gateway hosts supporting multipath failover.
  Linux, Microsoft Windows, and VMware hosts can connect to volumes using the iSCSI
  protocol, which makes them available like any other SCSI block device. This
  means SUSE Enterprise Storage 6 customers can effectively run a complete
  block-storage infrastructure subsystem on Ceph that provides all the
  features and benefits of a conventional SAN, enabling future growth.
 </p><p>
  This chapter introduces detailed information to set up a Ceph cluster
  infrastructure together with an iSCSI gateway so that the client hosts can
  use remotely stored data as local storage devices using the iSCSI protocol.
 </p><section class="sect1" id="ceph-iscsi-iscsi" data-id-title="iSCSI Block Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">iSCSI Block Storage</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi">#</a></h2></div></div></div><p>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
   is implemented as a service where a client (the initiator) talks to a server
   (the target) via a session on TCP port 3260. An iSCSI target's IP address
   and port are called an iSCSI portal, where a target can be exposed through
   one or more portals. The combination of a target and one or more portals is
   called the target portal group (TPG).
  </p><p>
   The underlying data link layer protocol for iSCSI is commonly Ethernet. More
   specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet or faster
   networks for optimal throughput. 10 Gigabit Ethernet connectivity between
   the iSCSI gateway and the back-end Ceph cluster is strongly recommended.
  </p><section class="sect2" id="ceph-iscsi-iscsi-target" data-id-title="The Linux Kernel iSCSI Target"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.1 </span><span class="title-name">The Linux Kernel iSCSI Target</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-target">#</a></h3></div></div></div><p>
    The Linux kernel iSCSI target was originally named LIO for linux-iscsi.org,
    the project's original domain and Web site. For some time, no fewer than
    four competing iSCSI target implementations were available for the Linux
    platform, but LIO ultimately prevailed as the single iSCSI reference
    target. The mainline kernel code for LIO uses the simple, but somewhat
    ambiguous name "target", distinguishing between "target core" and a variety
    of front-end and back-end target modules.
   </p><p>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol is
    supported by SUSE Enterprise Storage.
   </p><p>
    The most frequently used target back-end module is one that is capable of
    simply re-exporting any available block device on the target host. This
    module is named iblock. However, LIO also has an RBD-specific back-end
    module supporting parallelized multipath I/O access to RBD images.
   </p></section><section class="sect2" id="ceph-iscsi-iscsi-initiators" data-id-title="iSCSI Initiators"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.2 </span><span class="title-name">iSCSI Initiators</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-initiators">#</a></h3></div></div></div><p>
    This section introduces brief information on iSCSI initiators used on
    Linux, Microsoft Windows, and VMware platforms.
   </p><section class="sect3" id="id-1.4.5.4.5.5.3" data-id-title="Linux"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.1 </span><span class="title-name">Linux</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.3">#</a></h4></div></div></div><p>
     The standard initiator for the Linux platform is
     <code class="systemitem">open-iscsi</code>. <code class="systemitem">open-iscsi</code>
     launches a daemon, <code class="systemitem">iscsid</code>, which the user can
     then use to discover iSCSI targets on any given portal, log in to targets,
     and map iSCSI volumes. <code class="systemitem">iscsid</code> communicates with
     the SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <code class="systemitem">open-iscsi</code> initiator can be deployed in
     conjunction with the Device Mapper Multipath
     (<code class="systemitem">dm-multipath</code>) facility to provide a highly
     available iSCSI block device.
    </p></section><section class="sect3" id="id-1.4.5.4.5.5.4" data-id-title="Microsoft Windows and Hyper-V"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.2 </span><span class="title-name">Microsoft Windows and Hyper-V</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.4">#</a></h4></div></div></div><p>
     The default iSCSI initiator for the Microsoft Windows operating system is the
     Microsoft iSCSI initiator. The iSCSI service can be configured via a
     graphical user interface (GUI), and supports multipath I/O for high
     availability.
    </p></section><section class="sect3" id="id-1.4.5.4.5.5.5" data-id-title="VMware"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.2.3 </span><span class="title-name">VMware</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.5.5.5">#</a></h4></div></div></div><p>
     The default iSCSI initiator for VMware vSphere and ESX is the VMware
     ESX software iSCSI initiator, <code class="systemitem">vmkiscsi</code>. When
     enabled, it can be configured either from the vSphere client, or using the
     <code class="command">vmkiscsi-tool</code> command. You can then format storage
     volumes connected through the vSphere iSCSI storage adapter with VMFS, and
     use them like any other VM storage device. The VMware initiator also
     supports multipath I/O for high availability.
    </p></section></section></section><section class="sect1" id="ceph-iscsi-lrbd" data-id-title="General Information about ceph-iscsi"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">General Information about <code class="systemitem">ceph-iscsi</code></span> <a title="Permalink" class="permalink" href="#ceph-iscsi-lrbd">#</a></h2></div></div></div><p>
   <code class="systemitem">ceph-iscsi</code> combines the benefits of RADOS Block Devices with the ubiquitous
   versatility of iSCSI. By employing <code class="systemitem">ceph-iscsi</code> on an iSCSI target host (known
   as the iSCSI Gateway), any application that needs to make use of block storage can
   benefit from Ceph, even if it does not speak any Ceph client protocol.
   Instead, users can use iSCSI or any other target front-end protocol to
   connect to an LIO target, which translates all target I/O to RBD storage
   operations.
  </p><div class="figure" id="id-1.4.5.4.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme1.png" target="_blank"><img src="images/lrbd_scheme1.png" width="" alt="Ceph Cluster with a Single iSCSI Gateway"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.1: </span><span class="title-name">Ceph Cluster with a Single iSCSI Gateway </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.6.3">#</a></h6></div></div><p>
   <code class="systemitem">ceph-iscsi</code> is inherently highly-available and supports multipath operations.
   Thus, downstream initiator hosts can use multiple iSCSI gateways for both
   high availability and scalability. When communicating with an iSCSI
   configuration with more than one gateway, initiators may load-balance iSCSI
   requests across multiple gateways. In the event of a gateway failing, being
   temporarily unreachable, or being disabled for maintenance, I/O will
   transparently continue via another gateway.
  </p><div class="figure" id="id-1.4.5.4.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme2.png" target="_blank"><img src="images/lrbd_scheme2.png" width="" alt="Ceph Cluster with Multiple iSCSI Gateways"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.2: </span><span class="title-name">Ceph Cluster with Multiple iSCSI Gateways </span><a title="Permalink" class="permalink" href="#id-1.4.5.4.6.5">#</a></h6></div></div></section><section class="sect1" id="ceph-iscsi-deploy" data-id-title="Deployment Considerations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Deployment Considerations</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-deploy">#</a></h2></div></div></div><p>
   A minimum configuration of SUSE Enterprise Storage 6 with <code class="systemitem">ceph-iscsi</code>
   consists of the following components:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A Ceph storage cluster. The Ceph cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </p></li><li class="listitem"><p>
     An iSCSI target server running the LIO iSCSI target, configured via
     <code class="systemitem">ceph-iscsi</code>.
    </p></li><li class="listitem"><p>
     An iSCSI initiator host, running <code class="systemitem">open-iscsi</code>
     (Linux), the Microsoft iSCSI Initiator (Microsoft Windows), or any other compatible
     iSCSI initiator implementation.
    </p></li></ul></div><p>
   A recommended production configuration of SUSE Enterprise Storage 6 with
   <code class="systemitem">ceph-iscsi</code> consists of:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A Ceph storage cluster. A production Ceph cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running 10-12
     object storage daemons (OSDs), with no fewer than three dedicated MON
     hosts.
    </p></li><li class="listitem"><p>
     Several iSCSI target servers running the LIO iSCSI target, configured via
     <code class="systemitem">ceph-iscsi</code>. For iSCSI fail-over and load-balancing, these servers must run
     a kernel supporting the <code class="systemitem">target_core_rbd</code> module.
     Update packages are available from the SUSE Linux Enterprise Server maintenance channel.
    </p></li><li class="listitem"><p>
     Any number of iSCSI initiator hosts, running
     <code class="systemitem">open-iscsi</code> (Linux), the Microsoft iSCSI Initiator
     (Microsoft Windows), or any other compatible iSCSI initiator implementation.
    </p></li></ul></div></section><section class="sect1" id="ceph-iscsi-install" data-id-title="Installation and Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Installation and Configuration</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-install">#</a></h2></div></div></div><p>
   This section describes steps to install and configure an iSCSI Gateway on top of
   SUSE Enterprise Storage.
  </p><section class="sect2" id="id-1.4.5.4.8.3" data-id-title="Deploy the iSCSI Gateway to a Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.1 </span><span class="title-name">Deploy the iSCSI Gateway to a Ceph Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.3">#</a></h3></div></div></div><p>
    You can deploy the iSCSI Gateway either during the Ceph cluster deployment
    process, or add it to an existing cluster using DeepSea.
   </p><p>
    To include the iSCSI Gateway during the cluster deployment process, refer to
    <a class="xref" href="#policy-role-assignment" title="5.5.1.2. Role Assignment">Section 5.5.1.2, “Role Assignment”</a>.
   </p><p>
    To add the iSCSI Gateway to an existing cluster, refer to
    <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.2 “Adding New Roles to Nodes”</span>.
   </p></section><section class="sect2" id="id-1.4.5.4.8.4" data-id-title="Create RBD Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.2 </span><span class="title-name">Create RBD Images</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.4">#</a></h3></div></div></div><p>
    RBD images are created in the Ceph store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
    You can create a volume from any host that is able to connect to your
    storage cluster using the Ceph <code class="command">rbd</code> command line
    utility. This requires the client to have at least a minimal ceph.conf
    configuration file, and appropriate CephX authentication credentials.
   </p><p>
    To create a new volume for subsequent export via iSCSI, use the
    <code class="command">rbd create</code> command, specifying the volume size in
    megabytes. For example, in order to create a 100 GB volume named 'testvol'
    in the pool named 'iscsi-images', run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool iscsi-images create --size=102400 'testvol'</pre></div></section><section class="sect2" id="ceph-iscsi-rbd-export" data-id-title="Export RBD Images via iSCSI"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.3 </span><span class="title-name">Export RBD Images via iSCSI</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-export">#</a></h3></div></div></div><p>
    To export RBD images via iSCSI, you can use either Ceph Dashboard Web
    interface or the <code class="systemitem">ceph-iscsi</code> gwcli utility. In this section we will focus
    on gwcli only, demonstrating how to create an iSCSI target that exports
    an RBD image using the command line.
   </p><div id="id-1.4.5.4.8.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Only the following RBD image features are supported:
     <code class="option">layering</code>, <code class="option">striping (v2)</code>,
     <code class="option">exclusive-lock</code>, <code class="option">fast-diff</code>, and
     <code class="option">data-pool</code>. RBD images with any other feature enabled
     cannot be exported.
    </p></div><p>
    As <code class="systemitem">root</code>, start the iSCSI gateway command line interface:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>gwcli</pre></div><p>
    Go to <code class="literal">iscsi-targets</code> and create a target with the name
    <code class="literal">iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets
<code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; create iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</pre></div><p>
    Create the iSCSI gateways by specifying the gateway <code class="literal">name</code>
    and <code class="literal">ip</code> address:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; cd iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/gateways
<code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; create iscsi1 192.168.124.104
<code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; create iscsi2 192.168.124.105</pre></div><div id="id-1.4.5.4.8.5.10" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Use the <code class="literal">help</code> command to show the list of available
     commands in the current configuration node.
    </p></div><p>
    Add the RBD image with the name 'testvol' in the pool 'iscsi-images':
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; cd /disks
<code class="prompt user">gwcli &gt; </code> /disks&gt; attach iscsi-images/testvol</pre></div><p>
    Map the RBD image to the target:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/disks
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/disks&gt; add iscsi-images/testvol</pre></div><div id="id-1.4.5.4.8.5.15" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     You can use lower level tools, such as <code class="command">targetcli</code>, to
     query the local configuration, but not to modify it.
    </p></div><div id="id-1.4.5.4.8.5.16" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     You can use the <code class="command">ls</code> command to review the configuration.
     Some configuration nodes also support the <code class="command">info</code> command,
     which can be used to display more detailed information.
    </p></div><p>
    Note that, by default, ACL authentication is enabled so this target is not
    accessible yet. Check <a class="xref" href="#iscsi-lrbd-autentication" title="10.4.4. Authentication and Access Control">Section 10.4.4, “Authentication and Access Control”</a> for more
    information about authentication and access control.
   </p></section><section class="sect2" id="iscsi-lrbd-autentication" data-id-title="Authentication and Access Control"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.4 </span><span class="title-name">Authentication and Access Control</span> <a title="Permalink" class="permalink" href="#iscsi-lrbd-autentication">#</a></h3></div></div></div><p>
    iSCSI authentication is flexible and covers many authentication
    possibilities.
   </p><section class="sect3" id="id-1.4.5.4.8.6.3" data-id-title="No Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.1 </span><span class="title-name">No Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.3">#</a></h4></div></div></div><p>
     'No authentication' means that any initiator will be able to access any
     LUNs on the corresponding target. You can enable 'No authentication' by
     disabling the ACL authentication:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/hosts&gt; auth disable_acl</pre></div></section><section class="sect3" id="id-1.4.5.4.8.6.4" data-id-title="ACL Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.2 </span><span class="title-name">ACL Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.4">#</a></h4></div></div></div><p>
     When using initiator name based ACL authentication, only the defined
     initiators are allowed to connect. You can define an initiator by doing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/hosts&gt; create iqn.1996-04.de.suse:01:e6ca28cc9f20</pre></div><p>
     Defined initiators will be able to connect, but will only have access to
     the RBD images that were explicitly added to the initiator:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...:e6ca28cc9f20&gt; disk add rbd/testvol</pre></div></section><section class="sect3" id="chap-auth-password" data-id-title="CHAP Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.3 </span><span class="title-name">CHAP Authentication</span> <a title="Permalink" class="permalink" href="#chap-auth-password">#</a></h4></div></div></div><p>
     In addition to the ACL, you can enable the CHAP authentication by
     specifying a user name and password for each initiator:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts/iqn.1996-04.de.suse:01:e6ca28cc9f20
<code class="prompt user">gwcli &gt; </code> /iscsi-target...:e6ca28cc9f20&gt; auth username=common12 password=pass12345678</pre></div><div id="id-1.4.5.4.8.6.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      User names must have a length of 8 to 64 characters and can contain
      alphanumeric characters, '.', '@', '-', '_' or ':'.
     </p><p>
      Passwords must have a length of 12 to 16 characters and can contain
      alphanumeric characters, '@', '-', '_' or '/'.
     </p></div><p>
     Optionally, you can also enable the CHAP mutual authentication by
     specifying the <code class="option">mutual_username</code> and
     <code class="option">mutual_password</code> parameters in the <code class="command">auth</code>
     command.
    </p></section><section class="sect3" id="id-1.4.5.4.8.6.6" data-id-title="Discovery and Mutual Authentication"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.4.4 </span><span class="title-name">Discovery and Mutual Authentication</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.6.6">#</a></h4></div></div></div><p>
     <span class="emphasis"><em>Discovery authentication</em></span> is independent of the
     previous authentication methods. It requires credentials for browsing, it
     is optional, and can be configured by:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets
<code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; discovery_auth username=du123456 password=dp1234567890</pre></div><div id="id-1.4.5.4.8.6.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      User-names must have a length of 8 to 64 characters and can only contain
      letters, '.', '@', '-', '_' or ':'.
     </p><p>
      Passwords must have a length of 12 to 16 characters and can only contain
      letters, '@', '-', '_' or '/'.
     </p></div><p>
     Optionally, you can also specify the <code class="option">mutual_username</code> and
     <code class="option">mutual_password</code> parameters in the
     <code class="command">discovery_auth</code> command.
    </p><p>
     Discovery authentication can be disabled by using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; discovery_auth nochap</pre></div></section></section><section class="sect2" id="ceph-iscsi-rbd-advanced" data-id-title="Advanced Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.4.5 </span><span class="title-name">Advanced Settings</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-advanced">#</a></h3></div></div></div><p>
    <code class="systemitem">ceph-iscsi</code> can be configured with advanced parameters which are subsequently
    passed on to the LIO I/O target. The parameters are divided up into
    'target' and 'disk' parameters.
   </p><div id="id-1.4.5.4.8.7.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Unless otherwise noted, changing these parameters from the default setting
     is not recommended.
    </p></div><section class="sect3" id="id-1.4.5.4.8.7.4" data-id-title="Target Settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.5.1 </span><span class="title-name">Target Settings</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.7.4">#</a></h4></div></div></div><p>
     You can view the value of these settings by using the
     <code class="command">info</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol
<code class="prompt user">gwcli &gt; </code> /iscsi-target...i.<em class="replaceable">SYSTEM-ARCH</em>:testvol&gt; info</pre></div><p>
     And change a setting using the <code class="command">reconfigure</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...i.<em class="replaceable">SYSTEM-ARCH</em>:testvol&gt; reconfigure login_timeout 20</pre></div><p>
     The available 'target' settings are:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.7.4.7.1"><span class="term">default_cmdsn_depth</span></dt><dd><p>
        Default CmdSN (Command Sequence Number) depth. Limits the amount of
        requests that an iSCSI initiator can have outstanding at any moment.
       </p></dd><dt id="id-1.4.5.4.8.7.4.7.2"><span class="term">default_erl</span></dt><dd><p>
        Default error recovery level.
       </p></dd><dt id="id-1.4.5.4.8.7.4.7.3"><span class="term">login_timeout</span></dt><dd><p>
        Login timeout value in seconds.
       </p></dd><dt id="id-1.4.5.4.8.7.4.7.4"><span class="term">netif_timeout</span></dt><dd><p>
        NIC failure timeout in seconds.
       </p></dd><dt id="id-1.4.5.4.8.7.4.7.5"><span class="term">prod_mode_write_protect</span></dt><dd><p>
        If set to 1, prevents writes to LUNs.
       </p></dd></dl></div></section><section class="sect3" id="id-1.4.5.4.8.7.5" data-id-title="Disk Settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.4.5.2 </span><span class="title-name">Disk Settings</span> <a title="Permalink" class="permalink" href="#id-1.4.5.4.8.7.5">#</a></h4></div></div></div><p>
     You can view the value of these settings by using the
     <code class="command">info</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /disks/rbd/testvol
<code class="prompt user">gwcli &gt; </code> /disks/rbd/testvol&gt; info</pre></div><p>
     And change a setting using the <code class="command">reconfigure</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks/rbd/testvol&gt; reconfigure rbd/testvol emulate_pr 0</pre></div><p>
     The available 'disk' settings are:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.7.5.7.1"><span class="term">block_size</span></dt><dd><p>
        Block size of the underlying device.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.2"><span class="term">emulate_3pc</span></dt><dd><p>
        If set to 1, enables Third Party Copy.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.3"><span class="term">emulate_caw</span></dt><dd><p>
        If set to 1, enables Compare and Write.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.4"><span class="term">emulate_dpo</span></dt><dd><p>
        If set to 1, turns on Disable Page Out.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.5"><span class="term">emulate_fua_read</span></dt><dd><p>
        If set to 1, enables Force Unit Access read.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.6"><span class="term">emulate_fua_write</span></dt><dd><p>
        If set to 1, enables Force Unit Access write.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.7"><span class="term">emulate_model_alias</span></dt><dd><p>
        If set to 1, uses the back-end device name for the model alias.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.8"><span class="term">emulate_pr</span></dt><dd><p>
        If set to 0, support for SCSI Reservations, including Persistent Group
        Reservations, is disabled. While disabled, the SES iSCSI Gateway can
        ignore reservation state, resulting in improved request latency.
       </p><div id="id-1.4.5.4.8.7.5.7.8.2.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
         Setting backstore_emulate_pr to 0 is recommended if iSCSI initiators
         do not require SCSI Reservation support.
        </p></div></dd><dt id="id-1.4.5.4.8.7.5.7.9"><span class="term">emulate_rest_reord</span></dt><dd><p>
        If set to 0, the Queue Algorithm Modifier has Restricted Reordering.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.10"><span class="term">emulate_tas</span></dt><dd><p>
        If set to 1, enables Task Aborted Status.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.11"><span class="term">emulate_tpu</span></dt><dd><p>
        If set to 1, enables Thin Provisioning Unmap.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.12"><span class="term">emulate_tpws</span></dt><dd><p>
        If set to 1, enables Thin Provisioning Write Same.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.13"><span class="term">emulate_ua_intlck_ctrl</span></dt><dd><p>
        If set to 1, enables Unit Attention Interlock.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.14"><span class="term">emulate_write_cache</span></dt><dd><p>
        If set to 1, turns on Write Cache Enable.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.15"><span class="term">enforce_pr_isids</span></dt><dd><p>
        If set to 1, enforces persistent reservation ISIDs.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.16"><span class="term">is_nonrot</span></dt><dd><p>
        If set to 1, the backstore is a non-rotational device.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.17"><span class="term">max_unmap_block_desc_count</span></dt><dd><p>
        Maximum number of block descriptors for UNMAP.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.18"><span class="term">max_unmap_lba_count:</span></dt><dd><p>
        Maximum number of LBAs for UNMAP.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.19"><span class="term">max_write_same_len</span></dt><dd><p>
        Maximum length for WRITE_SAME.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.20"><span class="term">optimal_sectors</span></dt><dd><p>
        Optimal request size in sectors.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.21"><span class="term">pi_prot_type</span></dt><dd><p>
        DIF protection type.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.22"><span class="term">queue_depth</span></dt><dd><p>
        Queue depth.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.23"><span class="term">unmap_granularity</span></dt><dd><p>
        UNMAP granularity.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.24"><span class="term">unmap_granularity_alignment</span></dt><dd><p>
        UNMAP granularity alignment.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.25"><span class="term">force_pr_aptpl</span></dt><dd><p>
        When enabled, LIO will always write out the <span class="emphasis"><em>persistent
        reservation</em></span> state to persistent storage, regardless of
        whether or not the client has requested it via
        <code class="option">aptpl=1</code>. This has no effect with the kernel RBD
        back-end for LIO—it always persists PR state. Ideally, the
        <code class="option">target_core_rbd</code> option should force it to '1' and
        throw an error if someone tries to disable it via configfs.
       </p></dd><dt id="id-1.4.5.4.8.7.5.7.26"><span class="term">unmap_zeroes_data</span></dt><dd><p>
        Affects whether LIO will advertise LBPRZ to SCSI initiators, indicating
        that zeros will be read back from a region following UNMAP or WRITE
        SAME with an unmap bit.
       </p></dd></dl></div></section></section></section><section class="sect1" id="iscsi-tcmu" data-id-title="Exporting RADOS Block Device Images Using tcmu-runner"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.5 </span><span class="title-name">Exporting RADOS Block Device Images Using <code class="systemitem">tcmu-runner</code></span> <a title="Permalink" class="permalink" href="#iscsi-tcmu">#</a></h2></div></div></div><p>
   The <code class="systemitem">ceph-iscsi</code> supports both <code class="option">rbd</code> (kernel-based) and
   <code class="option">user:rbd</code> (tcmu-runner) backstores, making all the
   management transparent and independent of the backstore.
  </p><div id="id-1.4.5.4.9.3" data-id-title="Technology Preview" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Technology Preview</h6><p>
    <code class="systemitem">tcmu-runner</code> based iSCSI Gateway deployments are currently
    a technology preview.
   </p></div><p>
   Unlike kernel-based iSCSI Gateway deployments, <code class="systemitem">tcmu-runner</code>
   based iSCSI Gateways do not offer support for multipath I/O or SCSI Persistent
   Reservations.
  </p><p>
   To export an RADOS Block Device image using <code class="systemitem">tcmu-runner</code>, all you
   need to do is specify the <code class="option">user:rbd</code> backstore when attaching
   the disk:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks&gt; attach rbd/testvol backstore=user:rbd</pre></div><div id="id-1.4.5.4.9.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    When using <code class="systemitem">tcmu-runner</code>, the exported RBD image
    must have the <code class="option">exclusive-lock</code> feature enabled.
   </p></div></section></section><section class="chapter" id="cha-ceph-as-cephfs" data-id-title="Installation of CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11 </span><span class="title-name">Installation of CephFS</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-cephfs">#</a></h2></div></div></div><p>
  The Ceph file system (CephFS) is a POSIX-compliant file system that uses
  a Ceph storage cluster to store its data. CephFS uses the same cluster
  system as Ceph block devices, Ceph object storage with its S3 and Swift
  APIs, or native bindings (<code class="systemitem">librados</code>).
 </p><p>
  To use CephFS, you need to have a running Ceph storage cluster, and at
  least one running <span class="emphasis"><em>Ceph metadata server</em></span>.
 </p><section class="sect1" id="ceph-cephfs-limitations" data-id-title="Supported CephFS Scenarios and Guidance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Supported CephFS Scenarios and Guidance</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-limitations">#</a></h2></div></div></div><p>
   With SUSE Enterprise Storage 6, SUSE introduces official support for
   many scenarios in which the scale-out and distributed component CephFS is
   used. This entry describes hard limits and provides guidance for the
   suggested use cases.
  </p><p>
   A supported CephFS deployment must meet these requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Clients are SUSE Linux Enterprise Server 12 SP3 or newer, or SUSE Linux Enterprise Server 15 or newer, using the
     <code class="literal">cephfs</code> kernel module driver. The FUSE module is not
     supported.
    </p></li><li class="listitem"><p>
     CephFS quotas are supported in SUSE Enterprise Storage 6 and can be
     set on any subdirectory of the Ceph file system. The quota restricts
     either the number of <code class="literal">bytes</code> or <code class="literal">files</code>
     stored beneath the specified point in the directory hierarchy. For more
     information, see <span class="intraxref">Book “Administration Guide”, Chapter 28 “Clustered File System”, Section 28.6 “Setting CephFS Quotas”</span>.
    </p></li><li class="listitem"><p>
     CephFS supports file layout changes as documented in
     <a class="xref" href="#cephfs-layouts" title="11.3.4. File Layouts">Section 11.3.4, “File Layouts”</a>. However, while the file system is
     mounted by any client, new data pools may not be added to an existing
     CephFS file system (<code class="literal">ceph mds add_data_pool</code>). They may
     only be added while the file system is unmounted.
    </p></li><li class="listitem"><p>
     A minimum of one Metadata Server. SUSE recommends deploying several nodes with the
     MDS role. By default, additional MDS daemons start as
     <code class="literal">standby</code> daemons, acting as backups for the active MDS.
     Multiple active MDS daemons are also supported (refer to section
     <a class="xref" href="#ceph-cephfs-multimds" title="11.3.2. MDS Cluster Size">Section 11.3.2, “MDS Cluster Size”</a>).
    </p></li></ul></div></section><section class="sect1" id="ceph-cephfs-mds" data-id-title="Ceph Metadata Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.2 </span><span class="title-name">Ceph Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mds">#</a></h2></div></div></div><p>
   Ceph metadata server (MDS) stores metadata for the CephFS. Ceph block
   devices and Ceph object storage <span class="emphasis"><em>do not</em></span> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands—such as <code class="command">ls</code> or
   <code class="command">find</code>—without placing an enormous burden on the
   Ceph storage cluster.
  </p><section class="sect2" id="ceph-cephfs-mdf-add" data-id-title="Adding and Removing a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.1 </span><span class="title-name">Adding and Removing a Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mdf-add">#</a></h3></div></div></div><p>
    You can deploy MDS either during the initial cluster deployment process as
    described in <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>, or add it to an already
    deployed cluster as described in <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.1 “Adding New Cluster Nodes”</span>.
   </p><p>
    After you deploy your MDS, allow the <code class="literal">Ceph OSD/MDS</code>
    service in the firewall setting of the server where MDS is deployed: Start
    <code class="literal">yast</code>, navigate to <span class="guimenu">Security and
    Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
    Services</span> and in the <span class="guimenu">Service to
    Allow</span> drop–down menu select <span class="guimenu">Ceph
    OSD/MDS</span>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </p><p>
    You can remove a metadata server in your cluster as described in
    <span class="intraxref"/>.
   </p></section><section class="sect2" id="ceph-cephfs-mds-config" data-id-title="Configuring a Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.2.2 </span><span class="title-name">Configuring a Metadata Server</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-mds-config">#</a></h3></div></div></div><p>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <code class="filename">ceph.conf</code> configuration file.
   </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Metadata Server Settings </span><a title="Permalink" class="permalink" href="#id-1.4.5.5.6.4.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.5.6.4.3.2"><span class="term">mon force standby active</span></dt><dd><p>
       If set to 'true' (default), monitors force standby-replay to be active.
       Set under <code class="literal">[mon]</code> or <code class="literal">[global]</code>
       sections.
      </p></dd><dt id="id-1.4.5.5.6.4.3.3"><span class="term"><code class="option">mds cache memory limit</code></span></dt><dd><p>
       The soft memory limit (in bytes) that the MDS will enforce for its
       cache. Administrators should use this instead of the old <code class="option">mds
       cache size</code> setting. Defaults to 1 GB.
      </p></dd><dt id="id-1.4.5.5.6.4.3.4"><span class="term"><code class="option">mds cache reservation</code></span></dt><dd><p>
       The cache reservation (memory or inodes) for the MDS cache to maintain.
       When the MDS begins touching its reservation, it will recall client
       state until its cache size shrinks to restore the reservation. Defaults
       to 0.05.
      </p></dd><dt id="id-1.4.5.5.6.4.3.5"><span class="term">mds cache size</span></dt><dd><p>
       The number of inodes to cache. A value of 0 (default) indicates an
       unlimited number. It is recommended to use <code class="option">mds cache memory
       limit</code> to limit the amount of memory the MDS cache uses.
      </p></dd><dt id="id-1.4.5.5.6.4.3.6"><span class="term">mds cache mid</span></dt><dd><p>
       The insertion point for new items in the cache LRU (from the top).
       Default is 0.7.
      </p></dd><dt id="id-1.4.5.5.6.4.3.7"><span class="term">mds dir commit ratio</span></dt><dd><p>
       The fraction of directory that is dirty before Ceph commits using a
       full update instead of partial update. Default is 0.5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.8"><span class="term">mds dir max commit size</span></dt><dd><p>
       The maximum size of a directory update before Ceph breaks it into
       smaller transactions. Default is 90 MB.
      </p></dd><dt id="id-1.4.5.5.6.4.3.9"><span class="term">mds decay halflife</span></dt><dd><p>
       The half-life of MDS cache temperature. Default is 5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.10"><span class="term">mds beacon interval</span></dt><dd><p>
       The frequency in seconds of beacon messages sent to the monitor. Default
       is 4.
      </p></dd><dt id="id-1.4.5.5.6.4.3.11"><span class="term">mds beacon grace</span></dt><dd><p>
       The interval without beacons before Ceph declares an MDS laggy and
       possibly replaces it. Default is 15.
      </p></dd><dt id="id-1.4.5.5.6.4.3.12"><span class="term">mds blacklist interval</span></dt><dd><p>
       The blacklist duration for failed MDSs in the OSD map. This setting
       controls how long failed MDS daemons will stay in the OSD map blacklist.
       It has no effect on how long something is blacklisted when the
       administrator blacklists it manually. For example, the <code class="command">ceph osd
       blacklist add</code> command will still use the default blacklist
       time. Default is 24 * 60.
      </p></dd><dt id="id-1.4.5.5.6.4.3.13"><span class="term">mds reconnect timeout</span></dt><dd><p>
       The interval in seconds to wait for clients to reconnect during MDS
       restart. Default is 45.
      </p></dd><dt id="id-1.4.5.5.6.4.3.14"><span class="term">mds tick interval</span></dt><dd><p>
       How frequently the MDS performs internal periodic tasks. Default is 5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.15"><span class="term">mds dirstat min interval</span></dt><dd><p>
       The minimum interval in seconds to try to avoid propagating recursive
       stats up the tree. Default is 1.
      </p></dd><dt id="id-1.4.5.5.6.4.3.16"><span class="term">mds scatter nudge interval</span></dt><dd><p>
       How quickly dirstat changes propagate up. Default is 5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.17"><span class="term">mds client prealloc inos</span></dt><dd><p>
       The number of inode numbers to preallocate per client session. Default
       is 1000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.18"><span class="term">mds early reply</span></dt><dd><p>
       Determines whether the MDS should allow clients to see request results
       before they commit to the journal. Default is 'true'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.19"><span class="term">mds use tmap</span></dt><dd><p>
       Use trivial map for directory updates. Default is 'true'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.20"><span class="term">mds default dir hash</span></dt><dd><p>
       The function to use for hashing files across directory fragments.
       Default is 2 (that is 'rjenkins').
      </p></dd><dt id="id-1.4.5.5.6.4.3.21"><span class="term">mds log skip corrupt events</span></dt><dd><p>
       Determines whether the MDS should try to skip corrupt journal events
       during journal replay. Default is 'false'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.22"><span class="term">mds log max events</span></dt><dd><p>
       The maximum events in the journal before we initiate trimming. Set to -1
       (default) to disable limits.
      </p></dd><dt id="id-1.4.5.5.6.4.3.23"><span class="term">mds log max segments</span></dt><dd><p>
       The maximum number of segments (objects) in the journal before we
       initiate trimming. Set to -1 to disable limits. Default is 30.
      </p></dd><dt id="id-1.4.5.5.6.4.3.24"><span class="term">mds log max expiring</span></dt><dd><p>
       The maximum number of segments to expire in parallels. Default is 20.
      </p></dd><dt id="id-1.4.5.5.6.4.3.25"><span class="term">mds log eopen size</span></dt><dd><p>
       The maximum number of inodes in an EOpen event. Default is 100.
      </p></dd><dt id="id-1.4.5.5.6.4.3.26"><span class="term">mds bal sample interval</span></dt><dd><p>
       Determines how frequently to sample directory temperature for
       fragmentation decisions. Default is 3.
      </p></dd><dt id="id-1.4.5.5.6.4.3.27"><span class="term">mds bal replicate threshold</span></dt><dd><p>
       The maximum temperature before Ceph attempts to replicate metadata to
       other nodes. Default is 8000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.28"><span class="term">mds bal unreplicate threshold</span></dt><dd><p>
       The minimum temperature before Ceph stops replicating metadata to
       other nodes. Default is 0.
      </p></dd><dt id="id-1.4.5.5.6.4.3.29"><span class="term">mds bal split size</span></dt><dd><p>
       The maximum directory size before the MDS will split a directory
       fragment into smaller bits. Default is 10000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.30"><span class="term">mds bal split rd</span></dt><dd><p>
       The maximum directory read temperature before Ceph splits a directory
       fragment. Default is 25000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.31"><span class="term">mds bal split wr</span></dt><dd><p>
       The maximum directory write temperature before Ceph splits a directory
       fragment. Default is 10000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.32"><span class="term">mds bal split bits</span></dt><dd><p>
       The number of bits by which to split a directory fragment. Default is 3.
      </p></dd><dt id="id-1.4.5.5.6.4.3.33"><span class="term">mds bal merge size</span></dt><dd><p>
       The minimum directory size before Ceph tries to merge adjacent directory
       fragments. Default is 50.
      </p></dd><dt id="id-1.4.5.5.6.4.3.34"><span class="term">mds bal interval</span></dt><dd><p>
       The frequency in seconds of workload exchanges between MDSs. Default is
       10.
      </p></dd><dt id="id-1.4.5.5.6.4.3.35"><span class="term">mds bal fragment interval</span></dt><dd><p>
       The delay in seconds between a fragment being capable of splitting or
       merging, and execution of the fragmentation change. Default is 5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.36"><span class="term">mds bal fragment fast factor</span></dt><dd><p>
       The ratio by which fragments may exceed the split size before a split is
       executed immediately, skipping the fragment interval. Default is 1.5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.37"><span class="term">mds bal fragment size max</span></dt><dd><p>
       The maximum size of a fragment before any new entries are rejected with
       ENOSPC. Default is 100000.
      </p></dd><dt id="id-1.4.5.5.6.4.3.38"><span class="term">mds bal idle threshold</span></dt><dd><p>
       The minimum temperature before Ceph migrates a subtree back to its
       parent. Default is 0.
      </p></dd><dt id="id-1.4.5.5.6.4.3.39"><span class="term">mds bal mode</span></dt><dd><p>
       The method for calculating MDS load:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         0 = Hybrid.
        </p></li><li class="listitem"><p>
         1 = Request rate and latency.
        </p></li><li class="listitem"><p>
         2 = CPU load.
        </p></li></ul></div><p>
       Default is 0.
      </p></dd><dt id="id-1.4.5.5.6.4.3.40"><span class="term">mds bal min rebalance</span></dt><dd><p>
       The minimum subtree temperature before Ceph migrates. Default is 0.1.
      </p></dd><dt id="id-1.4.5.5.6.4.3.41"><span class="term">mds bal min start</span></dt><dd><p>
       The minimum subtree temperature before Ceph searches a subtree.
       Default is 0.2.
      </p></dd><dt id="id-1.4.5.5.6.4.3.42"><span class="term">mds bal need min</span></dt><dd><p>
       The minimum fraction of target subtree size to accept. Default is 0.8.
      </p></dd><dt id="id-1.4.5.5.6.4.3.43"><span class="term">mds bal need max</span></dt><dd><p>
       The maximum fraction of target subtree size to accept. Default is 1.2.
      </p></dd><dt id="id-1.4.5.5.6.4.3.44"><span class="term">mds bal midchunk</span></dt><dd><p>
       Ceph will migrate any subtree that is larger than this fraction of the
       target subtree size. Default is 0.3.
      </p></dd><dt id="id-1.4.5.5.6.4.3.45"><span class="term">mds bal minchunk</span></dt><dd><p>
       Ceph will ignore any subtree that is smaller than this fraction of the
       target subtree size. Default is 0.001.
      </p></dd><dt id="id-1.4.5.5.6.4.3.46"><span class="term">mds bal target removal min</span></dt><dd><p>
       The minimum number of balancer iterations before Ceph removes an old
       MDS target from the MDS map. Default is 5.
      </p></dd><dt id="id-1.4.5.5.6.4.3.47"><span class="term">mds bal target removal max</span></dt><dd><p>
       The maximum number of balancer iteration before Ceph removes an old
       MDS target from the MDS map. Default is 10.
      </p></dd><dt id="id-1.4.5.5.6.4.3.48"><span class="term">mds replay interval</span></dt><dd><p>
       The journal poll interval when in standby-replay mode ('hot standby').
       Default is 1.
      </p></dd><dt id="id-1.4.5.5.6.4.3.49"><span class="term">mds shutdown check</span></dt><dd><p>
       The interval for polling the cache during MDS shutdown. Default is 0.
      </p></dd><dt id="id-1.4.5.5.6.4.3.50"><span class="term">mds thrash fragments</span></dt><dd><p>
       Ceph will randomly fragment or merge directories. Default is 0.
      </p></dd><dt id="id-1.4.5.5.6.4.3.51"><span class="term">mds dump cache on map</span></dt><dd><p>
       Ceph will dump the MDS cache contents to a file on each MDS map.
       Default is 'false'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.52"><span class="term">mds dump cache after rejoin</span></dt><dd><p>
       Ceph will dump MDS cache contents to a file after rejoining the cache
       during recovery. Default is 'false'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.53"><span class="term">mds standby for name</span></dt><dd><p>
       An MDS daemon will standby for another MDS daemon of the name specified
       in this setting.
      </p></dd><dt id="id-1.4.5.5.6.4.3.54"><span class="term">mds standby for rank</span></dt><dd><p>
       An MDS daemon will standby for an MDS daemon of this rank. Default is
       -1.
      </p></dd><dt id="id-1.4.5.5.6.4.3.55"><span class="term">mds standby replay</span></dt><dd><p>
       Determines whether a Ceph MDS daemon should poll and replay the log of
       an active MDS ('hot standby'). Default is 'false'.
      </p></dd><dt id="id-1.4.5.5.6.4.3.56"><span class="term">mds min caps per client</span></dt><dd><p>
       Set the minimum number of capabilities a client may hold. Default is
       100.
      </p></dd><dt id="id-1.4.5.5.6.4.3.57"><span class="term">mds max ratio caps per client</span></dt><dd><p>
       Set the maximum ratio of current caps that may be recalled during MDS
       cache pressure. Default is 0.8.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Metadata Server Journaler Settings </span><a title="Permalink" class="permalink" href="#id-1.4.5.5.6.4.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.5.6.4.4.2"><span class="term">journaler write head interval</span></dt><dd><p>
       How frequently to update the journal head object. Default is 15.
      </p></dd><dt id="id-1.4.5.5.6.4.4.3"><span class="term">journaler prefetch periods</span></dt><dd><p>
       How many stripe periods to read ahead on journal replay. Default is 10.
      </p></dd><dt id="id-1.4.5.5.6.4.4.4"><span class="term">journal prezero periods</span></dt><dd><p>
       How many stripe periods to zero ahead of write position. Default 10.
      </p></dd><dt id="id-1.4.5.5.6.4.4.5"><span class="term">journaler batch interval</span></dt><dd><p>
       Maximum additional latency in seconds we incur artificially. Default is
       0.001.
      </p></dd><dt id="id-1.4.5.5.6.4.4.6"><span class="term">journaler batch max</span></dt><dd><p>
       Maximum number of bytes by which we will delay flushing. Default is 0.
      </p></dd></dl></div></section></section><section class="sect1" id="ceph-cephfs-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.3 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs">#</a></h2></div></div></div><p>
   When you have a healthy Ceph storage cluster with at least one Ceph
   metadata server, you can create and mount your Ceph file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </p><section class="sect2" id="ceph-cephfs-cephfs-create" data-id-title="Creating CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.1 </span><span class="title-name">Creating CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-create">#</a></h3></div></div></div><p>
    A CephFS requires at least two RADOS pools: one for
    <span class="emphasis"><em>data</em></span> and one for <span class="emphasis"><em>metadata</em></span>. When
    configuring these pools, you might consider:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </p></li><li class="listitem"><p>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </p></li></ul></div><p>
    When assigning a <code class="literal">role-mds</code> in the
    <code class="filename">policy.cfg</code>, the required pools are automatically
    created. You can manually create the pools <code class="literal">cephfs_data</code>
    and <code class="literal">cephfs_metadata</code> for manual performance tuning before
    setting up the Metadata Server. DeepSea will not create these pools if they already
    exist.
   </p><p>
    For more information on managing pools, see <span class="intraxref">Book “Administration Guide”, Chapter 22 “Managing Storage Pools”</span>.
   </p><p>
    To create the two required pools—for example, 'cephfs_data' and
    'cephfs_metadata'—with default settings for use with CephFS, run
    the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create cephfs_data <em class="replaceable">pg_num</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create cephfs_metadata <em class="replaceable">pg_num</em></pre></div><p>
    It is possible to use EC pools instead of replicated pools. We recommend to
    only use EC pools for low performance requirements and infrequent random
    access, for example cold storage, backups, archiving. CephFS on EC pools
    requires BlueStore to be enabled and the pool must have the
    <code class="literal">allow_ec_overwrite</code> option set. This option can be set by
    running <code class="command">ceph osd pool set ec_pool allow_ec_overwrites
    true</code>.
   </p><p>
    Erasure coding adds significant overhead to file system operations,
    especially small updates. This overhead is inherent to using erasure coding
    as a fault tolerance mechanism. This penalty is the trade off for
    significantly reduced storage space overhead.
   </p><p>
    When the pools are created, you may enable the file system with the
    <code class="command">ceph fs new</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs new <em class="replaceable">fs_name</em> <em class="replaceable">metadata_pool_name</em> <em class="replaceable">data_pool_name</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs new cephfs cephfs_metadata cephfs_data</pre></div><p>
    You can check that the file system was created by listing all available
    CephFSs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> <code class="option">fs ls</code>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</pre></div><p>
    When the file system has been created, your MDS will be able to enter an
    <span class="emphasis"><em>active</em></span> state. For example, in a single MDS system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> <code class="option">mds stat</code>
e5: 1/1/1 up</pre></div><div id="id-1.4.5.5.7.3.18" data-id-title="More Topics" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Topics</h6><p>
     You can find more information of specific tasks—for example
     mounting, unmounting, and advanced CephFS setup—in
     <span class="intraxref">Book “Administration Guide”, Chapter 28 “Clustered File System”</span>.
    </p></div></section><section class="sect2" id="ceph-cephfs-multimds" data-id-title="MDS Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.2 </span><span class="title-name">MDS Cluster Size</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-multimds">#</a></h3></div></div></div><p>
    A CephFS instance can be served by multiple active MDS daemons. All
    active MDS daemons that are assigned to a CephFS instance will distribute
    the file system's directory tree between themselves, and thus spread the
    load of concurrent clients. In order to add an active MDS daemon to a
    CephFS instance, a spare standby is needed. Either start an additional
    daemon or use an existing standby instance.
   </p><p>
    The following command will display the current number of active and passive
    MDS daemons.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mds stat</pre></div><p>
    The following command sets the number of active MDSs to two in a file
    system instance.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 2</pre></div><p>
    In order to shrink the MDS cluster prior to an update, set the
    <code class="option">max_mds</code> option so that only one instance remains:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 1</pre></div><p>
    We recommend at least one MDS is left as a standby daemon.
   </p></section><section class="sect2" id="ceph-cephfs-multimds-updates" data-id-title="MDS Cluster and Updates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.3 </span><span class="title-name">MDS Cluster and Updates</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-multimds-updates">#</a></h3></div></div></div><p>
    During Ceph updates, the feature flags on a file system instance may
    change (usually by adding new features). Incompatible daemons (such as the
    older versions) are not able to function with an incompatible feature set
    and will refuse to start. This means that updating and restarting one
    daemon can cause all other not yet updated daemons to stop and refuse to
    start. For this reason, we recommend shrinking the active MDS cluster to
    size one and stopping all standby daemons before updating Ceph. The
    manual steps for this update procedure are as follows:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the Ceph related packages using <code class="command">zypper</code>.
     </p></li><li class="step"><p>
      Shrink the active MDS cluster as described above to one instance and stop
      all standby MDS daemons using their <code class="systemitem">systemd</code> units on all other nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>systemctl stop ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Only then restart the single remaining MDS daemon, causing it to restart
      using the updated binary.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>systemctl restart ceph-mds\*.service ceph-mds.target</pre></div></li><li class="step"><p>
      Restart all other MDS daemons and reset the desired
      <code class="option">max_mds</code> setting.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>systemctl start ceph-mds.target</pre></div></li></ol></div></div><p>
    If you use DeepSea, it will follow this procedure in case the
    <span class="package">ceph</span> package was updated during stages 0 and 4. It is
    possible to perform this procedure while clients have the CephFS instance
    mounted and I/O is ongoing. Note however that there will be a very brief
    I/O pause while the active MDS restarts. Clients will recover
    automatically.
   </p><p>
    It is good practice to reduce the I/O load as much as possible before
    updating an MDS cluster. An idle MDS cluster will go through this update
    procedure quicker. Conversely, on a heavily loaded cluster with multiple
    MDS daemons it is essential to reduce the load in advance to prevent a
    single MDS daemon from being overwhelmed by ongoing I/O.
   </p></section><section class="sect2" id="cephfs-layouts" data-id-title="File Layouts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.4 </span><span class="title-name">File Layouts</span> <a title="Permalink" class="permalink" href="#cephfs-layouts">#</a></h3></div></div></div><p>
    The layout of a file controls how its contents are mapped to Ceph RADOS
    objects. You can read and write a file’s layout using <span class="emphasis"><em>virtual
    extended attributes</em></span> or <span class="emphasis"><em>xattrs</em></span> for shortly.
   </p><p>
    The name of the layout xattrs depends on whether a file is a regular file
    or a directory. Regular files’ layout xattrs are called
    <code class="literal">ceph.file.layout</code>, while directories’ layout xattrs are
    called <code class="literal">ceph.dir.layout</code>. Where examples refer to
    <code class="literal">ceph.file.layout</code>, substitute the
    <code class="literal">.dir.</code> part as appropriate when dealing with directories.
   </p><section class="sect3" id="id-1.4.5.5.7.6.4" data-id-title="Layout Fields"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.1 </span><span class="title-name">Layout Fields</span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.4">#</a></h4></div></div></div><p>
     The following attribute fields are recognized:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.7.6.4.3.1"><span class="term">pool</span></dt><dd><p>
        ID or name of a RADOS pool in which a file’s data objects will be
        stored.
       </p></dd><dt id="id-1.4.5.5.7.6.4.3.2"><span class="term">pool_namespace</span></dt><dd><p>
        RADOS namespace within a data pool to which the objects will be
        written. It is empty by default, meaning the default namespace.
       </p></dd><dt id="id-1.4.5.5.7.6.4.3.3"><span class="term">stripe_unit</span></dt><dd><p>
        The size in bytes of a block of data used in the RAID 0 distribution of
        a file. All stripe units for a file have equal size. The last stripe
        unit is typically incomplete—it represents the data at the end of
        the file as well as the unused 'space' beyond it up to the end of the
        fixed stripe unit size.
       </p></dd><dt id="id-1.4.5.5.7.6.4.3.4"><span class="term">stripe_count</span></dt><dd><p>
        The number of consecutive stripe units that constitute a RAID 0
        'stripe' of file data.
       </p></dd><dt id="id-1.4.5.5.7.6.4.3.5"><span class="term">object_size</span></dt><dd><p>
        The size in bytes of RADOS objects into which the file data is
        chunked.
       </p><div id="id-1.4.5.5.7.6.4.3.5.2.2" data-id-title="Object Sizes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Object Sizes</h6><p>
         RADOS enforces a configurable limit on object sizes. If you increase
         CephFS object sizes beyond that limit, then writes may not succeed.
         The OSD setting is <code class="option">osd_max_object_size</code>, which is
         128 MB by default. Very large RADOS objects may prevent smooth
         operation of the cluster, so increasing the object size limit past the
         default is not recommended.
        </p></div></dd></dl></div></section><section class="sect3" id="id-1.4.5.5.7.6.5" data-id-title="Reading Layout with getfattr"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.2 </span><span class="title-name">Reading Layout with <code class="command">getfattr</code></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.5">#</a></h4></div></div></div><p>
     Use the <code class="command">getfattr</code> command to read the layout information
     of an example file <code class="filename">file</code> as a single string:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>touch file
<code class="prompt user">root # </code>getfattr -n ceph.file.layout file
# file: file
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=419430</pre></div><p>
     Read individual layout fields:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>getfattr -n ceph.file.layout.pool file
# file: file
ceph.file.layout.pool="cephfs_data"
<code class="prompt user">root # </code>getfattr -n ceph.file.layout.stripe_unit file
# file: file
ceph.file.layout.stripe_unit="4194304"</pre></div><div id="id-1.4.5.5.7.6.5.6" data-id-title="Pool ID or Name" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Pool ID or Name</h6><p>
      When reading layouts, the pool will usually be indicated by name.
      However, in rare cases when pools have only just been created, the ID may
      be output instead.
     </p></div><p>
     Directories do not have an explicit layout until it is customized.
     Attempts to read the layout will fail if it has never been modified: this
     indicates that the layout of the next ancestor directory with an explicit
     layout will be used.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir dir
<code class="prompt user">root # </code>getfattr -n ceph.dir.layout dir
dir: ceph.dir.layout: No such attribute
<code class="prompt user">root # </code>setfattr -n ceph.dir.layout.stripe_count -v 2 dir
<code class="prompt user">root # </code>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs_data"</pre></div></section><section class="sect3" id="id-1.4.5.5.7.6.6" data-id-title="Writing Layouts with setfattr"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.3 </span><span class="title-name">Writing Layouts with <code class="command">setfattr</code></span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.6">#</a></h4></div></div></div><p>
     Use the <code class="command">setfattr</code> command to modify the layout fields of
     an example file <code class="command">file</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd lspools
0 rbd
1 cephfs_data
2 cephfs_metadata
<code class="prompt user">root # </code>setfattr -n ceph.file.layout.stripe_unit -v 1048576 file
<code class="prompt user">root # </code>setfattr -n ceph.file.layout.stripe_count -v 8 file
# Setting pool by ID:
<code class="prompt user">root # </code>setfattr -n ceph.file.layout.pool -v 1 file
# Setting pool by name:
<code class="prompt user">root # </code>setfattr -n ceph.file.layout.pool -v cephfs_data file</pre></div><div id="id-1.4.5.5.7.6.6.4" data-id-title="Empty File" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Empty File</h6><p>
      When the layout fields of a file are modified using
      <code class="command">setfattr</code>, this file needs to be empty otherwise an
      error will occur.
     </p></div></section><section class="sect3" id="id-1.4.5.5.7.6.7" data-id-title="Clearing Layouts"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.4 </span><span class="title-name">Clearing Layouts</span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.7">#</a></h4></div></div></div><p>
     If you want to remove an explicit layout from an example directory
     <code class="filename">mydir</code> and revert back to inheriting the layout of its
     ancestor, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>setfattr -x ceph.dir.layout mydir</pre></div><p>
     Similarly, if you have set the 'pool_namespace' attribute and wish to
     modify the layout to use the default namespace instead, run:
    </p><div class="verbatim-wrap"><pre class="screen"># Create a directory and set a namespace on it
<code class="prompt user">root # </code>mkdir mydir
<code class="prompt user">root # </code>setfattr -n ceph.dir.layout.pool_namespace -v foons mydir
<code class="prompt user">root # </code>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a pool_namespace=foons"

# Clear the namespace from the directory's layout
<code class="prompt user">root # </code>setfattr -x ceph.dir.layout.pool_namespace mydir
<code class="prompt user">root # </code>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a"</pre></div></section><section class="sect3" id="id-1.4.5.5.7.6.8" data-id-title="Inheritance of Layouts"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.5 </span><span class="title-name">Inheritance of Layouts</span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.8">#</a></h4></div></div></div><p>
     Files inherit the layout of their parent directory at creation time.
     However, subsequent changes to the parent directory’s layout do not affect
     children:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# file1 inherits its parent's layout
<code class="prompt user">root # </code>touch dir/file1
<code class="prompt user">root # </code>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# update the layout of the directory before creating a second file
<code class="prompt user">root # </code>setfattr -n ceph.dir.layout.stripe_count -v 4 dir
<code class="prompt user">root # </code>touch dir/file2

# file1's layout is unchanged
<code class="prompt user">root # </code>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# ...while file2 has the parent directory's new layout
<code class="prompt user">root # </code>getfattr -n ceph.file.layout dir/file2
# file: dir/file2
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"</pre></div><p>
     Files created as descendants of the directory also inherit its layout if
     the intermediate directories do not have layouts set:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
<code class="prompt user">root # </code>mkdir dir/childdir
<code class="prompt user">root # </code>getfattr -n ceph.dir.layout dir/childdir
dir/childdir: ceph.dir.layout: No such attribute
<code class="prompt user">root # </code>touch dir/childdir/grandchild
<code class="prompt user">root # </code>getfattr -n ceph.file.layout dir/childdir/grandchild
# file: dir/childdir/grandchild
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"</pre></div></section><section class="sect3" id="id-1.4.5.5.7.6.9" data-id-title="Adding a Data Pool to the Metadata Server"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.4.6 </span><span class="title-name">Adding a Data Pool to the Metadata Server</span> <a title="Permalink" class="permalink" href="#id-1.4.5.5.7.6.9">#</a></h4></div></div></div><p>
     Before you can use a pool with CephFS, you need to add it to the Metadata Server:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs add_data_pool cephfs cephfs_data_ssd
<code class="prompt user">cephadm@adm &gt; </code>ceph fs ls  # Pool should now show up
.... data pools: [cephfs_data cephfs_data_ssd ]</pre></div><div id="id-1.4.5.5.7.6.9.4" data-id-title="cephx Keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: cephx Keys</h6><p>
      Make sure that your cephx keys allow the client to access this new pool.
     </p></div><p>
     You can then update the layout on a directory in CephFS to use the pool
     you added:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /mnt/cephfs/myssddir
<code class="prompt user">root # </code>setfattr -n ceph.dir.layout.pool -v cephfs_data_ssd /mnt/cephfs/myssddir</pre></div><p>
     All new files created within that directory will now inherit its layout
     and place their data in your newly added pool. You may notice that the
     number of objects in your primary data pool continues to increase, even if
     files are being created in the pool you newly added. This is normal: the
     file data is stored in the pool specified by the layout, but a small
     amount of metadata is kept in the primary data pool for all files.
    </p></section></section></section></section><section class="chapter" id="cha-as-ganesha" data-id-title="Installation of NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12 </span><span class="title-name">Installation of NFS Ganesha</span> <a title="Permalink" class="permalink" href="#cha-as-ganesha">#</a></h2></div></div></div><p>
  NFS Ganesha provides NFS access to either the Object Gateway or the CephFS. In
  SUSE Enterprise Storage 6, NFS versions 3 and 4 are supported. NFS Ganesha
  runs in the user space instead of the kernel space and directly interacts
  with the Object Gateway or CephFS.
 </p><div id="id-1.4.5.6.4" data-id-title="Cross Protocol Access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Cross Protocol Access</h6><p>
   Native CephFS and NFS clients are not restricted by file locks obtained
   via Samba, and vice versa. Applications that rely on cross protocol file
   locking may experience data corruption if CephFS backed Samba share paths
   are accessed via other means.
  </p></div><section class="sect1" id="sec-as-ganesha-preparation" data-id-title="Preparation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Preparation</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation">#</a></h2></div></div></div><section class="sect2" id="sec-as-ganesha-preparation-general" data-id-title="General Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.1 </span><span class="title-name">General Information</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation-general">#</a></h3></div></div></div><p>
    To successfully deploy NFS Ganesha, you need to add a
    <code class="literal">role-ganesha</code> to your
    <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>. For details,
    see <a class="xref" href="#policy-configuration" title="5.5.1. The policy.cfg File">Section 5.5.1, “The <code class="filename">policy.cfg</code> File”</a>. NFS Ganesha also needs either a
    <code class="literal">role-rgw</code> or a <code class="literal">role-mds</code> present in the
    <code class="filename">policy.cfg</code>.
   </p><p>
    Although it is possible to install and run the NFS Ganesha server on an
    already existing Ceph node, we recommend running it on a dedicated host
    with access to the Ceph cluster. The client hosts are typically not part
    of the cluster, but they need to have network access to the NFS Ganesha
    server.
   </p><p>
    To enable the NFS Ganesha server at any point after the initial installation,
    add the <code class="literal">role-ganesha</code> to the
    <code class="filename">policy.cfg</code> and re-run at least DeepSea stages 2 and
    4. For details, see <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>.
   </p><p>
    NFS Ganesha is configured via the file
    <code class="filename">/etc/ganesha/ganesha.conf</code> that exists on the NFS Ganesha
    node. However, this file is overwritten each time DeepSea stage 4 is
    executed. Therefore we recommend to edit the template used by Salt, which
    is the file
    <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> on the
    Salt master. For details about the configuration file, see
    <span class="intraxref">Book “Administration Guide”, Chapter 30 “NFS Ganesha: Export Ceph Data via NFS”, Section 30.2 “Configuration”</span>.
   </p></section><section class="sect2" id="sec-as-ganesha-preparation-requirements" data-id-title="Summary of Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.2 </span><span class="title-name">Summary of Requirements</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-preparation-requirements">#</a></h3></div></div></div><p>
    The following requirements need to be met before DeepSea stages 2 and 4
    can be executed to install NFS Ganesha:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least one node needs to be assigned the
      <code class="literal">role-ganesha</code>.
     </p></li><li class="listitem"><p>
      You can define only one <code class="literal">role-ganesha</code> per minion.
     </p></li><li class="listitem"><p>
      NFS Ganesha needs either an Object Gateway or CephFS to work.
     </p></li><li class="listitem"><p>
      The kernel based NFS needs to be disabled on minions with the
      <code class="literal">role-ganesha</code> role.
     </p></li></ul></div></section></section><section class="sect1" id="sec-as-ganesha-basic-example" data-id-title="Example Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Example Installation</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-basic-example">#</a></h2></div></div></div><p>
   This procedure provides an example installation that uses both the Object Gateway and
   CephFS File System Abstraction Layers (FSAL) of NFS Ganesha.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If you have not done so, execute DeepSea stages 0 and 1 before
     continuing with this procedure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.0
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     After having executed stage 1 of DeepSea, edit the
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and add the
     line
    </p><div class="verbatim-wrap"><pre class="screen">role-ganesha/cluster/<em class="replaceable">NODENAME</em></pre></div><p>
     Replace <em class="replaceable">NODENAME</em> with the name of a node in
     your cluster.
    </p><p>
     Also make sure that a <code class="literal">role-mds</code> and a
     <code class="literal">role-rgw</code> are assigned.
    </p></li><li class="step"><p>
     Execute at least stages 2 and 4 of DeepSea. Running stage 3 in between
     is recommended.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.3 # optional but recommended
<code class="prompt user">root@master # </code><code class="command">salt-run</code> state.orch ceph.stage.4</pre></div></li><li class="step"><p>
     Verify that NFS Ganesha is working by checking that the NFS Ganesha service
     is running on the minion node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt</code> -I roles:ganesha service.status nfs-ganesha
<em class="replaceable">MINION_ID</em>:
    True</pre></div></li></ol></div></div></section><section class="sect1" id="sec-ganesha-active-active" data-id-title="Active-Active Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.3 </span><span class="title-name">Active-Active Configuration</span> <a title="Permalink" class="permalink" href="#sec-ganesha-active-active">#</a></h2></div></div></div><p>
   This section provides an example of simple active-active NFS Ganesha setup.
   The aim is to deploy two NFS Ganesha servers layered on top of the same
   existing CephFS. The servers will be two Ceph cluster nodes with
   separate addresses. The clients need to be distributed between them
   manually. <span class="quote">“<span class="quote">Failover</span>”</span> in this configuration means manually
   unmounting and remounting the other server on the client.
  </p><section class="sect2" id="sec-ganesha-active-active-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#sec-ganesha-active-active-prerequisites">#</a></h3></div></div></div><p>
    For our example configuration, you need the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Running Ceph cluster. See <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a> for
      details on deploying and configuring Ceph cluster by using DeepSea.
     </p></li><li class="listitem"><p>
      At least one configured CephFS. See
      <a class="xref" href="#cha-ceph-as-cephfs" title="Chapter 11. Installation of CephFS">Chapter 11, <em>Installation of CephFS</em></a> for more details on deploying and
      configuring CephFS.
     </p></li><li class="listitem"><p>
      Two Ceph cluster nodes with NFS Ganesha deployed. See
      <a class="xref" href="#cha-as-ganesha" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a> for more details on deploying
      NFS Ganesha.
     </p><div id="id-1.4.5.6.7.3.3.3.2" data-id-title="Use Dedicated Servers" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use Dedicated Servers</h6><p>
       Although NFS Ganesha nodes can share resources with other Ceph related
       services, we recommend to use dedicated servers to improve performance.
      </p></div></li></ul></div><p>
    After you deploy the NFS Ganesha nodes, verify that the cluster is
    operational and the default CephFS pools are there:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados lspools
cephfs_data
cephfs_metadata</pre></div></section><section class="sect2" id="sec-ganesha-active-active-configure" data-id-title="Configure NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.2 </span><span class="title-name">Configure NFS Ganesha</span> <a title="Permalink" class="permalink" href="#sec-ganesha-active-active-configure">#</a></h3></div></div></div><p>
    Check that both NFS Ganesha nodes have the file
    <code class="filename">/etc/ganesha/ganesha.conf</code> installed. Add the following
    blocks, if they do not exist yet, to the configuration file in order to
    enable RADOS as the recovery backend of NFS Ganesha.
   </p><div class="verbatim-wrap"><pre class="screen">NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<em class="replaceable">rados_pool</em>";
    namespace = "<em class="replaceable">pool_namespace</em>";
    nodeid = "<em class="replaceable">fqdn</em>"
    UserId = "<em class="replaceable">cephx_user_id</em>";
    Ceph_Conf = "<em class="replaceable">path_to_ceph.conf</em>"
}</pre></div><p>
    You can find out the values for <em class="replaceable">rados_pool</em> and
    <em class="replaceable">pool_namespace</em> by checking the already existing
    line in the configuration of the form:
   </p><div class="verbatim-wrap"><pre class="screen">%url rados://<em class="replaceable">rados_pool</em>/<em class="replaceable">pool_namespace</em>/...</pre></div><p>
    The value for <em class="replaceable">nodeid</em> option corresponds to the
    FQDN of the machine, and <em class="replaceable">UserId</em> and
    <em class="replaceable">Ceph_Conf</em> options value can be found in the
    already existing <em class="replaceable">RADOS_URLS</em> block.
   </p><p>
    Because legacy versions of NFS prevent us from lifting the grace period
    early and therefore prolong a server restart, we disable options for NFS
    prior to version 4.2. We also disable most of the NFS Ganesha caching as
    Ceph libraries do aggressive caching already.
   </p><p>
    The 'rados_cluster' recovery back-end stores its info in RADOS objects.
    Although it is not a lot of data, we want it highly available. We use the
    CephFS metadata pool for this purpose, and declare a new 'ganesha'
    namespace in it to keep it distinct from CephFS objects.
   </p><div id="id-1.4.5.6.7.4.9" data-id-title="Cluster Node IDs" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Cluster Node IDs</h6><p>
     Most of the configuration is identical between the two hosts, however the
     <code class="option">nodeid</code> option in the 'RADOS_KV' block needs to be a
     unique string for each node. By default, NFS Ganesha sets
     <code class="option">nodeid</code> to the host name of the node.
    </p><p>
     If you need to use different fixed values other than host names, you can
     for example set <code class="option">nodeid = 'a'</code> on one node and
     <code class="option">nodeid = 'b'</code> on the other one.
    </p></div></section><section class="sect2" id="ganesha-active-active-grace-db" data-id-title="Populate the Cluster Grace Database"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.3 </span><span class="title-name">Populate the Cluster Grace Database</span> <a title="Permalink" class="permalink" href="#ganesha-active-active-grace-db">#</a></h3></div></div></div><p>
    We need to verify that all of the nodes in the cluster know about each
    other. This done via a RADOS object that is shared between the hosts.
    NFS Ganesha uses this object to communicate the current state with regard to
    a grace period.
   </p><p>
    The <span class="package">nfs-ganesha-rados-grace</span> package contains a command
    line tool for querying and manipulating this database. If the package is
    not installed on at least one of the nodes, install it with
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install nfs-ganesha-rados-grace</pre></div><p>
    We will use the command to create the DB and add both
    <code class="option">nodeid</code>s. In our example, the two NFS Ganesha nodes are named
    <code class="literal">ses6min1.example.com</code> and
    <code class="literal">ses6min2.example.com</code> On one of the NFS Ganesha hosts, run
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<code class="prompt user">cephadm@adm &gt; </code>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<code class="prompt user">cephadm@adm &gt; </code>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E</pre></div><p>
    This creates the grace database and adds both 'ses6min1.example.com' and
    'ses6min2.example.com' to it. The last command dumps the current state.
    Newly added hosts are always considered to be enforcing the grace period so
    they both have the 'E' flag set. The 'cur' and 'rec' values show the
    current and recovery epochs, which is how we keep track of what hosts are
    allowed to perform recovery and when.
   </p></section><section class="sect2" id="ganesha-active-active-restart-servers" data-id-title="Restart NFS Ganesha Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.4 </span><span class="title-name">Restart NFS Ganesha Services</span> <a title="Permalink" class="permalink" href="#ganesha-active-active-restart-servers">#</a></h3></div></div></div><p>
    On both NFS Ganesha nodes, restart the related services:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart nfs-ganesha.service</pre></div><p>
    After the services are restarted, check the grace database:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com</pre></div><div id="id-1.4.5.6.7.6.6" data-id-title="Cleared the E Flag" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Cleared the 'E' Flag</h6><p>
     Note that both nodes have cleared their 'E' flags, indicating that they
     are no longer enforcing the grace period and are now in normal operation
     mode.
    </p></div></section><section class="sect2" id="ganesha-active-active-conclusion" data-id-title="Conclusion"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.5 </span><span class="title-name">Conclusion</span> <a title="Permalink" class="permalink" href="#ganesha-active-active-conclusion">#</a></h3></div></div></div><p>
    After you complete all the preceding steps, you can mount the exported NFS
    from either of the two NFS Ganesha servers, and perform normal NFS operations
    against them.
   </p><p>
    Our example configuration assumes that if one of the two NFS Ganesha servers
    goes down, you will restart it manually within 5 minutes. After 5 minutes,
    the Metadata Server may cancel the session that the NFS Ganesha client held and all of
    the state associated with it. If the session’s capabilities get cancelled
    before the rest of the cluster goes into the grace period, the server’s
    clients may not be able to recover all of their state.
   </p></section></section><section class="sect1" id="sec-as-ganesha-info" data-id-title="More Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.4 </span><span class="title-name">More Information</span> <a title="Permalink" class="permalink" href="#sec-as-ganesha-info">#</a></h2></div></div></div><p>
   More information can be found in <span class="intraxref">Book “Administration Guide”, Chapter 30 “NFS Ganesha: Export Ceph Data via NFS”</span>.
  </p></section></section></div><div class="part" id="containerized-ses-on-caasp" data-id-title="Cluster Deployment on Top of SUSE CaaS Platform 4 (Technology Preview)"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part IV </span><span class="title-name">Cluster Deployment on Top of SUSE CaaS Platform 4 (Technology Preview) </span><a title="Permalink" class="permalink" href="#containerized-ses-on-caasp">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-container-kubernetes"><span class="title-number">13 </span><span class="title-name">SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</span></a></span></li><dd class="toc-abstract"><p>
   Running containerized Ceph cluster on SUSE CaaS Platform is a technology preview. Do
   not deploy on a production Kubernetes cluster. This is not a supported version.
  </p></dd></ul></div><section class="chapter" id="cha-container-kubernetes" data-id-title="SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13 </span><span class="title-name">SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</span> <a title="Permalink" class="permalink" href="#cha-container-kubernetes">#</a></h2></div></div></div><div id="id-1.4.6.2.3" data-id-title="Technology Preview" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Technology Preview</h6><p>
   Running containerized Ceph cluster on SUSE CaaS Platform is a technology preview. Do
   not deploy on a production Kubernetes cluster. This is not a supported version.
  </p></div><p>
  This chapter describes how to deploy containerized SUSE Enterprise Storage
  6 on top of SUSE CaaS Platform 4 Kubernetes cluster.
 </p><section class="sect1" id="kube-consider" data-id-title="Considerations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">Considerations</span> <a title="Permalink" class="permalink" href="#kube-consider">#</a></h2></div></div></div><p>
   Before you start deploying, consider the following points:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     To run Ceph in Kubernetes, SUSE Enterprise Storage 6 uses an upstream
     project called Rook (<a class="link" href="https://rook.io/" target="_blank">https://rook.io/</a>).
    </p></li><li class="listitem"><p>
     Depending on the configuration, Rook may consume
     <span class="emphasis"><em>all</em></span> unused disks on all nodes in a Kubernetes cluster.
    </p></li><li class="listitem"><p>
     The setup requires privileged containers.
    </p></li></ul></div></section><section class="sect1" id="kube-prereq" data-id-title="Prerequisites"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.2 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#kube-prereq">#</a></h2></div></div></div><p>
   The minimum requirements and prerequisites to deploy SUSE Enterprise Storage
   6 on top of SUSE CaaS Platform 4 Kubernetes cluster are as follows:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A running SUSE CaaS Platform 4 cluster. You need to have an account with a SUSE CaaS Platform
     subscription. You can activate a 60-day free evaluation here
     <a class="link" href="https://www.suse.com/products/caas-platform/download/MkpwEt3Ub98~/?campaign_name=Eval:_CaaSP_4" target="_blank">https://www.suse.com/products/caas-platform/download/MkpwEt3Ub98~/?campaign_name=Eval:_CaaSP_4</a>.
    </p></li><li class="listitem"><p>
     At least three SUSE CaaS Platform worker nodes, with at least one additional disk
     attached to each worker node as storage for the OSD. We recommend four
     SUSE CaaS Platform worker nodes.
    </p></li><li class="listitem"><p>
     At least one OSD per worker node, with a minimum disk size of 5 GB.
    </p></li><li class="listitem"><p>
     Access to SUSE Enterprise Storage 6. You can get a trial subscription
     from here
     <a class="link" href="https://www.suse.com/products/suse-enterprise-storage/download/" target="_blank">https://www.suse.com/products/suse-enterprise-storage/download/</a>.
    </p></li><li class="listitem"><p>
     Access to a workstation that has access to the SUSE CaaS Platform cluster via
     <code class="literal">kubectl</code>. We recommend using the SUSE CaaS Platform master node as
     the workstation.
    </p></li><li class="listitem"><p>
     Ensure that the <code class="literal">SUSE-Enterprise-Storage-6-Pool</code> and
     <code class="literal">SUSE-Enterprise-Storage-6-Updates</code> repositories are
     configured on the management node to install the
     <span class="package">rook-k8s-yaml</span> RPM package.
    </p></li></ul></div></section><section class="sect1" id="kube-rook-manifests" data-id-title="Get Rook Manifests"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.3 </span><span class="title-name">Get Rook Manifests</span> <a title="Permalink" class="permalink" href="#kube-rook-manifests">#</a></h2></div></div></div><p>
   The Rook orchestrator uses configuration files in YAML format called
   <span class="emphasis"><em>manifests</em></span>. The manifests you need are included in the
   <span class="package">rook-k8s-yaml</span> RPM package. You can find this package in
   the SUSE Enterprise Storage 6 repository. Install it by running the
   following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install rook-k8s-yaml</pre></div></section><section class="sect1" id="kube-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.4 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#kube-install">#</a></h2></div></div></div><p>
   Rook-Ceph includes two main components: the 'operator' which is run by
   Kubernetes and allows creation of Ceph clusters, and the Ceph 'cluster'
   itself which is created and partially managed by the operator.
  </p><section class="sect2" id="kube-configure" data-id-title="Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.1 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#kube-configure">#</a></h3></div></div></div><section class="sect3" id="kube-configure-global" data-id-title="Global Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.1.1 </span><span class="title-name">Global Configuration</span> <a title="Permalink" class="permalink" href="#kube-configure-global">#</a></h4></div></div></div><p>
     The manifests used in this setup install all Rook and Ceph components
     in the 'rook-ceph' namespace. If you need to change it, adopt all
     references to the namespace in the Kubernetes manifests accordingly.
    </p><p>
     Depending on which features of Rook you intend to use, alter the 'Pod
     Security Policy' configuration in <code class="filename">common.yaml</code> to
     limit Rook's security requirements. Follow the comments in the manifest
     file.
    </p></section><section class="sect3" id="kube-config-operator" data-id-title="Operator Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.1.2 </span><span class="title-name">Operator Configuration</span> <a title="Permalink" class="permalink" href="#kube-config-operator">#</a></h4></div></div></div><p>
     The manifest <code class="filename">operator.yaml</code> configures the Rook
     operator. Normally, you do not need to change it. Find more information
     following the comments in the manifest file.
    </p></section><section class="sect3" id="kube-config-ceph" data-id-title="Ceph Cluster Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.1.3 </span><span class="title-name">Ceph Cluster Configuration</span> <a title="Permalink" class="permalink" href="#kube-config-ceph">#</a></h4></div></div></div><p>
     The manifest <code class="filename">cluster.yaml</code> is responsible for
     configuring the actual Ceph cluster which will run in Kubernetes. Find
     detailed description of all available options in the upstream Rook
     documentation at
     <a class="link" href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html" target="_blank">https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html</a>.
    </p><p>
     By default, Rook is configured to use all nodes that are not tainted
     with <code class="option">node-role.kubernetes.io/master:NoSchedule</code> and will
     obey configured placement settings (see
     <a class="link" href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html#placement-configuration-settings" target="_blank">https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html#placement-configuration-settings</a>).
     The following example disables such behavior and only uses the nodes
     explicitly listed in the nodes section:
    </p><div class="verbatim-wrap"><pre class="screen">storage:
  useAllNodes: false
  nodes:
    - name: caasp4-worker-0
    - name: caasp4-worker-1
    - name: caasp4-worker-2</pre></div><div id="id-1.4.6.2.8.3.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      By default, Rook is configured to use all free and empty disks on each
      node for use as Ceph storage.
     </p></div></section><section class="sect3" id="kube-config-docs" data-id-title="Documentation"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.1.4 </span><span class="title-name">Documentation</span> <a title="Permalink" class="permalink" href="#kube-config-docs">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The Rook-Ceph upstream documentation at
       <a class="link" href="https://rook.github.io/docs/rook/v1.3/ceph-storage.html" target="_blank">https://rook.github.io/docs/rook/v1.3/ceph-storage.html</a>
       contains more detailed information about configuring more advanced
       deployments. Use it as a reference for understanding the basics of
       Rook-Ceph before doing more advanced configurations.
      </p></li><li class="listitem"><p>
       Find more details about the SUSE CaaS Platform product at
       <a class="link" href="https://documentation.suse.com/suse-caasp/4.0/" target="_blank">https://documentation.suse.com/suse-caasp/4.0/</a>.
      </p></li></ul></div></section></section><section class="sect2" id="kube-config-rook-operator" data-id-title="Create the Rook Operator"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.2 </span><span class="title-name">Create the Rook Operator</span> <a title="Permalink" class="permalink" href="#kube-config-rook-operator">#</a></h3></div></div></div><p>
    Install the Rook-Ceph common components, CSI roles, and the Rook-Ceph
    operator by executing the following command on the SUSE CaaS Platform master node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>kubectl apply -f common.yaml -f operator.yaml</pre></div><p>
    <code class="filename">common.yaml</code> will create the 'rook-ceph' namespace,
    Ceph Custom Resource Definitions (CRDs) (see
    <a class="link" href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a>)
    to make Kubernetes aware of Ceph Objects (for example, 'CephCluster'), and
    the RBAC roles and Pod Security Policies (see
    <a class="link" href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank">https://kubernetes.io/docs/concepts/policy/pod-security-policy/</a>)
    which are necessary for allowing Rook to manage the cluster-specific
    resources.
   </p><div id="id-1.4.6.2.8.4.5" data-id-title="hostNetwork and hostPorts Usage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="option">hostNetwork</code> and <code class="option">hostPorts</code> Usage</h6><p>
     Allowing the usage of <code class="option">hostNetwork</code> is required when using
     <code class="option">hostNetwork: true</code> in the Cluster Resource Definition.
     Allowing the usage of <code class="option">hostPorts</code> in the
     <code class="literal">PodSecurityPolicy</code> is also required.
    </p></div><p>
    Verify the installation by running <code class="command">kubectl get pods -n
    rook-ceph</code> on SUSE CaaS Platform master node, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>kubectl get pods -n rook-ceph
NAME                                     READY   STATUS      RESTARTS   AGE
rook-ceph-agent-57c9j                    1/1     Running     0          22h
rook-ceph-agent-b9j4x                    1/1     Running     0          22h
rook-ceph-operator-cf6fb96-lhbj7         1/1     Running     0          22h
rook-discover-mb8gv                      1/1     Running     0          22h
rook-discover-tztz4                      1/1     Running     0          22h</pre></div></section><section class="sect2" id="kube-create-ceph-cluster" data-id-title="Create the Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.3 </span><span class="title-name">Create the Ceph Cluster</span> <a title="Permalink" class="permalink" href="#kube-create-ceph-cluster">#</a></h3></div></div></div><p>
    After you modify <code class="filename">cluster.yaml</code> according to your needs,
    you can create the Ceph cluster. Run the following command on the SUSE CaaS Platform
    master node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>kubectl apply -f cluster.yaml</pre></div><p>
    Watch the 'rook-ceph' namespace to see the Ceph cluster being created.
    You will see as many Ceph Monitors as configured in the
    <code class="filename">cluster.yaml</code> manifest (default is 3), one Ceph Manager, and
    as many Ceph OSDs as you have free disks.
   </p><div id="id-1.4.6.2.8.5.5" data-id-title="Temporary OSD Pods" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Temporary OSD Pods</h6><p>
     While bootstrapping the Ceph cluster, you will see some pods with the
     name
     <code class="literal">rook-ceph-osd-prepare-<em class="replaceable">NODE-NAME</em></code>
     run for a while and then terminate with the status 'Completed'. As their
     name implies, these pods provision Ceph OSDs. They are left without being
     deleted so that you can inspect their logs after their termination. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>kubectl get pods --namespace rook-ceph
NAME                                         READY  STATUS     RESTARTS  AGE
rook-ceph-agent-57c9j                        1/1    Running    0         22h
rook-ceph-agent-b9j4x                        1/1    Running    0         22h
rook-ceph-mgr-a-6d48564b84-k7dft             1/1    Running    0         22h
rook-ceph-mon-a-cc44b479-5qvdb               1/1    Running    0         22h
rook-ceph-mon-b-6c6565ff48-gm9wz             1/1    Running    0         22h
rook-ceph-operator-cf6fb96-lhbj7             1/1    Running    0         22h
rook-ceph-osd-0-57bf997cbd-4wspg             1/1    Running    0         22h
rook-ceph-osd-1-54cf468bf8-z8jhp             1/1    Running    0         22h
rook-ceph-osd-prepare-caasp4-worker-0-f2tmw  0/2    Completed  0         9m35s
rook-ceph-osd-prepare-caasp4-worker-1-qsfhz  0/2    Completed  0         9m33s
rook-ceph-tools-76c7d559b6-64rkw             1/1    Running    0         22h
rook-discover-mb8gv                          1/1    Running    0         22h
rook-discover-tztz4                          1/1    Running    0         22h</pre></div></div></section></section><section class="sect1" id="kube-using-rook" data-id-title="Using Rook as Storage for Kubernetes Workload"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.5 </span><span class="title-name">Using Rook as Storage for Kubernetes Workload</span> <a title="Permalink" class="permalink" href="#kube-using-rook">#</a></h2></div></div></div><p>
   Rook allows you to use three different types of storage:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.9.3.1"><span class="term"><span class="bold"><strong>Object Storage</strong></span></span></dt><dd><p>
      Object storage exposes an S3 API to the storage cluster for applications
      to put and get data. Refer to
      <a class="link" href="https://rook.io/docs/rook/v1.0/ceph-object.html" target="_blank">https://rook.io/docs/rook/v1.0/ceph-object.html</a> for
      a detailed description.
     </p></dd><dt id="id-1.4.6.2.9.3.2"><span class="term">Shared File System</span></dt><dd><p>
      A shared file system can be mounted with read/write permission from
      multiple pods. This is useful for applications that are clustered using a
      shared file system. Refer to
      <a class="link" href="https://rook.io/docs/rook/v1.0/ceph-filesystem.html" target="_blank">https://rook.io/docs/rook/v1.0/ceph-filesystem.html</a>
      for a detailed description.
     </p></dd><dt id="id-1.4.6.2.9.3.3"><span class="term">Block Storage</span></dt><dd><p>
      Block storage allows you to mount storage to a single pod. Refer to
      <a class="link" href="https://rook.io/docs/rook/v1.0/ceph-block.html" target="_blank">https://rook.io/docs/rook/v1.0/ceph-block.html</a> for a
      detailed description.
     </p></dd></dl></div></section><section class="sect1" id="kube-uninstall" data-id-title="Uninstalling Rook"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.6 </span><span class="title-name">Uninstalling Rook</span> <a title="Permalink" class="permalink" href="#kube-uninstall">#</a></h2></div></div></div><p>
   To uninstall Rook, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Delete any Kubernetes applications that are consuming Rook storage.
    </p></li><li class="step"><p>
     Delete all object, file, and/or block storage artifacts that you created
     by following <a class="xref" href="#kube-using-rook" title="13.5. Using Rook as Storage for Kubernetes Workload">Section 13.5, “Using Rook as Storage for Kubernetes Workload”</a>.
    </p></li><li class="step"><p>
     Delete the Ceph cluster, operator, and related resources:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>kubectl delete -f cluster.yaml
<code class="prompt user">root # </code>kubectl delete -f operator.yaml
<code class="prompt user">root # </code>kubectl delete -f common.yaml</pre></div></li><li class="step"><p>
     Delete the data on hosts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rm -rf /var/lib/rook</pre></div></li><li class="step"><p>
     If necessary, wipe the disks that were used by Rook. Refer to
     <a class="link" href="https://rook.github.io/docs/rook/v1.3/ceph-teardown.html" target="_blank">https://rook.github.io/docs/rook/v1.3/ceph-teardown.html</a>
     for more details.
    </p></li></ol></div></div></section></section></div><section class="appendix" id="id-1.4.7" data-id-title="Ceph Maintenance Updates Based on Upstream Nautilus Point Releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span> <a title="Permalink" class="permalink" href="#id-1.4.7">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 6 are based on the
  Nautilus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Nautilus series, SUSE Enterprise Storage 6 is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.5"><span class="name">Nautilus 14.2.20 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.12"><span class="name">Nautilus 14.2.18 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.12">#</a></h2></div><p>
  This release fixes a regression introduced in 14.2.17 in which the manager
  module tries to use a couple of Python modules that do not exist in some
  environments.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    This release fixes issues loading the dashboard and volumes manager modules
    in some environments.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.15"><span class="name">Nautilus 14.2.17 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.15">#</a></h2></div><p>
  This release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="varname">$pid</code> expansion in configuration paths such as
    <code class="literal">admin_socket</code> will now properly expand to the daemon PID
    for commands like <code class="command">ceph-mds</code> or
    <code class="command">ceph-osd</code>. Previously, only <code class="command">ceph-fuse</code>
    and <code class="command">rbd-nbd</code> expanded <code class="varname">$pid</code> with the
    actual daemon PID.
   </p></li><li class="listitem"><p>
    RADOS: PG removal has been optimized.
   </p></li><li class="listitem"><p>
    RADOS: Memory allocations are tracked in finer detail in BlueStore and
    displayed as a part of the <code class="command">dump_mempools</code> command.
   </p></li><li class="listitem"><p>
    CephFS: clients which acquire capabilities too quickly are throttled to
    prevent instability. See new config option
    <code class="option">mds_session_cap_acquisition_throttle</code> to control this
    behavior.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.18"><span class="name">Nautilus 14.2.16 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-27781 : OpenStack Manila use of
    <code class="command">ceph_volume_client.py</code> library allowed tenant access to
    any Ceph credentials' secret.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.21"><span class="name">Nautilus 14.2.15 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.21">#</a></h2></div><p>
  This release fixes a ceph-volume regression introduced in v14.2.13 and
  includes few other fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    ceph-volume: Fixes <code class="command">lvm batch –auto</code>, which breaks
    backward compatibility when using non rotational devices only (SSD and/or
    NVMe).
   </p></li><li class="listitem"><p>
    BlueStore: Fixes a bug in <code class="literal">collection_list_legacy</code> which
    makes PGs inconsistent during scrub when running OSDs older than 14.2.12
    with newer ones.
   </p></li><li class="listitem"><p>
    MGR: progress module can now be turned on or off, using the commands
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.24"><span class="name">Nautilus 14.2.14 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.24">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus, among other fixes across components.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE 2020-25660: Fix a regression in Messenger V2 replay attacks.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.27"><span class="name">Nautilus 14.2.13 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.27">#</a></h2></div><p>
  This release fixes a regression introduced in v14.2.12, and a few ceph-volume
  amd RGW fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a regression that caused breakage in clusters that referred to
    ceph-mon hosts using dns names instead of IP addresses in the
    <code class="option">mon_host</code> parameter in <code class="filename">ceph.conf</code>.
   </p></li><li class="listitem"><p>
    ceph-volume: the <code class="command">lvm batch</code> subcommand received a major
    rewrite.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.30"><span class="name">Nautilus 14.2.12 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.30">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="command">ceph df command</code> now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    MONs now have a config option <code class="option">mon_osd_warn_num_repaired</code>,
    10 by default. If any OSD has repaired more than this many I/O errors in
    stored data, a <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is
    generated. In order to allow clearing of the warning, a new command
    <code class="command">ceph tell osd.<em class="replaceable">SERVICE_ID</em>
    clear_shards_repaired <em class="replaceable">COUNT</em></code> has been
    added. By default, it will set the repair count to 0. If you want to be
    warned again if additional repairs are performed, you can provide a value
    to the command and specify the value of
    <code class="option">mon_osd_warn_num_repaired</code>. This command will be replaced
    in future releases by the health mute/unmute feature.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial MON to contact for Ceph tools
    and daemons using the <code class="option">mon_host_override config</code> option or
    <code class="option">--mon-host-override <em class="replaceable">IP</em></code>
    command-line switch. This generally should only be used for debugging and
    only affects initial communication with Ceph’s MON cluster.
   </p></li><li class="listitem"><p>
    Fix an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.33"><span class="name">Nautilus 14.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.33">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    RGW: The <code class="command">radosgw-admin</code> sub-commands dealing with orphans
    – <code class="command">radosgw-admin orphans find</code>, <code class="command">radosgw-admin
    orphans finish</code>, <code class="command">radosgw-admin orphans
    list-jobs</code> – have been deprecated. They have not been actively
    maintained and they store intermediate results on the cluster, which could
    fill a nearly-full cluster. They have been replaced by a tool, currently
    considered experimental, <code class="command">rgw-orphan-list</code>.
   </p></li><li class="listitem"><p>
    Now, when <code class="option">noscrub</code> and/or <code class="option">nodeep-scrub</code>
    flags are set globally or per pool, scheduled scrubs of the type disabled
    will be aborted. All user initiated scrubs are <span class="emphasis"><em>not</em></span>
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed a ceph-osd crash in committed OSD maps when there is a failure to
    encode the first incremental map.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.36"><span class="name">Nautilus 14.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.36">#</a></h2></div><p>
  This upstream release patched one security flaw:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li></ul></div><p>
  In addition to security flaws, this major upstream release brought a number
  of notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The pool parameter <code class="option">target_size_ratio</code>, used by the PG
    autoscaler, has changed meaning. It is now normalized across pools, rather
    than specifying an absolute ratio. If you have set target size ratios on
    any pools, you may want to set these pools to autoscale
    <code class="literal">warn</code> mode to avoid data movement during the upgrade:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_autoscale_mode warn</pre></div></li><li class="listitem"><p>
    The behaviour of the <code class="option">-o</code> argument to the RADOS tool has
    been reverted to its original behaviour of indicating an output file. This
    reverts it to a more consistent behaviour when compared to other tools.
    Specifying object size is now accomplished by using an upper case O
    <code class="option">-O</code>.
   </p></li><li class="listitem"><p>
    The format of MDSs in <code class="command">ceph fs dump</code> has changed.
   </p></li><li class="listitem"><p>
    Ceph will issue a health warning if a RADOS pool’s
    <code class="literal">size</code> is set to 1 or, in other words, the pool is
    configured with no redundancy. This can be fixed by setting the pool size
    to the minimum recommended value with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool-name</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The warning can be silenced with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    RGW: bucket listing performance on sharded bucket indexes has been notably
    improved by heuristically – and significantly, in many cases – reducing the
    number of entries requested from each bucket index shard.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.41"><span class="name">Nautilus 14.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.41">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS due to RGW GetObject header-splitting
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.389+gb0f23ac248.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.45"><span class="name">Nautilus 14.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.45">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The default value of <code class="option">bluestore_min_alloc_size_ssd</code> has been
    changed to 4K to improve performance across all workloads.
   </p></li><li class="listitem"><p>
    The following OSD memory config options related to BlueStore cache
    autotuning can now be configured during runtime:
   </p><div class="verbatim-wrap"><pre class="screen">osd_memory_base (default: 768 MB)
osd_memory_cache_min (default: 128 MB)
osd_memory_expected_fragmentation (default: 0.15)
osd_memory_target (default: 4 GB)</pre></div><p>
    You can set the above options by running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div></li><li class="listitem"><p>
    The Ceph Manager now accepts <code class="literal">profile rbd</code> and <code class="literal">profile
    rbd-read-only</code> user capabilities. You can use these capabilities
    to provide users access to MGR-based RBD functionality such as <code class="literal">rbd
    perf image iostat</code> and <code class="literal">rbd perf image iotop</code>.
   </p></li><li class="listitem"><p>
    The configuration value <code class="option">osd_calc_pg_upmaps_max_stddev</code> used
    for upmap balancing has been removed. Instead, use the Ceph Manager balancer
    configuration option <code class="option">upmap_max_deviation</code> which now is an
    integer number of PGs of deviation from the target PGs per OSD. You can set
    it with a following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/balancer/upmap_max_deviation 2</pre></div><p>
    The default <code class="option">upmap_max_deviation</code> is 5. There are situations
    where crush rules would not allow a pool to ever have completely balanced
    PGs. For example, if crush requires 1 replica on each of 3 racks, but there
    are fewer OSDs in 1 of the racks. In those cases, the configuration value
    can be increased.
   </p></li><li class="listitem"><p>
    CephFS: multiple active Metadata Server forward scrub is now rejected. Scrub is
    currently only permitted on a file system with a single rank. Reduce the
    ranks to one via <code class="command">ceph fs set <em class="replaceable">FS_NAME</em>
    max_mds 1</code>.
   </p></li><li class="listitem"><p>
    Ceph will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. This can be fixed
    by adjusting the pool to an adjacent power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">NEW_PG_NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.48"><span class="name">Nautilus 14.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.48">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1699: a path traversal flaw in Ceph Dashboard that could allow for
    potential information disclosure.
   </p></li><li class="listitem"><p>
    CVE-2020-1700: a flaw in the RGW beast front-end that could lead to denial
    of service from an unauthenticated client.
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.382+g8881d33957b.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.52"><span class="name">Nautilus 14.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.52">#</a></h2></div><p>
  This release fixed a Ceph Manager bug that caused MGRs becoming unresponsive on
  larger clusters. SES users were never exposed to the bug.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.54"><span class="name">Nautilus 14.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.54">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Health warnings are now issued if daemons have
    recently crashed.</strong></span> Ceph will now issue health warnings if
    daemons have recently crashed. Ceph has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash ls-new</pre></div><p>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive <em class="replaceable">CRASH-ID</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph crash archive-all</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong><code class="option">pg_num</code> must be a power of two,
    otherwise <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph
    will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Pool size needs to be greater than 1 otherwise
    <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph will issue a
    health warning if a RADOS pool’s size is set to 1 or if the pool is
    configured with no redundancy. Ceph will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> size <em class="replaceable">NUM-REPLICAS</em></pre></div><p>
    You can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</strong></span> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_ratio</code>,
    specifies a percentage of <code class="option">osd_heartbeat_grace</code> to determine
    the threshold. A value of zero disables the warning.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_time</code>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </p><p>
    A new command <code class="command">ceph daemon
    mgr.<em class="replaceable">MGR-NUMBER</em> dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </p><p>
    A new command <code class="command">ceph daemon osd.# dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Changes in the telemetry MGR module.</strong></span>
   </p><p>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <code class="literal">telemetry.ceph.com</code> in
    order to build and improve device failure prediction algorithms.
   </p><p>
    Telemetry reports information about CephFS file systems, including:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      How many MDS daemons (in total and per file system).
     </p></li><li class="listitem"><p>
      Which features are (or have been) enabled.
     </p></li><li class="listitem"><p>
      How many data pools.
     </p></li><li class="listitem"><p>
      Approximate file system age (year and the month of creation).
     </p></li><li class="listitem"><p>
      How many files, bytes, and snapshots.
     </p></li><li class="listitem"><p>
      How much metadata is being cached.
     </p></li></ul></div><p>
    Other miscellaneous information:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Which Ceph release the monitors are running.
     </p></li><li class="listitem"><p>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether RADOS cache tiering is enabled (and the mode).
     </p></li><li class="listitem"><p>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </p></li><li class="listitem"><p>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </p></li><li class="listitem"><p>
      Whether a separate OSD cluster network is being used.
     </p></li><li class="listitem"><p>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </p></li><li class="listitem"><p>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </p></li><li class="listitem"><p>
      Aggregate stats about the CRUSH Map, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </p></li></ul></div><p>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/channel_device false
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    You can view exactly what information will be reported first with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show        # see everything
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show device # just the device info
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show basic  # basic cluster info</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_recovery_reservations</code></strong></span>. It reveals the
    recovery locks held (<code class="option">in_progress</code>) and waiting in priority
    queues. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_recovery_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_scrub_reservations</code>. </strong></span> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_scrub_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports S3 Object Lock set of
    APIs.</strong></span> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports List Objects V2.</strong></span> RGW now
    supports List Objects V2 as specified at
    <a class="link" href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html</a>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.56"><span class="name">Nautilus 14.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.56">#</a></h2></div><p>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect SUSE Enterprise Storage customers
  because we did not ship a version based on 14.2.3.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.58"><span class="name">Nautilus 14.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.58">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a denial of service vulnerability where an unauthenticated client of
    Ceph Object Gateway could trigger a crash from an uncaught exception.
   </p></li><li class="listitem"><p>
    Nautilus-based librbd clients can now open images on Jewel clusters.
   </p></li><li class="listitem"><p>
    The Object Gateway <code class="option">num_rados_handles</code> has been removed. If you were
    using a value of <code class="option">num_rados_handles</code> greater than 1,
    multiply your current <code class="option">objecter_inflight_ops</code> and
    <code class="option">objecter_inflight_op_bytes</code> parameters by the old
    <code class="option">num_rados_handles</code> to get the same throttle behavior.
   </p></li><li class="listitem"><p>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </p></li><li class="listitem"><p>
    <code class="option">osd_deep_scrub_large_omap_object_key_threshold</code> has been
    lowered to detect an object with a large number of omap keys more easily.
   </p></li><li class="listitem"><p>
    The Ceph Dashboard now supports silencing Prometheus notifications.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.60"><span class="name">Nautilus 14.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.60">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="literal">no{up,down,in,out}</code> related commands have been
    revamped. There are now two ways to set the
    <code class="literal">no{up,down,in,out}</code> flags: the old command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set <em class="replaceable">FLAG</em></pre></div><p>
    which sets cluster-wide flags; and the new command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set-group <em class="replaceable">FLAGS</em> <em class="replaceable">WHO</em></pre></div><p>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </p></li><li class="listitem"><p>
    <code class="command">radosgw-admin</code> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of Object Gateway. Expire-stale objects are expired
    objects that should have been automatically erased but still exist and need
    to be listed and removed manually. One subcommand lists such objects and
    the other deletes them.
   </p></li><li class="listitem"><p>
    Earlier Nautilus releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new Nautilus BlueStore OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-Nautilus) breaks the pool
    utilization statistics reported by <code class="command">ceph df</code>. Until all
    OSDs have been reprovisioned or updated (via <code class="command">ceph-bluestore-tool
    repair</code>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <span class="emphasis"><em>all</em></span> OSDs are 14.2.2 or later, are Block Storage, and
    have been updated via the repair function if they were created prior to
    Nautilus.
   </p></li><li class="listitem"><p>
    The default value for <code class="option">mon_crush_min_required_version</code> has
    been changed from <code class="literal">firefly</code> to <code class="literal">hammer</code>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </p><p>
    If possible, we recommend that you set the oldest allowed client to
    <code class="literal">hammer</code> or later. To display what the current oldest
    allowed client is, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep min_compat_client</pre></div><p>
    If the current value is older than <code class="literal">hammer</code>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph features</pre></div><p>
    The newer <code class="literal">straw2</code> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <code class="literal">straw2</code> buckets to be used,
    including the <code class="literal">crush-compat</code> mode for the Balancer
    (<span class="intraxref">Book “Administration Guide”, Chapter 21 “Ceph Manager Modules”, Section 21.1 “Balancer”</span>).
   </p></li></ul></div><p>
  Find detailed information about the patch at
  <a class="link" href="https://download.suse.com/Download?buildid=D38A7mekBz4~" target="_blank">https://download.suse.com/Download?buildid=D38A7mekBz4~</a>
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.7.63"><span class="name">Nautilus 14.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.4.7.63">#</a></h2></div><p>
  This was the first point release following the original Nautilus release
  (14.2.0). The original ('General Availability' or 'GA') version of
  SUSE Enterprise Storage 6 was based on this point release.
 </p></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.4.8">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.4.8.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.4.8.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.2">#</a></span></dt><dd class="glossdef"><p>
     The node from which you run the <code class="command">ceph-deploy</code> utility to
     deploy Ceph on OSD nodes.
    </p></dd><dt id="id-1.4.8.3.3"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.3">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p><div id="id-1.4.8.3.3.2.2" data-id-title="Do Not Mix with S3 Buckets" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Mix with S3 Buckets</h6><p>
      S3 <span class="emphasis"><em>buckets</em></span> or <span class="emphasis"><em>containers</em></span>
      represent different terms meaning <span class="emphasis"><em>folders</em></span> for
      storing objects.
     </p></div></dd><dt id="id-1.4.8.3.4"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.4">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.4.8.3.5"><span><span class="glossterm">Monitor node, MON</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.5">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </p></dd><dt id="id-1.4.8.3.8"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.8">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.4.8.3.6"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.6">#</a></span></dt><dd class="glossdef"><p>
     Depending on context, <span class="emphasis"><em>Object Storage Device</em></span> or
     <span class="emphasis"><em>Object Storage Daemon</em></span>. The
     <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.4.8.3.7"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.7">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.4.8.3.9"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.9">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.4.8.3.10"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.10">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.4.8.3.12"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.12">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.4.8.3.11"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.4.8.3.11">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd></dl></div><div class="glossdiv" id="id-1.4.8.4" data-id-title="Ceph Specific Terms"><h3 class="title">Ceph Specific Terms</h3><dl><dt id="id-1.4.8.4.5"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.4.8.4.5">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies end user.
    </p></dd><dt id="id-1.4.8.4.2"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.8.4.2">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user’s data. Such a set
     consists of Ceph monitors and OSDs.
    </p><p>
     AKA <span class="quote">“<span class="quote">Ceph Object Store</span>”</span>.
    </p></dd><dt id="id-1.4.8.4.3"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.4.8.4.3">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.4.8.4.4"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.4.8.4.4">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd></dl></div><div class="glossdiv" id="id-1.4.8.5" data-id-title="Object Gateway Specific Terms"><h3 class="title">Object Gateway Specific Terms</h3><dl><dt id="id-1.4.8.5.3"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.4.8.5.3">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.4.8.5.2"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.4.8.5.2">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store.
    </p></dd></dl></div></section><section class="appendix" id="ap-deploy-docupdate" data-id-title="Documentation Updates"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">B </span><span class="title-name">Documentation Updates</span> <a title="Permalink" class="permalink" href="#ap-deploy-docupdate">#</a></h1></div></div></div><p>
  This chapter lists content changes for this document since the release of the
  latest maintenance update of SUSE Enterprise Storage 5. You can find changes related to
  the cluster deployment that apply to previous versions in
  <a class="link" href="https://documentation.suse.com/ses/5.5/html/ses-all/ap-deploy-docupdate.html" target="_blank">https://documentation.suse.com/ses/5.5/html/ses-all/ap-deploy-docupdate.html</a>.
 </p><p>
  The document was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-6mu" title="B.1. Maintenance update of SUSE Enterprise Storage 6 documentation">Section B.1, “Maintenance update of SUSE Enterprise Storage 6 documentation”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-depl-docupdates-6" title="B.2. June 2019 (Release of SUSE Enterprise Storage 6)">Section B.2, “June 2019 (Release of SUSE Enterprise Storage 6)”</a>
   </p></li></ul></div><section class="sect1" id="sec-depl-docupdates-6mu" data-id-title="Maintenance update of SUSE Enterprise Storage 6 documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">B.1 </span><span class="title-name">Maintenance update of SUSE Enterprise Storage 6 documentation</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-6mu">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Added a list of new features for Ceph 14.2.5 in the 'Ceph Maintenance
     Updates Based on Upstream 'Nautilus' Point Releases' appendix.
    </p></li><li class="listitem"><p>
     Suggested running <code class="command">rpmconfigcheck</code> to prevent losing
     local changes in <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>
     (<a class="link" href="https://jira.suse.com/browse/SES-348" target="_blank">https://jira.suse.com/browse/SES-348</a>).
    </p></li><li class="listitem"><p>
     Added <span class="intraxref">Book “Tuning Guide”, Chapter 8 “Improving Performance with LVM cache”</span>
     (<a class="link" href="https://jira.suse.com/browse/SES-269" target="_blank">https://jira.suse.com/browse/SES-269</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#cha-container-kubernetes" title="Chapter 13. SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster">Chapter 13, <em>SUSE Enterprise Storage 6 on Top of SUSE CaaS Platform 4 Kubernetes Cluster</em></a>
     (<a class="link" href="https://jira.suse.com/browse/SES-720" target="_blank">https://jira.suse.com/browse/SES-720</a>).
    </p></li><li class="listitem"><p>
     Added a tip on monitoring cluster nodes' status during upgrade in
     <a class="xref" href="#upgrade-adm" title="6.6. Upgrade the Admin Node">Section 6.6, “Upgrade the Admin Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1154568" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1154568</a>).
    </p></li><li class="listitem"><p>
     Made the network recommendations synchronized and more specific in
     <a class="xref" href="#ceph-install-ceph-deploy-network" title="2.1.1. Network Recommendations">Section 2.1.1, “Network Recommendations”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1156631" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1156631</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#upgrade-one-node-auto" title="6.5.2. Node Upgrade Using the SUSE Distribution Migration System">Section 6.5.2, “Node Upgrade Using the SUSE Distribution Migration System”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1154438" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1154438</a>).
    </p></li><li class="listitem"><p>
     Made the upgrade chapter sequential, <a class="xref" href="#cha-ceph-upgrade" title="Chapter 6. Upgrading from Previous Releases">Chapter 6, <em>Upgrading from Previous Releases</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1144709" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1144709</a>).
    </p></li><li class="listitem"><p>
     Added changelog entry for Ceph 14.2.4
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1151881" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1151881</a>).
    </p></li><li class="listitem"><p>
     Unified the pool name 'cephfs_metadata' in examples in
     <a class="xref" href="#cha-as-ganesha" title="Chapter 12. Installation of NFS Ganesha">Chapter 12, <em>Installation of NFS Ganesha</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1148548" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1148548</a>).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#ds-drive-groups-specs" title="5.5.2.1. Specification">Section 5.5.2.1, “Specification”</a> to include more realistic
     values
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1148216" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1148216</a>).
    </p></li><li class="listitem"><p>
     Added two new repositories for 'Module-Desktop' as our customers use
     mostly GUI in <a class="xref" href="#upgrade-one-node-manual" title="6.5.1. Manual Node Upgrade Using the Installer DVD">Section 6.5.1, “Manual Node Upgrade Using the Installer DVD”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1144897" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1144897</a>).
    </p></li><li class="listitem"><p>
     <span class="package">deepsea-cli</span> is not a dependency of
     <span class="package">deepsea</span> in <a class="xref" href="#deepsea-cli" title="5.4. DeepSea CLI">Section 5.4, “DeepSea CLI”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1143602" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1143602</a>).
    </p></li><li class="listitem"><p>
     Added a hint to migrate <code class="systemitem">ntpd</code> to
     <code class="systemitem">chronyd</code> in
     <a class="xref" href="#upgrade-ntp" title="6.2.9. Migrate from ntpd to chronyd">Section 6.2.9, “Migrate from <code class="systemitem">ntpd</code> to <code class="systemitem">chronyd</code>”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135185" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135185</a>).
    </p></li><li class="listitem"><p>
     Added <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.16 “Deactivating Tuned Profiles”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1130430" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1130430</a>).
    </p></li><li class="listitem"><p>
     Consider migrating whole OSD node in
     <a class="xref" href="#upgrade-osd-deployment" title="6.13.3. OSD Deployment">Section 6.13.3, “OSD Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138691" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138691</a>).
    </p></li><li class="listitem"><p>
     Added a point about migrating MDS names in
     <a class="xref" href="#before-upgrade-mds-names" title="6.2.6. Verify MDS Names">Section 6.2.6, “Verify MDS Names”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138804" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138804</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-depl-docupdates-6" data-id-title="June 2019 (Release of SUSE Enterprise Storage 6)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">B.2 </span><span class="title-name">June 2019 (Release of SUSE Enterprise Storage 6)</span> <a title="Permalink" class="permalink" href="#sec-depl-docupdates-6">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.4.9.7.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#ds-drive-groups" title="5.5.2. DriveGroups">Section 5.5.2, “DriveGroups”</a> (jsc#SES-548).
    </p></li><li class="listitem"><p>
     Rewrote <a class="xref" href="#cha-ceph-upgrade" title="Chapter 6. Upgrading from Previous Releases">Chapter 6, <em>Upgrading from Previous Releases</em></a> (jsc#SES-88).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-modify-ipv6" title="7.2.1. Enabling IPv6 for Ceph Cluster Deployment">Section 7.2.1, “Enabling IPv6 for Ceph Cluster Deployment”</a> (jsc#SES-409).
    </p></li><li class="listitem"><p>
     Made Block Storage the default storage back-end (Fate#325658).
    </p></li><li class="listitem"><p>
     Removed all references to external online documentation, replaced with the
     relevant content (Fate#320121).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.4.9.7.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added information about AppArmor during upgrade in
     <a class="xref" href="#before-upgrade-apparmor" title="6.2.5. Adjust AppArmor">Section 6.2.5, “Adjust AppArmor”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1137945" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1137945</a>).
    </p></li><li class="listitem"><p>
     Added a tip about orphaned packages in <a class="xref" href="#upgrade-one-node" title="6.5. Per-Node Upgrade Instructions">Section 6.5, “Per-Node Upgrade Instructions”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1136624" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1136624</a>).
    </p></li><li class="listitem"><p>
     Updated <code class="filename">profile-*</code> with
     <code class="filename">role-storage</code> in <a class="xref" href="#dev-env" title="Tip: Deploying Monitor Nodes without Defining OSD Profiles">Tip: Deploying Monitor Nodes without Defining OSD Profiles</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138181" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138181</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#upgrade-drive-groups" title="6.13. Migration from Profile-based Deployments to DriveGroups">Section 6.13, “Migration from Profile-based Deployments to DriveGroups”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135340" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135340</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#upgrade-drive-groups" title="6.13. Migration from Profile-based Deployments to DriveGroups">Section 6.13, “Migration from Profile-based Deployments to DriveGroups”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135340" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135340</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#upgrade-mds" title="6.8. Upgrade Metadata Servers">Section 6.8, “Upgrade Metadata Servers”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135064" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135064</a>).
    </p></li><li class="listitem"><p>
     MDS cluster needs to be shrunk in <a class="xref" href="#upgrade-mds" title="6.8. Upgrade Metadata Servers">Section 6.8, “Upgrade Metadata Servers”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1134826" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1134826</a>).
    </p></li><li class="listitem"><p>
     Changed configuration file to
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1129191" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1129191</a>).
    </p></li><li class="listitem"><p>
     Updated various parts of <span class="intraxref">Book “Administration Guide”, Chapter 29 “Exporting Ceph Data via Samba”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1101478" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1101478</a>).
    </p></li><li class="listitem"><p>
     <code class="filename">master_minion.sls</code> is gone in
     <a class="xref" href="#ceph-install-stack" title="5.3. Cluster Deployment">Section 5.3, “Cluster Deployment”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1090921" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1090921</a>).
    </p></li><li class="listitem"><p>
     Mentioned the <span class="package">deepsea-cli</span> package in
     <a class="xref" href="#deepsea-cli" title="5.4. DeepSea CLI">Section 5.4, “DeepSea CLI”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1087454" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1087454</a>).
    </p></li></ul></div></section></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/book_storage_deployment.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>