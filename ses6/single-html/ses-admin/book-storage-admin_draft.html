<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Administration Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Administration Guide | SES 6"/>
<meta name="description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Administration Guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Administration Guide | SES 6"/>
<meta property="og:description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration Guide | SES 6"/>
<meta name="twitter:description" content="SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the capabilities of the Ceph (http://ceph.com/) storage project w…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-admin">Administration Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-admin" data-id-title="Administration Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h1 class="title">Administration Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Alexandra</span> <span class="surname">Settle</span>, <span class="firstname">Liam</span> <span class="surname">Proven</span>, and <span class="firstname">Sven</span> <span class="surname">Seeberg</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>06/27/2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#id-1.3.2"><span class="title-name">About This Guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.2.7"><span class="title-name">Available Documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.8"><span class="title-name">Feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.9"><span class="title-name">Documentation Conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.10"><span class="title-name">About the Making of This Manual</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.11"><span class="title-name">Ceph Contributors</span></a></span></li></ul></li><li><span class="part"><a href="#part-cluster-managment"><span class="title-number">I </span><span class="title-name">Cluster Management</span></a></span><ul><li><span class="chapter"><a href="#id-1.3.3.2"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.3.2.4"><span class="title-number">1.1 </span><span class="title-name">Salt/DeepSea Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.3.3.2.5"><span class="title-number">1.2 </span><span class="title-name">Ceph Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.3.3.2.6"><span class="title-number">1.3 </span><span class="title-name">General Linux Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.3.3.2.7"><span class="title-number">1.4 </span><span class="title-name">Additional Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></span><ul><li><span class="sect1"><a href="#salt-adding-nodes"><span class="title-number">2.1 </span><span class="title-name">Adding New Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-adding-services"><span class="title-number">2.2 </span><span class="title-name">Adding New Roles to Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-node-removing"><span class="title-number">2.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#ds-mon"><span class="title-number">2.4 </span><span class="title-name">Redeploying Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="#salt-verify-encrypt-osd"><span class="title-number">2.5 </span><span class="title-name">Verify an Encrypted OSD</span></a></span></li><li><span class="sect1"><a href="#salt-node-add-disk"><span class="title-number">2.6 </span><span class="title-name">Adding an OSD Disk to a Node</span></a></span></li><li><span class="sect1"><a href="#salt-removing-osd"><span class="title-number">2.7 </span><span class="title-name">Removing an OSD</span></a></span></li><li><span class="sect1"><a href="#ds-osd-replace"><span class="title-number">2.8 </span><span class="title-name">Replacing an OSD Disk</span></a></span></li><li><span class="sect1"><a href="#ds-osd-recover"><span class="title-number">2.9 </span><span class="title-name">Recovering a Reinstalled OSD Node</span></a></span></li><li><span class="sect1"><a href="#moving-saltmaster"><span class="title-number">2.10 </span><span class="title-name">Moving the Admin Node to a New Server</span></a></span></li><li><span class="sect1"><a href="#salt-automated-installation"><span class="title-number">2.11 </span><span class="title-name">Automated Installation via Salt</span></a></span></li><li><span class="sect1"><a href="#deepsea-rolling-updates"><span class="title-number">2.12 </span><span class="title-name">Updating the Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#sec-salt-cluster-reboot"><span class="title-number">2.13 </span><span class="title-name">Halting or Rebooting Cluster</span></a></span></li><li><span class="sect1"><a href="#ds-custom-cephconf"><span class="title-number">2.14 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span></a></span></li><li><span class="sect1"><a href="#admin-apparmor"><span class="title-number">2.15 </span><span class="title-name">Enabling AppArmor Profiles</span></a></span></li><li><span class="sect1"><a href="#deactivate-tuned-profiles"><span class="title-number">2.16 </span><span class="title-name">Deactivating Tuned Profiles</span></a></span></li><li><span class="sect1"><a href="#deepsea-ceph-purge"><span class="title-number">2.17 </span><span class="title-name">Removing an Entire Ceph Cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></span><ul><li><span class="sect1"><a href="#backup-ceph"><span class="title-number">3.1 </span><span class="title-name">Back Up Ceph Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-deployment-backup-salt"><span class="title-number">3.2 </span><span class="title-name">Back Up Salt Configuration</span></a></span></li><li><span class="sect1"><a href="#sec-deployment-backup-deepsea"><span class="title-number">3.3 </span><span class="title-name">Back Up DeepSea Configuration</span></a></span></li><li><span class="sect1"><a href="#backup-config-files"><span class="title-number">3.4 </span><span class="title-name">Back Up Custom Configurations</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-dashboard"><span class="title-number">II </span><span class="title-name">Ceph Dashboard</span></a></span><ul><li><span class="chapter"><a href="#dashboard-about"><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></span></li><li><span class="chapter"><a href="#dashboard-webui-general"><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></span><ul><li><span class="sect1"><a href="#dashboard-webui-login"><span class="title-number">5.1 </span><span class="title-name">Log In</span></a></span></li><li><span class="sect1"><a href="#dashboard-util-menu"><span class="title-number">5.2 </span><span class="title-name">Utility Menu</span></a></span></li><li><span class="sect1"><a href="#dashboard-main-menu"><span class="title-number">5.3 </span><span class="title-name">Main Menu</span></a></span></li><li><span class="sect1"><a href="#dashboard-cpane"><span class="title-number">5.4 </span><span class="title-name">The Content Pane</span></a></span></li><li><span class="sect1"><a href="#dashboard-ui-common"><span class="title-number">5.5 </span><span class="title-name">Common Web UI Features</span></a></span></li><li><span class="sect1"><a href="#dashboard-widgets"><span class="title-number">5.6 </span><span class="title-name">Dashboard Widgets</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-user-mgmt"><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></span><ul><li><span class="sect1"><a href="#dashboard-listing-users"><span class="title-number">6.1 </span><span class="title-name">Listing Users</span></a></span></li><li><span class="sect1"><a href="#dashboard-adding-users"><span class="title-number">6.2 </span><span class="title-name">Adding New Users</span></a></span></li><li><span class="sect1"><a href="#dashboard-editing-users"><span class="title-number">6.3 </span><span class="title-name">Editing Users</span></a></span></li><li><span class="sect1"><a href="#dashboard-deleting-users"><span class="title-number">6.4 </span><span class="title-name">Deleting Users</span></a></span></li><li><span class="sect1"><a href="#dashboard-listing-user-roles"><span class="title-number">6.5 </span><span class="title-name">Listing User Roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-adding-roles"><span class="title-number">6.6 </span><span class="title-name">Adding Custom Roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-editing-roles"><span class="title-number">6.7 </span><span class="title-name">Editing Custom Roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-deleting-roles"><span class="title-number">6.8 </span><span class="title-name">Deleting Custom Roles</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-cluster"><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></span><ul><li><span class="sect1"><a href="#dashboard-cluster-hosts"><span class="title-number">7.1 </span><span class="title-name">Cluster Nodes</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-monitors"><span class="title-number">7.2 </span><span class="title-name">Ceph Monitors</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-osds"><span class="title-number">7.3 </span><span class="title-name">Ceph OSDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-config"><span class="title-number">7.4 </span><span class="title-name">Cluster Configuration</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-crushmap"><span class="title-number">7.5 </span><span class="title-name">CRUSH Map</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-mgr-plugins"><span class="title-number">7.6 </span><span class="title-name">Manager Modules</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-logs"><span class="title-number">7.7 </span><span class="title-name">Logs</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-pools"><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></span><ul><li><span class="sect1"><a href="#dashboard-pools-create"><span class="title-number">8.1 </span><span class="title-name">Adding a New Pool</span></a></span></li><li><span class="sect1"><a href="#dashboard-pools-delete"><span class="title-number">8.2 </span><span class="title-name">Deleting Pools</span></a></span></li><li><span class="sect1"><a href="#dashboard-pools-edit"><span class="title-number">8.3 </span><span class="title-name">Editing a Pool's Options</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-rbds"><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></span><ul><li><span class="sect1"><a href="#dashboard-rbds-details"><span class="title-number">9.1 </span><span class="title-name">Viewing Details about RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-configuration"><span class="title-number">9.2 </span><span class="title-name">Viewing RBD's Configuration</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-create"><span class="title-number">9.3 </span><span class="title-name">Creating RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbd-delete"><span class="title-number">9.4 </span><span class="title-name">Deleting RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-snapshots"><span class="title-number">9.5 </span><span class="title-name">RADOS Block Device Snapshots</span></a></span></li><li><span class="sect1"><a href="#dashboard-iscsi"><span class="title-number">9.6 </span><span class="title-name">Managing iSCSI Gateways</span></a></span></li><li><span class="sect1"><a href="#dash-rbd-qos"><span class="title-number">9.7 </span><span class="title-name">RBD Quality of Service (QoS)</span></a></span></li><li><span class="sect1"><a href="#dash-rbd-mirror"><span class="title-number">9.8 </span><span class="title-name">RBD Mirroring</span></a></span></li></ul></li><li><span class="chapter"><a href="#dash-webui-nfs"><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></span><ul><li><span class="sect1"><a href="#dash-webui-nfs-create"><span class="title-number">10.1 </span><span class="title-name">Adding NFS Exports</span></a></span></li><li><span class="sect1"><a href="#dash-webui-nfs-delete"><span class="title-number">10.2 </span><span class="title-name">Deleting NFS Exports</span></a></span></li><li><span class="sect1"><a href="#dash-webui-nfs-edit"><span class="title-number">10.3 </span><span class="title-name">Editing NFS Exports</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-mds"><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></span><ul><li><span class="sect1"><a href="#dashboard-mds-overview"><span class="title-number">11.1 </span><span class="title-name">Viewing CephFS Overview</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-ogw"><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></span><ul><li><span class="sect1"><a href="#dashboard-ogw-view"><span class="title-number">12.1 </span><span class="title-name">Viewing Object Gateways</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-user"><span class="title-number">12.2 </span><span class="title-name">Managing Object Gateway Users</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-bucket"><span class="title-number">12.3 </span><span class="title-name">Managing the Object Gateway Buckets</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-initial-configuration"><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></span><ul><li><span class="sect1"><a href="#dashboard-ssl"><span class="title-number">13.1 </span><span class="title-name">TLS/SSL Support</span></a></span></li><li><span class="sect1"><a href="#dashboard-hostname-port"><span class="title-number">13.2 </span><span class="title-name">Host Name and Port Number</span></a></span></li><li><span class="sect1"><a href="#dashboard-username-password"><span class="title-number">13.3 </span><span class="title-name">User Name and Password</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-enabling"><span class="title-number">13.4 </span><span class="title-name">Enabling the Object Gateway Management Front-end</span></a></span></li><li><span class="sect1"><a href="#dashboard-sso"><span class="title-number">13.5 </span><span class="title-name">Enable Single Sign-On</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-user-roles"><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></span><ul><li><span class="sect1"><a href="#dashboard-user-accounts"><span class="title-number">14.1 </span><span class="title-name">User Accounts</span></a></span></li><li><span class="sect1"><a href="#dashboard-permissions"><span class="title-number">14.2 </span><span class="title-name">User Roles and Permissions</span></a></span></li><li><span class="sect1"><a href="#dashboard-proxy"><span class="title-number">14.3 </span><span class="title-name">Reverse Proxies</span></a></span></li><li><span class="sect1"><a href="#dashboard-auditing"><span class="title-number">14.4 </span><span class="title-name">Auditing</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-operate"><span class="title-number">III </span><span class="title-name">Operating a Cluster</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">15 </span><span class="title-name">Introduction</span></a></span></li><li><span class="chapter"><a href="#ceph-operating-services"><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></span><ul><li><span class="sect1"><a href="#operate-services-systemd"><span class="title-number">16.1 </span><span class="title-name">Operating Ceph Cluster Related Services Using <code class="systemitem">systemd</code></span></a></span></li><li><span class="sect1"><a href="#Deepsea-restart"><span class="title-number">16.2 </span><span class="title-name">Restarting Ceph Services Using DeepSea</span></a></span></li><li><span class="sect1"><a href="#ceph-cluster-shutdown"><span class="title-number">16.3 </span><span class="title-name">Shutdown and Start of the Whole Ceph Cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></span><ul><li><span class="sect1"><a href="#monitor-status"><span class="title-number">17.1 </span><span class="title-name">Checking a Cluster's Status</span></a></span></li><li><span class="sect1"><a href="#monitor-health"><span class="title-number">17.2 </span><span class="title-name">Checking Cluster Health</span></a></span></li><li><span class="sect1"><a href="#monitor-watch"><span class="title-number">17.3 </span><span class="title-name">Watching a Cluster</span></a></span></li><li><span class="sect1"><a href="#monitor-stats"><span class="title-number">17.4 </span><span class="title-name">Checking a Cluster's Usage Stats</span></a></span></li><li><span class="sect1"><a href="#monitor-osdstatus"><span class="title-number">17.5 </span><span class="title-name">Checking OSD Status</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-fullosd"><span class="title-number">17.6 </span><span class="title-name">Checking for Full OSDs</span></a></span></li><li><span class="sect1"><a href="#monitor-monstatus"><span class="title-number">17.7 </span><span class="title-name">Checking Monitor Status</span></a></span></li><li><span class="sect1"><a href="#monitor-pgroupstatus"><span class="title-number">17.8 </span><span class="title-name">Checking Placement Group States</span></a></span></li><li><span class="sect1"><a href="#monitor-adminsocket"><span class="title-number">17.9 </span><span class="title-name">Using the Admin Socket</span></a></span></li><li><span class="sect1"><a href="#storage-capacity"><span class="title-number">17.10 </span><span class="title-name">Storage Capacity</span></a></span></li><li><span class="sect1"><a href="#op-mon-osd-pg"><span class="title-number">17.11 </span><span class="title-name">Monitoring OSDs and Placement Groups</span></a></span></li><li><span class="sect1"><a href="#op-osd-not-running"><span class="title-number">17.12 </span><span class="title-name">OSD Is Not Running</span></a></span></li></ul></li><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></span><ul><li><span class="sect1"><a href="#pillar-variables"><span class="title-number">18.1 </span><span class="title-name">Pillar Variables</span></a></span></li><li><span class="sect1"><a href="#grafana"><span class="title-number">18.2 </span><span class="title-name">Grafana</span></a></span></li><li><span class="sect1"><a href="#prometheus"><span class="title-number">18.3 </span><span class="title-name">Prometheus</span></a></span></li><li><span class="sect1"><a href="#alerting-alertmanager"><span class="title-number">18.4 </span><span class="title-name">Alertmanager</span></a></span></li><li><span class="sect1"><a href="#troubleshooting-alerts"><span class="title-number">18.5 </span><span class="title-name">Troubleshooting Alerts</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span><ul><li><span class="sect1"><a href="#storage-cephx-arch"><span class="title-number">19.1 </span><span class="title-name">Authentication Architecture</span></a></span></li><li><span class="sect1"><a href="#storage-cephx-keymgmt"><span class="title-number">19.2 </span><span class="title-name">Key Management</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></span><ul><li><span class="sect1"><a href="#datamgm-devices"><span class="title-number">20.1 </span><span class="title-name">Devices</span></a></span></li><li><span class="sect1"><a href="#datamgm-buckets"><span class="title-number">20.2 </span><span class="title-name">Buckets</span></a></span></li><li><span class="sect1"><a href="#datamgm-rules"><span class="title-number">20.3 </span><span class="title-name">Rule Sets</span></a></span></li><li><span class="sect1"><a href="#op-pgs"><span class="title-number">20.4 </span><span class="title-name">Placement Groups</span></a></span></li><li><span class="sect1"><a href="#op-crush"><span class="title-number">20.5 </span><span class="title-name">CRUSH Map Manipulation</span></a></span></li><li><span class="sect1"><a href="#scrubbing"><span class="title-number">20.6 </span><span class="title-name">Scrubbing</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-mgr-modules"><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></span><ul><li><span class="sect1"><a href="#mgr-modules-balancer"><span class="title-number">21.1 </span><span class="title-name">Balancer</span></a></span></li><li><span class="sect1"><a href="#mgr-modules-telemetry"><span class="title-number">21.2 </span><span class="title-name">Telemetry Module</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></span><ul><li><span class="sect1"><a href="#ceph-pools-associate"><span class="title-number">22.1 </span><span class="title-name">Associate Pools with an Application</span></a></span></li><li><span class="sect1"><a href="#ceph-pools-operate"><span class="title-number">22.2 </span><span class="title-name">Operating Pools</span></a></span></li><li><span class="sect1"><a href="#pools-migration"><span class="title-number">22.3 </span><span class="title-name">Pool Migration</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-pool"><span class="title-number">22.4 </span><span class="title-name">Pool Snapshots</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-pool-compression"><span class="title-number">22.5 </span><span class="title-name">Data Compression</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></span><ul><li><span class="sect1"><a href="#ceph-rbd-commands"><span class="title-number">23.1 </span><span class="title-name">Block Device Commands</span></a></span></li><li><span class="sect1"><a href="#storage-bp-integration-mount-rbd"><span class="title-number">23.2 </span><span class="title-name">Mounting and Unmounting</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-rbd"><span class="title-number">23.3 </span><span class="title-name">Snapshots</span></a></span></li><li><span class="sect1"><a href="#ceph-rbd-mirror"><span class="title-number">23.4 </span><span class="title-name">Mirroring</span></a></span></li><li><span class="sect1"><a href="#rbd-cache-settings"><span class="title-number">23.5 </span><span class="title-name">Cache Settings</span></a></span></li><li><span class="sect1"><a href="#rbd-qos"><span class="title-number">23.6 </span><span class="title-name">QoS Settings</span></a></span></li><li><span class="sect1"><a href="#rbd-readahead-settings"><span class="title-number">23.7 </span><span class="title-name">Read-ahead Settings</span></a></span></li><li><span class="sect1"><a href="#rbd-features"><span class="title-number">23.8 </span><span class="title-name">Advanced Features</span></a></span></li><li><span class="sect1"><a href="#rbd-old-clients-map"><span class="title-number">23.9 </span><span class="title-name">Mapping RBD Using Old Kernel Clients</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></span><ul><li><span class="sect1"><a href="#ec-prerequisite"><span class="title-number">24.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-default-profile"><span class="title-number">24.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-erasure-profiles"><span class="title-number">24.3 </span><span class="title-name">Erasure Code Profiles</span></a></span></li><li><span class="sect1"><a href="#ec-rbd"><span class="title-number">24.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></span><ul><li><span class="sect1"><a href="#ceph-config-runtime"><span class="title-number">25.1 </span><span class="title-name">Runtime Configuration</span></a></span></li><li><span class="sect1"><a href="#config-osd-and-bluestore"><span class="title-number">25.2 </span><span class="title-name">Ceph OSD and BlueStore</span></a></span></li><li><span class="sect1"><a href="#config-ogw"><span class="title-number">25.3 </span><span class="title-name">Ceph Object Gateway</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-dataccess"><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#sec-ceph-rgw-limits"><span class="title-number">26.1 </span><span class="title-name">Object Gateway Restrictions and Naming Limitations</span></a></span></li><li><span class="sect1"><a href="#ogw-deploy"><span class="title-number">26.2 </span><span class="title-name">Deploying the Object Gateway</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-operating"><span class="title-number">26.3 </span><span class="title-name">Operating the Object Gateway Service</span></a></span></li><li><span class="sect1"><a href="#ogw-config-parameters"><span class="title-number">26.4 </span><span class="title-name">Configuration Options</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-access"><span class="title-number">26.5 </span><span class="title-name">Managing Object Gateway Access</span></a></span></li><li><span class="sect1"><a href="#ogw-http-frontends"><span class="title-number">26.6 </span><span class="title-name">HTTP Front-ends</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-https"><span class="title-number">26.7 </span><span class="title-name">Enabling HTTPS/SSL for Object Gateways</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-sync"><span class="title-number">26.8 </span><span class="title-name">Synchronization Modules</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-ldap"><span class="title-number">26.9 </span><span class="title-name">LDAP Authentication</span></a></span></li><li><span class="sect1"><a href="#ogw-bucket-sharding"><span class="title-number">26.10 </span><span class="title-name">Bucket Index Sharding</span></a></span></li><li><span class="sect1"><a href="#ogw-keystone"><span class="title-number">26.11 </span><span class="title-name">Integrating OpenStack Keystone</span></a></span></li><li><span class="sect1"><a href="#ogw-storage-classes"><span class="title-number">26.12 </span><span class="title-name">Pool Placement and Storage Classes</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-fed"><span class="title-number">26.13 </span><span class="title-name">Multisite Object Gateways</span></a></span></li><li><span class="sect1"><a href="#ogw-haproxy"><span class="title-number">26.14 </span><span class="title-name">Load Balancing the Object Gateway Servers with HAProxy</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></span><ul><li><span class="sect1"><a href="#ceph-iscsi-connect"><span class="title-number">27.1 </span><span class="title-name">Connecting to <code class="systemitem">ceph-iscsi</code> Managed Targets</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-conclude"><span class="title-number">27.2 </span><span class="title-name">Conclusion</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></span><ul><li><span class="sect1"><a href="#ceph-cephfs-cephfs-mount"><span class="title-number">28.1 </span><span class="title-name">Mounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-unmount"><span class="title-number">28.2 </span><span class="title-name">Unmounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-fstab"><span class="title-number">28.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-activeactive"><span class="title-number">28.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-failover"><span class="title-number">28.5 </span><span class="title-name">Managing Failover</span></a></span></li><li><span class="sect1"><a href="#cephfs-quotas"><span class="title-number">28.6 </span><span class="title-name">Setting CephFS Quotas</span></a></span></li><li><span class="sect1"><a href="#cephfs-snapshots"><span class="title-number">28.7 </span><span class="title-name">Managing CephFS Snapshots</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></span><ul><li><span class="sect1"><a href="#cephfs-samba"><span class="title-number">29.1 </span><span class="title-name">Export CephFS via Samba Share</span></a></span></li><li><span class="sect1"><a href="#cephfs-ad"><span class="title-number">29.2 </span><span class="title-name">Samba Gateway Joining Active Directory</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></span><ul><li><span class="sect1"><a href="#ceph-nfsganesha-install"><span class="title-number">30.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-config"><span class="title-number">30.2 </span><span class="title-name">Configuration</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-customrole"><span class="title-number">30.3 </span><span class="title-name">Custom NFS Ganesha Roles</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-services"><span class="title-number">30.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-loglevel"><span class="title-number">30.5 </span><span class="title-name">Setting the Log Level</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-verify"><span class="title-number">30.6 </span><span class="title-name">Verifying the Exported NFS Share</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-mount"><span class="title-number">30.7 </span><span class="title-name">Mounting the Exported NFS Share</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-virt"><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></span><ul><li><span class="sect1"><a href="#ceph-libvirt-cfg-ceph"><span class="title-number">31.1 </span><span class="title-name">Configuring Ceph</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-virt-manager"><span class="title-number">31.2 </span><span class="title-name">Preparing the VM Manager</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-create-vm"><span class="title-number">31.3 </span><span class="title-name">Creating a VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-cfg-vm"><span class="title-number">31.4 </span><span class="title-name">Configuring the VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-summary"><span class="title-number">31.5 </span><span class="title-name">Summary</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></span><ul><li><span class="sect1"><a href="#ceph-kvm-install"><span class="title-number">32.1 </span><span class="title-name">Installation</span></a></span></li><li><span class="sect1"><a href="#ceph-kvm-usage"><span class="title-number">32.2 </span><span class="title-name">Usage</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.7"><span class="title-number">32.3 </span><span class="title-name">Creating Images with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.8"><span class="title-number">32.4 </span><span class="title-name">Resizing Images with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.9"><span class="title-number">32.5 </span><span class="title-name">Retrieving Image Info with QEMU</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.10"><span class="title-number">32.6 </span><span class="title-name">Running QEMU with RBD</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.11"><span class="title-number">32.7 </span><span class="title-name">Enabling Discard/TRIM</span></a></span></li><li><span class="sect1"><a href="#id-1.3.7.3.12"><span class="title-number">32.8 </span><span class="title-name">QEMU Cache Options</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-troubleshooting"><span class="title-number">VI </span><span class="title-name">FAQs, Tips and Troubleshooting</span></a></span><ul><li><span class="chapter"><a href="#storage-tips"><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></span><ul><li><span class="sect1"><a href="#tips-orphaned-partitions"><span class="title-number">33.1 </span><span class="title-name">Identifying Orphaned Partitions</span></a></span></li><li><span class="sect1"><a href="#tips-scrubbing"><span class="title-number">33.2 </span><span class="title-name">Adjusting Scrubbing</span></a></span></li><li><span class="sect1"><a href="#tips-stopping-osd-without-rebalancing"><span class="title-number">33.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span></a></span></li><li><span class="sect1"><a href="#Cluster-Time-Setting"><span class="title-number">33.4 </span><span class="title-name">Time Synchronization of Nodes</span></a></span></li><li><span class="sect1"><a href="#storage-bp-cluster-mntc-unbalanced"><span class="title-number">33.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span></a></span></li><li><span class="sect1"><a href="#storage-tips-ceph-btrfs-subvol"><span class="title-number">33.6 </span><span class="title-name">Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes</span></a></span></li><li><span class="sect1"><a href="#storage-bp-srv-maint-fds-inc"><span class="title-number">33.7 </span><span class="title-name">Increasing File Descriptors</span></a></span></li><li><span class="sect1"><a href="#storage-admin-integration"><span class="title-number">33.8 </span><span class="title-name">Integration with Virtualization Software</span></a></span></li><li><span class="sect1"><a href="#storage-bp-net-firewall"><span class="title-number">33.9 </span><span class="title-name">Firewall Settings for Ceph</span></a></span></li><li><span class="sect1"><a href="#storage-bp-network-test"><span class="title-number">33.10 </span><span class="title-name">Testing Network Performance</span></a></span></li><li><span class="sect1"><a href="#bp-flash-led-lights"><span class="title-number">33.11 </span><span class="title-name">How to Locate Physical Disks Using LED Lights</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-faqs"><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></span><ul><li><span class="sect1"><a href="#storage-bp-tuneups-pg-num"><span class="title-number">34.1 </span><span class="title-name">How Does the Number of Placement Groups Affect the Cluster Performance?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-tuneups-mix-ssd"><span class="title-number">34.2 </span><span class="title-name">Can I Use SSDs and Hard Disks on the Same Cluster?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-tuneups-ssd-tradeoffs"><span class="title-number">34.3 </span><span class="title-name">What are the Trade-offs of Using a Journal on SSD?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-diskfails"><span class="title-number">34.4 </span><span class="title-name">What Happens When a Disk Fails?</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-journalfails"><span class="title-number">34.5 </span><span class="title-name">What Happens When a Journal Disk Fails?</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-troubleshooting"><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></span><ul><li><span class="sect1"><a href="#storage-bp-report-bug"><span class="title-number">35.1 </span><span class="title-name">Reporting Software Problems</span></a></span></li><li><span class="sect1"><a href="#storage-bp-cluster-mntc-rados-striping"><span class="title-number">35.2 </span><span class="title-name">Sending Large Objects with <code class="command">rados</code> Fails with Full OSD</span></a></span></li><li><span class="sect1"><a href="#ceph-xfs-corruption"><span class="title-number">35.3 </span><span class="title-name">Corrupted XFS File system</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-toomanypgs"><span class="title-number">35.4 </span><span class="title-name">'Too Many PGs per OSD' Status Message</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-stuckinactive"><span class="title-number">35.5 </span><span class="title-name">'<span class="emphasis"><em>nn</em></span> pg stuck inactive' Status Message</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-osdweight"><span class="title-number">35.6 </span><span class="title-name">OSD Weight is 0</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-osddown"><span class="title-number">35.7 </span><span class="title-name">OSD is Down</span></a></span></li><li><span class="sect1"><a href="#storage-bp-performance-slowosd"><span class="title-number">35.8 </span><span class="title-name">Finding Slow OSDs</span></a></span></li><li><span class="sect1"><a href="#storage-bp-recover-clockskew"><span class="title-number">35.9 </span><span class="title-name">Fixing Clock Skew Warnings</span></a></span></li><li><span class="sect1"><a href="#storage-bp-performance-net-issues"><span class="title-number">35.10 </span><span class="title-name">Poor Cluster Performance Caused by Network Problems</span></a></span></li><li><span class="sect1"><a href="#trouble-jobcache"><span class="title-number">35.11 </span><span class="title-name"><code class="filename">/var</code> Running Out of Space</span></a></span></li><li><span class="sect1"><a href="#trouble-osd-panic"><span class="title-number">35.12 </span><span class="title-name">OSD Panic Occurs when Media Error Happens during FileStore Directory Split</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#app-stage1-custom"><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span></a></span></li><li><span class="appendix"><a href="#id-1.3.10"><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></span></li><li><span class="glossary"><a href="#id-1.3.11"><span class="title-name">Glossary</span></a></span></li><li><span class="appendix"><a href="#ap-adm-docupdate"><span class="title-number">C </span><span class="title-name">Documentation Updates</span></a></span><ul><li><span class="sect1"><a href="#sec-adm-docupdates-6mu"><span class="title-number">C.1 </span><span class="title-name">Maintenance update of SUSE Enterprise Storage 6 documentation</span></a></span></li><li><span class="sect1"><a href="#sec-adm-docupdates-6"><span class="title-number">C.2 </span><span class="title-name">June 2019 (Release of SUSE Enterprise Storage 6)</span></a></span></li></ul></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.3.4.3.3.4"><span class="number">5.1 </span><span class="name">Ceph Dashboard Login Screen</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.3.3.9"><span class="number">5.2 </span><span class="name">Ceph Dashboard Home Page</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.3.8.4.3"><span class="number">5.3 </span><span class="name">Status Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.3.8.5.3"><span class="number">5.4 </span><span class="name">performance Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.3.8.6.3"><span class="number">5.5 </span><span class="name">Capacity Widgets</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.5.4"><span class="number">6.1 </span><span class="name">User Management</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.6.3"><span class="number">6.2 </span><span class="name">Adding a User</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.9.4"><span class="number">6.3 </span><span class="name">User Roles</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.4.10.4"><span class="number">6.4 </span><span class="name">Adding a Role</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.4.3"><span class="number">7.1 </span><span class="name">Hosts</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.5.6"><span class="number">7.2 </span><span class="name">Ceph Monitors</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.6.3"><span class="number">7.3 </span><span class="name">Ceph OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.6.5"><span class="number">7.4 </span><span class="name">OSD Flags</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.6.7"><span class="number">7.5 </span><span class="name">OSD Recovery Priority</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.6.9"><span class="number">7.6 </span><span class="name">OSD Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.7.3"><span class="number">7.7 </span><span class="name">Cluster Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.8.4"><span class="number">7.8 </span><span class="name">CRUSH Map</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.9.3"><span class="number">7.9 </span><span class="name">Manager Modules</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.10.4"><span class="number">7.10 </span><span class="name">Logs</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.6.6"><span class="number">8.1 </span><span class="name">List of Pools</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.6.8.3"><span class="number">8.2 </span><span class="name">Adding a New Pool</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.5"><span class="number">9.1 </span><span class="name">List of RBD Images</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.6.3"><span class="number">9.2 </span><span class="name">RBD Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.7.3"><span class="number">9.3 </span><span class="name">RBD Configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.8.3"><span class="number">9.4 </span><span class="name">Adding a New RBD</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.10.4"><span class="number">9.5 </span><span class="name">RBD Snapshots</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.11.6"><span class="number">9.6 </span><span class="name">List of iSCSI Targets</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.11.8"><span class="number">9.7 </span><span class="name">iSCSI Target Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.11.9.3"><span class="number">9.8 </span><span class="name">Adding a New Target</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.6.3.5.2"><span class="number">9.9 </span><span class="name">Running <code class="systemitem">rbd-mirror</code> Daemons</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.7.3.2.2"><span class="number">9.10 </span><span class="name">Creating a Pool with RBD Application</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.7.3.3.2"><span class="number">9.11 </span><span class="name">Configuring the Replication Mode</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.7.3.4.2"><span class="number">9.12 </span><span class="name">Adding Peer Credentials</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.7.3.4.5"><span class="number">9.13 </span><span class="name">List of Replicated Pools</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.8.3.1.2"><span class="number">9.14 </span><span class="name">New RBD Image</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.8.3.2.2"><span class="number">9.15 </span><span class="name">New RBD Image Synchronized</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.13.8.3.2.3.3"><span class="number">9.16 </span><span class="name">RBD Images' Replication Status</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.6"><span class="number">10.1 </span><span class="name">List of NFS Exports</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.8"><span class="number">10.2 </span><span class="name">NFS Export Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.9.4"><span class="number">10.3 </span><span class="name">Adding a New NFS Export</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.8.11.4"><span class="number">10.4 </span><span class="name">Editing an NFS Export</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.9.4.4"><span class="number">11.1 </span><span class="name">CephFS Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.10.4.4"><span class="number">12.1 </span><span class="name">Gateway's Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.10.5.4"><span class="number">12.2 </span><span class="name">Gateway Users</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.10.5.5.3"><span class="number">12.3 </span><span class="name">Adding a New Gateway User</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.10.6.5.3"><span class="number">12.4 </span><span class="name">Gateway Bucket Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.4.14.6"><span class="number">17.1 </span><span class="name">Ceph Cluster</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.4.15.7.3"><span class="number">17.2 </span><span class="name">Peering Schema</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.4.15.8.18.1.2.2"><span class="number">17.3 </span><span class="name">Placement Groups Status</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.6.5.6"><span class="number">19.1 </span><span class="name">Basic <code class="systemitem">cephx</code> Authentication</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.6.5.8"><span class="number">19.2 </span><span class="name"><code class="systemitem">cephx</code> Authentication</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.6.5.10"><span class="number">19.3 </span><span class="name"><code class="systemitem">cephx</code> Authentication - MDS and OSD</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.7.12.7.3.3"><span class="number">20.1 </span><span class="name">OSDs with Mixed Device Classes</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-iterate-figure"><span class="number">20.2 </span><span class="name">Example Tree</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-mode-indep-figure"><span class="number">20.3 </span><span class="name">Node Replacement Methods</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.7.15.3.3"><span class="number">20.4 </span><span class="name">Placement Groups in a Pool</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.7.15.3.6"><span class="number">20.5 </span><span class="name">Placement Groups and OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.9.8.5.3.2.5"><span class="number">22.1 </span><span class="name">Pools before Migration</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.9.8.5.3.3.3"><span class="number">22.2 </span><span class="name">Cache Tier Setup</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.9.8.5.3.4.3"><span class="number">22.3 </span><span class="name">Data Flushing</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.9.8.5.3.5.4"><span class="number">22.4 </span><span class="name">Setting Overlay</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.9.8.5.3.6.3"><span class="number">22.5 </span><span class="name">Migration Complete</span></a></span></li><li><span class="figure"><a href="#id-1.3.5.10.5"><span class="number">23.1 </span><span class="name">RADOS Protocol</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.1.2"><span class="number">27.1 </span><span class="name">iSCSI Initiator Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.2.2"><span class="number">27.2 </span><span class="name">Discover Target Portal</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.3.2"><span class="number">27.3 </span><span class="name">Target Portals</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.4.2"><span class="number">27.4 </span><span class="name">Targets</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.6.2"><span class="number">27.5 </span><span class="name">iSCSI Target Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.3.7.2"><span class="number">27.6 </span><span class="name">Device Details</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.6.1.2"><span class="number">27.7 </span><span class="name">New Volume Wizard</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.6.2.2"><span class="number">27.8 </span><span class="name">Offline Disk Prompt</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.4.6.3.2"><span class="number">27.9 </span><span class="name">Confirm Volume Selections</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.2.2"><span class="number">27.10 </span><span class="name">iSCSI Initiator Properties</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.4.2"><span class="number">27.11 </span><span class="name">Add Target Server</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.5.2"><span class="number">27.12 </span><span class="name">Manage Multipath Devices</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.5.4"><span class="number">27.13 </span><span class="name">Paths Listing for Multipath</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.6.2"><span class="number">27.14 </span><span class="name">Add Storage Dialog</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.7.2"><span class="number">27.15 </span><span class="name">Custom Space Setting</span></a></span></li><li><span class="figure"><a href="#id-1.3.6.3.4.5.3.7.5"><span class="number">27.16 </span><span class="name">iSCSI Datastore Overview</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#id-1.3.6.5.6.8.6.3"><span class="number">29.1 </span><span class="name">Default Users and Group ID Blocks</span></a></span></li><li><span class="table"><a href="#id-1.3.6.5.6.8.6.6"><span class="number">29.2 </span><span class="name">ID Ranges</span></a></span></li><li><span class="table"><a href="#id-1.3.8.2.14.9"><span class="number">33.1 </span><span class="name">Third Party Storage Tools</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#ex-ds-rmnode"><span class="number">2.1 </span><span class="name">Removing a Salt minion from the Cluster</span></a></span></li><li><span class="example"><a href="#ex-ds-mignode"><span class="number">2.2 </span><span class="name">Migrating Nodes</span></a></span></li><li><span class="example"><a href="#ex-failed-node"><span class="number">2.3 </span><span class="name">Removal of a Failed Node</span></a></span></li><li><span class="example"><a href="#ex-failed-storage-node"><span class="number">2.4 </span><span class="name">Removal of a Failed Storage Node</span></a></span></li><li><span class="example"><a href="#id-1.3.5.4.15.10.4"><span class="number">17.1 </span><span class="name">Locating an Object</span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.7"><span class="number">18.1 </span><span class="name">Global Configuration</span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.8"><span class="number">18.2 </span><span class="name"><em class="replaceable">ROUTE</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.9"><span class="number">18.3 </span><span class="name"><em class="replaceable">INHIBIT_RULE</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.10"><span class="number">18.4 </span><span class="name"><em class="replaceable">HTTP_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.11"><span class="number">18.5 </span><span class="name"><em class="replaceable">RECEIVER</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.12"><span class="number">18.6 </span><span class="name"><em class="replaceable">EMAIL_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.13"><span class="number">18.7 </span><span class="name"><em class="replaceable">HIPCHAT_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.14"><span class="number">18.8 </span><span class="name"><em class="replaceable">PAGERDUTY_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.15"><span class="number">18.9 </span><span class="name"><em class="replaceable">PUSHOVER_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.16"><span class="number">18.10 </span><span class="name"><em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.17"><span class="number">18.11 </span><span class="name"><em class="replaceable">ACTION_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.18"><span class="number">18.12 </span><span class="name"><em class="replaceable">FIELD_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.19"><span class="number">18.13 </span><span class="name"><em class="replaceable">OPSGENIE_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.20"><span class="number">18.14 </span><span class="name"><em class="replaceable">VICTOROPS_CONFIG</em></span></a></span></li><li><span class="example"><a href="#alert-webhook"><span class="number">18.15 </span><span class="name"><em class="replaceable">WEBHOOK_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.3.22"><span class="number">18.16 </span><span class="name"><em class="replaceable">WECHAT_CONFIG</em></span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.4.10.1.2"><span class="number">18.17 </span><span class="name">Adding Custom Alerts to SUSE Enterprise Storage</span></a></span></li><li><span class="example"><a href="#id-1.3.5.5.8.5.3"><span class="number">18.18 </span><span class="name">SNMP Trap Configuration</span></a></span></li><li><span class="example"><a href="#id-1.3.5.7.12.7.7.4.1.2.6"><span class="number">20.1 </span><span class="name"><code class="command">crushtool --reclassify-root</code></span></a></span></li><li><span class="example"><a href="#id-1.3.5.7.12.7.7.4.3.2.2"><span class="number">20.2 </span><span class="name"><code class="command">crushtool --reclassify-bucket</code></span></a></span></li><li><span class="example"><a href="#id-1.3.5.12.6.6.2.3"><span class="number">25.1 </span><span class="name">Example Beast Configuration in <code class="filename">/etc/ceph/ceph.conf</code></span></a></span></li><li><span class="example"><a href="#id-1.3.5.12.6.6.3.3"><span class="number">25.2 </span><span class="name">Example Civetweb Configuration in <code class="filename">/etc/ceph/ceph.conf</code></span></a></span></li><li><span class="example"><a href="#id-1.3.6.2.11.6.5.3"><span class="number">26.1 </span><span class="name">Trivial Configuration</span></a></span></li><li><span class="example"><a href="#id-1.3.6.2.11.6.5.4"><span class="number">26.2 </span><span class="name">Non-trivial Configuration</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.3.1.6"><p>
  Copyright ©
2022

  SUSE LLC
 </p><p>
  Copyright © 2016, RedHat, Inc, and contributors.

 </p><p>
  The text of and illustrations in this document are licensed under a Creative
  Commons Attribution-Share Alike 4.0 International ("CC-BY-SA"). An
  explanation of CC-BY-SA is available at
  <a class="link" href="http://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">http://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
  In accordance with CC-BY-SA, if you distribute this document or an adaptation
  of it, you must provide the URL for the original version.
 </p><p>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International Corp.
  or its subsidiaries in the United States and/or other countries. All other
  trademarks are the property of their respective owners.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div><section class="preface" id="id-1.3.2" data-id-title="About This Guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About This Guide</span> <a title="Permalink" class="permalink" href="#id-1.3.2">#</a></h1></div></div></div><p>
  SUSE Enterprise Storage 6 is an extension to SUSE Linux Enterprise Server 15 SP1. It combines the
  capabilities of the Ceph (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>) storage
  project with the enterprise engineering and support of SUSE. SUSE Enterprise Storage
  6 provides IT organizations with the ability to deploy a
  distributed storage architecture that can support a number of use cases using
  commodity hardware platforms.
 </p><p>
  This guide helps you understand the concept of the SUSE Enterprise Storage
  6 with the main focus on managing and administrating the Ceph
  infrastructure. It also demonstrates how to use Ceph with other related
  solutions, such as OpenStack or KVM.
 </p><p>
  Many chapters in this manual contain links to additional documentation
  resources. These include additional documentation that is available on the
  system as well as documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p><section class="sect1" id="id-1.3.2.7" data-id-title="Available Documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available Documentation</span> <a title="Permalink" class="permalink" href="#id-1.3.2.7">#</a></h2></div></div></div><p>
  
  The following manuals are available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.7.4.1"><span class="term"><span class="intraxref">Book “Administration Guide”</span></span></dt><dd><p>
     The guide describes various administration tasks that are typically
     performed after the installation. The guide also introduces steps to
     integrate Ceph with virtualization solutions such as <code class="systemitem">libvirt</code>, Xen,
     or KVM, and ways to access objects stored in the cluster via iSCSI and
     RADOS gateways.
    </p></dd><dt id="id-1.3.2.7.4.2"><span class="term"><span class="intraxref">Book “Deployment Guide”</span></span></dt><dd><p>
     Guides you through the installation steps of the Ceph cluster and all
     services related to Ceph. The guide also illustrates a basic Ceph
     cluster structure and provides you with related terminology.
    </p></dd></dl></div><p>
  HTML versions of the product manuals can be found in the installed system
  under <code class="filename">/usr/share/doc/manual</code>. Find the latest
  documentation updates at <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>
  where you can download the manuals for your product in multiple formats.
 </p></section><section class="sect1" id="id-1.3.2.8" data-id-title="Feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Feedback</span> <a title="Permalink" class="permalink" href="#id-1.3.2.8">#</a></h2></div></div></div><p>
  Several feedback channels are available:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.8.4.1"><span class="term">Bugs and Enhancement Requests</span></dt><dd><p>
     For services and support options available for your product, refer to
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To report bugs for a product component, log in to the Novell Customer Center from
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a> and select <span class="guimenu">My Support</span> / <span class="guimenu">Service Request</span>.
    </p></dd><dt id="id-1.3.2.8.4.2"><span class="term">User Comments</span></dt><dd><p>
     We want to hear your comments and suggestions for this manual and the
     other documentation included with this product. If you have questions,
     suggestions, or corrections, contact doc-team@suse.com, or you can also
     click the <code class="literal">Report Documentation Bug</code> link beside each
     chapter or section heading.
    </p></dd><dt id="id-1.3.2.8.4.3"><span class="term">Mail</span></dt><dd><p>
     For feedback on the documentation of this product, you can also send a
     mail to <code class="literal">doc-team@suse.de</code>. Make sure to include the
     document title, the product version, and the publication date of the
     documentation. To report errors or suggest enhancements, provide a concise
     description of the problem and refer to the respective section number and
     page (or URL).
    </p></dd></dl></div></section><section class="sect1" id="id-1.3.2.9" data-id-title="Documentation Conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation Conventions</span> <a title="Permalink" class="permalink" href="#id-1.3.2.9">#</a></h2></div></div></div><p>
  The following typographical conventions are used in this manual:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">placeholder</em>: replace
    <em class="replaceable">placeholder</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: the environment variable PATH
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: users or groups
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: a key to press or a key combination; keys
    are shown in uppercase as on a keyboard
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> / <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Dancing Penguins</em></span> (Chapter
    <span class="emphasis"><em>Penguins</em></span>, ↑Another Manual): This is a reference
    to a chapter in another manual.
   </p></li></ul></div></section><section class="sect1" id="id-1.3.2.10" data-id-title="About the Making of This Manual"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">About the Making of This Manual</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10">#</a></h2></div></div></div><p>
  This book is written in GeekoDoc, a subset of DocBook (see
  <a class="link" href="http://www.docbook.org" target="_blank">http://www.docbook.org</a>). The XML source files were
  validated by <code class="command">xmllint</code>, processed by
  <code class="command">xsltproc</code>, and converted into XSL-FO using a customized
  version of Norman Walsh's stylesheets. The final PDF can be formatted through
  FOP from Apache or through XEP from RenderX. The authoring and publishing
  tools used to produce this manual are available in the package
  <code class="systemitem">daps</code>. The DocBook Authoring and
  Publishing Suite (DAPS) is developed as open source software. For more
  information, see <a class="link" href="http://daps.sf.net/" target="_blank">http://daps.sf.net/</a>.
 </p></section><section class="sect1" id="id-1.3.2.11" data-id-title="Ceph Contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph Contributors</span> <a title="Permalink" class="permalink" href="#id-1.3.2.11">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section></section><div class="part" id="part-cluster-managment" data-id-title="Cluster Management"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Cluster Management </span><a title="Permalink" class="permalink" href="#part-cluster-managment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#id-1.3.3.2"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></span></li><dd class="toc-abstract"><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p></dd><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span></a></span></li><dd class="toc-abstract"><p>
  After you deploy a Ceph cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </p></dd><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span></a></span></li><dd class="toc-abstract"><p>
  This chapter explains which parts of the Ceph cluster you should back up in
  order to be able to restore its functionality.
 </p></dd></ul></div><section class="chapter" id="id-1.3.3.2" data-id-title="User Privileges and Command Prompts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect1" id="id-1.3.3.2.4" data-id-title="Salt/DeepSea Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Salt/DeepSea Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.4">#</a></h2></div></div></div><p>
   These commands help you to deploy or upgrade the Ceph cluster, run
   commands on several (or all) cluster nodes at the same time, or assist you
   when adding or removing cluster nodes. The most frequently used are
   <code class="command">salt</code>, <code class="command">salt-run</code>, and
   <code class="command">deepsea</code>. You need to run Salt commands on the Salt master
   node (refer to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2 “Introduction to DeepSea”</span> for details) as
   <code class="systemitem">root</code>. These commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div></section><section class="sect1" id="id-1.3.3.2.5" data-id-title="Ceph Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Ceph Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.5">#</a></h2></div></div></div><p>
   These are lower level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">rbd</code>,
   <code class="command">radosgw-admin</code>, or <code class="command">crushtool</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   Safer and recommended option is to create a more restrictive individual key
   for each administrator user and put it in a directory where the users can
   read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.3.3.2.5.6" data-id-title="Path to Ceph Keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph Keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephadm</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth list</pre></div><div id="id-1.3.3.2.5.11" data-id-title="Commands for Specific Nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for Specific Nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code></pre></div></div></section><section class="sect1" id="id-1.3.3.2.6" data-id-title="General Linux Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">General Linux Commands</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.6">#</a></h2></div></div></div><p>
   Linux commands not related to Ceph or DeepSea, such as
   <code class="command">mount</code>, <code class="command">cat</code>, or
   <code class="command">openssl</code>, are introduced either with the <code class="prompt user">cephadm@adm &gt; </code>
   or <code class="prompt user">root # </code> prompts, depending on which privileges the related command
   requires.
  </p></section><section class="sect1" id="id-1.3.3.2.7" data-id-title="Additional Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#id-1.3.3.2.7">#</a></h2></div></div></div><p>
   For more information on Ceph key management, refer to
   <a class="xref" href="#storage-cephx-keymgmt" title="19.2. Key Management">Section 19.2, “Key Management”</a>.
  </p></section></section><section class="chapter" id="storage-salt-cluster" data-id-title="Salt Cluster Administration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Salt Cluster Administration</span> <a title="Permalink" class="permalink" href="#storage-salt-cluster">#</a></h2></div></div></div><p>
  After you deploy a Ceph cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </p><section class="sect1" id="salt-adding-nodes" data-id-title="Adding New Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Adding New Cluster Nodes</span> <a title="Permalink" class="permalink" href="#salt-adding-nodes">#</a></h2></div></div></div><p>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”</span>:
  </p><div id="id-1.3.3.3.4.3" data-id-title="Prevent Rebalancing" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Prevent Rebalancing</h6><p>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, add all OSDs you intend to add at the same time.
   </p><p>
    An additional way is to set the <code class="option">osd crush initial weight =
    0</code> option in the <code class="filename">ceph.conf</code> file before adding
    the OSDs:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Add <code class="option">osd crush initial weight = 0</code> to
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>.
     </p></li><li class="step"><p>
      Create the new configuration on the Salt master node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">SALT_MASTER_NODE</em>' state.apply ceph.configuration.create</pre></div><p>
      Or:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call state.apply ceph.configuration.create</pre></div></li><li class="step"><p>
      Apply the new configuration to the targeted OSD minions:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">OSD_MINIONS</em>' state.apply ceph.configuration</pre></div><div id="id-1.3.3.3.4.3.4.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If this is <span class="emphasis"><em>not</em></span> a new node, but you want to proceed
       as if it were, ensure you remove the
       <code class="filename">/etc/ceph/destroyedOSDs.yml</code> file from the node.
       Otherwise, any devices from the first attempt will be restored with
       their previous OSD ID and reweight.
      </p><p>
       Run the following commands:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt 'node*' state.apply ceph.osd</pre></div></div></li><li class="step"><p>
      After the new OSDs are added, adjust their weights as required with the
      <code class="command">ceph osd crush reweight</code> command in small increments.
      This allows the cluster to rebalance and become healthy between
      increasing increments so it does not overwhelm the cluster and clients
      accessing the cluster.
     </p></li></ol></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install SUSE Linux Enterprise Server 15 SP1 on the new node and configure its network setting so that
     it resolves the Salt master host name correctly. Verify that it has a proper
     connection to both public and cluster networks, and that time
     synchronization is correctly configured. Then install the
     <code class="systemitem">salt-minion</code> package:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div><p>
     If the Salt master's host name is different from <code class="literal">salt</code>,
     edit <code class="filename">/etc/salt/minion</code> and add the following:
    </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">DNS_name_of_your_salt_master</em></pre></div><p>
     If you performed any changes to the configuration files mentioned above,
     restart the <code class="systemitem">salt.minion</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
     On the Salt master, accept the salt key of the new node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept <em class="replaceable">NEW_NODE_KEY</em></pre></div></li><li class="step"><p>
     Verify that <code class="filename">/srv/pillar/ceph/deepsea_minions.sls</code>
     targets the new Salt minion and/or set the proper DeepSea grain. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2.2.1 “Matching the Minion Name”</span> or
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”, Running Deployment Stages</span> for more details.
    </p></li><li class="step"><p>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information DeepSea expects.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div><div id="id-1.3.3.3.4.4.4.3" data-id-title="Possible Restart of DeepSea stage 0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Possible Restart of DeepSea stage 0</h6><p>
      If the Salt master rebooted after its kernel update, you need to restart
      DeepSea stage 0.
     </p></div></li><li class="step"><p>
     Run the discovery stage. It will write new file entries in the
     <code class="filename">/srv/pillar/ceph/proposals</code> directory, where you can
     edit relevant .yml files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1</pre></div></li><li class="step"><p>
     Optionally, change
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="step"><p>
     Run the configuration stage. It reads everything under
     <code class="filename">/srv/pillar/ceph</code> and updates the pillar accordingly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div><p>
     Pillar stores data which you can access with the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> pillar.items</pre></div><div id="id-1.3.3.3.4.4.7.5" data-id-title="Modifying OSDs Layout" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Modifying OSD's Layout</h6><p>
      If you want to modify the default OSD's layout and change the drive
      groups configuration, follow the procedure described in
      <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span>.
     </p></div></li><li class="step"><p>
     The configuration and deployment stages include newly added nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div></section><section class="sect1" id="salt-adding-services" data-id-title="Adding New Roles to Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Adding New Roles to Nodes</span> <a title="Permalink" class="permalink" href="#salt-adding-services">#</a></h2></div></div></div><p>
   You can deploy all types of supported roles with DeepSea. See
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1.2 “Role Assignment”</span> for more information on supported
   role types and examples of matching them.
  </p><p>
   To add a new service to an existing node, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Adapt <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> to match
     the existing host with a new role. For more details, refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>. For example, if you need to run an
     Object Gateway on a MON node, the line is similar to:
    </p><div class="verbatim-wrap"><pre class="screen">role-rgw/xx/x/example.mon-1.sls</pre></div></li><li class="step"><p>
     Run stage 2 to update the pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2</pre></div></li><li class="step"><p>
     Run stage 3 to deploy core services, or stage 4 to deploy optional
     services. Running both stages does not hurt.
    </p></li></ol></div></div></section><section class="sect1" id="salt-node-removing" data-id-title="Removing and Reinstalling Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Removing and Reinstalling Cluster Nodes</span> <a title="Permalink" class="permalink" href="#salt-node-removing">#</a></h2></div></div></div><div id="id-1.3.3.3.6.2" data-id-title="Removing a Cluster Node Temporarily" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a Cluster Node Temporarily</h6><p>
    The Salt master expects all minions to be present in the cluster and
    responsive. If a minion breaks and is not responsive anymore, it causes
    problems to the Salt infrastructure, mainly to DeepSea and Ceph Dashboard.
   </p><p>
    Before you fix the minion, delete its key from the Salt master temporarily:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">MINION_HOST_NAME</em></pre></div><p>
    After the minion is fixed, add its key to the Salt master again:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -a <em class="replaceable">MINION_HOST_NAME</em></pre></div></div><p>
   To remove a role from a cluster, edit
   <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> and remove the
   corresponding line(s). Then run stages 2 and 5 as described in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
  </p><div id="id-1.3.3.3.6.4" data-id-title="Removing OSDs from Cluster" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Removing OSDs from Cluster</h6><p>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </p><p>
    Before running stage 5 to do the actual removal, always check which OSDs
    are going to be removed by DeepSea:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run rescinded.ids</pre></div></div><p>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </p><p>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </p><div id="id-1.3.3.3.6.7" data-id-title="Preserving Partitions Created by Other Methods" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Preserving Partitions Created by Other Methods</h6><p>
    Disk drives previously configured by other methods, such as
    <code class="command">ceph-deploy</code>, may still contain partitions. DeepSea
    will not automatically destroy these. The administrator must reclaim these
    drives manually.
   </p></div><div class="complex-example"><div class="example" id="ex-ds-rmnode" data-id-title="Removing a Salt minion from the Cluster"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.1: </span><span class="title-name">Removing a Salt minion from the Cluster </span><a title="Permalink" class="permalink" href="#ex-ds-rmnode">#</a></h6></div><div class="example-contents"><p>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <code class="filename">policy.cfg</code> are similar to the following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
role-storage/cluster/data*.sls
[...]</pre></div><p>
    Then to remove the Salt minion 'data2.ceph', change the lines to the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">[...]
# Hardware Profile
role-storage/cluster/data[1,3-6]*.sls
[...]</pre></div><p>
    Also keep in mind to adapt your drive_groups.yml file to match the new
    targets.
   </p><div class="verbatim-wrap"><pre class="screen">    [...]
    drive_group_name:
      target: 'data[1,3-6]*'
    [...]</pre></div><p>
    Then run stage 2, check which OSDs are going to be removed, and finish by
    running stage 5:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-ds-mignode" data-id-title="Migrating Nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.2: </span><span class="title-name">Migrating Nodes </span><a title="Permalink" class="permalink" href="#ex-ds-mignode">#</a></h6></div><div class="example-contents"><p>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    Object Gateway while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </p><p>
    After running stages 0 and 1 (see <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”, Running Deployment Stages</span>) for the
    new hardware, you named the new gateway <code class="literal">rgw1</code>. If the
    node <code class="literal">data8</code> needs the Object Gateway role removed and the storage
    role added, and the current <code class="filename">policy.cfg</code> looks like
    this:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
role-storage/cluster/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</pre></div><p>
    Then change it to:
   </p><div class="verbatim-wrap"><pre class="screen"># Hardware Profile
role-storage/cluster/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</pre></div><p>
    Run stages 2 to 4, check which OSDs are going to be possibly removed, and
    finish by running stage 5. Stage 3 will add <code class="literal">data8</code> as a
    storage node. For a moment, <code class="literal">data8</code> will have both roles.
    Stage 4 will add the Object Gateway role to <code class="literal">rgw1</code> and stage 5 will
    remove the Object Gateway role from <code class="literal">data8</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run rescinded.ids
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5</pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-failed-node" data-id-title="Removal of a Failed Node"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.3: </span><span class="title-name">Removal of a Failed Node </span><a title="Permalink" class="permalink" href="#ex-failed-node">#</a></h6></div><div class="example-contents"><p>
    If the Salt minion is not responding and the administrator is unable to
    resolve the issue, we recommend removing the Salt key:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">MINION_ID</em></pre></div></div></div></div><div class="complex-example"><div class="example" id="ex-failed-storage-node" data-id-title="Removal of a Failed Storage Node"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 2.4: </span><span class="title-name">Removal of a Failed Storage Node </span><a title="Permalink" class="permalink" href="#ex-failed-storage-node">#</a></h6></div><div class="example-contents"><p>
    When a server fails (due to network, power, or other issues), it means that
    all the OSDs are dead. Issue the following commands for
    <span class="emphasis"><em>each</em></span> OSD on the failed storage node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd purge-actual $id --yes-i-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph auth del osd.$id</pre></div><p>
    Running the <code class="command">ceph osd purge-actual</code> command is equivalent
    to the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph destroy $id
<code class="prompt user">cephadm@adm &gt; </code>ceph osd rm $id
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush remove osd.$id</pre></div></div></div></div></section><section class="sect1" id="ds-mon" data-id-title="Redeploying Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Redeploying Monitor Nodes</span> <a title="Permalink" class="permalink" href="#ds-mon">#</a></h2></div></div></div><p>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </p><div id="id-1.3.3.3.7.3" data-id-title="The Minimum Is Three Monitor Nodes" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: The Minimum Is Three Monitor Nodes</h6><p>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only two monitor nodes, you need to
    temporarily assign the monitor role to other cluster nodes before you
    redeploy the failed monitor nodes. After you redeploy the failed monitor
    nodes, you can uninstall the temporary monitor roles.
   </p><p>
    For more information on adding new nodes/roles to the Ceph cluster, see
    <a class="xref" href="#salt-adding-nodes" title="2.1. Adding New Cluster Nodes">Section 2.1, “Adding New Cluster Nodes”</a> and
    <a class="xref" href="#salt-adding-services" title="2.2. Adding New Roles to Nodes">Section 2.2, “Adding New Roles to Nodes”</a>.
   </p><p>
    For more information on removing cluster nodes, refer to
    <a class="xref" href="#salt-node-removing" title="2.3. Removing and Reinstalling Cluster Nodes">Section 2.3, “Removing and Reinstalling Cluster Nodes”</a>.
   </p></div><p>
   There are two basic degrees of a Ceph node failure:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The Salt minion host is broken either physically or on the OS level, and
     does not respond to the <code class="command">salt
     '<em class="replaceable">minion_name</em>' test.ping</code> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p></li><li class="listitem"><p>
     The monitor related services failed and refuse to recover, but the host
     responds to the <code class="command">salt '<em class="replaceable">minion_name</em>'
     test.ping</code> call. In such case, follow these steps:
    </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Edit <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code> on the
     Salt master, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes. For
     example:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]</pre></div></li><li class="step"><p>
     Run DeepSea stages 2 to 5 to apply the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.2
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.3
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.4
<code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.5</pre></div></li></ol></div></div></section><section class="sect1" id="salt-verify-encrypt-osd" data-id-title="Verify an Encrypted OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Verify an Encrypted OSD</span> <a title="Permalink" class="permalink" href="#salt-verify-encrypt-osd">#</a></h2></div></div></div><p>
   After using DeepSea to deploy an OSD, you may want to verify that the OSD
   is encrypted.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check the output of <code class="command">ceph-volume lvm list</code> (it should be
     run as root on the node where the OSDs in question are located):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-volume lvm list

  ====== osd.3 =======

    [block]       /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7

        block device              /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7
        block uuid                m5F10p-tUeo-6ZGP-UjxJ-X3cd-Ec5B-dNGXvG
        cephx lockbox secret
        cluster fsid              413d9116-e4f6-4211-a53b-89aa219f1cf2
        cluster name              ceph
        crush device class        None
        encrypted                 0
        osd fsid                  f8596bf7-000f-4186-9378-170b782359dc
        osd id                    3
        type                      block
        vdo                       0
        devices                   /dev/vdb

  ====== osd.7 =======

    [block]       /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987

        block device              /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987
        block uuid                1y3qcS-ZG01-Y7Z1-B3Kv-PLr6-jbm6-8B79g6
        cephx lockbox secret
        cluster fsid              413d9116-e4f6-4211-a53b-89aa219f1cf2
        cluster name              ceph
        crush device class        None
        encrypted                 0
        osd fsid                  0f9a8002-4c81-4f5f-93a6-255252cac2c4
        osd id                    7
        type                      block
        vdo                       0
        devices                   /dev/vdc</pre></div><p>
     Note the line that says <code class="literal">encrypted 0</code>. This means the OSD
     is not encrypted. The possible values are as follows:
    </p><div class="verbatim-wrap"><pre class="screen">  encrypted                 0  = not encrypted
  encrypted                 1  = encrypted</pre></div><p>
     If you get the following error, it means the node where you are running
     the command does not have any OSDs on it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-volume lvm list
No valid Ceph lvm devices found</pre></div><p>
     If you have deployed a cluster with an OSD for which <code class="command">ceph-volume
     lvm list</code> shows <code class="literal">encrypted 1</code>, the OSD is
     encrypted. If you are unsure, proceed to step two.
    </p></li><li class="step"><p>
     Ceph OSD encryption-at-rest relies on the Linux kernel's
     <code class="literal">dm-crypt</code> subsystem and the Linux Unified Key Setup
     ("LUKS"). When creating an encrypted OSD, <code class="command">ceph-volume</code>
     creates an encrypted logical volume and saves the corresponding
     <code class="literal">dm-crypt</code> secret key in the Ceph Monitor data store. When the
     OSD is to be started, <code class="command">ceph-volume</code> ensures the device is
     mounted, retrieves the <code class="literal">dm-crypt</code> secret key from the
     Ceph Monitor's, and decrypts the underlying device. This creates a new device,
     containing the unencrypted data, and this is the device the Ceph OSD
     daemon is started on.
    </p><p>
     The OSD does not know whether the underlying logical volume is encrypted
     or not, there is no <code class="command">ceph osd command</code> that returns this
     information. However, it is possible to query LUKS for it, as follows.
    </p><p>
     First, get the device of the OSD logical volume you are interested in.
     This can be obtained from the <code class="command">ceph-volume lvm list</code>
     output:
    </p><div class="verbatim-wrap"><pre class="screen">block device              /dev/ceph-d9f09cf7-a2a4-4ddc-b5ab-b1fa4096f713/osd-data-71f62502-4c85-4944-9860-312241d41bb7</pre></div><p>
     Then, dump the LUKS header from that device:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cryptsetup luksDump OSD_BLOCK_DEVICE</pre></div><p>
     if the OSD is <span class="emphasis"><em>not</em></span> encrypted, the output is as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">Device /dev/ceph-38914e8d-f512-44a7-bbee-3c20a684753d/osd-data-0f385f9e-ce5c-45b9-917d-7f8c08537987 is not a valid LUKS device.</pre></div><p>
     If the OSD <span class="emphasis"><em>is</em></span> encrypted, the output is as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cryptsetup luksDump /dev/ceph-1ce61157-81be-427d-83ad-7337f05d8514/osd-data-89230c92-3ace-4685-97ff-6fa059cef63a
  LUKS header information for /dev/ceph-1ce61157-81be-427d-83ad-7337f05d8514/osd-data-89230c92-3ace-4685-97ff-6fa059cef63a

  Version:        1
  Cipher name:    aes
  Cipher mode:    xts-plain64
  Hash spec:      sha256
  Payload offset: 4096
  MK bits:        256
  MK digest:      e9 41 85 f1 1b a3 54 e2 48 6a dc c2 50 26 a5 3b 79 b0 f2 2e
  MK salt:        4c 8c 9d 1f 72 1a 88 6c 06 88 04 72 81 7b e4 bb
                  b1 70 e1 c2 7c c5 3b 30 6d f7 c8 9c 7c ca 22 7d
  MK iterations:  118940
  UUID:           7675f03b-58e3-47f2-85fc-3bafcf1e589f

  Key Slot 0: ENABLED
          Iterations:             1906500
          Salt:                   8f 1f 7f f4 eb 30 5a 22 a5 b4 14 07 cc da dc 48
                                  b5 e9 87 ef 3b 9b 24 72 59 ea 1a 0a ec 61 e6 42
          Key material offset:    8
          AF stripes:             4000
  Key Slot 1: DISABLED
  Key Slot 2: DISABLED
  Key Slot 3: DISABLED
  Key Slot 4: DISABLED
  Key Slot 5: DISABLED
  Key Slot 6: DISABLED
  Key Slot 7: DISABLED</pre></div><p>
     Since decrypting the data on an encrypted OSD disk requires knowledge of
     the corresponding <code class="literal">dm-crypt</code> secret key, OSD encryption
     provides protection for cases when a disk drive that was used as an OSD is
     decommissioned, lost, or stolen.
    </p></li></ol></div></div></section><section class="sect1" id="salt-node-add-disk" data-id-title="Adding an OSD Disk to a Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Adding an OSD Disk to a Node</span> <a title="Permalink" class="permalink" href="#salt-node-add-disk">#</a></h2></div></div></div><p>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <span class="intraxref">Step 12</span> in
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span> for more details. Adapt
   <code class="filename">/srv/salt/ceph/configuration/files/drive_groups.yml</code>
   accordingly (refer to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span> for details). After
   saving the file, run DeepSea's stage 3:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">deepsea</code> stage run ceph.stage.3</pre></div></section><section class="sect1" id="salt-removing-osd" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="#salt-removing-osd">#</a></h2></div></div></div><p>
   You can remove a Ceph OSD from the cluster by running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_ID</em></pre></div><p>
   <em class="replaceable">OSD_ID</em> needs to be a number of the OSD without
   the <code class="literal">osd.</code> prefix. For example, from
   <code class="literal">osd.3</code> only use the digit <code class="literal">3</code>.
  </p><section class="sect2" id="osd-removal-multiple" data-id-title="Removing Multiple OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.1 </span><span class="title-name">Removing Multiple OSDs</span> <a title="Permalink" class="permalink" href="#osd-removal-multiple">#</a></h3></div></div></div><p>
    Use the same procedure as mentioned in <a class="xref" href="#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>
    but simply supply multiple OSD IDs:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove 2 6 11 15
Removing osd 2 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.2 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 6 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.6 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 11 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.11 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 15 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.15 is safe to destroy
Purging from the crushmap
Zapping the device


2:
True
6:
True
11:
True
15:
True</pre></div></section><section class="sect2" id="remove-all-osds-per-host" data-id-title="Removing All OSDs on a Host"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.2 </span><span class="title-name">Removing All OSDs on a Host</span> <a title="Permalink" class="permalink" href="#remove-all-osds-per-host">#</a></h3></div></div></div><p>
    To remove all OSDs on a specific host, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_HOST_NAME</em></pre></div></section><section class="sect2" id="osd-forced-removal" data-id-title="Removing Broken OSDs Forcefully"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.3 </span><span class="title-name">Removing Broken OSDs Forcefully</span> <a title="Permalink" class="permalink" href="#osd-forced-removal">#</a></h3></div></div></div><p>
    There are cases when removing an OSD gracefully (see
    <a class="xref" href="#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>) fails. This may happen, for example,
    if the OSD or its journal, WAL or DB are broken, when it suffers from
    hanging I/O operations, or when the OSD disk fails to unmount.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.remove <em class="replaceable">OSD_ID</em> force=True</pre></div><div id="id-1.3.3.3.10.7.4" data-id-title="Hanging Mounts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Hanging Mounts</h6><p>
     If a partition is still mounted on the disk being removed, the command
     will exit with the 'Unmount failed - check for processes on
     <em class="replaceable">DEVICE</em>' message. You can then list all
     processes that access the file system with the <code class="command">fuser -m
     <em class="replaceable">DEVICE</em></code>. If <code class="command">fuser</code>
     returns nothing, try manual <code class="command">unmount
     <em class="replaceable">DEVICE</em></code> and watch the output of
     <code class="command">dmesg</code> or <code class="command">journalctl</code> commands.
    </p></div></section><section class="sect2" id="validate-osd-lvm" data-id-title="Validating OSD LVM Metadata"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.7.4 </span><span class="title-name">Validating OSD LVM Metadata</span> <a title="Permalink" class="permalink" href="#validate-osd-lvm">#</a></h3></div></div></div><p>
    After removing an OSD using the <code class="command">salt-run osd.remove
    <em class="replaceable">ID</em></code> or through other ceph commands, LVM
    metadata may not be completely removed. This means that if you want to
    re-deploy a new OSD, old LVM metadata would be used.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      First, check if the OSD has been removed:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm list</pre></div><p>
      Even if one of the OSD's has been removed successfully, it can still be
      listed. For example, if you removed <code class="literal">osd.2</code>, the
      following would be the output:
     </p><div class="verbatim-wrap"><pre class="screen">  ====== osd.2 =======

  [block] /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380

  block device /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380
  block uuid kH9aNy-vnCT-ExmQ-cAsI-H7Gw-LupE-cvSJO9
  cephx lockbox secret
  cluster fsid 6b6bbac4-eb11-45cc-b325-637e3ff9fa0c
  cluster name ceph
  crush device class None
  encrypted 0
  osd fsid aac51485-131c-442b-a243-47c9186067db
  osd id 2
  type block
  vdo 0
  devices /dev/sda</pre></div><p>
      In this example, you can see that <code class="literal">osd.2</code> is still
      located in <code class="filename">/dev/sda</code>.
     </p></li><li class="step"><p>
      Validate the LVM metadata on the OSD node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume inventory</pre></div><p>
      The output from running <code class="command">ceph-volume inventory</code> marks
      the <code class="filename">/dev/sda</code> availablity as
      <code class="literal">False</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen">  Device Path Size rotates available Model name
  /dev/sda 40.00 GB True False QEMU HARDDISK
  /dev/sdb 40.00 GB True False QEMU HARDDISK
  /dev/sdc 40.00 GB True False QEMU HARDDISK
  /dev/sdd 40.00 GB True False QEMU HARDDISK
  /dev/sde 40.00 GB True False QEMU HARDDISK
  /dev/sdf 40.00 GB True False QEMU HARDDISK
  /dev/vda 25.00 GB True False</pre></div></li><li class="step"><p>
      Run the following command on the OSD node to remove the LVM metadata
      completely:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm zap --osd-id <em class="replaceable">ID</em> --destroy</pre></div></li><li class="step"><p>
      Run the <code class="command">inventory</code> command again to verify that the
      <code class="filename">/dev/sda</code> availability returns
      <code class="literal">True</code>. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume inventory
Device Path Size rotates available Model name
/dev/sda 40.00 GB True True QEMU HARDDISK
/dev/sdb 40.00 GB True False QEMU HARDDISK
/dev/sdc 40.00 GB True False QEMU HARDDISK
/dev/sdd 40.00 GB True False QEMU HARDDISK
/dev/sde 40.00 GB True False QEMU HARDDISK
/dev/sdf 40.00 GB True False QEMU HARDDISK
/dev/vda 25.00 GB True False</pre></div><p>
      LVM metadata has been removed. You can safely run the
      <code class="command">dd</code> command on the device.
     </p></li><li class="step"><p>
      The OSD can now be re-deployed without needing to reboot the OSD node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div></section></section><section class="sect1" id="ds-osd-replace" data-id-title="Replacing an OSD Disk"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Replacing an OSD Disk</span> <a title="Permalink" class="permalink" href="#ds-osd-replace">#</a></h2></div></div></div><p>
   There are several reasons why you may need to replace an OSD disk, for
   example:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The OSD disk failed or is soon going to fail based on SMART information,
     and can no longer be used to store data safely.
    </p></li><li class="listitem"><p>
     You need to upgrade the OSD disk, for example to increase its size.
    </p></li><li class="listitem"><p>
     You need to change the OSD disk layout.
    </p></li><li class="listitem"><p>
     You plan to move from a non-LVM to a LVM-based layout.
    </p></li></ul></div><p>
   The replacement procedure is the same for both cases. It is also valid for
   both default and customized CRUSH Maps.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Suppose that, for example, '5' is the ID of the OSD whose disk needs to be
     replaced. The following command marks it as
     <span class="bold"><strong>destroyed</strong></span> in the CRUSH Map but leaves
     its original ID:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run osd.replace 5</pre></div><div id="id-1.3.3.3.11.5.1.3" data-id-title="osd.replace and osd.remove" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">osd.replace</code> and <code class="command">osd.remove</code></h6><p>
      The Salt's <code class="command">osd.replace</code> and
      <code class="command">osd.remove</code> (see <a class="xref" href="#salt-removing-osd" title="2.7. Removing an OSD">Section 2.7, “Removing an OSD”</a>)
      commands are identical except that <code class="command">osd.replace</code> leaves
      the OSD as 'destroyed' in the CRUSH Map while
      <code class="command">osd.remove</code> removes all traces from the CRUSH Map.
     </p></div></li><li class="step"><p>
     Manually replace the failed/upgraded OSD drive.
    </p></li><li class="step"><p>
     If you want to modify the default OSD's layout and change the DriveGroups
     configuration, follow the procedure described in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.2 “DriveGroups”</span>.
    </p></li><li class="step"><p>
     Run the deployment stage 3 to deploy the replaced OSD disk:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><div id="id-1.3.3.3.11.6" data-id-title="Shared device failure" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Shared device failure</h6><p>
    If a shared device for DB/WAL fails, you need to perform the replacement
    procedure for all OSDs that share the failed device.
   </p></div></section><section class="sect1" id="ds-osd-recover" data-id-title="Recovering a Reinstalled OSD Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">Recovering a Reinstalled OSD Node</span> <a title="Permalink" class="permalink" href="#ds-osd-recover">#</a></h2></div></div></div><p>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Reinstall the base SUSE Linux Enterprise operating system on the node where the OS broke.
     Install the <span class="package">salt-minion</span> packages on the OSD node,
     delete the old Salt minion key on the Salt master, and register the new
     Salt minion's key with the Salt master. For more information on the initial
     deployment, see <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p></li><li class="step"><p>
     Instead of running the whole of stage 0, run the following parts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.sync
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.packages.common
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.mines
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.updates</pre></div></li><li class="step"><p>
     Copy the <code class="filename">ceph.conf</code> to the OSD node, and then activate
     the OSD:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' state.apply ceph.configuration
<code class="prompt user">root@master # </code>salt '<em class="replaceable">osd_node</em>' cmd.run "ceph-volume lvm activate --all"</pre></div></li><li class="step"><p>
     Verify activation with one of the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph -s
# OR
<code class="prompt user">root@master # </code>ceph osd tree</pre></div></li><li class="step"><p>
     To ensure consistency across the cluster, run the DeepSea stages in the
     following order:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.1
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.5
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
     Run DeepSea stage 0:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </p></li><li class="step"><p>
     Get Prometheus' node exporter installed/running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">RECOVERED_MINION</em>' \
 state.apply ceph.monitoring.prometheus.exporters.node_exporter</pre></div></li><li class="step"><p>
     Remove unnecessary Salt grains (best after all OSDs have been migrated
     to LVM):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I roles:storage grains.delkey ceph</pre></div></li></ol></div></div></section><section class="sect1" id="moving-saltmaster" data-id-title="Moving the Admin Node to a New Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">Moving the Admin Node to a New Server</span> <a title="Permalink" class="permalink" href="#moving-saltmaster">#</a></h2></div></div></div><p>
   If you need to replace the Admin Node host with a new one, you need to move the
   Salt master and DeepSea files. Use your favorite synchronization tool for
   transferring the files. In this procedure, we use <code class="command">rsync</code>
   because it is a standard tool available in SUSE Linux Enterprise Server 15 SP1 software repositories.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop <code class="systemitem">salt-master</code> and
     <code class="systemitem">salt-minion</code> services on the old
     Admin Node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-master.service
<code class="prompt user">root@master # </code>systemctl stop salt-minion.service</pre></div></li><li class="step"><p>
     Configure Salt on the new Admin Node so that the Salt master and Salt minions
     communicate. Find more details in <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>.
    </p><div id="id-1.3.3.3.13.3.2.2" data-id-title="Transition of Salt Minions" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Transition of Salt Minions</h6><p>
      To ease the transition of Salt minions to the new Admin Node, remove the
      original Salt master's public key from each of them:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>rm /etc/salt/pki/minion/minion_master.pub
<code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></div></li><li class="step"><p>
     Verify that the <span class="package">deepsea</span> package is installed and
     install it if required.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install deepsea</pre></div></li><li class="step"><p>
     Customize the <code class="filename">policy.cfg</code> file by changing the
     <code class="literal">role-master</code> line. Find more details in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="step"><p>
     Synchronize <code class="filename">/srv/pillar</code> and
     <code class="filename">/srv/salt</code> directories from the old Admin Node to the new
     one.
    </p><div id="id-1.3.3.3.13.3.5.2" data-id-title="rsync Dry Run and Symbolic Links" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">rsync</code> Dry Run and Symbolic Links</h6><p>
      If possible, try synchronizing the files in a dry run first to see which
      files will be transferred (<code class="command">rsync</code>'s option
      <code class="option">-n</code>). Also, include symbolic links
      (<code class="command">rsync</code>'s option <code class="option">-a</code>). For
      <code class="command">rsync</code>, the synchronize command will look as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>rsync -avn /srv/pillar/ <em class="replaceable">NEW-ADMIN-HOSTNAME:</em>/srv/pillar</pre></div></div></li><li class="step"><p>
     If you made custom changes to files outside of
     <code class="filename">/srv/pillar</code> and <code class="filename">/srv/salt</code>, for
     example in <code class="filename">/etc/salt/master</code> or
     <code class="filename">/etc/salt/master.d</code>, synchronize them as well.
    </p></li><li class="step"><p>
     Now you can run DeepSea stages from the new Admin Node. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2 “Introduction to DeepSea”</span> for their detailed description.
    </p></li></ol></div></div></section><section class="sect1" id="salt-automated-installation" data-id-title="Automated Installation via Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Automated Installation via Salt</span> <a title="Permalink" class="permalink" href="#salt-automated-installation">#</a></h2></div></div></div><p>
   The installation can be automated by using the Salt reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a Ceph cluster with the specified behavior.
  </p><div id="id-1.3.3.3.14.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Salt cannot perform dependency checks based on reactor events. There is a
    real risk of putting your Salt master into a death spiral.
   </p></div><p>
   The automated installation requires the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A properly created
     <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>.
    </p></li><li class="listitem"><p>
     Prepared custom <code class="filename">global.yml</code> placed to the
     <code class="filename">/srv/pillar/ceph/stack</code> directory.
    </p></li></ul></div><p>
   The default reactor configuration will only run stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </p><p>
   When the first salt-minion starts, stage 0 will begin. A lock prevents
   multiple instances. When all minions complete stage 0, stage 1 will begin.
  </p><p>
   If the operation is performed properly, edit the file
  </p><div class="verbatim-wrap"><pre class="screen">/etc/salt/master.d/reactor.conf</pre></div><p>
   and replace the following line
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/discovery.sls</pre></div><p>
   with
  </p><div class="verbatim-wrap"><pre class="screen">- /srv/salt/ceph/reactor/all_stages.sls</pre></div><p>
   Verify that the line is not commented out.
  </p></section><section class="sect1" id="deepsea-rolling-updates" data-id-title="Updating the Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">Updating the Cluster Nodes</span> <a title="Permalink" class="permalink" href="#deepsea-rolling-updates">#</a></h2></div></div></div><p>
   Keep the Ceph cluster nodes up-to-date by applying rolling updates
   regularly.
  </p><section class="sect2" id="rolling-updates-repos" data-id-title="Software Repositories"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.1 </span><span class="title-name">Software Repositories</span> <a title="Permalink" class="permalink" href="#rolling-updates-repos">#</a></h3></div></div></div><p>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Upgrading from Previous Releases”, Section 6.5.1 “Manual Node Upgrade Using the Installer DVD”</span> for a complete list of the
    required repositories.
   </p></section><section class="sect2" id="rolling-upgrades-staging" data-id-title="Repository Staging"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.2 </span><span class="title-name">Repository Staging</span> <a title="Permalink" class="permalink" href="#rolling-upgrades-staging">#</a></h3></div></div></div><p>
    If you use a staging tool—for example, SUSE Manager, Subscription Management Tool, or
    Repository Mirroring Tool—that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for SUSE Linux Enterprise Server and SUSE Enterprise Storage are
    created at the same point in time.
   </p><p>
    We strongly recommend to use a staging tool to apply patches which have
    <code class="literal">frozen</code> or <code class="literal">staged</code> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </p></section><section class="sect2" id="rolling-updates-patch-or-dup" data-id-title="zypper patch or zypper dup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.3 </span><span class="title-name"><code class="command">zypper patch</code> or <code class="command">zypper dup</code></span> <a title="Permalink" class="permalink" href="#rolling-updates-patch-or-dup">#</a></h3></div></div></div><p>
    By default, cluster nodes are upgraded using the <code class="command">zypper
    dup</code> command. If you prefer to update the system using
    <code class="command">zypper patch</code> instead, edit
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
    following line:
   </p><div class="verbatim-wrap"><pre class="screen">update_method_init: zypper-patch</pre></div></section><section class="sect2" id="rolling-updates-reboots" data-id-title="Cluster Node Reboots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.4 </span><span class="title-name">Cluster Node Reboots</span> <a title="Permalink" class="permalink" href="#rolling-updates-reboots">#</a></h3></div></div></div><p>
    During the update, cluster nodes may be optionally rebooted if their kernel
    was upgraded by the update. If you want to eliminate the possibility of a
    forced reboot of potentially all nodes, either verify that the latest
    kernel is installed and running on Ceph nodes, or disable automatic node
    reboots as described in <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Customizing the Default Configuration”, Section 7.1.5 “Updates and Reboots during Stage 0”</span>.
   </p></section><section class="sect2" id="id-1.3.3.3.15.7" data-id-title="Downtime of Ceph Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.5 </span><span class="title-name">Downtime of Ceph Services</span> <a title="Permalink" class="permalink" href="#id-1.3.3.3.15.7">#</a></h3></div></div></div><p>
    Depending on the configuration, cluster nodes may be rebooted during the
    update as described in <a class="xref" href="#rolling-updates-reboots" title="2.12.4. Cluster Node Reboots">Section 2.12.4, “Cluster Node Reboots”</a>. If there
    is a single point of failure for services such as Object Gateway, Samba Gateway, NFS Ganesha,
    or iSCSI, the client machines may be temporarily disconnected from
    services whose nodes are being rebooted.
   </p></section><section class="sect2" id="rolling-updates-running" data-id-title="Running the Update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.12.6 </span><span class="title-name">Running the Update</span> <a title="Permalink" class="permalink" href="#rolling-updates-running">#</a></h3></div></div></div><p>
    To update the software packages on all cluster nodes to the latest version,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Update the <span class="package">deepsea</span>, <span class="package">salt-master</span>,
      and <span class="package">salt-minion</span> packages and restart relevant services
      on the Salt master:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'roles:master' state.apply ceph.updates.master</pre></div></li><li class="step"><p>
      Update and restart the <span class="package">salt-minion</span> package on all
      cluster nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'cluster:ceph' state.apply ceph.updates.salt</pre></div></li><li class="step"><p>
      Update all other software packages on the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.0</pre></div></li><li class="step"><p>
      Restart Ceph related services:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart</pre></div></li></ol></div></div></section></section><section class="sect1" id="sec-salt-cluster-reboot" data-id-title="Halting or Rebooting Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.13 </span><span class="title-name">Halting or Rebooting Cluster</span> <a title="Permalink" class="permalink" href="#sec-salt-cluster-reboot">#</a></h2></div></div></div><p>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Tell the Ceph cluster not to mark OSDs as out:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd set noout</pre></div></li><li class="step"><p>
     Stop daemons and nodes in the following order:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Storage clients
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph Monitor
      </p></li></ol></div></li><li class="step"><p>
     If required, perform maintenance tasks.
    </p></li><li class="step"><p>
     Start the nodes and servers in the reverse order of the shutdown process:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Ceph Monitor
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Storage clients
      </p></li></ol></div></li><li class="step"><p>
     Remove the noout flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd unset noout</pre></div></li></ol></div></div></section><section class="sect1" id="ds-custom-cephconf" data-id-title="Adjusting ceph.conf with Custom Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.14 </span><span class="title-name">Adjusting <code class="filename">ceph.conf</code> with Custom Settings</span> <a title="Permalink" class="permalink" href="#ds-custom-cephconf">#</a></h2></div></div></div><p>
   If you need to put custom settings into the <code class="filename">ceph.conf</code>
   file, you can do so by modifying the configuration files in the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d</code>
   directory:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     global.conf
    </p></li><li class="listitem"><p>
     mon.conf
    </p></li><li class="listitem"><p>
     mgr.conf
    </p></li><li class="listitem"><p>
     mds.conf
    </p></li><li class="listitem"><p>
     osd.conf
    </p></li><li class="listitem"><p>
     client.conf
    </p></li><li class="listitem"><p>
     rgw.conf
    </p></li></ul></div><div id="id-1.3.3.3.17.4" data-id-title="Unique rgw.conf" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unique <code class="filename">rgw.conf</code></h6><p>
    The Object Gateway offers a lot of flexibility and is unique compared to the other
    <code class="filename">ceph.conf</code> sections. All other Ceph components have
    static headers such as <code class="literal">[mon]</code> or
    <code class="literal">[osd]</code>. The Object Gateway has unique headers such as
    <code class="literal">[client.rgw.rgw1]</code>. This means that the
    <code class="filename">rgw.conf</code> file needs a header entry. For examples, see
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw.conf</code></pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="filename">/srv/salt/ceph/configuration/files/rgw-ssl.conf</code></pre></div><p>
    See <a class="xref" href="#ceph-rgw-https" title="26.7. Enabling HTTPS/SSL for Object Gateways">Section 26.7, “Enabling HTTPS/SSL for Object Gateways”</a> for more examples.
   </p></div><div id="id-1.3.3.3.17.5" data-id-title="Run stage 3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Run stage 3</h6><p>
    After you make custom changes to the above mentioned configuration files,
    run stages 3 and 4 to apply these changes to the cluster nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></div><p>
   These files are included from the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
   template file, and correspond to the different sections that the Ceph
   configuration file accepts. Putting a configuration snippet in the correct
   file enables DeepSea to place it into the correct section. You do not need
   to add any of the section headers.
  </p><div id="id-1.3.3.3.17.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <code class="literal">[osd.1]</code>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </p></div><section class="sect2" id="id-1.3.3.3.17.8" data-id-title="Overriding the Defaults"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.14.1 </span><span class="title-name">Overriding the Defaults</span> <a title="Permalink" class="permalink" href="#id-1.3.3.3.17.8">#</a></h3></div></div></div><p>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.j2</code>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</code>
    file:
   </p><div class="verbatim-wrap"><pre class="screen">auth cluster required = none
auth service required = none
auth client required = none</pre></div><p>
    When redefining the default values, Ceph related tools such as
    <code class="command">rados</code> may issue warnings that specific values from the
    <code class="filename">ceph.conf.j2</code> were redefined in
    <code class="filename">global.conf</code>. These warnings are caused by one
    parameter assigned twice in the resulting <code class="filename">ceph.conf</code>.
   </p><p>
    As a workaround for this specific case, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Change the current directory to
      <code class="filename">/srv/salt/ceph/configuration/create</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/create</pre></div></li><li class="step"><p>
      Copy <code class="filename">default.sls</code> to <code class="filename">custom.sls</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp default.sls custom.sls</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom.sls</code> and change
      <code class="option">ceph.conf.j2</code> to <code class="option">custom-ceph.conf.j2</code>.
     </p></li><li class="step"><p>
      Change current directory to
      <code class="filename">/srv/salt/ceph/configuration/files</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/configuration/files</pre></div></li><li class="step"><p>
      Copy <code class="filename">ceph.conf.j2</code> to
      <code class="filename">custom-ceph.conf.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp ceph.conf.j2 custom-ceph.conf.j2</pre></div></li><li class="step"><p>
      Edit <code class="filename">custom-ceph.conf.j2</code> and delete the following
      line:
     </p><div class="verbatim-wrap"><pre class="screen">{% include "ceph/configuration/files/rbd.conf" %}</pre></div><p>
      Edit <code class="filename">global.yml</code> and add the following line:
     </p><div class="verbatim-wrap"><pre class="screen">configuration_create: custom</pre></div></li><li class="step"><p>
      Refresh the pillar:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">target</em> saltutil.pillar_refresh</pre></div></li><li class="step"><p>
      Run stage 3:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></li></ol></div></div><p>
    Now you should have only one entry for each value definition. To re-create
    the configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.configuration.create</pre></div><p>
    and then verify the contents of
    <code class="filename">/srv/salt/ceph/configuration/cache/ceph.conf</code>.
   </p></section><section class="sect2" id="id-1.3.3.3.17.9" data-id-title="Including Configuration Files"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.14.2 </span><span class="title-name">Including Configuration Files</span> <a title="Permalink" class="permalink" href="#id-1.3.3.3.17.9">#</a></h3></div></div></div><p>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <code class="filename">osd.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</pre></div><p>
    In the previous example, the <code class="filename">osd1.conf</code>,
    <code class="filename">osd2.conf</code>, <code class="filename">osd3.conf</code>, and
    <code class="filename">osd4.conf</code> files contain the configuration options
    specific to the related OSD.
   </p><div id="id-1.3.3.3.17.9.5" data-id-title="Runtime Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Runtime Configuration</h6><p>
     Changes made to Ceph configuration files take effect after the related
     Ceph daemons restart. See <a class="xref" href="#ceph-config-runtime" title="25.1. Runtime Configuration">Section 25.1, “Runtime Configuration”</a> for more
     information on changing the Ceph runtime configuration.
    </p></div></section></section><section class="sect1" id="admin-apparmor" data-id-title="Enabling AppArmor Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.15 </span><span class="title-name">Enabling AppArmor Profiles</span> <a title="Permalink" class="permalink" href="#admin-apparmor">#</a></h2></div></div></div><p>
   AppArmor is a security solution that confines programs by a specific profile.
   For more details, refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-apparmor.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-apparmor.html</a>.
  </p><p>
   DeepSea provides three states for AppArmor profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular AppArmor state, run:
  </p><div class="verbatim-wrap"><pre class="screen">salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<em class="replaceable">STATE</em></pre></div><p>
   To put the AppArmor profiles in an 'enforce' state:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce</pre></div><p>
   To put the AppArmor profiles in a 'complain' state:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain</pre></div><p>
   To disable the AppArmor profiles:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable</pre></div><div id="id-1.3.3.3.18.11" data-id-title="Enabling the AppArmor Service" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Enabling the AppArmor Service</h6><p>
    Each of these three calls verifies if AppArmor is installed and installs it if
    not, and starts and enables the related <code class="systemitem">systemd</code> service. DeepSea will
    warn you if AppArmor was installed and started/enabled in another way and
    therefore runs without DeepSea profiles.
   </p></div></section><section class="sect1" id="deactivate-tuned-profiles" data-id-title="Deactivating Tuned Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.16 </span><span class="title-name">Deactivating Tuned Profiles</span> <a title="Permalink" class="permalink" href="#deactivate-tuned-profiles">#</a></h2></div></div></div><p>
   By default, DeepSea deploys Ceph clusters with tuned profiles active on
   Ceph Monitor, Ceph Manager, and Ceph OSD nodes. In some cases, you may need to permanently
   deactivate tuned profiles. To do so, put the following lines in
   <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and re-run stage 3:
  </p><div class="verbatim-wrap"><pre class="screen">alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3</pre></div></section><section class="sect1" id="deepsea-ceph-purge" data-id-title="Removing an Entire Ceph Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.17 </span><span class="title-name">Removing an Entire Ceph Cluster</span> <a title="Permalink" class="permalink" href="#deepsea-ceph-purge">#</a></h2></div></div></div><p>
   The <code class="command">ceph.purge</code> runner removes the entire Ceph cluster.
   This way you can clean the cluster environment when testing different
   setups. After the <code class="command">ceph.purge</code> completes, the Salt
   cluster is reverted back to the state at the end of DeepSea stage 1. You
   can then either change the <code class="filename">policy.cfg</code> (see
   <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>), or proceed to DeepSea stage 2
   with the same setup.
  </p><p>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the Ceph
   cluster by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety
<code class="prompt user">root@master # </code>salt-run state.orch ceph.purge</pre></div><div id="id-1.3.3.3.20.5" data-id-title="Disabling Ceph Cluster Removal" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling Ceph Cluster Removal</h6><p>
    If you want to prevent anyone from running the
    <code class="command">ceph.purge</code> runner, create a file named
    <code class="filename">disabled.sls</code> in the
    <code class="filename">/srv/salt/ceph/purge</code> directory and insert the
    following line in the
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">purge_init: disabled</pre></div></div><div id="id-1.3.3.3.20.6" data-id-title="Rescind Custom Roles" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Rescind Custom Roles</h6><p>
    If you previously created custom roles for Ceph Dashboard (refer to
    <a class="xref" href="#dashboard-adding-roles" title="6.6. Adding Custom Roles">Section 6.6, “Adding Custom Roles”</a> and
    <a class="xref" href="#dashboard-permissions" title="14.2. User Roles and Permissions">Section 14.2, “User Roles and Permissions”</a> for detailed information), you need
    to take manual steps to purge them before running the
    <code class="command">ceph.purge</code> runner. For example, if the custom role for
    Object Gateway is named 'us-east-1', then follow these steps:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cd /srv/salt/ceph/rescind
<code class="prompt user">root@master # </code>rsync -a rgw/ us-east-1
<code class="prompt user">root@master # </code>sed -i 's!rgw!us-east-1!' us-east-1/*.sls</pre></div></div></section></section><section class="chapter" id="cha-deployment-backup" data-id-title="Backing Up Cluster Configuration and Data"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Backing Up Cluster Configuration and Data</span> <a title="Permalink" class="permalink" href="#cha-deployment-backup">#</a></h2></div></div></div><p>
  This chapter explains which parts of the Ceph cluster you should back up in
  order to be able to restore its functionality.
 </p><section class="sect1" id="backup-ceph" data-id-title="Back Up Ceph Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Back Up Ceph Configuration</span> <a title="Permalink" class="permalink" href="#backup-ceph">#</a></h2></div></div></div><p>
   Back up the <code class="filename">/etc/ceph</code> directory. It contains crucial
   cluster configuration. You will need the backup of
   <code class="filename">/etc/ceph</code> for example when you need to replace the
   Admin Node.
  </p></section><section class="sect1" id="sec-deployment-backup-salt" data-id-title="Back Up Salt Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Back Up Salt Configuration</span> <a title="Permalink" class="permalink" href="#sec-deployment-backup-salt">#</a></h2></div></div></div><p>
   You need to back up the <code class="filename">/etc/salt/</code> directory. It
   contains the Salt configuration files, for example the Salt master key and
   accepted client keys.
  </p><p>
   The Salt files are not strictly required for backing up the Admin Node, but
   make redeploying the Salt cluster easier. If there is no backup of these
   files, the Salt minions need to be registered again at the new Admin Node.
  </p><div id="id-1.3.3.4.5.4" data-id-title="Security of the Salt Master Private Key" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Security of the Salt Master Private Key</h6><p>
    Make sure that the backup of the Salt master private key is stored in a safe
    location. The Salt master key can be used to manipulate all cluster nodes.
   </p></div><p>
   After restoring the <code class="filename">/etc/salt</code> directory from a backup,
   restart the Salt services:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">systemctl</code> restart salt-master
<code class="prompt user">root@master # </code><code class="command">systemctl</code> restart salt-minion</pre></div></section><section class="sect1" id="sec-deployment-backup-deepsea" data-id-title="Back Up DeepSea Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.3 </span><span class="title-name">Back Up DeepSea Configuration</span> <a title="Permalink" class="permalink" href="#sec-deployment-backup-deepsea">#</a></h2></div></div></div><p>
   All files required by DeepSea are stored in
   <code class="filename">/srv/pillar/</code>, <code class="filename">/srv/salt/</code> and
   <code class="filename">/etc/salt/master.d</code>.
  </p><p>
   If you need to redeploy the Admin Node, install the DeepSea package on the new
   node and move the backed up data back into the directories. DeepSea can
   then be used again without any further changes being required. Before using
   DeepSea again, make sure that all Salt minions are correctly registered
   on the Admin Node.
  </p></section><section class="sect1" id="backup-config-files" data-id-title="Back Up Custom Configurations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.4 </span><span class="title-name">Back Up Custom Configurations</span> <a title="Permalink" class="permalink" href="#backup-config-files">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Prometheus data and customization.
    </p></li><li class="listitem"><p>
     Grafana customization.
    </p></li><li class="listitem"><p>
     Verify that you have a record of existing openATTIC users so that you can
     create new accounts for these users in the Ceph Dashboard.
    </p></li><li class="listitem"><p>
     Manual changes to <code class="filename">ceph.conf</code> outside of DeepSea.
    </p></li><li class="listitem"><p>
     Manual changes to the iSCSI configuration outside of DeepSea.
    </p></li><li class="listitem"><p>
     Ceph keys.
    </p></li><li class="listitem"><p>
     CRUSH Map and CRUSH rules. Save the decompiled CRUSH Map including CRUSH
     rules into <code class="filename">crushmap-backup.txt</code> by running the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</pre></div></li><li class="listitem"><p>
     Samba Gateway configuration. If you are using a single gateway, backup
     <code class="filename">/etc/samba/smb.conf</code>. If you are using HA setup,
     backup also CTDB and Pacemaker configuration files. Refer to
     <a class="xref" href="#cha-ses-cifs" title="Chapter 29. Exporting Ceph Data via Samba">Chapter 29, <em>Exporting Ceph Data via Samba</em></a> for details on what configuration is used
     by Samba Gateways.
    </p></li><li class="listitem"><p>
     NFS Ganesha configuration. Only needed when using HA setup. Refer to
     <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 30. NFS Ganesha: Export Ceph Data via NFS">Chapter 30, <em>NFS Ganesha: Export Ceph Data via NFS</em></a> for details on what configuration is
     used by NFS Ganesha.
    </p></li></ul></div></section></section></div><div class="part" id="part-dashboard" data-id-title="Ceph Dashboard"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Ceph Dashboard </span><a title="Permalink" class="permalink" href="#part-dashboard">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#dashboard-about"><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span></a></span></li><dd class="toc-abstract"><p>The Ceph Dashboard is a module that adds a built-in Web based monitoring and administration application to the Ceph Manager (refer to Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.2.3 “Ceph Nodes and Daemons” for more details on Ceph Manager). You no longer need …</p></dd><li><span class="chapter"><a href="#dashboard-webui-general"><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span></a></span></li><dd class="toc-abstract"><p>
   To log in to the dashboard Web application, point your browser to its URL
   including the port number. You can find its address by running
  </p></dd><li><span class="chapter"><a href="#dashboard-user-mgmt"><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span></a></span></li><dd class="toc-abstract"><p>
  Dashboard user management performed by Ceph commands on the command line
  was already introduced in <a class="xref" href="#dashboard-user-roles" title="Chapter 14. Managing Users and Roles on the Command Line">Chapter 14, <em>Managing Users and Roles on the Command Line</em></a>.
 </p></dd><li><span class="chapter"><a href="#dashboard-cluster"><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span></a></span></li><dd class="toc-abstract"><p>
  The <span class="guimenu">Cluster</span> menu item lets you view detailed information
  about Ceph cluster hosts, OSDs, MONs, CRUSH Map, and the content of log
  files.
 </p></dd><li><span class="chapter"><a href="#dashboard-pools"><span class="title-number">8 </span><span class="title-name">Managing Pools</span></a></span></li><dd class="toc-abstract"><p>
   For more general information about Ceph pools, refer to
   <a class="xref" href="#ceph-pools" title="Chapter 22. Managing Storage Pools">Chapter 22, <em>Managing Storage Pools</em></a>. For information specific to erasure code
   pools, refer to <a class="xref" href="#cha-ceph-erasure" title="Chapter 24. Erasure Coded Pools">Chapter 24, <em>Erasure Coded Pools</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-rbds"><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span></a></span></li><dd class="toc-abstract"><p>
  To list all available RADOS Block Devices (RBDs), click
  <span class="guimenu">Block</span> / <span class="guimenu">Images</span>
  from the main menu.
 </p></dd><li><span class="chapter"><a href="#dash-webui-nfs"><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span></a></span></li><dd class="toc-abstract"><p>
   For more general information about NFS Ganesha, refer to
   <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 30. NFS Ganesha: Export Ceph Data via NFS">Chapter 30, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-mds"><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span></a></span></li><dd class="toc-abstract"><p>
   To find detailed information about CephFS, refer to
   <a class="xref" href="#cha-ceph-cephfs" title="Chapter 28. Clustered File System">Chapter 28, <em>Clustered File System</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-ogw"><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span></a></span></li><dd class="toc-abstract"><p>
   For more general information about Object Gateway, refer to
   <a class="xref" href="#cha-ceph-gw" title="Chapter 26. Ceph Object Gateway">Chapter 26, <em>Ceph Object Gateway</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-initial-configuration"><span class="title-number">13 </span><span class="title-name">Manual Configuration</span></a></span></li><dd class="toc-abstract"><p>
  This section introduces advanced information for users that prefer
  configuring dashboard's settings manually on the command line.
 </p></dd><li><span class="chapter"><a href="#dashboard-user-roles"><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span></a></span></li><dd class="toc-abstract"><p>
  This section describes how to manage user accounts used by the Ceph Dashboard.
  It helps you create or modify user accounts, as well as set proper user roles
  and permissions.
 </p></dd></ul></div><section class="chapter" id="dashboard-about" data-id-title="About Ceph Dashboard"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">About Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#dashboard-about">#</a></h2></div></div></div><p>
  The Ceph Dashboard is a module that adds a built-in Web based monitoring and
  administration application to the Ceph Manager (refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.2.3 “Ceph Nodes and Daemons”</span> for more details on Ceph Manager). You
  no longer need to know complex Ceph related commands to manage and monitor
  your Ceph cluster. You can either use the Ceph Dashboard's intuitive Web
  interface, or its built-in REST API.
 </p><p>
  The Ceph Dashboard is automatically enabled and configured with DeepSea's
  stage 3 during the deployment procedure (see
  <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span>). In a Ceph cluster with multiple
  Ceph Manager instances, only the dashboard running on the currently active Ceph Manager
  daemon will serve incoming requests. Accessing the dashboard's TCP port on
  any of the other Ceph Manager instances that are currently on standby will perform
  an HTTP redirect (303) to the currently active Ceph Manager's dashboard URL. This
  way, you can point your browser to any of the Ceph Manager instances in order to
  access the dashboard. Consider this behavior when securing access with
  firewall or planning for HA setup.
 </p></section><section class="chapter" id="dashboard-webui-general" data-id-title="Dashboards Web User Interface"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Dashboard's Web User Interface</span> <a title="Permalink" class="permalink" href="#dashboard-webui-general">#</a></h2></div></div></div><section class="sect1" id="dashboard-webui-login" data-id-title="Log In"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Log In</span> <a title="Permalink" class="permalink" href="#dashboard-webui-login">#</a></h2></div></div></div><p>
   To log in to the dashboard Web application, point your browser to its URL
   including the port number. You can find its address by running
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr services | grep dashboard
"dashboard": "https://ses-dash-node.example.com:8443/",</pre></div><div class="figure" id="id-1.3.4.3.3.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_login.png" target="_blank"><img src="images/dashboard_login.png" width="" alt="Ceph Dashboard Login Screen"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">Ceph Dashboard Login Screen </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.3.4">#</a></h6></div></div><p>
   You need a user account in order to log in to the dashboard Web application.
   DeepSea creates a default user 'admin' with administrator privileges for
   you. If you decide to log in with the default 'admin' user, retrieve the
   corresponding password by running
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call grains.get dashboard_creds</pre></div><div id="id-1.3.4.3.3.7" data-id-title="Custom User Account" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom User Account</h6><p>
    If you do not want to use the default 'admin' account to access the
    Ceph Dashboard, create a custom user account with administrator privileges.
    Refer to <a class="xref" href="#dashboard-user-roles" title="Chapter 14. Managing Users and Roles on the Command Line">Chapter 14, <em>Managing Users and Roles on the Command Line</em></a> for more details.
   </p></div><p>
   The dashboard user interface is graphically divided into several
   <span class="emphasis"><em>blocks</em></span>: the <span class="emphasis"><em>utility menu</em></span>, the
   <span class="emphasis"><em>main menu</em></span>, and the main <span class="emphasis"><em>content
   pane</em></span>.
  </p><div class="figure" id="id-1.3.4.3.3.9"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_homepage.png" target="_blank"><img src="images/dashboard_homepage.png" width="" alt="Ceph Dashboard Home Page"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.2: </span><span class="title-name">Ceph Dashboard Home Page </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.3.9">#</a></h6></div></div></section><section class="sect1" id="dashboard-util-menu" data-id-title="Utility Menu"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Utility Menu</span> <a title="Permalink" class="permalink" href="#dashboard-util-menu">#</a></h2></div></div></div><p>
   The top right part of the screen contains a utility menu. It includes
   general tasks related more to the dashboard than to the Ceph cluster. By
   clicking its items, you can access the following topics:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Change the language of the dashboard's user interface. You can choose from
     Czech, English, German, Spanish, French, Portuguese, Chinese, or
     Indonesian.
    </p></li><li class="listitem"><p>
     Display a list of Ceph related tasks that are running in the background.
    </p></li><li class="listitem"><p>
     View and erase recent dashboard notifications.
    </p></li><li class="listitem"><p>
     Display a list of links that refer to the information about the dashboard,
     its complete documentation, and an overview of its REST API.
    </p></li><li class="listitem"><p>
     Manage the dashboard's users and user roles. Refer to
     <a class="xref" href="#dashboard-user-roles" title="Chapter 14. Managing Users and Roles on the Command Line">Chapter 14, <em>Managing Users and Roles on the Command Line</em></a> for more detailed command line
     descriptions.
    </p></li><li class="listitem"><p>
     Log out of the dashboard.
    </p></li></ul></div></section><section class="sect1" id="dashboard-main-menu" data-id-title="Main Menu"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.3 </span><span class="title-name">Main Menu</span> <a title="Permalink" class="permalink" href="#dashboard-main-menu">#</a></h2></div></div></div><p>
   The dashboard's main menu occupies the top left part of the screen. It
   covers the following topics:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.5.3.1"><span class="term"><span class="guimenu">Dashboard</span></span></dt><dd><p>
      Return to Ceph Dashboard's home page.
     </p></dd><dt id="id-1.3.4.3.5.3.2"><span class="term"><span class="guimenu">Cluster</span></span></dt><dd><p>
      View detailed information about overall cluster configuration and
      CRUSH Map, its hosts, Ceph OSDs, Ceph Monitors, and the content of a log file.
     </p></dd><dt id="id-1.3.4.3.5.3.3"><span class="term"><span class="guimenu">Pools</span></span></dt><dd><p>
      View and manage cluster pools.
     </p></dd><dt id="id-1.3.4.3.5.3.4"><span class="term"><span class="guimenu">Block</span></span></dt><dd><p>
      View and manage block devices and their iSCSI exports.
     </p></dd><dt id="id-1.3.4.3.5.3.5"><span class="term"><span class="guimenu">NFS</span></span></dt><dd><p>
      View and manage NFS Ganesha deployments.
     </p></dd><dt id="id-1.3.4.3.5.3.6"><span class="term"><span class="guimenu">Filesystems</span></span></dt><dd><p>
      View and manage CephFSs.
     </p></dd><dt id="id-1.3.4.3.5.3.7"><span class="term"><span class="guimenu">Object Gateway</span></span></dt><dd><p>
      View and manage Object Gateway's daemons, users, and buckets.
     </p></dd></dl></div></section><section class="sect1" id="dashboard-cpane" data-id-title="The Content Pane"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.4 </span><span class="title-name">The Content Pane</span> <a title="Permalink" class="permalink" href="#dashboard-cpane">#</a></h2></div></div></div><p>
   The content pane occupies the main part of the dashboard's screen. The
   dashboard home page shows plenty of helpful widgets to inform you briefly
   about the current status of the cluster, capacity, and performance
   information.
  </p></section><section class="sect1" id="dashboard-ui-common" data-id-title="Common Web UI Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.5 </span><span class="title-name">Common Web UI Features</span> <a title="Permalink" class="permalink" href="#dashboard-ui-common">#</a></h2></div></div></div><p>
   In Ceph Dashboard, you often work with <span class="emphasis"><em>lists</em></span>—for
   example, lists of pools, OSD nodes, or RBD devices. All lists will
   automatically refresh themselves by default every 5 seconds. The following
   common widgets help you manage or adjust these list:
  </p><p>
   Click <span class="inlinemediaobject"><img src="images/oa_widget_reload.png" width=""/></span> to trigger a manual refresh of the list.
  </p><p>
   Click <span class="inlinemediaobject"><img src="images/oa_widget_columns.png" width=""/></span> to display or hide individual table columns.
  </p><p>
   Click <span class="inlinemediaobject"><img src="images/oa_widget_rows.png" width=""/></span> and select how many rows to display on a single page.
  </p><p>
   Click inside <span class="inlinemediaobject"><img src="images/oa_widget_search.png" width=""/></span> and filter the rows by typing the string to search for.
  </p><p>
   Use <span class="inlinemediaobject"><img src="images/oa_widget_pager.png" width=""/></span> to change the currently displayed page if the list
   spans across multiple pages.
  </p></section><section class="sect1" id="dashboard-widgets" data-id-title="Dashboard Widgets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.6 </span><span class="title-name">Dashboard Widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets">#</a></h2></div></div></div><p>
   Each dashboard widget shows specific status information related to a
   specific aspect of a running Ceph cluster. Some widgets are active links
   and after clicking them, they will redirect you to a related detailed page
   of the topic they represent.
  </p><div id="id-1.3.4.3.8.3" data-id-title="More Details on Mouse Over" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Details on Mouse Over</h6><p>
    Some graphical widgets show you more detail when you move the mouse over
    them.
   </p></div><section class="sect2" id="dashboard-widgets-status" data-id-title="Status Widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.6.1 </span><span class="title-name">Status Widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-status">#</a></h3></div></div></div><p>
    <span class="guimenu">Status</span> widgets give you a brief overview about the
    cluster's current status.
   </p><div class="figure" id="id-1.3.4.3.8.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_status.png" target="_blank"><img src="images/dashboard_widgets_status.png" width="" alt="Status Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.3: </span><span class="title-name">Status Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.8.4.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.8.4.4.1"><span class="term"><span class="guimenu">Cluster Status</span></span></dt><dd><p>
       Presents basic information about the cluster's health.
      </p></dd><dt id="id-1.3.4.3.8.4.4.2"><span class="term"><span class="guimenu">Monitors</span></span></dt><dd><p>
       Shows the number of running monitors and their quorum.
      </p></dd><dt id="id-1.3.4.3.8.4.4.3"><span class="term"><span class="guimenu">OSDs</span></span></dt><dd><p>
       Shows the total number of OSDs, as well as the number of 'up' and 'in'
       OSDs.
      </p></dd><dt id="id-1.3.4.3.8.4.4.4"><span class="term"><span class="guimenu">Manager Daemons</span></span></dt><dd><p>
       Shows the number of active and standby Ceph Manager daemons.
      </p></dd><dt id="id-1.3.4.3.8.4.4.5"><span class="term"><span class="guimenu">Hosts</span></span></dt><dd><p>
       Shows the total number of cluster nodes.
      </p></dd><dt id="id-1.3.4.3.8.4.4.6"><span class="term"><span class="guimenu">Object Gateways</span></span></dt><dd><p>
       Shows the number of running Object Gateways.
      </p></dd><dt id="id-1.3.4.3.8.4.4.7"><span class="term"><span class="guimenu">Metadata Servers</span></span></dt><dd><p>
       Shows the number of Metadata Servers.
      </p></dd><dt id="id-1.3.4.3.8.4.4.8"><span class="term"><span class="guimenu">iSCSI Gateway</span></span></dt><dd><p>
       Shows the number of configured iSCSI gateways.
      </p></dd></dl></div></section><section class="sect2" id="dashboard-widgets-performance" data-id-title="Performance Widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.6.2 </span><span class="title-name">Performance Widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-performance">#</a></h3></div></div></div><p>
    <span class="guimenu">Performance</span> widgets refer to basic performance data of
    Ceph clients.
   </p><div class="figure" id="id-1.3.4.3.8.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_perf.png" target="_blank"><img src="images/dashboard_widgets_perf.png" width="" alt="performance Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.4: </span><span class="title-name">performance Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.8.5.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.8.5.4.1"><span class="term"><span class="guimenu">Client IOPS</span></span></dt><dd><p>
       The amount of clients' read and write operations per second.
      </p></dd><dt id="id-1.3.4.3.8.5.4.2"><span class="term"><span class="guimenu">Client Throughput</span></span></dt><dd><p>
       The sum of clients' read and write operations per second.
      </p></dd><dt id="id-1.3.4.3.8.5.4.3"><span class="term"><span class="guimenu">Client Read/Write</span></span></dt><dd><p>
       Visualizes clients' read/write ratio.
      </p></dd><dt id="id-1.3.4.3.8.5.4.4"><span class="term"><span class="guimenu">Recovery Throughput</span></span></dt><dd><p>
       The throughput of data recovered per second.
      </p></dd><dt id="id-1.3.4.3.8.5.4.5"><span class="term"><span class="guimenu">Scrub</span></span></dt><dd><p>
       Shows the scrub (see <a class="xref" href="#op-pg-scrubpg" title="20.4.9. Scrubbing a Placement Group">Section 20.4.9, “Scrubbing a Placement Group”</a>) status. It is
       either disabled, enabled, or active.
      </p></dd></dl></div></section><section class="sect2" id="dashboard-widgets-capacity" data-id-title="Capacity Widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.6.3 </span><span class="title-name">Capacity Widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-capacity">#</a></h3></div></div></div><p>
    <span class="guimenu">Capacity</span> widgets show brief information about the
    storage capacity.
   </p><div class="figure" id="id-1.3.4.3.8.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_capacity.png" target="_blank"><img src="images/dashboard_widgets_capacity.png" width="" alt="Capacity Widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.5: </span><span class="title-name">Capacity Widgets </span><a title="Permalink" class="permalink" href="#id-1.3.4.3.8.6.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.3.8.6.4.1"><span class="term"><span class="guimenu">Pools</span></span></dt><dd><p>
       Shows the number of pools in the cluster.
      </p></dd><dt id="id-1.3.4.3.8.6.4.2"><span class="term"><span class="guimenu">Raw Capacity</span></span></dt><dd><p>
       Shows the ratio of used and available raw storage capacity.
      </p></dd><dt id="id-1.3.4.3.8.6.4.3"><span class="term"><span class="guimenu">Objects</span></span></dt><dd><p>
       Shows the number of data objects stored in the cluster.
      </p></dd><dt id="id-1.3.4.3.8.6.4.4"><span class="term"><span class="guimenu">PGs per OSD</span></span></dt><dd><p>
       Shows the number of placement groups per OSD.
      </p></dd><dt id="id-1.3.4.3.8.6.4.5"><span class="term"><span class="guimenu">PG Status</span></span></dt><dd><p>
       Displays a chart of the placement groups according to their status.
      </p></dd></dl></div></section></section></section><section class="chapter" id="dashboard-user-mgmt" data-id-title="Managing Dashboard Users and Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Managing Dashboard Users and Roles</span> <a title="Permalink" class="permalink" href="#dashboard-user-mgmt">#</a></h2></div></div></div><p>
  Dashboard user management performed by Ceph commands on the command line
  was already introduced in <a class="xref" href="#dashboard-user-roles" title="Chapter 14. Managing Users and Roles on the Command Line">Chapter 14, <em>Managing Users and Roles on the Command Line</em></a>.
 </p><p>
  This section describes how to manage user accounts by using the Dashboard Web
  user interface.
 </p><section class="sect1" id="dashboard-listing-users" data-id-title="Listing Users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Listing Users</span> <a title="Permalink" class="permalink" href="#dashboard-listing-users">#</a></h2></div></div></div><p>
   Click <span class="inlinemediaobject"><img src="images/dash_icon_gear.png" width=""/></span> in the utility menu and select <span class="guimenu">User
   Management</span>.
  </p><p>
   The list contains each user's user name, full name, e-mail, and list of
   assigned roles.
  </p><div class="figure" id="id-1.3.4.4.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_users.png" target="_blank"><img src="images/dash_users.png" width="" alt="User Management"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.1: </span><span class="title-name">User Management </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.5.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-adding-users" data-id-title="Adding New Users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Adding New Users</span> <a title="Permalink" class="permalink" href="#dashboard-adding-users">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Add</span> in the top left of the table heading to add a
   new user. Enter their user name, password, and optionally a full name and an
   e-mail.
  </p><div class="figure" id="id-1.3.4.4.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_user_add.png" target="_blank"><img src="images/dash_user_add.png" width="" alt="Adding a User"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.2: </span><span class="title-name">Adding a User </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.6.3">#</a></h6></div></div><p>
   Click the little pen icon to assign predefined roles to the user. Confirm
   with <span class="guimenu">Create User</span>.
  </p></section><section class="sect1" id="dashboard-editing-users" data-id-title="Editing Users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Editing Users</span> <a title="Permalink" class="permalink" href="#dashboard-editing-users">#</a></h2></div></div></div><p>
   Click a user's table row and then select <span class="guimenu">Edit</span> to edit
   details about the user. Confirm with <span class="guimenu">Update User</span>.
  </p></section><section class="sect1" id="dashboard-deleting-users" data-id-title="Deleting Users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.4 </span><span class="title-name">Deleting Users</span> <a title="Permalink" class="permalink" href="#dashboard-deleting-users">#</a></h2></div></div></div><p>
   Click a user's table row and then select <span class="guimenu">Delete</span> to delete
   the user account. Activate the <span class="guimenu">Yes, I am sure</span> check box
   and confirm with <span class="guimenu">Delete User</span>.
  </p></section><section class="sect1" id="dashboard-listing-user-roles" data-id-title="Listing User Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.5 </span><span class="title-name">Listing User Roles</span> <a title="Permalink" class="permalink" href="#dashboard-listing-user-roles">#</a></h2></div></div></div><p>
   Click <span class="inlinemediaobject"><img src="images/dash_icon_gear.png" width=""/></span> in the utility menu and select <span class="guimenu">User
   Management</span>. Then click the <span class="guimenu">Roles</span> tab.
  </p><p>
   The list contains each role's name, description, and whether it is a system
   role.
  </p><div class="figure" id="id-1.3.4.4.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_roles.png" target="_blank"><img src="images/dash_roles.png" width="" alt="User Roles"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.3: </span><span class="title-name">User Roles </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.9.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-adding-roles" data-id-title="Adding Custom Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.6 </span><span class="title-name">Adding Custom Roles</span> <a title="Permalink" class="permalink" href="#dashboard-adding-roles">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Add</span> in the top left of the table heading to add a
   new custom role. Enter its name, description, and set required permissions
   for individual topics.
  </p><div id="id-1.3.4.4.10.3" data-id-title="Purging Custom Roles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Purging Custom Roles</h6><p>
    If you create custom user roles and intend to remove the Ceph cluster
    with the <code class="command">ceph.purge</code> runner later on, you need to purge
    the custom roles first. Find more details in
    <a class="xref" href="#deepsea-ceph-purge" title="2.17. Removing an Entire Ceph Cluster">Section 2.17, “Removing an Entire Ceph Cluster”</a>.
   </p></div><div class="figure" id="id-1.3.4.4.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_roles_add.png" target="_blank"><img src="images/dash_roles_add.png" width="" alt="Adding a Role"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.4: </span><span class="title-name">Adding a Role </span><a title="Permalink" class="permalink" href="#id-1.3.4.4.10.4">#</a></h6></div></div><div id="id-1.3.4.4.10.5" data-id-title="Multiple Activation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple Activation</h6><p>
    By activating the check box that precedes the topic name, you activate all
    permissions for that topic. By activating the <span class="guimenu">All</span> check
    box, you activate all permissions for all the topics.
   </p></div><p>
   Confirm with <span class="guimenu">Create Role</span>.
  </p></section><section class="sect1" id="dashboard-editing-roles" data-id-title="Editing Custom Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.7 </span><span class="title-name">Editing Custom Roles</span> <a title="Permalink" class="permalink" href="#dashboard-editing-roles">#</a></h2></div></div></div><p>
   Click a custom role's table row and then select <span class="guimenu">Edit</span> in
   the top left of the table heading to edit a description and permissions of
   the custom role. Confirm with <span class="guimenu">Update Role</span>.
  </p></section><section class="sect1" id="dashboard-deleting-roles" data-id-title="Deleting Custom Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.8 </span><span class="title-name">Deleting Custom Roles</span> <a title="Permalink" class="permalink" href="#dashboard-deleting-roles">#</a></h2></div></div></div><p>
   Click a role's table row and then select <span class="guimenu">Delete</span> to delete
   the role. Activate the <span class="guimenu">Yes, I am sure</span> check box and
   confirm with <span class="guimenu">Delete Role</span>.
  </p></section></section><section class="chapter" id="dashboard-cluster" data-id-title="Viewing Cluster Internals"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Viewing Cluster Internals</span> <a title="Permalink" class="permalink" href="#dashboard-cluster">#</a></h2></div></div></div><p>
  The <span class="guimenu">Cluster</span> menu item lets you view detailed information
  about Ceph cluster hosts, OSDs, MONs, CRUSH Map, and the content of log
  files.
 </p><section class="sect1" id="dashboard-cluster-hosts" data-id-title="Cluster Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Cluster Nodes</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-hosts">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> / <span class="guimenu">Hosts</span>
   to view a list of cluster nodes.
  </p><div class="figure" id="id-1.3.4.5.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_hosts.png" target="_blank"><img src="images/dash_hosts.png" width="" alt="Hosts"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.1: </span><span class="title-name">Hosts </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.3">#</a></h6></div></div><p>
   Click a node name in the <span class="guimenu">Hostname</span> column to view
   performance details of the node.
  </p><p>
   The <span class="guimenu">Services</span> column lists all daemons that are running on
   each related node. Click a daemon name to view its detailed configuration.
  </p></section><section class="sect1" id="dashboard-cluster-monitors" data-id-title="Ceph Monitors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Ceph Monitors</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-monitors">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> / <span class="guimenu">Monitors</span>
   to view a list of cluster nodes with running Ceph monitors. The list
   includes each monitor's rank number, public IP address, and number of open
   sessions.
  </p><p>
   The list is divided into two tables: one for Ceph Monitors that
   <span class="emphasis"><em>are</em></span> in quorum, and the second one for Ceph Monitors that
   <span class="emphasis"><em>are not</em></span> in quorum.
  </p><p>
   Click a node name in the <span class="guimenu">Name</span> column to view the related
   Ceph Monitor configuration.
  </p><p>
   The <span class="guimenu">Status</span> table shows general statistics about the
   running Ceph Monitors.
  </p><div class="figure" id="id-1.3.4.5.5.6"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_mons.png" target="_blank"><img src="images/dash_mons.png" width="" alt="Ceph Monitors"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.2: </span><span class="title-name">Ceph Monitors </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.5.6">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-osds" data-id-title="Ceph OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.3 </span><span class="title-name">Ceph OSDs</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-osds">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> / <span class="guimenu">OSDs</span>
   to view a list of nodes with running OSD daemons/disks. The list includes
   each node's name, status, number of placement groups, size, usage,
   reads/writes chart in time, and the rate of read/write operations per
   second.
  </p><div class="figure" id="id-1.3.4.5.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds.png" target="_blank"><img src="images/dash_osds.png" width="" alt="Ceph OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.3: </span><span class="title-name">Ceph OSDs </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.3">#</a></h6></div></div><p>
   Click <span class="guimenu">Set Cluster-wide Flags</span> in the table heading to pop
   up a window with a list of flags that apply to the whole cluster. You can
   activate or deactivate individual flags, and confirm with
   <span class="guimenu">Submit</span>.
  </p><div class="figure" id="id-1.3.4.5.6.5"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds_flags.png" target="_blank"><img src="images/dash_osds_flags.png" width="" alt="OSD Flags"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.4: </span><span class="title-name">OSD Flags </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.5">#</a></h6></div></div><p>
   Click <span class="guimenu">Set Cluster-wide Recovery Priority</span> in the table
   heading to open a pop-up window with a list of OSD recovery priorities that
   apply to the whole cluster. You can activate the preferred priority profile,
   and fine tune the individual values below. Confirm with
   <span class="guimenu">Submit</span>.
  </p><div class="figure" id="id-1.3.4.5.6.7"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds_recover.png" target="_blank"><img src="images/dash_osds_recover.png" width="" alt="OSD Recovery Priority"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.5: </span><span class="title-name">OSD Recovery Priority </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.7">#</a></h6></div></div><p>
   Click a node name in the <span class="guimenu">Host</span> column to view an extended
   table with details about the OSD setting and performance. Browsing through
   several tabs, you can see lists of <span class="guimenu">Attributes</span>,
   <span class="guimenu">Metadata</span>, <span class="guimenu">Performance counter</span>, and a
   graphical <span class="guimenu">Histogram</span> of reads and writes.
  </p><div class="figure" id="id-1.3.4.5.6.9"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_isd_details.png" target="_blank"><img src="images/dash_isd_details.png" width="" alt="OSD Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.6: </span><span class="title-name">OSD Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.6.9">#</a></h6></div></div><div id="id-1.3.4.5.6.10" data-id-title="Perform Specific Tasks on OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Perform Specific Tasks on OSDs</h6><p>
    After you click an OSD node name, its table row changes color slightly,
    meaning that you can now perform a task on the node. Click the down arrow
    in the top left of the table heading and select a task to perform, such as
    <span class="guimenu">Deep Scrub</span>. Optionally enter a required value for the
    task, and confirm to run it on the node.
   </p></div></section><section class="sect1" id="dashboard-cluster-config" data-id-title="Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.4 </span><span class="title-name">Cluster Configuration</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-config">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> / <span class="guimenu">Configuration</span>
   to view a complete list of Ceph cluster configuration options. The list
   contains the name of the option, its short description, and its current and
   default values.
  </p><div class="figure" id="id-1.3.4.5.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_cluster_config.png" target="_blank"><img src="images/dash_cluster_config.png" width="" alt="Cluster Configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.7: </span><span class="title-name">Cluster Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.7.3">#</a></h6></div></div><p>
   Click a specific option row to highlight and see detailed information about
   the option, such as its type of value, minimum and maximum permitted values,
   whether it can be updated at runtime, and many more.
  </p><p>
   After highlighting a specific option, you can edit its value(s) by clicking
   the <span class="guimenu">Edit</span> button in the top left of the table heading.
   Confirm changes with <span class="guimenu">Save</span>.
  </p></section><section class="sect1" id="dashboard-cluster-crushmap" data-id-title="CRUSH Map"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.5 </span><span class="title-name">CRUSH Map</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-crushmap">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Cluster</span> / <span class="guimenu">CRUSH
   map</span> to view a CRUSH Map of the cluster. For more
   general information on CRUSH Maps, refer to <a class="xref" href="#op-crush" title="20.5. CRUSH Map Manipulation">Section 20.5, “CRUSH Map Manipulation”</a>.
  </p><p>
   Click the root, nodes, or individual OSDs to view more detailed information,
   such as crush weight, depth in the map tree, device class of the OSD, and
   many more.
  </p><div class="figure" id="id-1.3.4.5.8.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_crushmap.png" target="_blank"><img src="images/dash_crushmap.png" width="" alt="CRUSH Map"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.8: </span><span class="title-name">CRUSH Map </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.8.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-mgr-plugins" data-id-title="Manager Modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.6 </span><span class="title-name">Manager Modules</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-mgr-plugins">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Cluster</span> / <span class="guimenu">Manager
   modules</span> to view a list of available Ceph Manager modules.
   Each line consists of a module name and information on whether it is
   currently enabled or not.
  </p><div class="figure" id="id-1.3.4.5.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_mgr_modules.png" target="_blank"><img src="images/dash_mgr_modules.png" width="" alt="Manager Modules"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.9: </span><span class="title-name">Manager Modules </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.9.3">#</a></h6></div></div><p>
   After highlighting a specific module, you can see its detailed settings in
   the <span class="guimenu">Details</span> table below. Edit them by clicking the
   <span class="guimenu">Edit</span> button in the top left of the table heading. Confirm
   changes with <span class="guimenu">Update</span>.
  </p></section><section class="sect1" id="dashboard-cluster-logs" data-id-title="Logs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.7 </span><span class="title-name">Logs</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-logs">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> / <span class="guimenu">Logs</span>
   to view a list of cluster's recent log entries. Each line consists of a time
   stamp, the type of the log entry, and the logged message itself.
  </p><p>
   Click the <span class="guimenu">Audit Logs</span> tab to view log entries of the
   auditing subsystem. Refer to <a class="xref" href="#dashboard-auditing" title="14.4. Auditing">Section 14.4, “Auditing”</a> for
   commands to enable or disable auditing.
  </p><div class="figure" id="id-1.3.4.5.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_logs.png" target="_blank"><img src="images/dash_logs.png" width="" alt="Logs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.10: </span><span class="title-name">Logs </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.10.4">#</a></h6></div></div></section></section><section class="chapter" id="dashboard-pools" data-id-title="Managing Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Managing Pools</span> <a title="Permalink" class="permalink" href="#dashboard-pools">#</a></h2></div></div></div><div id="id-1.3.4.6.3" data-id-title="More Information on Pools" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on Pools</h6><p>
   For more general information about Ceph pools, refer to
   <a class="xref" href="#ceph-pools" title="Chapter 22. Managing Storage Pools">Chapter 22, <em>Managing Storage Pools</em></a>. For information specific to erasure code
   pools, refer to <a class="xref" href="#cha-ceph-erasure" title="Chapter 24. Erasure Coded Pools">Chapter 24, <em>Erasure Coded Pools</em></a>.
  </p></div><p>
  To list all available pools, click <span class="guimenu">Pools</span> from the main
  menu.
 </p><p>
  The list shows each pool's name, type, related application, placement group
  status, replica size, erasure coded profile, usage, and read/write
  statistics.
 </p><div class="figure" id="id-1.3.4.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools.png" target="_blank"><img src="images/oa_pools.png" width="" alt="List of Pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.1: </span><span class="title-name">List of Pools </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.6">#</a></h6></div></div><p>
  To view more detailed information about a pool, activate its table row.
 </p><section class="sect1" id="dashboard-pools-create" data-id-title="Adding a New Pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">Adding a New Pool</span> <a title="Permalink" class="permalink" href="#dashboard-pools-create">#</a></h2></div></div></div><p>
   To add a new pool, click <span class="guimenu">Add</span> in the top left of the pools
   table. In the pool form you can enter the pool's name, type, number of
   placement groups, and additional information, such as pool's applications,
   and compression mode and algorithm. The pool form itself pre-calculates the
   number of placement groups that best suited to this specific pool. The
   calculation is based on the amount of OSDs in the cluster and the selected
   pool type with its specific settings. As soon as a placement groups number
   is set manually, it will be replaced by a calculated number. Confirm with
   <span class="guimenu">Create pool</span>.
  </p><div class="figure" id="id-1.3.4.6.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools_add.png" target="_blank"><img src="images/oa_pools_add.png" width="" alt="Adding a New Pool"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.2: </span><span class="title-name">Adding a New Pool </span><a title="Permalink" class="permalink" href="#id-1.3.4.6.8.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-pools-delete" data-id-title="Deleting Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.2 </span><span class="title-name">Deleting Pools</span> <a title="Permalink" class="permalink" href="#dashboard-pools-delete">#</a></h2></div></div></div><p>
   To delete a pool, click its table row and click <span class="guimenu">Delete</span> in
   the top left of the pools table.
  </p></section><section class="sect1" id="dashboard-pools-edit" data-id-title="Editing a Pools Options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.3 </span><span class="title-name">Editing a Pool's Options</span> <a title="Permalink" class="permalink" href="#dashboard-pools-edit">#</a></h2></div></div></div><p>
   To edit a pool's options, click its table row and select
   <span class="guimenu">Edit</span> in the top left of the pools table.
  </p><p>
   You can change the name of the pool, increase the number of placement
   groups, change the list of the pool's applications and compression settings.
   Confirm with <span class="guimenu">Edit pool</span>.
  </p></section></section><section class="chapter" id="dashboard-rbds" data-id-title="Managing RADOS Block Devices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9 </span><span class="title-name">Managing RADOS Block Devices</span> <a title="Permalink" class="permalink" href="#dashboard-rbds">#</a></h2></div></div></div><p>
  To list all available RADOS Block Devices (RBDs), click
  <span class="guimenu">Block</span> / <span class="guimenu">Images</span>
  from the main menu.
 </p><p>
  The list shows brief information about the device, such as the device's name,
  the related pool name, size of the device, number and size of objects on the
  device.
 </p><div class="figure" id="id-1.3.4.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images.png" target="_blank"><img src="images/dash_rbd_images.png" width="" alt="List of RBD Images"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.1: </span><span class="title-name">List of RBD Images </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.5">#</a></h6></div></div><section class="sect1" id="dashboard-rbds-details" data-id-title="Viewing Details about RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Viewing Details about RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-details">#</a></h2></div></div></div><p>
   To view more detailed information about a device, click its row in the
   table:
  </p><div class="figure" id="id-1.3.4.7.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_details.png" target="_blank"><img src="images/dash_rbd_images_details.png" width="" alt="RBD Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.2: </span><span class="title-name">RBD Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.6.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-rbds-configuration" data-id-title="Viewing RBDs Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.2 </span><span class="title-name">Viewing RBD's Configuration</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-configuration">#</a></h2></div></div></div><p>
   To view detailed configuration of a device, click its row in the table and
   then the <span class="guimenu">Configuration</span> tab in the lower table:
  </p><div class="figure" id="id-1.3.4.7.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_config.png" target="_blank"><img src="images/dash_rbd_images_config.png" width="" alt="RBD Configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.3: </span><span class="title-name">RBD Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.7.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-rbds-create" data-id-title="Creating RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.3 </span><span class="title-name">Creating RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-create">#</a></h2></div></div></div><p>
   To add a new device, click <span class="guimenu">Add</span> in the top left of the
   table heading and do the following on the <span class="guimenu">Create RBD</span>
   screen:
  </p><div class="figure" id="id-1.3.4.7.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_add.png" target="_blank"><img src="images/oa_rbd_add.png" width="" alt="Adding a New RBD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.4: </span><span class="title-name">Adding a New RBD </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.8.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Enter the name of the new device. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware Requirements and Recommendations”, Section 2.11 “Naming Limitations”</span>
     for naming limitations.
    </p></li><li class="step"><p>
     Select the pool with the 'rbd' application assigned from which the new RBD
     device will be created.
    </p></li><li class="step"><p>
     Specify the size of the new device.
    </p></li><li class="step"><p>
     Specify additional options for the device. To fine-tune the device
     parameters, click <span class="guimenu">Advanced</span> and enter values for object
     size, stripe unit, or stripe count. To enter Quality of Service (QoS)
     limits, click <span class="guimenu">Quality of Service</span> and enter them.
    </p></li><li class="step"><p>
     Confirm with <span class="guimenu">Create RBD</span>.
    </p></li></ol></div></div></section><section class="sect1" id="dashboard-rbd-delete" data-id-title="Deleting RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.4 </span><span class="title-name">Deleting RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbd-delete">#</a></h2></div></div></div><p>
   To delete a device, click its row in the table and select
   <span class="guimenu">Delete</span> in the top left of the table heading. Confirm the
   deletion with <span class="guimenu">Delete RBD</span>.
  </p><div id="id-1.3.4.7.9.3" data-id-title="Moving RBDs to Trash" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Moving RBDs to Trash</h6><p>
    Deleting an RBD is an irreversible action. If you <span class="guimenu">Move to
    Trash</span> instead, you can restore the device later on by selecting
    it on the <span class="guimenu">Trash</span> tab of the main table and clicking
    <span class="guimenu">Restore</span> in the top left of the table heading.
   </p></div></section><section class="sect1" id="dashboard-rbds-snapshots" data-id-title="RADOS Block Device Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.5 </span><span class="title-name">RADOS Block Device Snapshots</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-snapshots">#</a></h2></div></div></div><p>
   To create a RADOS Block Device snapshot, click the device's table row and in the
   <span class="guimenu">Snapshots</span> tab below the main table, click
   <span class="guimenu">Create</span> in the top left of the table heading. Enter the
   snapshot's name and confirm with <span class="guimenu">Create Snapshot</span>.
  </p><p>
   After selecting a snapshot, you can perform additional actions on the
   device, such as rename, protect, clone, copy, or delete.
   <span class="guimenu">Rollback</span> restores the device's state from the current
   snapshot.
  </p><div class="figure" id="id-1.3.4.7.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_snapshots.png" target="_blank"><img src="images/dash_rbd_images_snapshots.png" width="" alt="RBD Snapshots"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.5: </span><span class="title-name">RBD Snapshots </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.10.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-iscsi" data-id-title="Managing iSCSI Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.6 </span><span class="title-name">Managing iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi">#</a></h2></div></div></div><div id="id-1.3.4.7.11.2" data-id-title="More Information on iSCSI Gateways" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on iSCSI Gateways</h6><p>
    For more general information about iSCSI Gateways, refer to
    <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span> and <a class="xref" href="#cha-ceph-iscsi" title="Chapter 27. Ceph iSCSI Gateway">Chapter 27, <em>Ceph iSCSI Gateway</em></a>.
   </p></div><p>
   To list all available gateways and mapped images, click
   <span class="guimenu">Block</span> / <span class="guimenu">iSCSI</span>
   from the main menu. An <span class="guimenu">Overview</span> tab opens, listing
   currently configured iSCSI Gateways and mapped RBD images.
  </p><p>
   The <span class="guimenu">Gateways</span> table lists each gateway's state, number of
   iSCSI targets, and number of sessions. The <span class="guimenu">Images</span> table
   lists each mapped image's name, related pool name backstore type, and other
   statistical details.
  </p><p>
   The <span class="guimenu">Targets</span> tab lists currently configured iSCSI
   targets.
  </p><div class="figure" id="id-1.3.4.7.11.6"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws.png" target="_blank"><img src="images/dash_igws.png" width="" alt="List of iSCSI Targets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.6: </span><span class="title-name">List of iSCSI Targets </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.11.6">#</a></h6></div></div><p>
   To view more detailed information about a target, click its table row. A
   tree-structured schema opens, listing disks, portals, initiators, and
   groups. Click an item to expand it and view its detailed contents,
   optionally with a related configuration in the table on the right.
  </p><div class="figure" id="id-1.3.4.7.11.8"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws_status.png" target="_blank"><img src="images/dash_igws_status.png" width="" alt="iSCSI Target Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.7: </span><span class="title-name">iSCSI Target Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.11.8">#</a></h6></div></div><section class="sect2" id="dashboard-iscsi-create" data-id-title="Adding iSCSI Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.6.1 </span><span class="title-name">Adding iSCSI Targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-create">#</a></h3></div></div></div><p>
    To add a new iSCSI target, click <span class="guimenu">Add</span> in the top left
    of the <span class="guimenu">Targets</span> table and enter the required information.
   </p><div class="figure" id="id-1.3.4.7.11.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws_add.png" target="_blank"><img src="images/dash_igws_add.png" width="" alt="Adding a New Target"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.8: </span><span class="title-name">Adding a New Target </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.11.9.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Enter the target address of the new gateway.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add portal</span> and select one or multiple iSCSI
      portals from the list.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add image</span> and select one or multiple RBD images
      for the gateway.
     </p></li><li class="step"><p>
      If you need to use authentication to access the gateway, activate the
      <span class="guimenu">Authentication</span> check box and enter the credentials.
      You can find more advanced authentication options after activating
      <span class="guimenu">Mutual authentication</span> and <span class="guimenu">Discovery
      authentication</span>.
     </p></li><li class="step"><p>
      Confirm with <span class="guimenu">Create Target</span>.
     </p></li></ol></div></div></section><section class="sect2" id="dashboard-iscsi-edit" data-id-title="Editing iSCSI Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.6.2 </span><span class="title-name">Editing iSCSI Targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-edit">#</a></h3></div></div></div><p>
    To edit an existing iSCSI target, click its row in the
    <span class="guimenu">Targets</span> table and click <span class="guimenu">Edit</span> in the
    top left of the table.
   </p><p>
    You can then modify the iSCSI target, add or delete portals, and add or
    delete related RBD images. You can also adjust authentication information
    for the gateway.
   </p></section><section class="sect2" id="dashboard-iscsi-delete" data-id-title="Deleting iSCSI Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.6.3 </span><span class="title-name">Deleting iSCSI Targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-delete">#</a></h3></div></div></div><p>
    To delete an iSCSI target, click its table row and select
    <span class="guimenu">Delete</span> in the top left of the gateways table. Activate
    <span class="guimenu">Yes, I am sure</span> and confirm with <span class="guimenu">Delete
    iSCSI</span>.
   </p></section></section><section class="sect1" id="dash-rbd-qos" data-id-title="RBD Quality of Service (QoS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.7 </span><span class="title-name">RBD Quality of Service (QoS)</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos">#</a></h2></div></div></div><div id="id-1.3.4.7.12.2" data-id-title="For More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: For More Information</h6><p>
    For more general information and a description of RBD QoS configuration
    options, refer to <a class="xref" href="#rbd-qos" title="23.6. QoS Settings">Section 23.6, “QoS Settings”</a>.
   </p></div><p>
   The QoS options can be configured at different levels.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Globally
    </p></li><li class="listitem"><p>
     On a per-pool basis
    </p></li><li class="listitem"><p>
     On a per-image basis
    </p></li></ul></div><p>
   The <span class="emphasis"><em>global</em></span> configuration is at the top of the list and
   will be used for all newly created RBD images and for those images that do
   not override these values on the pool or RBD image layer. An option value
   specified globally can be overridden on a per-pool or per-image basis.
   Options specified on a pool will be applied to all RBD images of that pool
   unless overridden by a configuration option set on an image. Options
   specified on an image will override options specified on a pool and will
   override options specified globally.
  </p><p>
   This way it is possible to define defaults globally, adapt them for all RBD
   images of a specific pool, and override the pool configuration for
   individual RBD images.
  </p><section class="sect2" id="dash-rbd-qos-global" data-id-title="Configuring Options Globally"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.1 </span><span class="title-name">Configuring Options Globally</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-global">#</a></h3></div></div></div><p>
    To configure RBD options globally, select
    <span class="guimenu">Cluster</span> / <span class="guimenu">Configuration</span> from the main menu.
   </p><p>
    To list all available global configuration options, click
    <span class="guimenu">Level</span> / <span class="guimenu">Advanced</span>. Then filter the results of the
    table by filtering for 'rbd_qos' in the search field. This lists all
    available configuration options for QoS. To change a value, click its row
    in the table, then select <span class="guimenu">Edit</span> at the top left of the
    table. The <span class="guimenu">Edit</span> dialog contains six different fields for
    specifying values. The RBD configuration option values are required in the
    <span class="guimenu">mgr</span> text box. Note that unlike the other dialogs, this
    one does not allow you to specify the value in convenient units. You need
    to set these values in either bytes or IOPS, depending on the option you
    are editing.
   </p></section><section class="sect2" id="dash-rbd-qos-pool-create" data-id-title="Configuring Options on a New Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.2 </span><span class="title-name">Configuring Options on a New Pool</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-pool-create">#</a></h3></div></div></div><p>
    To create a new pool and configure RBD configuration options on it, click
    <span class="guimenu">Pools</span> / <span class="guimenu">Create</span>. Select
    <span class="guimenu">replicated</span> as pool type. You will then need to add the
    <span class="guimenu">rbd</span> application tag to the pool to be able to configure
    the RBD QoS options.
   </p><div id="id-1.3.4.7.12.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     It is not possible to configure RBD QoS configuration options on an
     erasure coded pool. To configure the RBD QoS options for erasure coded
     pools, you need to edit the replicated metadata pool of an RBD image. The
     configuration will then be applied to the erasure coded data pool of that
     image.
    </p></div></section><section class="sect2" id="dash-rbd-qos-pool-edit" data-id-title="Configuring Options on an Existing Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.3 </span><span class="title-name">Configuring Options on an Existing Pool</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-pool-edit">#</a></h3></div></div></div><p>
    To configure RBD QoS options on an existing pool, click
    <span class="guimenu">Pools</span>, then click the pool's table row and select
    <span class="guimenu">Edit</span> at the top left of the table.
   </p><p>
    You should see the <span class="guimenu">RBD Configuration</span> section in the
    dialog, followed by a <span class="guimenu">Quality of Service</span> section.
   </p><div id="id-1.3.4.7.12.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If you see neither the <span class="guimenu">RBD Configuration</span> nor the
     <span class="guimenu">Quality of Service</span> section, you are likely either
     editing an <span class="emphasis"><em>erasure coded</em></span> pool, which cannot be used
     to set RBD configuration options, or the pool is not configured to be used
     by RBD images. In the latter case, assign the <span class="guimenu">rbd</span>
     application tag to the pool and the corresponding configuration sections
     will show up.
    </p></div></section><section class="sect2" id="dash-rbd-qos-qos" data-id-title="Configuration Options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.4 </span><span class="title-name">Configuration Options</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-qos">#</a></h3></div></div></div><p>
    Click <span class="guimenu">Quality of Service +</span> to expand the configuration
    options. A list of all available options will show up. The units of the
    configuration options are already shown in the text boxes. In case of any
    bytes per second (BPS) option, you are free to use shortcuts such as '1M'
    or '5G'. They will be automatically converted to '1 MB/s' and '5 GB/s'
    respectively.
   </p><p>
    By clicking the reset button to the right of each text box, any value set
    on the pool will be removed. This does not remove configuration values of
    options configured globally or on an RBD image.
   </p></section><section class="sect2" id="dash-rbd-qos-image-create" data-id-title="Creating RBD QoS Options with a New RBD Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.5 </span><span class="title-name">Creating RBD QoS Options with a New RBD Image</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-create">#</a></h3></div></div></div><p>
    To create an RBD image with RBD QoS options set on that image, select
    <span class="guimenu">Block</span> / <span class="guimenu">Images</span>
    and then click <span class="guimenu">Create</span>. Click <span class="guimenu">Advanced</span>
    to expand the advanced configuration section. Click <span class="guimenu">Quality of
    Service</span> to open all available configuration options.
   </p></section><section class="sect2" id="dash-rbd-qos-image-edit" data-id-title="Editing RBD QoS Options on Existing Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.6 </span><span class="title-name">Editing RBD QoS Options on Existing Images</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-edit">#</a></h3></div></div></div><p>
    To edit RBD QoS options on an existing image, select
    <span class="guimenu">Block</span> / <span class="guimenu">Images</span>, then click the pool's table row,
    and lastly click <span class="guimenu">Edit</span>. The edit dialog will show up.
    Click <span class="guimenu">Advanced</span> to expand the advanced configuration
    section. Click <span class="guimenu">Quality of Service</span> to open all available
    configuration options.
   </p></section><section class="sect2" id="dash-rbd-qos-image-copy" data-id-title="Changing Configuration Options When Copying or Cloning Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.7.7 </span><span class="title-name">Changing Configuration Options When Copying or Cloning Images</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-copy">#</a></h3></div></div></div><p>
    If an RBD image is cloned or copied, the values set on that particular
    image will be copied too, by default. If you want to change them while
    copying or cloning, you can do so by specifying the updated configuration
    values in the copy/clone dialog, the same way as when creating or editing
    an RBD image. Doing so will only set (or reset) the values for the RBD
    image that is copied or cloned. This operation changes neither the source
    RBD image configuration, nor the global configuration.
   </p><p>
    If you choose to reset the option value on copying/cloning, no value for
    that option will be set on that image. This means that any value of that
    option specified for the parent pool will be used if the parent pool has
    the value configured. Otherwise, the global default will be used.
   </p></section></section><section class="sect1" id="dash-rbd-mirror" data-id-title="RBD Mirroring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.8 </span><span class="title-name">RBD Mirroring</span> <a title="Permalink" class="permalink" href="#dash-rbd-mirror">#</a></h2></div></div></div><div id="id-1.3.4.7.13.2" data-id-title="General Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: General Information</h6><p>
    For general information and the command line approach to RADOS Block Device mirroring,
    refer to <a class="xref" href="#ceph-rbd-mirror" title="23.4. Mirroring">Section 23.4, “Mirroring”</a>.
   </p></div><p>
   You can use the Ceph Dashboard to configure replication of RBD images between
   two or more clusters.
  </p><section class="sect2" id="rbd-mirror-primary-secondary" data-id-title="Primary Cluster and Secondary Cluster(s)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.8.1 </span><span class="title-name">Primary Cluster and Secondary Cluster(s)</span> <a title="Permalink" class="permalink" href="#rbd-mirror-primary-secondary">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Primary</em></span> cluster is where the original pool with images
    is created. <span class="emphasis"><em>Secondary</em></span> cluster(s) is where the
    pool/images are replicated from the <span class="emphasis"><em>primary</em></span> cluster.
   </p><div id="id-1.3.4.7.13.4.3" data-id-title="Relative Naming" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Relative Naming</h6><p>
     The <span class="emphasis"><em>primary</em></span> and <span class="emphasis"><em>secondary</em></span> terms
     can be relative in the context of replication because they relate more to
     individual pools than to clusters. For example, in two-way replication,
     one pool can be mirrored from the <span class="emphasis"><em>primary</em></span> cluster to
     the <span class="emphasis"><em>secondary</em></span> one, while another pool can be mirrored
     from the <span class="emphasis"><em>secondary</em></span> cluster to the
     <span class="emphasis"><em>primary</em></span> one.
    </p></div></section><section class="sect2" id="rbd-mirroring-replication-modes" data-id-title="Replication Modes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.8.2 </span><span class="title-name">Replication Modes</span> <a title="Permalink" class="permalink" href="#rbd-mirroring-replication-modes">#</a></h3></div></div></div><p>
    There are two modes of data replication:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Using the <span class="emphasis"><em>pool</em></span> mode, you replicate all the RBD
      images in a pool.
     </p></li><li class="listitem"><p>
      Using the <span class="emphasis"><em>image</em></span> mode, you can activate mirroring
      only for specific image(s) in a pool.
     </p></li></ul></div></section><section class="sect2" id="rbd-mirror-gui-daemon" data-id-title="Configure the rbd-mirror Daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.8.3 </span><span class="title-name">Configure the <code class="systemitem">rbd-mirror</code> Daemon</span> <a title="Permalink" class="permalink" href="#rbd-mirror-gui-daemon">#</a></h3></div></div></div><p>
    The <code class="systemitem">rbd-mirror</code> daemon performs the
    actual cluster data replication. To install, configure, and run it, follow
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      The <code class="systemitem">rbd-mirror</code> daemon needs to
      run on one of the nodes on the <span class="emphasis"><em>secondary</em></span> cluster
      other than the Admin Node. Because it is not installed by default, install it:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper install rbd-mirror</pre></div></li><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster, create a unique Ceph user
      ID for the <code class="systemitem">rbd-mirror</code> daemon
      process. In this example, we will use 'uid1' as the user ID:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create client.rbd-mirror.uid1 \
 mon 'profile rbd-mirror' osd 'profile rbd'
[client.rbd-mirror.uid1]
	key = AQBbDJddZKLBIxAAdsmSCCjXoKwzGkGmCpUQ9g==</pre></div></li><li class="step"><p>
      On the node where you previously installed the
      <span class="package">rbd-mirror</span> package on the
      <span class="emphasis"><em>secondary</em></span> cluster, create the same Ceph user and
      save the output to a keyring:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ceph auth get-or-create client.rbd-mirror.uid1 \
 mon 'profile rbd-mirror' osd 'profile rbd' \
 &gt; /etc/ceph/ceph.client.rbd-mirror.uid1.keyring</pre></div></li><li class="step"><p>
      On the same node, enable and run the
      <code class="systemitem">rbd-mirror</code> service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl enable ceph-rbd-mirror@rbd-mirror.uid1
<code class="prompt user">root@minion &gt; </code>systemctl start ceph-rbd-mirror@rbd-mirror.uid1
<code class="prompt user">root@minion &gt; </code>systemctl status ceph-rbd-mirror@rbd-mirror.uid1
● ceph-rbd-mirror@rbd-mirror.uid1.service - Ceph rbd mirror daemon
   Loaded: loaded (/usr/lib/systemd/system/ceph-rbd-mirror@.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2019-10-04 07:48:53 EDT; 2 days ago
 Main PID: 212434 (rbd-mirror)
    Tasks: 47
   CGroup: /system.slice/system-ceph\x2drbd\x2dmirror.slice/ceph-rbd-mirror@rbd-mirror.uid1.service
           └─212434 /usr/bin/rbd-mirror -f --cluster ceph --id rbd-mirror.test --setuser ceph --setgroup ceph

Oct 04 07:48:53 doc-ses6min4 systemd[1]: Started Ceph rbd mirror daemon.</pre></div></li><li class="step"><p>
      On the <span class="emphasis"><em>secondary</em></span> cluster's Ceph Dashboard, navigate to
      <span class="guimenu">Block</span> / <span class="guimenu">Mirroring</span>.
      The <span class="guimenu">Daemons</span> table to the left shows actively running
      <code class="systemitem">rbd-mirror</code> daemons and their health.
     </p><div class="figure" id="id-1.3.4.7.13.6.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-daemon.png" target="_blank"><img src="images/rbd-mirror-daemon.png" width="" alt="Running rbd-mirror Daemons"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.9: </span><span class="title-name">Running <code class="systemitem">rbd-mirror</code> Daemons </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.6.3.5.2">#</a></h6></div></div></li></ol></div></div></section><section class="sect2" id="rbd-mirror-gui" data-id-title="Configure Pool Replication in Ceph Dashboard"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.8.4 </span><span class="title-name">Configure Pool Replication in Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#rbd-mirror-gui">#</a></h3></div></div></div><p>
    The <code class="systemitem">rbd-mirror</code> daemon needs to have
    access to the primary cluster to be able to mirror RBD images. Therefore
    you need to create a <span class="emphasis"><em>peer</em></span> Ceph user account on the
    primary cluster and let the secondary cluster know about its keyring:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster, create a new
      'client.rbd-mirror-peer' user that will be used for data replication:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
[client.rbd-mirror-peer]
	key = AQBbDJddZKLBIxAAdsmSCCjXoKwzGkGmCpUQ9g==</pre></div></li><li class="step"><p>
      On both the <span class="emphasis"><em>primary</em></span> and
      <span class="emphasis"><em>secondary</em></span> cluster, create a pool with an identical
      name and assign the 'rbd' application to it. Refer to
      <a class="xref" href="#dashboard-pools-create" title="8.1. Adding a New Pool">Section 8.1, “Adding a New Pool”</a> for more details on creating a
      new pool.
     </p><div class="figure" id="id-1.3.4.7.13.7.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool.png" target="_blank"><img src="images/rbd-mirror-pool.png" width="" alt="Creating a Pool with RBD Application"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.10: </span><span class="title-name">Creating a Pool with RBD Application </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.7.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      On both the <span class="emphasis"><em>primary</em></span> and
      <span class="emphasis"><em>secondary</em></span> cluster's dashboards, navigate to
      <span class="guimenu">Block</span> / <span class="guimenu">Mirroring</span>.
      In the <span class="guimenu">Pools</span> table on the right, click the name of the
      pool to replicate, and after clicking <span class="guimenu">Edit Mode</span>,
      select the replication mode. In this example, we will work with a
      <span class="emphasis"><em>pool</em></span> replication mode, which means that all images
      within a given pool will be replicated. Confirm with
      <span class="guimenu">Update</span>.
     </p><div class="figure" id="id-1.3.4.7.13.7.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-mode.png" target="_blank"><img src="images/rbd-mirror-pool-mode.png" width="" alt="Configuring the Replication Mode"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.11: </span><span class="title-name">Configuring the Replication Mode </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.7.3.3.2">#</a></h6></div></div><div id="id-1.3.4.7.13.7.3.3.3" data-id-title="Error or Warning on the Primary Cluster" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Error or Warning on the Primary Cluster</h6><p>
       After updating the replication mode, an error or warning flag will
       appear in the corresponding right column. That is because the pool has
       no peer user for replication assigned yet. Ignore this flag for the
       <span class="emphasis"><em>primary</em></span> cluster as we assign a peer user to the
       <span class="emphasis"><em>secondary</em></span> cluster only.
      </p></div></li><li class="step"><p>
      On the <span class="emphasis"><em>secondary</em></span> cluster's Dashboard, navigate to
      <span class="guimenu">Block</span> / <span class="guimenu">Mirroring</span>.
      Register the 'client.rbd-mirror-peer' user keyring to the mirrored pool
      by clicking the pool's name and selecting <span class="guimenu">Add Peer</span>.
      Provide the <span class="emphasis"><em>primary</em></span> cluster's details:
     </p><div class="figure" id="id-1.3.4.7.13.7.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-peer.png" target="_blank"><img src="images/rbd-mirror-pool-peer.png" width="" alt="Adding Peer Credentials"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.12: </span><span class="title-name">Adding Peer Credentials </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.7.3.4.2">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.13.7.3.4.3.1"><span class="term"><span class="guimenu">Cluster Name</span></span></dt><dd><p>
         An arbitrary unique string that identifies the primary cluster, such
         as 'primary'. The cluster name needs to be different from the real
         secondary cluster's name.
        </p></dd><dt id="id-1.3.4.7.13.7.3.4.3.2"><span class="term"><span class="guimenu">CephX ID</span></span></dt><dd><p>
         The Ceph user ID that you created as a mirroring peer. In this
         example it is 'rbd-mirror-peer'.
        </p></dd><dt id="id-1.3.4.7.13.7.3.4.3.3"><span class="term"><span class="guimenu">Monitor Addresses</span></span></dt><dd><p>
         Comma separated list of IP addresses/host names of the primary
         cluster's Ceph Monitor nodes.
        </p></dd><dt id="id-1.3.4.7.13.7.3.4.3.4"><span class="term"><span class="guimenu">CephX Key</span></span></dt><dd><p>
         The key related to the peer user ID. You can retrieve it by running
         the following example command on the primary cluster:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth print_key client.rbd-mirror-peer</pre></div></dd></dl></div><p>
      Confirm with <span class="guimenu">Submit</span>.
     </p><div class="figure" id="id-1.3.4.7.13.7.3.4.5"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-list.png" target="_blank"><img src="images/rbd-mirror-pool-list.png" width="" alt="List of Replicated Pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.13: </span><span class="title-name">List of Replicated Pools </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.7.3.4.5">#</a></h6></div></div></li></ol></div></div></section><section class="sect2" id="rbd-mirror-test" data-id-title="Verify That RBD Image Replication Works"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.8.5 </span><span class="title-name">Verify That RBD Image Replication Works</span> <a title="Permalink" class="permalink" href="#rbd-mirror-test">#</a></h3></div></div></div><p>
    When the <code class="systemitem">rbd-mirror</code> daemon is
    running and RBD image replication is configured on the Ceph Dashboard, it is
    time to verify whether the replication actually works:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster's Ceph Dashboard, create an RBD
      image so that its parent pool is the pool that you already created for
      replication purposes. Enable the <code class="option">Exclusive lock</code> and
      <code class="option">Journaling</code> features for the image. Refer to
      <a class="xref" href="#dashboard-rbds-create" title="9.3. Creating RBDs">Section 9.3, “Creating RBDs”</a> for details on how to create RBD
      images.
     </p><div class="figure" id="id-1.3.4.7.13.8.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-image.png" target="_blank"><img src="images/rbd-mirror-image.png" width="" alt="New RBD Image"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.14: </span><span class="title-name">New RBD Image </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.8.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      After you create the image that you want to replicate, open the
      <span class="emphasis"><em>secondary</em></span> cluster's Ceph Dashboard and navigate to
      <span class="guimenu">Block</span> / <span class="guimenu">Mirroring</span>.
      The <span class="guimenu">Pools</span> table on the right will reflect the change
      in the number of <span class="guimenu"># Remote</span> images and synchronize the
      number of <span class="guimenu"># Local</span> images.
     </p><div class="figure" id="id-1.3.4.7.13.8.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-list-1.png" target="_blank"><img src="images/rbd-mirror-pool-list-1.png" width="" alt="New RBD Image Synchronized"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.15: </span><span class="title-name">New RBD Image Synchronized </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.8.3.2.2">#</a></h6></div></div><div id="id-1.3.4.7.13.8.3.2.3" data-id-title="Replication Progress" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Replication Progress</h6><p>
       The <span class="guimenu">Images</span> table at the bottom of the page shows the
       status of replication of RBD images. The <span class="guimenu">Issues</span> tab
       includes possible problems, the <span class="guimenu">Syncing</span> tab displays
       the progress of image replication, and the <span class="guimenu">Ready</span> tab
       lists all images with successful replication.
      </p><div class="figure" id="id-1.3.4.7.13.8.3.2.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-images-status.png" target="_blank"><img src="images/rbd-mirror-images-status.png" width="" alt="RBD Images' Replication Status"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.16: </span><span class="title-name">RBD Images' Replication Status </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.13.8.3.2.3.3">#</a></h6></div></div></div></li><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster, write data to the RBD image.
      On the <span class="emphasis"><em>secondary</em></span> cluster's Ceph Dashboard, navigate to
      <span class="guimenu">Block</span> / <span class="guimenu">Images</span>
      and monitor whether the corresponding image's size is growing as the data
      on the primary cluster is written.
     </p></li></ol></div></div></section></section></section><section class="chapter" id="dash-webui-nfs" data-id-title="Managing NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Managing NFS Ganesha</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs">#</a></h2></div></div></div><div id="id-1.3.4.8.3" data-id-title="More Information on NFS Ganesha" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on NFS Ganesha</h6><p>
   For more general information about NFS Ganesha, refer to
   <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 30. NFS Ganesha: Export Ceph Data via NFS">Chapter 30, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>.
  </p></div><p>
  To list all available NFS exports, click <span class="guimenu">NFS</span> from the main
  menu.
 </p><p>
  The list shows each export's directory, daemon host name, type of storage
  back-end, and access type.
 </p><div class="figure" id="id-1.3.4.8.6"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs.png" target="_blank"><img src="images/oa_nfs.png" width="" alt="List of NFS Exports"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.1: </span><span class="title-name">List of NFS Exports </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.6">#</a></h6></div></div><p>
  To view more detailed information about an NFS export, click its table row in
  the table.
 </p><div class="figure" id="id-1.3.4.8.8"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_status.png" target="_blank"><img src="images/oa_nfs_status.png" width="" alt="NFS Export Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.2: </span><span class="title-name">NFS Export Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.8">#</a></h6></div></div><section class="sect1" id="dash-webui-nfs-create" data-id-title="Adding NFS Exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Adding NFS Exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-create">#</a></h2></div></div></div><p>
   To add a new NFS export, click <span class="guimenu">Add</span> in the top left of the
   exports table and enter the required information.
  </p><div id="id-1.3.4.8.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The following example uses <code class="literal">admin</code> for the Object Gateway User. For
    more information on the permissions for other users, see
    <a class="xref" href="#auth-capabilities" title="19.2.1.2. Authorization and Capabilities">Section 19.2.1.2, “Authorization and Capabilities”</a>.
   </p></div><div class="figure" id="id-1.3.4.8.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_add.png" target="_blank"><img src="images/oa_nfs_add.png" width="" alt="Adding a New NFS Export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.3: </span><span class="title-name">Adding a New NFS Export </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.9.4">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Select one or more NFS Ganesha daemons that will run the export.
    </p></li><li class="step"><p>
     Select a storage back-end—either <span class="guimenu">CephFS</span> or
     <span class="guimenu">Object Gateway</span>.
    </p></li><li class="step"><p>
     Select a user ID and other back-end related options.
    </p></li><li class="step"><p>
     Enter the directory path for the NFS export. If the directory does not
     exist on the server, it will be created.
    </p></li><li class="step"><p>
     Specify other NFS related options, such as supported NFS protocol version,
     pseudo, access type, squashing, or transport protocol.
    </p></li><li class="step"><p>
     If you need to limit access to specific clients only, click <span class="guimenu">Add
     clients</span> and add their IP addresses together with access type and
     squashing options.
    </p></li><li class="step"><p>
     Confirm with <span class="guimenu">Submit</span>.
    </p></li></ol></div></div></section><section class="sect1" id="dash-webui-nfs-delete" data-id-title="Deleting NFS Exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">Deleting NFS Exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-delete">#</a></h2></div></div></div><p>
   To delete an export, click its table row and select
   <span class="guimenu">Delete</span> in the top left of the exports table.
  </p></section><section class="sect1" id="dash-webui-nfs-edit" data-id-title="Editing NFS Exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Editing NFS Exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-edit">#</a></h2></div></div></div><p>
   To edit an existing export, click its table row and click
   <span class="guimenu">Edit</span> in the top left of the exports table.
  </p><p>
   You can then adjust all the details of the NFS export.
  </p><div class="figure" id="id-1.3.4.8.11.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_edit.png" target="_blank"><img src="images/oa_nfs_edit.png" width="" alt="Editing an NFS Export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 10.4: </span><span class="title-name">Editing an NFS Export </span><a title="Permalink" class="permalink" href="#id-1.3.4.8.11.4">#</a></h6></div></div></section></section><section class="chapter" id="dashboard-mds" data-id-title="Managing Ceph File Systems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11 </span><span class="title-name">Managing Ceph File Systems</span> <a title="Permalink" class="permalink" href="#dashboard-mds">#</a></h2></div></div></div><div id="id-1.3.4.9.3" data-id-title="For More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: For More Information</h6><p>
   To find detailed information about CephFS, refer to
   <a class="xref" href="#cha-ceph-cephfs" title="Chapter 28. Clustered File System">Chapter 28, <em>Clustered File System</em></a>.
  </p></div><section class="sect1" id="dashboard-mds-overview" data-id-title="Viewing CephFS Overview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Viewing CephFS Overview</span> <a title="Permalink" class="permalink" href="#dashboard-mds-overview">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Filesystems</span> from the main menu to view the overview
   of configured file systems. The main table shows each file system's name,
   date of creation, and whether it is enabled or not.
  </p><p>
   By clicking on a file system's table row, you reveal details about its rank
   and pools added to the file system.
  </p><div class="figure" id="id-1.3.4.9.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_cephfs_details.png" target="_blank"><img src="images/dash_cephfs_details.png" width="" alt="CephFS Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 11.1: </span><span class="title-name">CephFS Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.9.4.4">#</a></h6></div></div><p>
   At the bottom of the screen, you can see statistics counting the number of
   related MDS inodes and client requests, collected in real time.
  </p></section></section><section class="chapter" id="dashboard-ogw" data-id-title="Managing Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12 </span><span class="title-name">Managing Object Gateways</span> <a title="Permalink" class="permalink" href="#dashboard-ogw">#</a></h2></div></div></div><div id="id-1.3.4.10.3" data-id-title="More Information on Object Gateway" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information on Object Gateway</h6><p>
   For more general information about Object Gateway, refer to
   <a class="xref" href="#cha-ceph-gw" title="Chapter 26. Ceph Object Gateway">Chapter 26, <em>Ceph Object Gateway</em></a>.
  </p></div><section class="sect1" id="dashboard-ogw-view" data-id-title="Viewing Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Viewing Object Gateways</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-view">#</a></h2></div></div></div><p>
   To view a list of configured Object Gateways, click <span class="guimenu">Object
   Gateway</span> / <span class="guimenu">Daemons</span>. The list includes
   the ID of the gateway, host name of the cluster node where the gateway
   daemon is running, and the gateway's version number.
  </p><p>
   Click a gateway's table row to view detailed information about the gateway.
   The <span class="guimenu">Performance Counters</span> tab shows details about
   read/write operations and cache statistics.
  </p><div class="figure" id="id-1.3.4.10.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_details.png" target="_blank"><img src="images/dash_ogw_details.png" width="" alt="Gateway's Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.1: </span><span class="title-name">Gateway's Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.10.4.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-ogw-user" data-id-title="Managing Object Gateway Users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Managing Object Gateway Users</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Object
   Gateway</span> / <span class="guimenu">Users</span> to view a list of
   existing Object Gateway users.
  </p><p>
   Click a user's table row to view details about the user account, such as
   status information or the user and bucket quota details.
  </p><div class="figure" id="id-1.3.4.10.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_users_details.png" target="_blank"><img src="images/dash_ogw_users_details.png" width="" alt="Gateway Users"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.2: </span><span class="title-name">Gateway Users </span><a title="Permalink" class="permalink" href="#id-1.3.4.10.5.4">#</a></h6></div></div><section class="sect2" id="dashboard-ogw-user-create" data-id-title="Adding a New Gateway User"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.1 </span><span class="title-name">Adding a New Gateway User</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-create">#</a></h3></div></div></div><p>
    To add a new gateway user, click <span class="guimenu">Add</span> in the top left of
    the table heading. Fill in their credentials, details about the S3 key and
    user/bucket quota, then confirm with <span class="guimenu">Add</span>.
   </p><div class="figure" id="id-1.3.4.10.5.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_user_add.png" target="_blank"><img src="images/dash_ogw_user_add.png" width="" alt="Adding a New Gateway User"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.3: </span><span class="title-name">Adding a New Gateway User </span><a title="Permalink" class="permalink" href="#id-1.3.4.10.5.5.3">#</a></h6></div></div></section><section class="sect2" id="dashboard-ogw-user-delete" data-id-title="Deleting Gateway Users"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.2 </span><span class="title-name">Deleting Gateway Users</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-delete">#</a></h3></div></div></div><p>
    To delete a gateway user, click the user's table row and select
    <span class="guimenu">Delete</span> in the top left of the table heading. Activate
    the <span class="guimenu">Yes, I am sure</span> check box and confirm with
    <span class="guimenu">Delete User</span>.
   </p></section><section class="sect2" id="dashboard-ogw-user-edit" data-id-title="Editing Gateway User Details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.3 </span><span class="title-name">Editing Gateway User Details</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-edit">#</a></h3></div></div></div><p>
    To change gateway user details, click the user's table row and select
    <span class="guimenu">Edit</span> in the top left of the table heading.
   </p><p>
    Modify basic or additional user information, such as their capabilities,
    keys, sub-users, and quota information. Confirm with
    <span class="guimenu">Update</span>.
   </p><p>
    The <span class="guimenu">Keys</span> tab includes a read-only list of the gateway's
    users and their access and secret keys. To view the keys, click a user name
    in the list and then select <span class="guimenu">Show</span> in the top left of the
    table heading. In the <span class="guimenu">S3 Key</span> dialog, click the 'eye'
    icon to unveil the keys, or click the clipboard icon to copy the related
    key to the clipboard.
   </p></section></section><section class="sect1" id="dashboard-ogw-bucket" data-id-title="Managing the Object Gateway Buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.3 </span><span class="title-name">Managing the Object Gateway Buckets</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket">#</a></h2></div></div></div><p>
   Object Gateway (OGW) buckets implement the functionality of OpenStack Swift
   containers. Object Gateway buckets serve as containers for storing data objects.
  </p><p>
   Click <span class="guimenu">Object
   Gateway</span> / <span class="guimenu">Buckets</span> to view a list of
   OGW buckets.
  </p><section class="sect2" id="dashboard-ogw-bucket-create" data-id-title="Adding a New Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.1 </span><span class="title-name">Adding a New Bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-create">#</a></h3></div></div></div><p>
    To add a new OGW bucket, click <span class="guimenu">Add</span> in the top left of
    the table heading. Enter the bucket's name and select its owner. Confirm
    with <span class="guimenu">Add</span>.
   </p></section><section class="sect2" id="dashboard-ogw-bucket-view" data-id-title="Viewing Bucket Details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.2 </span><span class="title-name">Viewing Bucket Details</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-view">#</a></h3></div></div></div><p>
    To view detailed information about an OGW bucket, click its table row.
   </p><div class="figure" id="id-1.3.4.10.6.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_bucket_details.png" target="_blank"><img src="images/dash_ogw_bucket_details.png" width="" alt="Gateway Bucket Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.4: </span><span class="title-name">Gateway Bucket Details </span><a title="Permalink" class="permalink" href="#id-1.3.4.10.6.5.3">#</a></h6></div></div><div id="id-1.3.4.10.6.5.4" data-id-title="Bucket Quota" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Bucket Quota</h6><p>
     Below the <span class="guimenu">Details</span> table, you can find details about the
     bucket quota settings.
    </p></div></section><section class="sect2" id="dashboard-ogw-bucket-edit" data-id-title="Updating the Owner of a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.3 </span><span class="title-name">Updating the Owner of a Bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-edit">#</a></h3></div></div></div><p>
    Click a bucket table row, then select <span class="guimenu">Edit</span> in the top
    left of the table heading.
   </p><p>
    Update the owner of the bucket and confirm with <span class="guimenu">Update</span>.
   </p></section><section class="sect2" id="dashboard-ogw-bucket-delete" data-id-title="Deleting a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.4 </span><span class="title-name">Deleting a Bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-delete">#</a></h3></div></div></div><p>
    To delete an OGW bucket, click its table row and select
    <span class="guimenu">Delete</span> in the top left of the table heading. Activate
    the <span class="guimenu">Yes, I am sure</span> check box, and confirm with
    <span class="guimenu">Delete bucket</span>.
   </p></section></section></section><section class="chapter" id="dashboard-initial-configuration" data-id-title="Manual Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13 </span><span class="title-name">Manual Configuration</span> <a title="Permalink" class="permalink" href="#dashboard-initial-configuration">#</a></h2></div></div></div><p>
  This section introduces advanced information for users that prefer
  configuring dashboard's settings manually on the command line.
 </p><section class="sect1" id="dashboard-ssl" data-id-title="TLS/SSL Support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">TLS/SSL Support</span> <a title="Permalink" class="permalink" href="#dashboard-ssl">#</a></h2></div></div></div><p>
   All HTTP connections to the dashboard are secured with SSL/TLS by default. A
   secure connection requires an SSL certificate. You can either use a
   self-signed certificate, or generate a certificate and have a well known
   certificate authority (CA) sign it.
  </p><div id="id-1.3.4.11.4.3" data-id-title="Disabling SSL" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling SSL</h6><p>
    You may want to disable the SSL support for a specific reason. For example,
    if the dashboard is running behind a proxy that does not support SSL.
   </p><p>
    Use caution when disabling SSL as <span class="bold"><strong>user names and
    passwords</strong></span> will be sent to the dashboard
    <span class="bold"><strong>unencrypted</strong></span>.
   </p><p>
    To disable SSL, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/ssl false</pre></div></div><div id="id-1.3.4.11.4.4" data-id-title="Restart Ceph Manager Processes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Restart Ceph Manager Processes</h6><p>
    You need to restart the Ceph Manager processes manually after changing the SSL
    certificate and key. You can do so by either running
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr fail<em class="replaceable">ACTIVE-MANAGER-NAME</em></pre></div><p>
    or by disabling and re-enabling the dashboard module, which also triggers
    the manager to respawn itself:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module disable dashboard
<code class="prompt user">cephadm@adm &gt; </code>ceph mgr module enable dashboard</pre></div></div><section class="sect2" id="self-sign-certificates" data-id-title="Self-signed Certificates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.1 </span><span class="title-name">Self-signed Certificates</span> <a title="Permalink" class="permalink" href="#self-sign-certificates">#</a></h3></div></div></div><p>
    Creating a self-signed certificate for secure communication is simple. This
    way you can get the dashboard running quickly.
   </p><div id="id-1.3.4.11.4.5.3" data-id-title="Web Browsers Complain" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Web Browsers Complain</h6><p>
     Most Web browsers will complain about a self-signed certificate and
     require explicit confirmation before establishing a secure connection to
     the dashboard.
    </p></div><p>
    To generate and install a self-signed certificate, use the following
    built-in command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard create-self-signed-cert</pre></div></section><section class="sect2" id="self-sign-certificates-openssl" data-id-title="Self-signed or Trusted Third-party Certificate with OpenSSL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.2 </span><span class="title-name">Self-signed or Trusted Third-party Certificate with OpenSSL</span> <a title="Permalink" class="permalink" href="#self-sign-certificates-openssl">#</a></h3></div></div></div><p>
    OpenSSL is an open-source command-line tool that is commonly used to
    generate private keys, create Certificate Signing Requests (CSR), install
    your SSL/TLS certificate, and identify certificate information. The
    following instructions illustrate how to generate a self-signed or trusted
    third-party certificate using OpenSSL:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Generate a Private Key:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl genrsa -des3 -out server.key 2048</pre></div><p>
      Type the passphrase to protect the key.
     </p></li><li class="step"><p>
      Generate a CSR:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -new -key server.key -out server.csr</pre></div><p>
      Enter the passphrase, and fill in the <code class="literal">Country Name</code>,
      <code class="literal">State or Province Name</code>, <code class="literal">Locality
      Name</code>, <code class="literal">Organization Name</code>,
      <code class="literal">Organizational Unit Name</code>, <code class="literal">Common
      Name</code>, <code class="literal">Email Address</code>.
     </p><div id="id-1.3.4.11.4.6.3.2.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The <code class="literal">Common Name</code> should be the FQDN of the server. For
       example, <code class="literal">server.mydomain.com</code>.
      </p></div><p>
      When asked for a <code class="literal">challenge</code> password and optional
      company name, leave it blank.
     </p></li><li class="step"><p>
      To sign the certificate, select from the following options:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p><span class="formalpara-title">Trusted Third-party Certificate Authority. </span>
         Send the CSR to the third party for their signing. The following files
         should be received: Server certificate (public key) and the
         Intermediate CA and the bundles that chain to the Trusted Root CA.
        </p></li><li class="listitem"><p><span class="formalpara-title">Self-signed. </span>
         Sign the certificate with OpenSSL:
        </p><div class="verbatim-wrap"><pre class="screen">openssl x509 -req <span class="strong"><strong>-days 730</strong></span> -in server.csr -signkey server.key -out server.crt</pre></div><p>
        Increase or decrease the value <em class="replaceable">730</em> as
        needed. This is the number of days for which the certificate is valid.
       </p></li></ul></div></li><li class="step"><p><span class="step-optional">(Optional)</span> 
      If needed, create a concatenated PEM file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout key.pem -out cert.pem</pre></div></li></ol></div></div></section><section class="sect2" id="cert-sign-CA" data-id-title="Certificates Signed by CA"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.3 </span><span class="title-name">Certificates Signed by CA</span> <a title="Permalink" class="permalink" href="#cert-sign-CA">#</a></h3></div></div></div><p>
    To properly secure the connection to the dashboard and to eliminate Web
    browser complaints about a self-signed certificate, we recommend using a
    certificate that is signed by a CA.
   </p><p>
    You can generate a certificate key pair with a command similar to the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>openssl req -new -nodes -x509 \
  -subj "/O=IT/CN=ceph-mgr-dashboard" -days 3650 \
  -keyout dashboard.key -out dashboard.crt -extensions v3_ca</pre></div><p>
    The above command outputs <code class="filename">dashboard.key</code> and
    <code class="filename">dashboard.crt</code> files. After you get the
    <code class="filename">dashboard.crt</code> file signed by a CA, enable it for all
    Ceph Manager instances by running the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-ssl-certificate -i dashboard.crt
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-ssl-certificate-key -i dashboard.key</pre></div><div id="id-1.3.4.11.4.7.7" data-id-title="Different Certificates for Each Manager Instance" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Different Certificates for Each Manager Instance</h6><p>
     If you require different certificates for each Ceph Manager instance, modify the
     commands and include the name of the instance as follows. Replace
     <em class="replaceable">NAME</em> with the name of the Ceph Manager instance
     (usually the related host name):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-ssl-certificate <em class="replaceable">NAME</em> -i dashboard.crt
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-ssl-certificate-key <em class="replaceable">NAME</em> -i dashboard.key</pre></div></div></section><section class="sect2" id="cert-sign-custom-CA" data-id-title="Certificates Signed with a Custom CA"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.4 </span><span class="title-name">Certificates Signed with a Custom CA</span> <a title="Permalink" class="permalink" href="#cert-sign-custom-CA">#</a></h3></div></div></div><p>
    The following procedure needs to be followed once to create the root CA.
   </p><div id="id-1.3.4.11.4.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This is the key used to sign the certificate requests. Anyone holding this
     can sign certificates on your behalf.
    </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the Root Key:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl genrsa -des3 -out rootCA.key 4096</pre></div><div id="id-1.3.4.11.4.8.4.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If you want a non-password protected key, remove the
       <code class="option">-des3</code> option.
      </p></div></li><li class="step"><p>
      Create and self-sign the root certificate:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.crt</pre></div></li></ol></div></div><p>
    The following procedure needs to be followed for each server that needs a
    trusted certificate from our CA.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create the certificate key:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl genrsa -out mydomain.com.key 2048</pre></div><p>
      The certificate signing request is where you specify the details for the
      certificate you want to generate. This request is processed by the owner
      of the Root Key to generate the certificate.
     </p></li><li class="step"><p>
      These are two ways to create the CSR:
     </p><div id="id-1.3.4.11.4.8.6.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       When creating the certificate signing request, it is important to
       specify the <code class="literal">Common Name</code> providing the IP address or
       domain name for the service, otherwise the certificate cannot be
       verified.
      </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Interactive method. For example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -new -key mydomain.com.key -out mydomain.com.csr</pre></div><p>
        You will then be prmopted for information. For example, the
        <code class="literal">Country Name</code>, <code class="literal">Organization Name</code>,
        and <code class="literal">Email Address</code>.
       </p></li><li class="listitem"><p>
        One-liner method. This is where instead of being interactively
        prompted, you include the information up front. For example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -new -sha256 -key mydomain.com.key -subj "/C=US/ST=CA/O=MyOrg, Inc./CN=mydomain.com" -out mydomain.com.csr</pre></div><p>
        If you need to pass additional configuration in the one-liner method,
        you can use the <code class="option">-config</code> parameter. For example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -new -sha256 \
      -key mydomain.com.key \
      -subj "/C=US/ST=CA/O=MyOrg, Inc./CN=mydomain.com" \
      -reqexts SAN \
      -config &lt;(cat /etc/ssl/openssl.cnf \
          &lt;(printf "\n[SAN]\nsubjectAltName=DNS:mydomain.com,DNS:www.mydomain.com")) \
      -out mydomain.com.csr</pre></div></li></ul></div></li><li class="step"><p>
      Verify the CSR content:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl req -in mydomain.com.csr -noout -text</pre></div></li><li class="step"><p>
      Generate the certificate using the <code class="literal">mydomain</code> CSR and
      key along with the CA Root Key:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl x509 -req -in mydomain.com.csr -CA rootCA.crt -CAkey rootCA.key -CAcreateserial -out mydomain.com.crt -days 500 -sha256</pre></div></li><li class="step"><p>
      Verify the certificate's content:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>openssl x509 -in mydomain.com.crt -text -noout</pre></div></li></ol></div></div></section></section><section class="sect1" id="dashboard-hostname-port" data-id-title="Host Name and Port Number"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.2 </span><span class="title-name">Host Name and Port Number</span> <a title="Permalink" class="permalink" href="#dashboard-hostname-port">#</a></h2></div></div></div><p>
   The Ceph Dashboard Web application binds to a specific TCP/IP address and TCP
   port. By default, the currently active Ceph Manager that hosts the dashboard binds
   to TCP port 8443 (or 8080 when SSL is disabled).
  </p><p>
   The dashboard Web application binds to "::" by default, which corresponds to
   all available IPv4 and IPv6 addresses. You can change the IP address and
   port number of the Web application so that they apply to all Ceph Manager instances
   by using the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/server_addr <em class="replaceable">IP_ADDRESS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/server_port <em class="replaceable">PORT_NUMBER</em></pre></div><div id="id-1.3.4.11.5.5" data-id-title="Configure Ceph Manager Instances Separately" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Configure Ceph Manager Instances Separately</h6><p>
    Since each <code class="systemitem">ceph-mgr</code> daemon hosts
    its own instance of the dashboard, you may need to configure them
    separately. Change the IP address and port number for a specific manager
    instance by using the following commands (replace
    <em class="replaceable">NAME</em> with the ID of the
    <code class="systemitem">ceph-mgr</code> instance):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/<em class="replaceable">NAME</em>/server_addr <em class="replaceable">IP_ADDRESS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/<em class="replaceable">NAME</em>/server_port <em class="replaceable">PORT_NUMBER</em></pre></div></div><div id="id-1.3.4.11.5.6" data-id-title="List Configured Endpoints" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: List Configured Endpoints</h6><p>
    The <code class="command">ceph mgr services</code> command displays all endpoints
    that are currently configured. Look for the 'dashboard' key to obtain the
    URL for accessing the dashboard.
   </p></div></section><section class="sect1" id="dashboard-username-password" data-id-title="User Name and Password"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.3 </span><span class="title-name">User Name and Password</span> <a title="Permalink" class="permalink" href="#dashboard-username-password">#</a></h2></div></div></div><p>
   If you do not want to use the default administrator account, create a
   different user account and associate it with at least one role. We provide a
   set of predefined system roles that you can use. For more details refer to
   <a class="xref" href="#dashboard-user-roles" title="Chapter 14. Managing Users and Roles on the Command Line">Chapter 14, <em>Managing Users and Roles on the Command Line</em></a>.
  </p><p>
   To create a user with administrator privileges, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-create <em class="replaceable">USER_NAME</em> <em class="replaceable">PASSWORD</em> administrator</pre></div></section><section class="sect1" id="dashboard-ogw-enabling" data-id-title="Enabling the Object Gateway Management Front-end"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.4 </span><span class="title-name">Enabling the Object Gateway Management Front-end</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-enabling">#</a></h2></div></div></div><p>
   To use the Object Gateway management functionality of the dashboard, you need to
   provide the login credentials of a user with the 'system' flag enabled:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If you do not have a user with the 'system' flag, create one:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USER_ID</em> --display-name=<em class="replaceable">DISPLAY_NAME</em> --system</pre></div><p>
     Take note of the 'access_key' and 'secret_key' keys in the output of the
     command.
    </p></li><li class="step"><p>
     You can also obtain the credentials of an existing user by using the
     <code class="command">radosgw-admin</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user info --uid=<em class="replaceable">USER_ID</em></pre></div></li><li class="step"><p>
     Provide the received credentials to the dashboard:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-access-key <em class="replaceable">ACCESS_KEY</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-secret-key <em class="replaceable">SECRET_KEY</em></pre></div></li></ol></div></div><p>
   There are several points to consider:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The host name and port number of the Object Gateway are determined automatically.
    </p></li><li class="listitem"><p>
     If multiple zones are used, it will automatically determine the host
     within the master zonegroup and master zone. This is sufficient for most
     setups, but in some circumstances you may want to set the host name and
     port manually:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-host <em class="replaceable">HOST</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-port <em class="replaceable">PORT</em></pre></div></li><li class="listitem"><p>
     These are additional settings that you may need:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-scheme <em class="replaceable">SCHEME</em>  # http or https
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-admin-resource <em class="replaceable">ADMIN_RESOURCE</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-user-id <em class="replaceable">USER_ID</em></pre></div></li><li class="listitem"><p>
     If you are using a self-signed certificate
     (<a class="xref" href="#dashboard-ssl" title="13.1. TLS/SSL Support">Section 13.1, “TLS/SSL Support”</a>) in your Object Gateway setup, disable
     certificate verification in the dashboard to avoid refused connections
     caused by certificates signed by an unknown CA or not matching the host
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rgw-api-ssl-verify False</pre></div></li><li class="listitem"><p>
     If the Object Gateway takes too long to process requests and the dashboard runs
     into timeouts, the timeout value can be adjusted (default is 45 seconds):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-rest-requests-timeout <em class="replaceable">SECONDS</em></pre></div></li></ul></div></section><section class="sect1" id="dashboard-sso" data-id-title="Enable Single Sign-On"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.5 </span><span class="title-name">Enable Single Sign-On</span> <a title="Permalink" class="permalink" href="#dashboard-sso">#</a></h2></div></div></div><p>
   <span class="emphasis"><em>Single Sign-On</em></span> (SSO) is an access control method that enables
   users to log in with a single ID and password to multiple applications
   simultaneously.
  </p><p>
   The Ceph Dashboard supports external authentication of users via the SAML 2.0
   protocol. Because <span class="emphasis"><em>authorization</em></span> is still performed by
   the dashboard, you first need to create user accounts and associate them
   with the desired roles. However, the <span class="emphasis"><em>authentication</em></span>
   process can be performed by an existing <span class="emphasis"><em>Identity
   Provider</em></span> (IdP).
  </p><p>
   To configure Single Sign-On, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard sso setup saml2 <em class="replaceable">CEPH_DASHBOARD_BASE_URL</em> \
 <em class="replaceable">IDP_METADATA</em> <em class="replaceable">IDP_USERNAME_ATTRIBUTE</em> \
 <em class="replaceable">IDP_ENTITY_ID</em> <em class="replaceable">SP_X_509_CERT</em> \
 <em class="replaceable">SP_PRIVATE_KEY</em></pre></div><p>
   Parameters:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.11.8.7.1"><span class="term"><em class="replaceable">CEPH_DASHBOARD_BASE_URL</em></span></dt><dd><p>
      Base URL where Ceph Dashboard is accessible (for example,
      'https://cephdashboard.local').
     </p></dd><dt id="id-1.3.4.11.8.7.2"><span class="term"><em class="replaceable">IDP_METADATA</em></span></dt><dd><p>
      URL, file path, or content of the IdP metadata XML (for example,
      'https://myidp/metadata').
     </p></dd><dt id="id-1.3.4.11.8.7.3"><span class="term"><em class="replaceable">IDP_USERNAME_ATTRIBUTE</em></span></dt><dd><p>
      Optional. Attribute that will be used to get the user name from the
      authentication response. Defaults to 'uid'.
     </p></dd><dt id="id-1.3.4.11.8.7.4"><span class="term"><em class="replaceable">IDP_ENTITY_ID</em></span></dt><dd><p>
      Optional. Use when more than one entity ID exists on the IdP metadata.
     </p></dd><dt id="id-1.3.4.11.8.7.5"><span class="term"><em class="replaceable">SP_X_509_CERT</em> / <em class="replaceable">SP_PRIVATE_KEY</em></span></dt><dd><p>
      Optional. File path or content of the certificate that will be used by
      Ceph Dashboard (Service Provider) for signing and encryption.
     </p></dd></dl></div><div id="id-1.3.4.11.8.8" data-id-title="SAML Requests" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: SAML Requests</h6><p>
    The issuer value of SAML requests will follow this pattern:
   </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CEPH_DASHBOARD_BASE_URL</em>/auth/saml2/metadata</pre></div></div><p>
   To display the current SAML 2.0 configuration, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard sso show saml2</pre></div><p>
   To disable Single Sign-On, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard sso disable</pre></div><p>
   To check if SSO is enabled, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard sso status</pre></div><p>
   To enable SSO, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard sso enable saml2</pre></div></section></section><section class="chapter" id="dashboard-user-roles" data-id-title="Managing Users and Roles on the Command Line"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14 </span><span class="title-name">Managing Users and Roles on the Command Line</span> <a title="Permalink" class="permalink" href="#dashboard-user-roles">#</a></h2></div></div></div><p>
  This section describes how to manage user accounts used by the Ceph Dashboard.
  It helps you create or modify user accounts, as well as set proper user roles
  and permissions.
 </p><section class="sect1" id="dashboard-user-accounts" data-id-title="User Accounts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.1 </span><span class="title-name">User Accounts</span> <a title="Permalink" class="permalink" href="#dashboard-user-accounts">#</a></h2></div></div></div><p>
   The Ceph Dashboard supports managing multiple user accounts. Each user account
   consists of a user name, a password (stored in encrypted form using
   <code class="literal">bcrypt</code>), an optional name, and an optional e-mail
   address.
  </p><p>
   User accounts are stored in Ceph Monitor’s configuration database and are
   globally shared across all Ceph Manager instances.
  </p><p>
   Use the following commands to manage user accounts:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.4.5.1"><span class="term">Show existing users:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-show [<em class="replaceable">USERNAME</em>]</pre></div></dd><dt id="id-1.3.4.12.4.5.2"><span class="term">Create a new user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-create <em class="replaceable">USERNAME</em> [<em class="replaceable">PASSWORD</em>] [<em class="replaceable">ROLENAME</em>] [<em class="replaceable">NAME</em>] [<em class="replaceable">EMAIL</em>]</pre></div></dd><dt id="id-1.3.4.12.4.5.3"><span class="term">Delete a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-delete <em class="replaceable">USERNAME</em></pre></div></dd><dt id="id-1.3.4.12.4.5.4"><span class="term">Change a user's password:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-set-password <em class="replaceable">USERNAME</em> <em class="replaceable">PASSWORD</em></pre></div></dd><dt id="id-1.3.4.12.4.5.5"><span class="term">Modify a user's name and email:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-set-info <em class="replaceable">USERNAME</em> <em class="replaceable">NAME</em> <em class="replaceable">EMAIL</em></pre></div></dd></dl></div></section><section class="sect1" id="dashboard-permissions" data-id-title="User Roles and Permissions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.2 </span><span class="title-name">User Roles and Permissions</span> <a title="Permalink" class="permalink" href="#dashboard-permissions">#</a></h2></div></div></div><p>
   This section describes what security scopes you can assign to a user role,
   how to manage user roles and assign them to user accounts.
  </p><section class="sect2" id="id-1.3.4.12.5.3" data-id-title="Security Scopes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.2.1 </span><span class="title-name">Security Scopes</span> <a title="Permalink" class="permalink" href="#id-1.3.4.12.5.3">#</a></h3></div></div></div><p>
    User accounts are associated with a set of roles that define which parts of
    the dashboard can be accessed by the user. The dashboard parts are grouped
    within a <span class="emphasis"><em>security</em></span> scope. Security scopes are
    predefined and static. The following security scopes are currently
    available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.5.3.3.1"><span class="term">hosts</span></dt><dd><p>
       Includes all features related to the <span class="guimenu">Hosts</span> menu
       entry.
      </p></dd><dt id="id-1.3.4.12.5.3.3.2"><span class="term">config-opt</span></dt><dd><p>
       Includes all features related to the management of Ceph configuration
       options.
      </p></dd><dt id="id-1.3.4.12.5.3.3.3"><span class="term">pool</span></dt><dd><p>
       Includes all features related to pool management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.4"><span class="term">osd</span></dt><dd><p>
       Includes all features related to the Ceph OSD management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.5"><span class="term">monitor</span></dt><dd><p>
       Includes all features related to the Ceph Monitor management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.6"><span class="term">rbd-image</span></dt><dd><p>
       Includes all features related to the RADOS Block Device image management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.7"><span class="term">rbd-mirroring</span></dt><dd><p>
       Includes all features related to the RADOS Block Device mirroring management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.8"><span class="term">iscsi</span></dt><dd><p>
       Includes all features related to iSCSI management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.9"><span class="term">rgw</span></dt><dd><p>
       Includes all features related to the Object Gateway management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.10"><span class="term">cephfs</span></dt><dd><p>
       Includes all features related to CephFS management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.11"><span class="term">manager</span></dt><dd><p>
       Includes all features related to the Ceph Manager management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.12"><span class="term">log</span></dt><dd><p>
       Includes all features related to Ceph logs management.
      </p></dd><dt id="id-1.3.4.12.5.3.3.13"><span class="term">grafana</span></dt><dd><p>
       Includes all features related to the Grafana proxy.
      </p></dd><dt id="id-1.3.4.12.5.3.3.14"><span class="term">dashboard-settings</span></dt><dd><p>
       Allows changing dashboard settings.
      </p></dd></dl></div></section><section class="sect2" id="id-1.3.4.12.5.4" data-id-title="User Roles"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.2.2 </span><span class="title-name">User Roles</span> <a title="Permalink" class="permalink" href="#id-1.3.4.12.5.4">#</a></h3></div></div></div><p>
    A <span class="emphasis"><em>role</em></span> specifies a set of mappings between a
    <span class="emphasis"><em>security scope</em></span> and a set of
    <span class="emphasis"><em>permissions</em></span>. There are four types of permissions:
    'read', 'create', 'update', and 'delete'.
   </p><p>
    The following example specifies a role where a user has 'read' and 'create'
    permissions for features related to pool management, and has full
    permissions for features related to RBD image management:
   </p><div class="verbatim-wrap"><pre class="screen">{
  'role': 'my_new_role',
  'description': 'My new role',
  'scopes_permissions': {
    'pool': ['read', 'create'],
    'rbd-image': ['read', 'create', 'update', 'delete']
  }
}</pre></div><p>
    The dashboard already provides a set of predefined roles that we call
    <span class="emphasis"><em>system roles</em></span>. You can instantly use them after a fresh
    Ceph Dashboard installation:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.5.4.6.1"><span class="term">administrator</span></dt><dd><p>
       Provides full permissions for all security scopes.
      </p></dd><dt id="id-1.3.4.12.5.4.6.2"><span class="term">read-only</span></dt><dd><p>
       Provides read permission for all security scopes except the dashboard
       settings.
      </p></dd><dt id="id-1.3.4.12.5.4.6.3"><span class="term">block-manager</span></dt><dd><p>
       Provides full permissions for 'rbd-image', 'rbd-mirroring', and 'iscsi'
       scopes.
      </p></dd><dt id="id-1.3.4.12.5.4.6.4"><span class="term">rgw-manager</span></dt><dd><p>
       Provides full permissions for the 'rgw' scope.
      </p></dd><dt id="id-1.3.4.12.5.4.6.5"><span class="term">cluster-manager</span></dt><dd><p>
       Provides full permissions for the 'hosts', 'osd', 'monitor', 'manager',
       and 'config-opt' scopes.
      </p></dd><dt id="id-1.3.4.12.5.4.6.6"><span class="term">pool-manager</span></dt><dd><p>
       Provides full permissions for the 'pool' scope.
      </p></dd><dt id="id-1.3.4.12.5.4.6.7"><span class="term">cephfs-manager</span></dt><dd><p>
       Provides full permissions for the 'cephfs' scope.
      </p></dd></dl></div><section class="sect3" id="id-1.3.4.12.5.4.7" data-id-title="Managing Custom Roles"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">14.2.2.1 </span><span class="title-name">Managing Custom Roles</span> <a title="Permalink" class="permalink" href="#id-1.3.4.12.5.4.7">#</a></h4></div></div></div><p>
     You can create new user roles by using the following commands:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.5.4.7.3.1"><span class="term">Create a new role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-create <em class="replaceable">ROLENAME</em> [<em class="replaceable">DESCRIPTION</em>]</pre></div></dd><dt id="id-1.3.4.12.5.4.7.3.2"><span class="term">Delete a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-delete <em class="replaceable">ROLENAME</em></pre></div></dd><dt id="id-1.3.4.12.5.4.7.3.3"><span class="term">Add scope permissions to a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-add-scope-perms <em class="replaceable">ROLENAME</em> <em class="replaceable">SCOPENAME</em> <em class="replaceable">PERMISSION</em> [<em class="replaceable">PERMISSION</em>...]</pre></div></dd><dt id="id-1.3.4.12.5.4.7.3.4"><span class="term">Delete scope permissions from a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-del-perms <em class="replaceable">ROLENAME</em> <em class="replaceable">SCOPENAME</em></pre></div></dd></dl></div></section><section class="sect3" id="id-1.3.4.12.5.4.8" data-id-title="Assigning Roles to User Accounts"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">14.2.2.2 </span><span class="title-name">Assigning Roles to User Accounts</span> <a title="Permalink" class="permalink" href="#id-1.3.4.12.5.4.8">#</a></h4></div></div></div><p>
     Use the following commands to assign roles to users:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.5.4.8.3.1"><span class="term">Set user roles:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-set-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd><dt id="id-1.3.4.12.5.4.8.3.2"><span class="term">Add additional roles to a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-add-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd><dt id="id-1.3.4.12.5.4.8.3.3"><span class="term">Delete roles from a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-del-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd></dl></div><div id="id-1.3.4.12.5.4.8.4" data-id-title="Purging Custom Roles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Purging Custom Roles</h6><p>
      If you create custom user roles and intend to remove the Ceph cluster
      with the <code class="command">ceph.purge</code> runner later on, you need to purge
      the custom roles first. Find more details in
      <a class="xref" href="#deepsea-ceph-purge" title="2.17. Removing an Entire Ceph Cluster">Section 2.17, “Removing an Entire Ceph Cluster”</a>.
     </p></div></section><section class="sect3" id="id-1.3.4.12.5.4.9" data-id-title="Example: Creating a User and a Custom Role"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">14.2.2.3 </span><span class="title-name">Example: Creating a User and a Custom Role</span> <a title="Permalink" class="permalink" href="#id-1.3.4.12.5.4.9">#</a></h4></div></div></div><p>
     This section illustrates a procedure for creating a user account capable
     of managing RBD images, viewing and creating Ceph pools, and having
     read-only access to any other scopes.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a new user named 'tux':
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-create tux <em class="replaceable">PASSWORD</em></pre></div></li><li class="step"><p>
       Create a role and specify scope permissions:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-create rbd/pool-manager
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-add-scope-perms rbd/pool-manager \
 rbd-image read create update delete
<code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-role-add-scope-perms rbd/pool-manager pool read create</pre></div></li><li class="step"><p>
       Associate the roles with the 'tux' user:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard ac-user-set-roles tux rbd/pool-manager read-only</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="dashboard-proxy" data-id-title="Reverse Proxies"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.3 </span><span class="title-name">Reverse Proxies</span> <a title="Permalink" class="permalink" href="#dashboard-proxy">#</a></h2></div></div></div><p>
   If you are accessing the dashboard via a reverse proxy configuration, you
   may need to service it under a URL prefix. To get the dashboard to use
   hyperlinks that include your prefix, you can set the
   <code class="option">url_prefix</code> setting:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/dashboard/url_prefix <em class="replaceable">URL_PREFIX</em></pre></div><p>
   Then you can access the dashboard at
   <code class="literal">http://<em class="replaceable">HOST_NAME</em>:<em class="replaceable">PORT_NUMBER</em>/<em class="replaceable">URL_PREFIX</em>/</code>.
  </p></section><section class="sect1" id="dashboard-auditing" data-id-title="Auditing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.4 </span><span class="title-name">Auditing</span> <a title="Permalink" class="permalink" href="#dashboard-auditing">#</a></h2></div></div></div><p>
   The Ceph Dashboard's REST API can log PUT, POST, and DELETE requests to the
   Ceph audit log. Logging is disabled by default, but you can enable it with
   the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-audit-api-enabled true</pre></div><p>
   If enabled, the following parameters are logged per each request:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.12.7.5.1"><span class="term">from</span></dt><dd><p>
      The origin of the request, for example 'https://[::1]:44410'.
     </p></dd><dt id="id-1.3.4.12.7.5.2"><span class="term">path</span></dt><dd><p>
      The REST API path, for example '/api/auth'.
     </p></dd><dt id="id-1.3.4.12.7.5.3"><span class="term">method</span></dt><dd><p>
      'PUT', 'POST', or 'DELETE'.
     </p></dd><dt id="id-1.3.4.12.7.5.4"><span class="term">user</span></dt><dd><p>
      The name of the user (or ‘None’).
     </p></dd></dl></div><p>
   An example log entry looks like this:
  </p><div class="verbatim-wrap"><pre class="screen">2019-02-06 10:33:01.302514 mgr.x [INF] [DASHBOARD] \
 from='https://[::ffff:127.0.0.1]:37022' path='/api/rgw/user/exu' method='PUT' \
 user='admin' params='{"max_buckets": "1000", "display_name": "Example User", "uid": "exu", "suspended": "0", "email": "user@example.com"}'</pre></div><div id="id-1.3.4.12.7.8" data-id-title="Disable Logging of Request Payload" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disable Logging of Request Payload</h6><p>
    The logging of the request payload (the list of arguments and their values)
    is enabled by default. You can disable it as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-audit-api-log-payload false</pre></div></div></section></section></div><div class="part" id="part-operate" data-id-title="Operating a Cluster"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Operating a Cluster </span><a title="Permalink" class="permalink" href="#part-operate">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">15 </span><span class="title-name">Introduction</span></a></span></li><dd class="toc-abstract"><p>
  In this part of the manual you will learn how to start or stop Ceph
  services, monitor a cluster's state, use and modify CRUSH Maps, or manage
  storage pools.
 </p></dd><li><span class="chapter"><a href="#ceph-operating-services"><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span></a></span></li><dd class="toc-abstract"><p>
  You can operate Ceph services either using <code class="systemitem">systemd</code> or using DeepSea.
 </p></dd><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span></a></span></li><dd class="toc-abstract"><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups, and Metadata Servers.
 </p></dd><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span></a></span></li><dd class="toc-abstract"><p>In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting stack on the Salt master. Users have to define the Prometheus role for Prometheus and Alertmanager, and the Grafana role for Grafana. When multiple nodes are assigned with the Prometheus or Grafana role, a highly avail…</p></dd><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span></li><dd class="toc-abstract"><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p></dd><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">20 </span><span class="title-name">Stored Data Management</span></a></span></li><dd class="toc-abstract"><p>The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OSDs directly rather than through a centralized server or broker. With an algorithmically determined method of storing and retrieving data, Ceph avoids a…</p></dd><li><span class="chapter"><a href="#cha-mgr-modules"><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span></a></span></li><dd class="toc-abstract"><p>The architecture of the Ceph Manager (refer to Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.2.3 “Ceph Nodes and Daemons” for a brief introduction) allows extending its functionality via modules, such as 'dashboard' (see Part II, “Ceph Dashboard”), 'prometheus' (…</p></dd><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span></a></span></li><dd class="toc-abstract"><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p></dd><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span></a></span></li><dd class="toc-abstract"><p>A block is a sequence of bytes, for example a 4 MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, such as hard disks, CDs, floppy disks. The ubiquity of block device interfaces makes a virtual block device an ideal candidate to interact with …</p></dd><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span></a></span></li><dd class="toc-abstract"><p>Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality of replicated pools (for example, they cannot store metadata for RBD pools), but require less raw storage. A default erasure pool capable of s…</p></dd><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span></a></span></li><dd class="toc-abstract"><p>
  This chapter provides a list of important Ceph cluster settings and their
  description. The settings are sorted by topic.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-operating" data-id-title="Introduction"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15 </span><span class="title-name">Introduction</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating">#</a></h2></div></div></div><p>
  In this part of the manual you will learn how to start or stop Ceph
  services, monitor a cluster's state, use and modify CRUSH Maps, or manage
  storage pools.
 </p><p>
  It also includes advanced topics, for example how to manage users and
  authentication in general, how to manage pool and RADOS device snapshots, how
  to set up erasure coded pools, or how to increase the cluster performance
  with cache tiering.
 </p></section><section class="chapter" id="ceph-operating-services" data-id-title="Operating Ceph Services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16 </span><span class="title-name">Operating Ceph Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services">#</a></h2></div></div></div><p>
  You can operate Ceph services either using <code class="systemitem">systemd</code> or using DeepSea.
 </p><section class="sect1" id="operate-services-systemd" data-id-title="Operating Ceph Cluster Related Services Using systemd"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.1 </span><span class="title-name">Operating Ceph Cluster Related Services Using <code class="systemitem">systemd</code></span> <a title="Permalink" class="permalink" href="#operate-services-systemd">#</a></h2></div></div></div><p>
   Use the <code class="command">systemctl</code> command to operate all Ceph related
   services. The operation takes place on the node you are currently logged in
   to. You need to have <code class="systemitem">root</code> privileges to be able to operate on Ceph
   services.
  </p><section class="sect2" id="ceph-operating-services-targets" data-id-title="Starting, Stopping, and Restarting Services Using Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.1.1 </span><span class="title-name">Starting, Stopping, and Restarting Services Using Targets</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-targets">#</a></h3></div></div></div><p>
    To simplify starting, stopping, and restarting all the services of a
    particular type (for example all Ceph services, or all MONs, or all OSDs)
    on a node, Ceph provides the following <code class="systemitem">systemd</code> unit files:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</pre></div><p>
    To start/stop/restart all Ceph services on the node, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph.target
<code class="prompt user">root # </code>systemctl stop ceph.target
<code class="prompt user">root # </code>systemctl restart ceph.target</pre></div><p>
    To start/stop/restart all OSDs on the node, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd.target
<code class="prompt user">root # </code>systemctl stop ceph-osd.target
<code class="prompt user">root # </code>systemctl restart ceph-osd.target</pre></div><p>
    Commands for the other targets are analogous.
   </p></section><section class="sect2" id="ceph-operating-services-individual" data-id-title="Starting, Stopping, and Restarting Individual Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.1.2 </span><span class="title-name">Starting, Stopping, and Restarting Individual Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-individual">#</a></h3></div></div></div><p>
    You can operate individual services using the following parameterized
    <code class="systemitem">systemd</code> unit files:
   </p><div class="verbatim-wrap"><pre class="screen">ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-mgr@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service</pre></div><p>
    To use these commands, you first need to identify the name of the service
    you want to operate. See
    <a class="xref" href="#ceph-operating-services-finding-names" title="16.1.3. Identifying Individual Services">Section 16.1.3, “Identifying Individual Services”</a> to learn more about
    services identification.
   </p><p>
    To start, stop or restart the <code class="literal">osd.1</code> service, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd@1.service
<code class="prompt user">root # </code>systemctl stop ceph-osd@1.service
<code class="prompt user">root # </code>systemctl restart ceph-osd@1.service</pre></div><p>
    Commands for the other service types are analogous.
   </p></section><section class="sect2" id="ceph-operating-services-finding-names" data-id-title="Identifying Individual Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.1.3 </span><span class="title-name">Identifying Individual Services</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-finding-names">#</a></h3></div></div></div><p>
    You can find out the names/numbers of a particular type of service in
    several ways. The following commands provide results for
    <code class="literal">ceph*</code> services. You can run them on any node of the
    Ceph cluster.
   </p><p>
    To list all (even inactive) services of type <code class="literal">ceph*</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl list-units --all --type=service ceph*</pre></div><p>
    To list only the inactive services, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl list-units --all --state=inactive --type=service ceph*</pre></div><p>
    You can also use <code class="command">salt</code> to query services across multiple
    nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">TARGET</em> cmd.shell \
 "systemctl list-units --all --type=service ceph* | sed -e '/^$/,$ d'"</pre></div><p>
    Query storage nodes only:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -I 'roles:storage' cmd.shell \
 'systemctl list-units --all --type=service ceph*'</pre></div></section><section class="sect2" id="ceph-operating-services-status" data-id-title="Service Status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.1.4 </span><span class="title-name">Service Status</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-status">#</a></h3></div></div></div><p>
    You can query <code class="systemitem">systemd</code> for the status of services. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl status ceph-osd@1.service
<code class="prompt user">root # </code>systemctl status ceph-mon@<em class="replaceable">HOSTNAME</em>.service</pre></div><p>
    Replace <em class="replaceable">HOSTNAME</em> with the host name the daemon
    is running on.
   </p><p>
    If you do not know the exact name/number of the service, see
    <a class="xref" href="#ceph-operating-services-finding-names" title="16.1.3. Identifying Individual Services">Section 16.1.3, “Identifying Individual Services”</a>.
   </p></section></section><section class="sect1" id="Deepsea-restart" data-id-title="Restarting Ceph Services Using DeepSea"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.2 </span><span class="title-name">Restarting Ceph Services Using DeepSea</span> <a title="Permalink" class="permalink" href="#Deepsea-restart">#</a></h2></div></div></div><p>
   After applying updates to the cluster nodes, the affected Ceph related
   services need to be restarted. Normally, restarts are performed
   automatically by DeepSea. This section describes how to restart the
   services manually.
  </p><div id="id-1.3.5.3.5.3" data-id-title="Watching the Restart" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Watching the Restart</h6><p>
    The process of restarting the cluster may take some time. You can watch the
    events by using the Salt event bus by running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.event pretty=True</pre></div><p>
    Another command to monitor active jobs is
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run jobs.active</pre></div></div><section class="sect2" id="deepsea-restart-all" data-id-title="Restarting All Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.1 </span><span class="title-name">Restarting All Services</span> <a title="Permalink" class="permalink" href="#deepsea-restart-all">#</a></h3></div></div></div><div id="id-1.3.5.3.5.4.2" data-id-title="Interruption of Services" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Interruption of Services</h6><p>
     If Ceph related services—specifically iSCSI or
     NFS Ganesha—are configured as single points of access with no High Availability
     setup, restarting them will result in their temporary outage as viewed
     from the client side.
    </p></div><div id="id-1.3.5.3.5.4.3" data-id-title="Samba Not Managed by DeepSea" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Samba Not Managed by DeepSea</h6><p>
     Because DeepSea and the Ceph Dashboard do not currently support Samba
     deployments, you need to manage Samba related services manually. For
     more details, see <a class="xref" href="#cha-ses-cifs" title="Chapter 29. Exporting Ceph Data via Samba">Chapter 29, <em>Exporting Ceph Data via Samba</em></a>.
    </p></div><p>
    To restart <span class="emphasis"><em>all</em></span> services on the cluster, run the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      For DeepSea prior to version 0.8.4, the Metadata Server, iSCSI Gateway, Object Gateway, and
      NFS Ganesha services restart in parallel.
     </p></li><li class="listitem"><p>
      For DeepSea 0.8.4 and newer, all roles you have configured restart in
      the following order: Ceph Monitor, Ceph Manager, Ceph OSD, Metadata Server, Object Gateway, iSCSI Gateway, NFS Ganesha.
      To keep the downtime low and to find potential issues as early as
      possible, nodes are restarted sequentially. For example, only one
      monitoring node is restarted at a time.
     </p></li></ul></div><p>
    The command waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </p></section><section class="sect2" id="deepsea-restart-specific" data-id-title="Restarting Specific Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.2.2 </span><span class="title-name">Restarting Specific Services</span> <a title="Permalink" class="permalink" href="#deepsea-restart-specific">#</a></h3></div></div></div><p>
    To restart a specific service on the cluster, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.<em class="replaceable">service_name</em></pre></div><p>
    For example, to restart all Object Gateways, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.rgw</pre></div><p>
    You can use the following targets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mon</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mgr</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.osd</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.mds</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.rgw</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.igw</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.restart.ganesha</pre></div><p>
    The restart orchestration checks if the installated binary is newer than
    the current one, or if configuration changes exist for this daemon and only
    restarts in those cases. If you run the above command and nothing happens,
    this is due to these conditions. See
    <a class="xref" href="#ceph-operating-services-individual" title="16.1.2. Starting, Stopping, and Restarting Individual Services">Section 16.1.2, “Starting, Stopping, and Restarting Individual Services”</a> for more information.
   </p></section></section><section class="sect1" id="ceph-cluster-shutdown" data-id-title="Shutdown and Start of the Whole Ceph Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.3 </span><span class="title-name">Shutdown and Start of the Whole Ceph Cluster</span> <a title="Permalink" class="permalink" href="#ceph-cluster-shutdown">#</a></h2></div></div></div><p>
   There are occasions when you need to stop all Ceph related services in the
   cluster in the recommended order, and then be able to simply start them
   again. For example, in case of a planned power outage.
  </p><div class="procedure" id="id-1.3.5.3.6.3" data-id-title="Shutting Down the Whole Ceph Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 16.1: </span><span class="title-name">Shutting Down the Whole Ceph Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Shut down or disconnect any clients accessing the cluster.
    </p></li><li class="step"><p>
     To prevent CRUSH from automatically rebalancing the cluster, set the
     cluster to <code class="literal">noout</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd set noout</pre></div></li><li class="step"><p>
     Disable safety measures and run the <code class="command">ceph.shutdown</code>
     runner:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run disengage.safety
<code class="prompt user">root@master # </code>salt-run state.orch ceph.shutdown</pre></div></li><li class="step"><p>
     Power off all cluster nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -C 'G@deepsea:*' cmd.run "shutdown -h"</pre></div></li></ol></div></div><div class="procedure" id="id-1.3.5.3.6.4" data-id-title="Starting the Whole Ceph Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 16.2: </span><span class="title-name">Starting the Whole Ceph Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.5.3.6.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Power on the Admin Node.
    </p></li><li class="step"><p>
     Power on the Ceph Monitor nodes.
    </p></li><li class="step"><p>
     Power on the Ceph OSD nodes.
    </p></li><li class="step"><p>
     Unset the previously set <code class="literal">noout</code> flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd unset noout</pre></div></li><li class="step"><p>
     Power on all configured gateways.
    </p></li><li class="step"><p>
     Power on or connect cluster clients.
    </p></li></ol></div></div></section></section><section class="chapter" id="ceph-monitor" data-id-title="Determining Cluster State"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17 </span><span class="title-name">Determining Cluster State</span> <a title="Permalink" class="permalink" href="#ceph-monitor">#</a></h2></div></div></div><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups, and Metadata Servers.
 </p><div id="id-1.3.5.4.4" data-id-title="Interactive Mode" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Interactive Mode</h6><p>
   To run the <code class="command">ceph</code> tool in an interactive mode, type
   <code class="command">ceph</code> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <code class="command">ceph</code> commands in a row. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</pre></div></div><section class="sect1" id="monitor-status" data-id-title="Checking a Clusters Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.1 </span><span class="title-name">Checking a Cluster's Status</span> <a title="Permalink" class="permalink" href="#monitor-status">#</a></h2></div></div></div><p>
   To check a cluster's status, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph status</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -s</pre></div><p>
   In interactive mode, type <code class="command">status</code> and press
   <span class="keycap">Enter</span>.
  </p><div class="verbatim-wrap"><pre class="screen">ceph&gt; status</pre></div><p>
   Ceph will print the cluster status. For example, a tiny Ceph cluster
   consisting of one monitor and two OSDs may print the following:
  </p><div class="verbatim-wrap"><pre class="screen">cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</pre></div></section><section class="sect1" id="monitor-health" data-id-title="Checking Cluster Health"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.2 </span><span class="title-name">Checking Cluster Health</span> <a title="Permalink" class="permalink" href="#monitor-health">#</a></h2></div></div></div><p>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</pre></div><div id="id-1.3.5.4.6.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -c <em class="replaceable">/path/to/conf</em> -k <em class="replaceable">/path/to/keyring</em> health</pre></div></div><p>
   The Ceph cluster returns one of the following health codes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.6.6.1"><span class="term">OSD_DOWN</span></dt><dd><p>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </p><p>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<code class="filename">/var/log/ceph/ceph-osd.*</code>) may contain debugging
      information.
     </p></dd><dt id="id-1.3.5.4.6.6.2"><span class="term">OSD_<em class="replaceable">crush type</em>_DOWN, for example OSD_HOST_DOWN</span></dt><dd><p>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </p></dd><dt id="id-1.3.5.4.6.6.3"><span class="term">OSD_ORPHAN</span></dt><dd><p>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rm osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.5.4.6.6.4"><span class="term">OSD_OUT_OF_ORDER_FULL</span></dt><dd><p>
      The usage thresholds for <span class="emphasis"><em>backfillfull</em></span> (defaults to
      0.90), <span class="emphasis"><em>nearfull</em></span> (defaults to 0.85),
      <span class="emphasis"><em>full</em></span> (defaults to 0.95), and/or
      <span class="emphasis"><em>failsafe_full</em></span> are not ascending. In particular, we
      expect <span class="emphasis"><em>backfillfull</em></span> &lt;
      <span class="emphasis"><em>nearfull</em></span>, <span class="emphasis"><em>nearfull</em></span> &lt;
      <span class="emphasis"><em>full</em></span>, and <span class="emphasis"><em>full</em></span> &lt;
      <span class="emphasis"><em>failsafe_full</em></span>.
     </p><p>
      To read the current values, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%</pre></div><p>
      The thresholds can be adjusted with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div></dd><dt id="id-1.3.5.4.6.6.5"><span class="term">OSD_FULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>full</em></span> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div><p>
      The currently defined <span class="emphasis"><em>full</em></span> ratio can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep full_ratio</pre></div><p>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div><p>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </p></dd><dt id="id-1.3.5.4.6.6.6"><span class="term">OSD_BACKFILLFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>backfillfull</em></span>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.4.6.6.7"><span class="term">OSD_NEARFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>nearfull</em></span>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.4.6.6.8"><span class="term">OSDMAP_FLAGS</span></dt><dd><p>
      One or more cluster flags of interest has been set. With the exception of
      <span class="emphasis"><em>full</em></span>, these flags can be set or cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set <em class="replaceable">flag</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset <em class="replaceable">flag</em></pre></div><p>
      These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.6.6.8.2.4.1"><span class="term">full</span></dt><dd><p>
         The cluster is flagged as full and cannot service writes.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.2"><span class="term">pauserd, pausewr</span></dt><dd><p>
         Paused reads or writes.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.3"><span class="term">noup</span></dt><dd><p>
         OSDs are not allowed to start.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.4"><span class="term">nodown</span></dt><dd><p>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <span class="emphasis"><em>down</em></span>.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.5"><span class="term">noin</span></dt><dd><p>
         OSDs that were previously marked <span class="emphasis"><em>out</em></span> will not be
         marked back <span class="emphasis"><em>in</em></span> when they start.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.6"><span class="term">noout</span></dt><dd><p>
         <span class="emphasis"><em>Down</em></span> OSDs will not automatically be marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.7"><span class="term">nobackfill, norecover, norebalance</span></dt><dd><p>
         Recovery or data rebalancing is suspended.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.8"><span class="term">noscrub, nodeep_scrub</span></dt><dd><p>
         Scrubbing (see <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) is disabled.
        </p></dd><dt id="id-1.3.5.4.6.6.8.2.4.9"><span class="term">notieragent</span></dt><dd><p>
         Cache tiering activity is suspended.
        </p></dd></dl></div></dd><dt id="id-1.3.5.4.6.6.9"><span class="term">OSD_FLAGS</span></dt><dd><p>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.6.6.9.2.2.1"><span class="term">noup</span></dt><dd><p>
         OSD is not allowed to start.
        </p></dd><dt id="id-1.3.5.4.6.6.9.2.2.2"><span class="term">nodown</span></dt><dd><p>
         Failure reports for this OSD will be ignored.
        </p></dd><dt id="id-1.3.5.4.6.6.9.2.2.3"><span class="term">noin</span></dt><dd><p>
         If this OSD was previously marked <span class="emphasis"><em>out</em></span>
         automatically after a failure, it will not be marked
         <span class="emphasis"><em>in</em></span> when it starts.
        </p></dd><dt id="id-1.3.5.4.6.6.9.2.2.4"><span class="term">noout</span></dt><dd><p>
         If this OSD is down, it will not be automatically marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd></dl></div><p>
      Per-OSD flags can be set and cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd add-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd rm-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em></pre></div></dd><dt id="id-1.3.5.4.6.6.10"><span class="term">OLD_CRUSH_TUNABLES</span></dt><dd><p>
      The CRUSH Map is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <code class="option">mon_crush_min_required_version</code>
      configuration option.
     </p></dd><dt id="id-1.3.5.4.6.6.11"><span class="term">OLD_CRUSH_STRAW_CALC_VERSION</span></dt><dd><p>
      The CRUSH Map is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The CRUSH Map should be
      updated to use the newer method (<code class="option">straw_calc_version</code>=1).
     </p></dd><dt id="id-1.3.5.4.6.6.12"><span class="term">CACHE_POOL_NO_HIT_SET</span></dt><dd><p>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_type <em class="replaceable">type</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_period <em class="replaceable">period-in-seconds</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_count <em class="replaceable">number-of-hitsets</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_fpp <em class="replaceable">target-false-positive-rate</em></pre></div><p>
      For more information on cache tiering, see
      <span class="intraxref">Book “Tuning Guide”, Chapter 7 “Cache Tiering”</span>.
     </p></dd><dt id="id-1.3.5.4.6.6.13"><span class="term">OSD_NO_SORTBITWISE</span></dt><dd><p>
      No pre-Luminous v12 OSDs are running but the <code class="option">sortbitwise</code>
      flag has not been set. You need to set the <code class="option">sortbitwise</code>
      flag before Luminous v12 or newer OSDs can start:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set sortbitwise</pre></div></dd><dt id="id-1.3.5.4.6.6.14"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df detail</pre></div><p>
      You can either raise the pool quota with
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_objects <em class="replaceable">num-objects</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_bytes <em class="replaceable">num-bytes</em></pre></div><p>
      or delete some existing data to reduce usage.
     </p></dd><dt id="id-1.3.5.4.6.6.15"><span class="term">PG_AVAILABILITY</span></dt><dd><p>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow I/O
      requests to be serviced. Problematic PG states include
      <span class="emphasis"><em>peering</em></span>, <span class="emphasis"><em>stale</em></span>,
      <span class="emphasis"><em>incomplete</em></span>, and the lack of
      <span class="emphasis"><em>active</em></span> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.5.4.6.6.16"><span class="term">PG_DEGRADED</span></dt><dd><p>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <span class="emphasis"><em>degraded</em></span> or
      <span class="emphasis"><em>undersized</em></span> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <span class="emphasis"><em>clean</em></span> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.5.4.6.6.17"><span class="term">PG_DEGRADED_FULL</span></dt><dd><p>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <span class="emphasis"><em>backfill_toofull</em></span> or
      <span class="emphasis"><em>recovery_toofull</em></span> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <span class="emphasis"><em>backfillfull</em></span> threshold.
     </p></dd><dt id="id-1.3.5.4.6.6.18"><span class="term">PG_DAMAGED</span></dt><dd><p>
      Data scrubbing (see <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <span class="emphasis"><em>inconsistent</em></span> or
      <span class="emphasis"><em>snaptrim_error</em></span> flag is set, indicating an earlier
      scrub operation found a problem, or that the <span class="emphasis"><em>repair</em></span>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </p></dd><dt id="id-1.3.5.4.6.6.19"><span class="term">OSD_SCRUB_ERRORS</span></dt><dd><p>
      Recent OSD scrubs have uncovered inconsistencies.
     </p></dd><dt id="id-1.3.5.4.6.6.20"><span class="term">CACHE_POOL_NEAR_FULL</span></dt><dd><p>
      A cache tier pool is nearly full. Full in this context is determined by
      the <span class="emphasis"><em>target_max_bytes</em></span> and
      <span class="emphasis"><em>target_max_objects</em></span> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_objects <em class="replaceable">objects</em></pre></div><p>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </p><p>
      Find more information about cache tiering in
      <span class="intraxref">Book “Tuning Guide”, Chapter 7 “Cache Tiering”</span>.
     </p></dd><dt id="id-1.3.5.4.6.6.21"><span class="term">TOO_FEW_PGS</span></dt><dd><p>
      The number of PGs in use is below the configurable threshold of
      <code class="option">mon_pg_warn_min_per_osd</code> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </p></dd><dt id="id-1.3.5.4.6.6.22"><span class="term">TOO_MANY_PGS</span></dt><dd><p>
      The number of PGs in use is above the configurable threshold of
      <code class="option">mon_pg_warn_max_per_osd</code> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the Ceph Managers and Ceph Monitors.
     </p><p>
      While the <code class="option">pg_num</code> value for existing pools cannot be
      reduced. The <code class="option">pgp_num</code> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <code class="option">pgp_num</code> value can
      be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">value</em></pre></div></dd><dt id="id-1.3.5.4.6.6.23"><span class="term">SMALLER_PGP_NUM</span></dt><dd><p>
      One or more pools has a <code class="option">pgp_num</code> value less than
      <code class="option">pg_num</code>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <code class="option">pgp_num</code> to match
      <code class="option">pg_num</code>, triggering the data migration, with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">pg_num_value</em></pre></div></dd><dt id="id-1.3.5.4.6.6.24"><span class="term">MANY_OBJECTS_PER_PG</span></dt><dd><p>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration option on the
      monitors.
     </p></dd><dt id="id-1.3.5.4.6.6.25"><span class="term">POOL_APP_NOT_ENABLED¶</span></dt><dd><p>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd pool init <em class="replaceable">pool_name</em></pre></div><p>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable foo</pre></div></dd><dt id="id-1.3.5.4.6.6.26"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <code class="option">mon_pool_quota_crit_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.3.5.4.6.6.27"><span class="term">POOL_NEAR_FULL</span></dt><dd><p>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <code class="option">mon_pool_quota_warn_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.3.5.4.6.6.28"><span class="term">OBJECT_MISPLACED</span></dt><dd><p>
      One or more objects in the cluster are not stored on the node where the
      cluster wants them to be. This is an indication that data migration
      caused by a recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in itself. Data consistency is never at
      risk, and old copies of objects are never removed until the desired
      number of new copies (in the desired locations) are present.
     </p></dd><dt id="id-1.3.5.4.6.6.29"><span class="term">OBJECT_UNFOUND</span></dt><dd><p>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on the OSDs that
      are currently up. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, the down OSD that has the most recent copy of the
      unfound object can be brought back up. Candidate OSDs can be identified
      from the peering state for the PG(s) responsible for the unfound object:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.3.5.4.6.6.30"><span class="term">REQUEST_SLOW</span></dt><dd><p>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">id</em> ops</pre></div><p>
      You can see a summary of the slowest recent requests:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">id</em> dump_historic_ops</pre></div><p>
      You can find the location of an OSD with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd find osd.<em class="replaceable">id</em></pre></div></dd><dt id="id-1.3.5.4.6.6.31"><span class="term">REQUEST_STUCK</span></dt><dd><p>
      One or more OSD requests have been blocked for a relatively long time,
      for example 4096 seconds. This is an indication that either the cluster
      has been unhealthy for an extended period of time (for example, not
      enough running OSDs or inactive PGs) or there is some internal problem
      with the OSD.
     </p></dd><dt id="id-1.3.5.4.6.6.32"><span class="term">PG_NOT_SCRUBBED</span></dt><dd><p>
      One or more PGs have not been scrubbed (see <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>)
      recently. PGs are normally scrubbed every
      <code class="option">mon_scrub_interval</code> seconds, and this warning triggers
      when <code class="option">mon_warn_not_scrubbed</code> such intervals have elapsed
      without a scrub. PGs will not scrub if they are not flagged as clean,
      which may happen if they are misplaced or degraded (see PG_AVAILABILITY
      and PG_DEGRADED above). You can manually initiate a scrub of a clean PG
      with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg scrub <em class="replaceable">pgid</em></pre></div></dd><dt id="id-1.3.5.4.6.6.33"><span class="term">PG_NOT_DEEP_SCRUBBED</span></dt><dd><p>
      One or more PGs has not been deep scrubbed (see
      <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) recently. PGs are normally scrubbed every
      <code class="option">osd_deep_mon_scrub_interval</code> seconds, and this warning
      triggers when <code class="option">mon_warn_not_deep_scrubbed</code> seconds have
      elapsed without a scrub. PGs will not (deep) scrub if they are not
      flagged as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg deep-scrub <em class="replaceable">pgid</em></pre></div></dd></dl></div><div id="id-1.3.5.4.6.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph -c <em class="replaceable">/path/to/conf</em> -k <em class="replaceable">/path/to/keyring</em> health</pre></div></div></section><section class="sect1" id="monitor-watch" data-id-title="Watching a Cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.3 </span><span class="title-name">Watching a Cluster</span> <a title="Permalink" class="permalink" href="#monitor-watch">#</a></h2></div></div></div><p>
   You can find the immediate state of the cluster using <code class="command">ceph
   -s</code>. For example, a tiny Ceph cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 &gt; max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean</pre></div><p>
   The output provides the following information:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Cluster ID
    </p></li><li class="listitem"><p>
     Cluster health status
    </p></li><li class="listitem"><p>
     The monitor map epoch and the status of the monitor quorum
    </p></li><li class="listitem"><p>
     The OSD map epoch and the status of OSDs
    </p></li><li class="listitem"><p>
     The status of Ceph Managers
    </p></li><li class="listitem"><p>
     The status of Object Gateways
    </p></li><li class="listitem"><p>
     The placement group map version
    </p></li><li class="listitem"><p>
     The number of placement groups and pools
    </p></li><li class="listitem"><p>
     The <span class="emphasis"><em>notional</em></span> amount of data stored and the number of
     objects stored
    </p></li><li class="listitem"><p>
     The total amount of data stored
    </p></li></ul></div><div id="id-1.3.5.4.7.6" data-id-title="How Ceph Calculates Data Usage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: How Ceph Calculates Data Usage</h6><p>
    The <code class="literal">used</code> value reflects the actual amount of raw storage
    used. The <code class="literal">xxx GB / xxx GB</code> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because Ceph creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </p></div><p>
   Other commands that display immediate status information are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">ceph pg stat</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph osd pool stats</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df detail</code>
    </p></li></ul></div><p>
   To get the information updated in real time, put any of these commands
   (including <code class="command">ceph -s</code>) as an argument of the
   <code class="command">watch</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>watch -n 10 'ceph -s'</pre></div><p>
   Press <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>
   when you are tired of watching.
  </p></section><section class="sect1" id="monitor-stats" data-id-title="Checking a Clusters Usage Stats"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.4 </span><span class="title-name">Checking a Cluster's Usage Stats</span> <a title="Permalink" class="permalink" href="#monitor-stats">#</a></h2></div></div></div><p>
   To check a cluster’s data usage and distribution among pools, use the
   <code class="command">ceph df</code> command. To get more details, use <code class="command">ceph
   df detail</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
    TOTAL     40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
POOLS:
    POOL             ID     STORED     OBJECTS    USED       %USED    MAX AVAIL
    iscsi-images      1     3.9 KiB          8    769 KiB        0       10 GiB
    cephfs_data       2     1.6 KiB          5    960 KiB        0       10 GiB
    cephfs_metadata   3      54 KiB         22    1.5 MiB        0       10 GiB
[...]</pre></div><p>
   The <code class="literal">RAW STORAGE</code> section of the output provides an
   overview of the amount of storage your cluster uses for your data.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">CLASS</code>: The storage class of the device. Refer to
     <a class="xref" href="#crush-devclasses" title="20.1.1. Device Classes">Section 20.1.1, “Device Classes”</a> for more details on device classes.
    </p></li><li class="listitem"><p>
     <code class="literal">SIZE</code>: The overall storage capacity of the cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">AVAIL</code>: The amount of free space available in the
     cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">USED</code>: The space (accumulated over all OSDs) allocated
     purely for data objects kept at block device.
    </p></li><li class="listitem"><p>
     <code class="literal">RAW USED</code>: The sum of 'USED' space and space
     allocated/reserved at block device for Ceph purposes, for example BlueFS
     part for BlueStore.
    </p></li><li class="listitem"><p>
     <code class="literal">% RAW USED</code>: The percentage of raw storage used. Use
     this number in conjunction with the <code class="literal">full ratio</code> and
     <code class="literal">near full ratio</code> to ensure that you are not reaching
     your cluster’s capacity. See <a class="xref" href="#storage-capacity" title="17.10. Storage Capacity">Section 17.10, “Storage Capacity”</a> for
     additional details.
    </p><div id="id-1.3.5.4.8.5.6.2" data-id-title="Cluster Fill Level" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Cluster Fill Level</h6><p>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </p><p>
      Use the command <code class="command">ceph osd df tree</code> to list the fill
      level of all OSDs.
     </p></div></li></ul></div><p>
   The <code class="literal">POOLS</code> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <span class="emphasis"><em>does not</em></span> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">POOL</code>: The name of the pool.
    </p></li><li class="listitem"><p>
     <code class="literal">ID</code>: The pool ID.
    </p></li><li class="listitem"><p>
     <code class="literal">STORED</code>: The amount of data stored by the user.
    </p></li><li class="listitem"><p>
     <code class="literal">OBJECTS</code>: The notional number of objects stored per
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">USED</code>: The amount of space allocated purely for data by
     all OSD nodes in kB.
    </p></li><li class="listitem"><p>
     <code class="literal">%USED</code>: The notional percentage of storage used per
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">MAX AVAIL</code>: The maximum available space in the given
     pool.
    </p></li></ul></div><div id="id-1.3.5.4.8.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    <code class="literal">USED</code>and %<code class="literal">USED</code> amounts will not add up
    to the <code class="literal">RAW USED</code> and <code class="literal">%RAW USED</code> amounts
    in the <code class="literal">RAW STORAGE</code> section of the output.
   </p></div></section><section class="sect1" id="monitor-osdstatus" data-id-title="Checking OSD Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.5 </span><span class="title-name">Checking OSD Status</span> <a title="Permalink" class="permalink" href="#monitor-osdstatus">#</a></h2></div></div></div><p>
   You can check OSDs to ensure they are up and on by executing:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump</pre></div><p>
   You can also view OSDs according to their position in the CRUSH map.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tree</pre></div><p>
   Ceph will print a CRUSH tree with a host, its OSDs, whether they are up
   and their weight.
  </p><div class="verbatim-wrap"><pre class="screen"># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</pre></div></section><section class="sect1" id="storage-bp-monitoring-fullosd" data-id-title="Checking for Full OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.6 </span><span class="title-name">Checking for Full OSDs</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-fullosd">#</a></h2></div></div></div><p>
   Ceph prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <code class="command">mon osd full ratio</code>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <code class="command">mon osd nearfull ratio</code> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </p><p>
   Full OSD nodes will be reported by <code class="command">ceph health</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</pre></div><p>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </p><div id="id-1.3.5.4.10.8" data-id-title="Preventing Full OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Preventing Full OSDs</h6><p>
    After an OSD becomes full—it uses 100% of its disk space—it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Each OSD's disk space (usually mounted under
      <code class="filename">/var/lib/ceph/osd/osd-{1,2..}</code>) needs to be placed on
      a dedicated underlying disk or partition.
     </p></li><li class="listitem"><p>
      Check the Ceph configuration files and make sure that Ceph does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </p></li><li class="listitem"><p>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </p></li></ul></div></div></section><section class="sect1" id="monitor-monstatus" data-id-title="Checking Monitor Status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.7 </span><span class="title-name">Checking Monitor Status</span> <a title="Permalink" class="permalink" href="#monitor-monstatus">#</a></h2></div></div></div><p>
   After you start the cluster and before first reading and/or writing data,
   check the Ceph Monitors' quorum status. When the cluster is already serving
   requests, check the Ceph Monitors' status periodically to ensure that they are
   running.
  </p><p>
   To display the monitor map, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon dump</pre></div><p>
   To check the quorum status for the monitor cluster, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph quorum_status</pre></div><p>
   Ceph will return the quorum status. For example, a Ceph cluster
   consisting of three monitors may return the following:
  </p><div class="verbatim-wrap"><pre class="screen">{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</pre></div></section><section class="sect1" id="monitor-pgroupstatus" data-id-title="Checking Placement Group States"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.8 </span><span class="title-name">Checking Placement Group States</span> <a title="Permalink" class="permalink" href="#monitor-pgroupstatus">#</a></h2></div></div></div><p>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <code class="literal">active</code> and
   <code class="literal">clean</code>. For a detailed discussion, refer to
   <a class="xref" href="#op-mon-osd-pg" title="17.11. Monitoring OSDs and Placement Groups">Section 17.11, “Monitoring OSDs and Placement Groups”</a>.
  </p></section><section class="sect1" id="monitor-adminsocket" data-id-title="Using the Admin Socket"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.9 </span><span class="title-name">Using the Admin Socket</span> <a title="Permalink" class="permalink" href="#monitor-adminsocket">#</a></h2></div></div></div><p>
   
   The Ceph admin socket allows you to query a daemon via a socket interface.
   By default, Ceph sockets reside under <code class="filename">/var/run/ceph</code>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph --admin-daemon /var/run/ceph/<em class="replaceable">socket-name</em></pre></div><p>
   To view the available admin socket commands, execute the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph --admin-daemon /var/run/ceph/<em class="replaceable">socket-name</em> help</pre></div><p>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to <a class="xref" href="#ceph-config-runtime" title="25.1. Runtime Configuration">Section 25.1, “Runtime Configuration”</a> for details.
  </p><p>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <code class="command">ceph tell</code>
   <em class="replaceable">daemon-type</em>.<em class="replaceable">id</em>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </p></section><section class="sect1" id="storage-capacity" data-id-title="Storage Capacity"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.10 </span><span class="title-name">Storage Capacity</span> <a title="Permalink" class="permalink" href="#storage-capacity">#</a></h2></div></div></div><p>
   When a Ceph storage cluster gets close to its maximum capacity, Ceph
   prevents you from writing to or reading from Ceph OSDs as a safety measure to
   prevent data loss. Therefore, letting a production cluster approach its full
   ratio is not a good practice, because it sacrifices high availability. The
   default full ratio is set to .95, meaning 95% of capacity. This a very
   aggressive setting for a test cluster with a small number of OSDs.
  </p><div id="id-1.3.5.4.14.3" data-id-title="Increase Storage Capacity" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increase Storage Capacity</h6><p>
    When monitoring your cluster, be alert to warnings related to the
    <code class="literal">nearfull</code> ratio. It means that a failure of some OSDs
    could result in a temporary service disruption if one or more OSDs fails.
    Consider adding more OSDs to increase storage capacity.
   </p></div><p>
   A common scenario for test clusters involves a system administrator removing
   a Ceph OSD from the Ceph storage cluster to watch the cluster rebalance. Then
   removing another Ceph OSD, and so on until the cluster eventually reaches the
   full ratio and locks up. We recommend a bit of capacity planning even with a
   test cluster. Planning enables you to estimate how much spare capacity you
   will need in order to maintain high availability. Ideally, you want to plan
   for a series of Ceph OSD failures where the cluster can recover to an
   <code class="literal">active + clean</code> state without replacing those Ceph OSDs
   immediately. You can run a cluster in an <code class="literal">active +
   degraded</code> state, but this is not ideal for normal operating
   conditions.
  </p><p>
   The following diagram depicts a simplistic Ceph storage cluster containing
   33 Ceph nodes with one Ceph OSD per host, each of them reading from and
   writing to a 3 TB drive. This exemplary cluster has a maximum actual
   capacity of 99 TB. The <code class="option">mon osd full ratio</code> option is set to
   0.95. If the cluster falls to 5 TB of the remaining capacity, it will not
   allow the clients to read and write data. Therefore the storage cluster’s
   operating capacity is 95 TB, not 99 TB.
  </p><div class="figure" id="id-1.3.5.4.14.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_cluster.png" target="_blank"><img src="images/ceph_cluster.png" width="" alt="Ceph Cluster"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.1: </span><span class="title-name">Ceph Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.14.6">#</a></h6></div></div><p>
   It is normal in such a cluster for one or two OSDs to fail. A less frequent
   but reasonable scenario involves a rack’s router or power supply failing,
   which brings down multiple OSDs simultaneously (for example, OSDs 7-12). In
   such a scenario, you should still strive for a cluster that can remain
   operational and achieve an <code class="literal">active + clean</code>
   state—even if that means adding a few hosts with additional OSDs in
   short order. If your capacity usage is too high, you may not lose data. But
   you could still sacrifice data availability while resolving an outage within
   a failure domain if capacity usage of the cluster exceeds the full ratio.
   For this reason, we recommend at least some rough capacity planning.
  </p><p>
   Identify two numbers for your cluster:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     The number of OSDs.
    </p></li><li class="listitem"><p>
     The total capacity of the cluster.
    </p></li></ol></div><p>
   If you divide the total capacity of your cluster by the number of OSDs in
   your cluster, you will find the mean average capacity of an OSD within your
   cluster. Consider multiplying that number by the number of OSDs you expect
   will fail simultaneously during normal operations (a relatively small
   number). Finally, multiply the capacity of the cluster by the full ratio to
   arrive at a maximum operating capacity. Then, subtract the number of the
   amount of data from the OSDs you expect to fail to arrive at a reasonable
   full ratio. Repeat the foregoing process with a higher number of OSD
   failures (a rack of OSDs) to arrive at a reasonable number for a near full
   ratio.
  </p><p>
   The following settings only apply on cluster creation and are then stored in
   the OSD map:
  </p><div class="verbatim-wrap"><pre class="screen">[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70</pre></div><div id="id-1.3.5.4.14.13" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    These settings only apply during cluster creation. Afterward they need to
    be changed in the OSD Map using the <code class="command">ceph osd
    set-nearfull-ratio</code> and <code class="command">ceph osd set-full-ratio</code>
    commands.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.14.14.1"><span class="term">mon osd full ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered
      <code class="literal">full</code>. Default is .95
     </p></dd><dt id="id-1.3.5.4.14.14.2"><span class="term">mon osd backfillfull ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered too
      <code class="literal">full</code> to backfill. Default is .90
     </p></dd><dt id="id-1.3.5.4.14.14.3"><span class="term">mon osd nearfull ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered
      <code class="literal">nearfull</code>. Default is .85
     </p></dd></dl></div><div id="id-1.3.5.4.14.15" data-id-title="Check OSD Weight" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Check OSD Weight</h6><p>
    If some OSDs are <code class="literal">nearfull</code>, but others have plenty of
    capacity, you may have a problem with the CRUSH weight for the
    <code class="literal">nearfull</code> OSDs.
   </p></div></section><section class="sect1" id="op-mon-osd-pg" data-id-title="Monitoring OSDs and Placement Groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.11 </span><span class="title-name">Monitoring OSDs and Placement Groups</span> <a title="Permalink" class="permalink" href="#op-mon-osd-pg">#</a></h2></div></div></div><p>
   High availability and high reliability require a fault-tolerant approach to
   managing hardware and software issues. Ceph has no single
   point-of-failure, and can service requests for data in a 'degraded' mode.
   Ceph’s data placement introduces a layer of indirection to ensure that
   data does not bind directly to particular OSD addresses. This means that
   tracking down system faults requires finding the placement group and the
   underlying OSDs at root of the problem.
  </p><div id="id-1.3.5.4.15.3" data-id-title="Access in Case of Failure" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Access in Case of Failure</h6><p>
    A fault in one part of the cluster may prevent you from accessing a
    particular object. That does not mean that you cannot access other objects.
    When you run into a fault, follow the steps for monitoring your OSDs and
    placement groups. Then begin troubleshooting.
   </p></div><p>
   Ceph is generally self-repairing. However, when problems persist,
   monitoring OSDs and placement groups will help you identify the problem.
  </p><section class="sect2" id="op-mon-osds" data-id-title="Monitoring OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.1 </span><span class="title-name">Monitoring OSDs</span> <a title="Permalink" class="permalink" href="#op-mon-osds">#</a></h3></div></div></div><p>
    An OSD’s status is either <span class="emphasis"><em>in the cluster</em></span> ('in') or
    <span class="emphasis"><em>out of the cluster</em></span> ('out'). At the same time, it is
    either <span class="emphasis"><em>up and running</em></span> ('up') or it is <span class="emphasis"><em>down
    and not running</em></span> ('down'). If an OSD is 'up', it may be either in
    the cluster (you can read and write data) or out of the cluster. If it was
    in the cluster and recently moved out of the cluster, Ceph will migrate
    placement groups to other OSDs. If an OSD is out of the cluster, CRUSH will
    not assign placement groups to it. If an OSD is 'down', it should also be
    'out'.
   </p><div id="id-1.3.5.4.15.5.3" data-id-title="Unhealthy State" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unhealthy State</h6><p>
     If an OSD is 'down' and 'in', there is a problem and the cluster will not
     be in a healthy state.
    </p></div><p>
    If you execute a command such as <code class="command">ceph health</code>,
    <code class="command">ceph -s</code> or <code class="command">ceph -w</code>, you may notice
    that the cluster does not always echo back <code class="literal">HEALTH OK</code>.
    With regard to OSDs, you should expect that the cluster will
    <span class="emphasis"><em>not</em></span> echo <code class="literal">HEALTH OK</code> under the
    following circumstances:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You have not started the cluster yet (it will not respond).
     </p></li><li class="listitem"><p>
      You have just started or restarted the cluster and it is not ready yet,
      because the placement groups are being created and the OSDs are in the
      process of peering.
     </p></li><li class="listitem"><p>
      You have just added or removed an OSD.
     </p></li><li class="listitem"><p>
      You have just modified your cluster map.
     </p></li></ul></div><p>
    An important aspect of monitoring OSDs is to ensure that when the cluster
    is up and running, all the OSDs in the cluster are up and running, too. To
    see if all the OSDs are running, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd stat
x osds: y up, z in; epoch: eNNNN</pre></div><p>
    The result should tell you the total number of OSDs (x), how many are 'up'
    (y), how many are 'in' (z), and the map epoch (eNNNN). If the number of
    OSDs that are 'in' the cluster is more than the number of OSDs that are
    'up', execute the following command to identify the
    <code class="literal">ceph-osd</code> daemons that are not running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000</pre></div><p>
    If an OSD with, for example, ID 1 is down, start it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>sudo systemctl start ceph-osd@1.service</pre></div><p>
    See <a class="xref" href="#op-osd-not-running" title="17.12. OSD Is Not Running">Section 17.12, “OSD Is Not Running”</a> for problems associated with OSDs
    that have stopped or that will not restart.
   </p></section><section class="sect2" id="op-pgsets" data-id-title="Placement Group Sets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.2 </span><span class="title-name">Placement Group Sets</span> <a title="Permalink" class="permalink" href="#op-pgsets">#</a></h3></div></div></div><p>
    When CRUSH assigns placement groups to OSDs, it looks at the number of
    replicas for the pool and assigns the placement group to OSDs such that
    each replica of the placement group gets assigned to a different OSD. For
    example, if the pool requires three replicas of a placement group, CRUSH
    may assign them to <code class="literal">osd.1</code>, <code class="literal">osd.2</code> and
    <code class="literal">osd.3</code> respectively. CRUSH actually seeks a pseudo-random
    placement that will take into account failure domains you set in your
    CRUSH Map, so you will rarely see placement groups assigned to nearest
    neighbor OSDs in a large cluster. We refer to the set of OSDs that should
    contain the replicas of a particular placement group as the <span class="emphasis"><em>acting set</em></span>. In
    some cases, an OSD in the acting set is down or otherwise not able to
    service requests for objects in the placement group. When these situations
    arise, it may match one of the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You added or removed an OSD. Then, CRUSH reassigned the placement group
      to other OSDs and therefore changed the composition of the <span class="emphasis"><em>acting set</em></span>,
      causing the migration of data with a 'backfill' process.
     </p></li><li class="listitem"><p>
      An OSD was 'down', was restarted, and is now recovering.
     </p></li><li class="listitem"><p>
      An OSD in the <span class="emphasis"><em>acting set</em></span> is 'down' or unable to service requests, and
      another OSD has temporarily assumed its duties.
     </p><p>
      Ceph processes a client request using the <span class="emphasis"><em>up set</em></span>, which is the set of
      OSDs that will actually handle the requests. In most cases, the <span class="emphasis"><em>up set</em></span>
      and the <span class="emphasis"><em>acting set</em></span> are virtually identical. When they are not, it may
      indicate that Ceph is migrating data, an OSD is recovering, or that
      there is a problem (for example, Ceph usually echoes a <code class="literal">HEALTH
      WARN</code> state with a 'stuck stale' message in such scenarios).
     </p></li></ul></div><p>
    To retrieve a list of placement groups, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>;ceph pg dump</pre></div><p>
    To view which OSDs are within the <span class="emphasis"><em>acting set</em></span> or the <span class="emphasis"><em>up set</em></span> for a given
    placement group, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg map<em class="replaceable">PG_NUM</em>
osdmap eNNN pg <em class="replaceable">RAW_PG_NUM</em> (<em class="replaceable">PG_NUM</em>) -&gt; up [0,1,2] acting [0,1,2]</pre></div><p>
    The result should tell you the osdmap epoch (eNNN), the placement group
    number (<em class="replaceable">PG_NUM</em>), the OSDs in the <span class="emphasis"><em>up set</em></span> ('up'),
    and the OSDs in the <span class="emphasis"><em>acting set</em></span> ('acting'):
   </p><div id="id-1.3.5.4.15.6.9" data-id-title="Cluster Problem Indicator" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Cluster Problem Indicator</h6><p>
     If the <span class="emphasis"><em>up set</em></span> and <span class="emphasis"><em>acting set</em></span> do not match, this may be an indicator
     either of the cluster rebalancing itself, or of a potential problem with
     the cluster.
    </p></div></section><section class="sect2" id="op-peering" data-id-title="Peering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.3 </span><span class="title-name">Peering</span> <a title="Permalink" class="permalink" href="#op-peering">#</a></h3></div></div></div><p>
    Before you can write data to a placement group, it must be in an 'active'
    state, and it should be in a 'clean' state. For Ceph to determine the
    current state of a placement group, the primary OSD of the placement group
    (the first OSD in the <span class="emphasis"><em>acting set</em></span>), peers with the secondary and tertiary
    OSDs to establish agreement on the current state of the placement group
    (assuming a pool with three replicas of the PG).
   </p><div class="figure" id="id-1.3.5.4.15.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_peering.png" target="_blank"><img src="images/ceph_peering.png" width="" alt="Peering Schema"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.2: </span><span class="title-name">Peering Schema </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.7.3">#</a></h6></div></div></section><section class="sect2" id="op-mon-pg-states" data-id-title="Monitoring Placement Group States"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.4 </span><span class="title-name">Monitoring Placement Group States</span> <a title="Permalink" class="permalink" href="#op-mon-pg-states">#</a></h3></div></div></div><p>
    If you execute a command such as <code class="command">ceph health</code>,
    <code class="command">ceph -s</code> or <code class="command">ceph -w</code>, you may notice
    that the cluster does not always echo back the <code class="literal">HEALTH OK</code>
    message. After you check to see if the OSDs are running, you should also
    check placement group states.
   </p><p>
    Expect that the cluster will <span class="bold"><strong>not</strong></span> echo
    <code class="literal">HEALTH OK</code> in a number of placement group peering-related
    circumstances:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You have just created a pool and placement groups have not peered yet.
     </p></li><li class="listitem"><p>
      The placement groups are recovering.
     </p></li><li class="listitem"><p>
      You have just added an OSD to or removed an OSD from the cluster.
     </p></li><li class="listitem"><p>
      You have just modified your CRUSH Map and your placement groups are
      migrating.
     </p></li><li class="listitem"><p>
      There is inconsistent data in different replicas of a placement group.
     </p></li><li class="listitem"><p>
      Ceph is scrubbing a placement group’s replicas.
     </p></li><li class="listitem"><p>
      Ceph does not have enough storage capacity to complete backfilling
      operations.
     </p></li></ul></div><p>
    If one of the above mentioned circumstances causes Ceph to echo
    <code class="literal">HEALTH WARN</code>, do not panic. In many cases, the cluster
    will recover on its own. In some cases, you may need to take action. An
    important aspect of monitoring placement groups is to ensure that when the
    cluster is up and running, all placement groups are 'active' and preferably
    in the 'clean state'. To see the status of all placement groups, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail</pre></div><p>
    The result should tell you the total number of placement groups (x), how
    many placement groups are in a particular state such as 'active+clean' (y)
    and the amount of data stored (z).
   </p><p>
    In addition to the placement group states, Ceph will also echo back the
    amount of storage capacity used (aa), the amount of storage capacity
    remaining (bb), and the total storage capacity for the placement group.
    These numbers can be important in a few cases:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You are reaching your <code class="option">near full ratio</code> or <code class="option">full
      ratio</code>.
     </p></li><li class="listitem"><p>
      Your data is not getting distributed across the cluster because of an
      error in your CRUSH configuration.
     </p></li></ul></div><div id="id-1.3.5.4.15.8.10" data-id-title="Placement Group IDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Placement Group IDs</h6><p>
     Placement group IDs consist of the pool number (not pool name) followed by
     a period (.) and the placement group ID—a hexadecimal number. You
     can view pool numbers and their names from the output of <code class="command">ceph osd
     lspools</code>. For example, the default pool <code class="literal">rbd</code>
     corresponds to pool number 0. A fully qualified placement group ID has the
     following form:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">POOL_NUM</em>.<em class="replaceable">PG_ID</em></pre></div><p>
     And it typically looks like this:
    </p><div class="verbatim-wrap"><pre class="screen">0.1f</pre></div></div><p>
    To retrieve a list of placement groups, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg dump</pre></div><p>
    You can also format the output in JSON format and save it to a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg dump -o <em class="replaceable">FILE_NAME</em> --format=json</pre></div><p>
    To query a particular placement group, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg <em class="replaceable">POOL_NUM</em>.<em class="replaceable">PG_ID</em> query</pre></div><p>
    The following list describes the common placement group states in detail.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.4.15.8.18.1"><span class="term">CREATING</span></dt><dd><p>
       When you create a pool, it will create the number of placement groups
       you specified. Ceph will echo 'creating' when it is creating one or
       more placement groups. When they are created, the OSDs that are part of
       the placement group’s <span class="emphasis"><em>acting set</em></span> will peer. When peering is complete,
       the placement group status should be 'active+clean', which means that a
       Ceph client can begin writing to the placement group.
      </p><div class="figure" id="id-1.3.5.4.15.8.18.1.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pg_creating.png" target="_blank"><img src="images/ceph_pg_creating.png" width="" alt="Placement Groups Status"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.3: </span><span class="title-name">Placement Groups Status </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.8.18.1.2.2">#</a></h6></div></div></dd><dt id="id-1.3.5.4.15.8.18.2"><span class="term">PEERING</span></dt><dd><p>
       When Ceph is peering a placement group, it is bringing the OSDs that
       store the replicas of the placement group into agreement about the state
       of the objects and metadata in the placement group. When Ceph
       completes peering, this means that the OSDs that store the placement
       group agree about the current state of the placement group. However,
       completion of the peering process does
       <span class="bold"><strong>not</strong></span> mean that each replica has the
       latest contents.
      </p><div id="id-1.3.5.4.15.8.18.2.2.2" data-id-title="Authoritative History" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Authoritative History</h6><p>
        Ceph will <span class="bold"><strong>not</strong></span> acknowledge a write
        operation to a client until all OSDs of the <span class="emphasis"><em>acting set</em></span> persist the
        write operation. This practice ensures that at least one member of the
        <span class="emphasis"><em>acting set</em></span> will have a record of every acknowledged write operation
        since the last successful peering operation.
       </p><p>
        With an accurate record of each acknowledged write operation, Ceph
        can construct and enlarge a new authoritative history of the placement
        group—a complete and fully ordered set of operations that, if
        performed, would bring an OSD’s copy of a placement group up to date.
       </p></div></dd><dt id="id-1.3.5.4.15.8.18.3"><span class="term">ACTIVE</span></dt><dd><p>
       When Ceph completes the peering process, a placement group may become
       'active'. The 'active' state means that the data in the placement group
       is generally available in the primary placement group and the replicas
       for read and write operations.
      </p></dd><dt id="id-1.3.5.4.15.8.18.4"><span class="term">CLEAN</span></dt><dd><p>
       When a placement group is in the 'clean' state, the primary OSD and the
       replica OSDs have successfully peered and there are no stray replicas
       for the placement group. Ceph replicated all objects in the placement
       group the correct number of times.
      </p></dd><dt id="id-1.3.5.4.15.8.18.5"><span class="term">DEGRADED</span></dt><dd><p>
       When a client writes an object to the primary OSD, the primary OSD is
       responsible for writing the replicas to the replica OSDs. After the
       primary OSD writes the object to storage, the placement group will
       remain in a 'degraded' state until the primary OSD has received an
       acknowledgement from the replica OSDs that Ceph created the replica
       objects successfully.
      </p><p>
       The reason a placement group can be 'active+degraded' is that an OSD may
       be 'active' even though it does not hold all of the objects yet. If an
       OSD goes down, Ceph marks each placement group assigned to the OSD as
       'degraded'. The OSDs must peer again when the OSD comes back up.
       However, a client can still write a new object to a degraded placement
       group if it is 'active'.
      </p><p>
       If an OSD is 'down' and the 'degraded' condition persists, Ceph may
       mark the down OSD as 'out' of the cluster and remap the data from the
       'down' OSD to another OSD. The time between being marked 'down' and
       being marked 'out' is controlled by the <code class="option">mon osd down out
       interval</code> option, which is set to 600 seconds by default.
      </p><p>
       A placement group can also be 'degraded' because Ceph cannot find one
       or more objects that should be in the placement group. While you cannot
       read or write to unfound objects, you can still access all of the other
       objects in the 'degraded' placement group.
      </p></dd><dt id="id-1.3.5.4.15.8.18.6"><span class="term">RECOVERING</span></dt><dd><p>
       Ceph was designed for fault-tolerance at a scale where hardware and
       software problems are ongoing. When an OSD goes 'down', its contents may
       fall behind the current state of other replicas in the placement groups.
       When the OSD is back 'up', the contents of the placement groups must be
       updated to reflect the current state. During that time period, the OSD
       may reflect a 'recovering' state.
      </p><p>
       Recovery is not always trivial, because a hardware failure may cause a
       cascading failure of multiple OSDs. For example, a network switch for a
       rack or cabinet may fail, which can cause the OSDs of a number of host
       machines to fall behind the current state of the cluster. Each of the
       OSDs must recover when the fault is resolved.
      </p><p>
       Ceph provides a number of settings to balance the resource contention
       between new service requests and the need to recover data objects and
       restore the placement groups to the current state. The <code class="option">osd
       recovery delay start</code> setting allows an OSD to restart, re-peer
       and even process some replay requests before starting the recovery
       process. The <code class="option">osd recovery thread timeout</code> sets a thread
       timeout, because multiple OSDs may fail, restart and re-peer at
       staggered rates. The <code class="option">osd recovery max active</code> setting
       limits the number of recovery requests an OSD will process
       simultaneously to prevent the OSD from failing to serve. The <code class="option">osd
       recovery max chunk</code> setting limits the size of the recovered
       data chunks to prevent network congestion.
      </p></dd><dt id="id-1.3.5.4.15.8.18.7"><span class="term">BACK FILLING</span></dt><dd><p>
       When a new OSD joins the cluster, CRUSH will reassign placement groups
       from OSDs in the cluster to the newly added OSD. Forcing the new OSD to
       accept the reassigned placement groups immediately can put excessive
       load on the new OSD. Backfilling the OSD with the placement groups
       allows this process to begin in the background. When backfilling is
       complete, the new OSD will begin serving requests when it is ready.
      </p><p>
       During the backfill operations, you may see one of several states:
       'backfill_wait' indicates that a backfill operation is pending, but is
       not yet in progress; 'backfill' indicates that a backfill operation is
       in progress; 'backfill_too_full' indicates that a backfill operation was
       requested, but could not be completed because of insufficient storage
       capacity. When a placement group cannot be backfilled, it may be
       considered 'incomplete'.
      </p><p>
       Ceph provides a number of settings to manage the load associated with
       reassigning placement groups to an OSD (especially a new OSD). By
       default, <code class="option">osd max backfills</code> sets the maximum number of
       concurrent backfills to or from an OSD to 10. The <code class="option">backfill full
       ratio</code> enables an OSD to refuse a backfill request if the OSD is
       approaching its full ratio (90%, by default) and change with
       <code class="command">ceph osd set-backfillfull-ratio</code> command. If an OSD
       refuses a backfill request, the <code class="option">osd backfill retry
       interval</code> enables an OSD to retry the request (after 10 seconds,
       by default). OSDs can also set <code class="option">osd backfill scan min</code>
       and <code class="option">osd backfill scan max</code> to manage scan intervals (64
       and 512, by default).
      </p></dd><dt id="id-1.3.5.4.15.8.18.8"><span class="term">REMAPPED</span></dt><dd><p>
       When the <span class="emphasis"><em>acting set</em></span> that services a placement group changes, the data
       migrates from the old <span class="emphasis"><em>acting set</em></span> to the new <span class="emphasis"><em>acting set</em></span>. It may take
       some time for a new primary OSD to service requests. So it may ask the
       old primary to continue to service requests until the placement group
       migration is complete. When data migration completes, the mapping uses
       the primary OSD of the new <span class="emphasis"><em>acting set</em></span>.
      </p></dd><dt id="id-1.3.5.4.15.8.18.9"><span class="term">STALE</span></dt><dd><p>
       While Ceph uses heartbeats to ensure that hosts and daemons are
       running, the <code class="literal">ceph-osd</code> daemons may also get into a
       'stuck' state where they are not reporting statistics in a timely manner
       (for example, a temporary network fault). By default, OSD daemons report
       their placement group, boot and failure statistics every half second
       (0.5), which is more frequent than the heartbeat thresholds. If the
       primary OSD of a placement group’s <span class="emphasis"><em>acting set</em></span> fails to report to the
       monitor or if other OSDs have reported the primary OSD as 'down', the
       monitors will mark the placement group as 'stale'.
      </p><p>
       When you start your cluster, it is common to see the 'stale' state until
       the peering process completes. After your cluster has been running for a
       while, seeing placement groups in the 'stale' state indicates that the
       primary OSD for those placement groups is down or not reporting
       placement group statistics to the monitor.
      </p></dd></dl></div></section><section class="sect2" id="op-pg-stuck-states" data-id-title="Identifying Troubled Placement Groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.5 </span><span class="title-name">Identifying Troubled Placement Groups</span> <a title="Permalink" class="permalink" href="#op-pg-stuck-states">#</a></h3></div></div></div><p>
    As previously noted, a placement group is not necessarily problematic
    because its state is not 'active+clean'. Generally, Ceph’s ability to
    self repair may not be working when placement groups get stuck. The stuck
    states include:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="bold"><strong>Unclean</strong></span>: Placement groups contain
      objects that are not replicated the required number of times. They should
      be recovering.
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Inactive</strong></span>: Placement groups cannot
      process reads or writes because they are waiting for an OSD with the most
      up-to-date data to come back up.
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Stale</strong></span>: Placement groups are in an
      unknown state, because the OSDs that host them have not reported to the
      monitor cluster in a while (configured by the <code class="option">mon osd report
      timeout</code> option).
     </p></li></ul></div><p>
    To identify stuck placement groups, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]</pre></div></section><section class="sect2" id="op-pg-objectfinding" data-id-title="Finding an Object Location"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.11.6 </span><span class="title-name">Finding an Object Location</span> <a title="Permalink" class="permalink" href="#op-pg-objectfinding">#</a></h3></div></div></div><p>
    To store object data in the Ceph Object Store, a Ceph client needs to
    set an object name and specify a related pool. The Ceph client retrieves
    the latest cluster map and the CRUSH algorithm calculates how to map the
    object to a placement group, and then calculates how to assign the
    placement group to an OSD dynamically. To find the object location, all you
    need is the object name and the pool name. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd map <em class="replaceable">POOL_NAME</em> <em class="replaceable">OBJECT_NAME</em> [<em class="replaceable">NAMESPACE</em>]</pre></div><div class="complex-example"><div class="example" id="id-1.3.5.4.15.10.4" data-id-title="Locating an Object"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 17.1: </span><span class="title-name">Locating an Object </span><a title="Permalink" class="permalink" href="#id-1.3.5.4.15.10.4">#</a></h6></div><div class="example-contents"><p>
     As an example, let us create an object. Specify an object name
     'test-object-1', a path to an example file 'testfile.txt' containing some
     object data, and a pool name 'data' using the <code class="command">rados put</code>
     command on the command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados put test-object-1 testfile.txt --pool=data</pre></div><p>
     To verify that the Ceph Object Store stored the object, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p data ls</pre></div><p>
     Now, identify the object location. Ceph will output the object’s
     location:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)</pre></div><p>
     To remove the example object, simply delete it using the <code class="command">rados
     rm</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados rm test-object-1 --pool=data</pre></div></div></div></div></section></section><section class="sect1" id="op-osd-not-running" data-id-title="OSD Is Not Running"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.12 </span><span class="title-name">OSD Is Not Running</span> <a title="Permalink" class="permalink" href="#op-osd-not-running">#</a></h2></div></div></div><p>
   Under normal circumstances, simply restarting the
   <code class="literal">ceph-osd</code> daemon will allow it to rejoin the cluster and
   recover.
  </p><section class="sect2" id="op-osd-not-start" data-id-title="An OSD Will Not Start"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.12.1 </span><span class="title-name">An OSD Will Not Start</span> <a title="Permalink" class="permalink" href="#op-osd-not-start">#</a></h3></div></div></div><p>
    If you start your cluster and an OSD will not start, check the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="bold"><strong>Configuration File</strong></span>: If you were not able
      to get OSDs running from a new installation, check your configuration
      file to ensure it conforms (for example, <code class="literal">host</code> and not
      <code class="literal">hostname</code>).
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Check Paths</strong></span>: Check the paths in your
      configuration, and the actual paths themselves for data and journals. If
      you separate the OSD data from the journal data and there are errors in
      your configuration file or in the actual mounts, you may have trouble
      starting OSDs. If you want to store the journal on a block device, you
      need to partition your journal disk and assign one partition per OSD.
     </p></li><li class="listitem"><p>
      <span class="bold"><strong>Check Max Threadcount</strong></span>: If you have a
      node with a lot of OSDs, you may be hitting the default maximum number of
      threads (usually 32,000), especially during recovery. You can increase
      the number of threads using the <code class="command">sysctl</code> command to see
      if increasing the maximum number of threads to the maximum possible
      number of threads allowed (for example, 4194303) will help:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>sysctl -w kernel.pid_max=4194303</pre></div><p>
      If increasing the maximum thread count resolves the issue, you can make
      it permanent by including a <code class="option">kernel.pid_max</code> setting in
      the <code class="filename">/etc/sysctl.conf</code> file:
     </p><div class="verbatim-wrap"><pre class="screen">kernel.pid_max = 4194303</pre></div></li></ul></div></section><section class="sect2" id="op-osd-failed" data-id-title="An OSD Failed"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.12.2 </span><span class="title-name">An OSD Failed</span> <a title="Permalink" class="permalink" href="#op-osd-failed">#</a></h3></div></div></div><p>
    When the <code class="literal">ceph-osd</code> process dies, the monitor will learn
    about the failure from surviving <code class="literal">ceph-osd</code> daemons and
    report it via the <code class="command">ceph health</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health
HEALTH_WARN 1/3 in osds are down</pre></div><p>
    Specifically, you will get a warning whenever there are
    <code class="literal">ceph-osd</code> processes that are marked 'in' and 'down'. You
    can identify which<code class="literal"> ceph-osds</code> are down with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080</pre></div><p>
    If there is a disk failure or other fault preventing
    <code class="literal">ceph-osd</code> from functioning or restarting, an error
    message should be present in its log file in
    <code class="filename">/var/log/ceph</code>.
   </p><p>
    If the daemon stopped because of a heartbeat failure, the underlying kernel
    file system may be unresponsive. Check the <code class="command">dmesg</code> command
    output for disk or other kernel errors.
   </p></section><section class="sect2" id="op-no-disk-space" data-id-title="No Free Disk Space"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.12.3 </span><span class="title-name">No Free Disk Space</span> <a title="Permalink" class="permalink" href="#op-no-disk-space">#</a></h3></div></div></div><p>
    Ceph prevents you from writing to a full OSD to prevent losing data. In
    an operational cluster, you should receive a warning when your cluster is
    getting near its full ratio. The <code class="option">mon osd full ratio</code> option
    defaults to 0.95, or 95% of capacity before it stops clients from writing
    data. The <code class="option">mon osd backfillfull ratio</code> defaults to 0.90, or
    90% of capacity when it blocks backfills from starting. The OSD nearfull
    ratio defaults to 0.85, or 85% of capacity when it generates a health
    warning. You can change the value of 'nearfull' with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">0.0 to 1.0</em></pre></div><p>
    Full cluster issues usually arise when testing how Ceph handles an OSD
    failure on a small cluster. When one node has a high percentage of the
    cluster’s data, the cluster can easily eclipse its 'nearfull' and 'full'
    ratio immediately. If you are testing how Ceph reacts to OSD failures on
    a small cluster, you should leave sufficient free disk space and consider
    temporarily lowering the OSD full ratio, OSD backfillfull ratio and OSD
    nearfull ratio using these commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">0.0 to 1.0</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">0.0 to 1.0</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">0.0 to 1.0</em></pre></div><p>
    Full Ceph OSD will be reported by <code class="command">ceph health</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health
HEALTH_WARN 1 nearfull osd(s)</pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%</pre></div><p>
    The best way to deal with a full cluster is to add new Ceph OSDs, allowing the
    cluster to redistribute data to the newly available storage.
   </p><p>
    If you cannot start an OSD because it is full, you may delete some data by
    deleting some placement group directories in the full OSD.
   </p><div id="id-1.3.5.4.16.5.12" data-id-title="Deleting a Placement Group Directory" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Deleting a Placement Group Directory</h6><p>
     If you choose to delete a placement group directory on a full OSD,
     <span class="bold"><strong>do not</strong></span> delete the same placement group
     directory on another full OSD, or you may <span class="bold"><strong>lose
     data</strong></span>. You <span class="bold"><strong>must</strong></span> maintain at
     least one copy of your data on at least one OSD.
    </p></div></section></section></section><section class="chapter" id="monitoring-alerting" data-id-title="Monitoring and Alerting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18 </span><span class="title-name">Monitoring and Alerting</span> <a title="Permalink" class="permalink" href="#monitoring-alerting">#</a></h2></div></div></div><p>
  In SUSE Enterprise Storage 6, DeepSea no longer deploys a monitoring and alerting
  stack on the Salt master. Users have to define the Prometheus role for
  Prometheus and Alertmanager, and the Grafana role for Grafana. When
  multiple nodes are assigned with the Prometheus or Grafana role, a highly
  available setup is deployed.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Prometheus</strong></span> is the monitoring and
    alerting toolkit.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Alertmanager</strong></span> handles alerts sent by the
    Prometheus server.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Grafana</strong></span> is the visualization and
    alerting software.
   </p></li><li class="listitem"><p>
    The <code class="systemitem">prometheus-node_exporter</code> is the
    service running on all Salt minions.
   </p></li></ul></div><p>
  The Prometheus configuration and <span class="emphasis"><em>scrape</em></span> targets
  (exporting daemons) are setup automatically by DeepSea. DeepSea also
  deploys a list of default alerts, for example <code class="literal">health
  error</code>, <code class="literal">10% OSDs down</code>, or <code class="literal">pgs
  inactive</code>.
 </p><section class="sect1" id="pillar-variables" data-id-title="Pillar Variables"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.1 </span><span class="title-name">Pillar Variables</span> <a title="Permalink" class="permalink" href="#pillar-variables">#</a></h2></div></div></div><p>
   The Salt pillar is a key-value store that provides information and
   configuration values to minions. It is available to all minions, each with
   differing content. The Salt pillar is pre-populated with default values and
   can be customized in two different ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong><code class="filename">/srv/pillar/ceph/stack/global.yml</code></strong></span>:
     to change pillar variables for all nodes.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="filename">/srv/pillar/ceph/stack/<em class="replaceable">CLUSTER_NAME</em>/minions/<em class="replaceable">HOST</em></code></strong></span>:
     to change specific minion configurations.
    </p></li></ul></div><p>
   The pillar variables below are available to all nodes by default:
  </p><div class="verbatim-wrap"><pre class="screen">  monitoring:
    alertmanager:
      config: salt://path/to/config
      additional_flags: ''
    grafana:
      ssl_cert: False # self-signed certs are created by default
      ssl_key: False # self-signed certs are created by default
    prometheus:
      # pass additional configration to prometheus
      additional_flags: ''
      alert_relabel_config: []
      rule_files: []
      # per exporter config variables
      scrape_interval:
        ceph: 10
        node_exporter: 10
        prometheus: 10
        grafana: 10
      relabel_config:
        alertmanager: []
        ceph: []
        node_exporter: []
        prometheus: []
        grafana: []
      metric_relabel_config:
        ceph: []
        node_exporter: []
        prometheus: []
        grafana: []
      target_partition:
        ceph: '1/1'
        node_exporter: '1/1'
        prometheus: '1/1'
        grafana: '1/1'</pre></div></section><section class="sect1" id="grafana" data-id-title="Grafana"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.2 </span><span class="title-name">Grafana</span> <a title="Permalink" class="permalink" href="#grafana">#</a></h2></div></div></div><section class="sect2" id="grafana-certs" data-id-title="Grafana SSL/TLS certificates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.2.1 </span><span class="title-name">Grafana SSL/TLS certificates</span> <a title="Permalink" class="permalink" href="#grafana-certs">#</a></h3></div></div></div><p>
    All traffic is encrypted through Grafana. You can either supply your own
    SSL certs or create self-signed one.
   </p><p>
    Grafana uses the following variables:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="bold"><strong><code class="literal">ssl_cert</code></strong></span>
     </p></li><li class="listitem"><p>
      <span class="bold"><strong><code class="literal">ssl_key</code></strong></span>
     </p></li></ul></div><p>
    The Ceph Dashboard embeds the Grafana dashboards via HTML
    <code class="literal">iframe</code> elements. If Grafana is configured without
    SSL/TLS support, or if SSL is using self-signed certificates, and if the
    SSL support in the dashboard has been enabled (which is the default
    configuration), then most browsers will block the embedding of insecure
    content into a secured web page. If you can not see the embedded Grafana
    dashboards in Ceph Dashboard, check your browser's documentation on how to
    unblock mixed content or how to accept self-signed certificates.
    Alternatively, consider enabling SSL/TLS support in Grafana, using a
    certificate that is issued by a certificate authority (CA) known to the
    browser.
   </p><p>
    For more information on supplying your own SSL certificates, see
    <a class="xref" href="#cert-sign-CA" title="13.1.3. Certificates Signed by CA">Section 13.1.3, “Certificates Signed by CA”</a>. For generating a self-signed or trusted
    third-party certificate using OpenSSL, see
    <a class="xref" href="#self-sign-certificates-openssl" title="13.1.2. Self-signed or Trusted Third-party Certificate with OpenSSL">Section 13.1.2, “Self-signed or Trusted Third-party Certificate with OpenSSL”</a>. For creating your own
    CA-signed certificate, see <a class="xref" href="#self-sign-certificates" title="13.1.1. Self-signed Certificates">Section 13.1.1, “Self-signed Certificates”</a>. For
    creating your own custom CA signed certificate, see
    <a class="xref" href="#cert-sign-custom-CA" title="13.1.4. Certificates Signed with a Custom CA">Section 13.1.4, “Certificates Signed with a Custom CA”</a>.
   </p></section><section class="sect2" id="grafana-url" data-id-title="Configuring Grafana frontend URL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.2.2 </span><span class="title-name">Configuring Grafana frontend URL</span> <a title="Permalink" class="permalink" href="#grafana-url">#</a></h3></div></div></div><p>
    The Ceph Dashboard backend requires the Grafana URL to be able to verify the
    existence of Grafana dashboards before the frontend even loads them. Due
    to the nature of how Grafana is implemented in Ceph Dashboard, this means
    that two working connections are required in order to be able to see
    Grafana graphs in Ceph Dashboard:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The backend (Ceph Manager module) needs to verify the existence of the requested
      graph. If this request succeeds, it lets the frontend know that it can
      safely access Grafana.
     </p></li><li class="listitem"><p>
      The frontend then requests the Grafana graphs directly from the user's
      browser using an <code class="literal">iframe</code>. The Grafana instance is
      accessed directly without any detour through Ceph Dashboard.
     </p></li></ul></div><p>
    Now, it might be the case that your environment makes it difficult for the
    user's browser to directly access the URL configured in Ceph Dashboard. To
    solve this issue, a separate URL can be configured which will solely be
    used to tell the frontend (the user's browser) which URL it should use to
    access Grafana.
   </p><p>
    To change the URL that is returned to the frontend issue the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard set-grafana-frontend-api-url <em class="replaceable">GRAFANA-SERVER-URL</em></pre></div><p>
    If no value is set for that option, it will simply fall back to the value
    of the <em class="replaceable">GRAFANA_API_URL</em> option, which is set
    automatically by DeepSea. If set, it will instruct the browser to use
    this URL to access Grafana.
   </p></section></section><section class="sect1" id="prometheus" data-id-title="Prometheus"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.3 </span><span class="title-name">Prometheus</span> <a title="Permalink" class="permalink" href="#prometheus">#</a></h2></div></div></div><p>
   The exporter based configuration that can be passed through the pillar.
   These groups map to exporters that provide data. The node exporter is
   present on all nodes, Ceph is exported by the Ceph Manager nodes, Prometheus
   and Grafana is exported by the respective Prometheus and Grafana
   nodes.
  </p><p>
   Prometheus uses the following variables:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">scrape_interval</code></strong></span>:
     change the scrape interval, how often an exporter is to be scraped.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">target_partition</code></strong></span>:
     partition scrape targets when multiple Prometheus instnaces are deployed
     and have some instances scrape only part of all exporter instances.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">relabel_config</code></strong></span>:
     dynamically rewrites the label set of a target before it gets scraped.
     Multiple relabeling steps can be configured per scrape configuration.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong><code class="literal">metrics_relabel_config</code></strong></span>:
     applied to samples as the last step before ingestion.
    </p></li></ul></div><section class="sect2" id="prometheus-security-model" data-id-title="Security Model"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.3.1 </span><span class="title-name">Security Model</span> <a title="Permalink" class="permalink" href="#prometheus-security-model">#</a></h3></div></div></div><p>
    Prometheus' security model presumes that untrusted users have access to
    the Prometheus HTTP endpoint and logs. Untrusted users have access to all
    the (meta-)data Prometheus collects that is contained in the database,
    plus a variety of operational and debugging information.
   </p><p>
    However, Prometheus' HTTP API is limited to read-only operations.
    Configurations cannot be changed using the API, and secrets are not
    exposed. Moreover, Prometheus has some built-in measures to mitigate the
    impact of denial of service attacks.
   </p></section></section><section class="sect1" id="alerting-alertmanager" data-id-title="Alertmanager"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.4 </span><span class="title-name">Alertmanager</span> <a title="Permalink" class="permalink" href="#alerting-alertmanager">#</a></h2></div></div></div><p>
   The Alertmanager handles alerts sent by the Prometheus server. It takes
   care of deduplicating, grouping, and routing them to the correct receiver.
   It also takes care of silencing of alerts. Alertmanager is configured via
   the command line flags and a configuration file that defines inhibition
   rules, notification routing and notification receivers.
  </p><section class="sect2" id="id-1.3.5.5.8.3" data-id-title="Configuration File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.1 </span><span class="title-name">Configuration File</span> <a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3">#</a></h3></div></div></div><p>
    Alertmanager's configuration is different for each deployment. Therefore,
    DeepSea does not ship any related defaults. You need to provide your own
    <code class="filename">alertmanager.yml</code> configuration file. The
    <span class="package">alertmanager</span> package by default installs a configuration
    file <code class="filename">/etc/prometheus/alertmanager.yml</code> which can serve
    as an example configuration. If you prefer to have your Alertmanager
    configuration managed by DeepSea, add the following key to your pillar,
    for example to the
    <code class="filename">/srv/pillar/ceph/stack/ceph/minions/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
    file:
   </p><p>
    For a complete example of Alertmanager's configuration file, see
    <a class="xref" href="#troubleshooting-alerts" title="18.5. Troubleshooting Alerts">Section 18.5, “Troubleshooting Alerts”</a>.
   </p><div class="verbatim-wrap"><pre class="screen">monitoring:
  alertmanager:
    /path/to/your/alertmanager/config.yml</pre></div><p>
    Alertmanager's configuration file is written in the YAML format. It
    follows the scheme described below. Parameters in brackets are optional.
    For non-list parameters the default value is used. The following generic
    placeholders are used in the scheme:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.8.3.6.1"><span class="term"><em class="replaceable">DURATION</em></span></dt><dd><p>
       A duration matching the regular expression
       <code class="literal">[0-9]+(ms|[smhdwy])</code>
      </p></dd><dt id="id-1.3.5.5.8.3.6.2"><span class="term"><em class="replaceable">LABELNAME</em></span></dt><dd><p>
       A string matching the regular expression
       <code class="literal">[a-zA-Z_][a-zA-Z0-9_]*</code>
      </p></dd><dt id="id-1.3.5.5.8.3.6.3"><span class="term"><em class="replaceable">LABELVALUE</em></span></dt><dd><p>
       A string of Unicode characters.
      </p></dd><dt id="id-1.3.5.5.8.3.6.4"><span class="term"><em class="replaceable">FILEPATH</em></span></dt><dd><p>
       A valid path in the current working directory.
      </p></dd><dt id="id-1.3.5.5.8.3.6.5"><span class="term"><em class="replaceable">BOOLEAN</em></span></dt><dd><p>
       A Boolean that can take the values 'true' or 'false'.
      </p></dd><dt id="id-1.3.5.5.8.3.6.6"><span class="term"><em class="replaceable">STRING</em></span></dt><dd><p>
       A regular string.
      </p></dd><dt id="id-1.3.5.5.8.3.6.7"><span class="term"><em class="replaceable">SECRET</em></span></dt><dd><p>
       A regular string that is a secret, for example a password.
      </p></dd><dt id="id-1.3.5.5.8.3.6.8"><span class="term"><em class="replaceable">TMPL_STRING</em></span></dt><dd><p>
       A string which is template-expanded before usage.
      </p></dd><dt id="id-1.3.5.5.8.3.6.9"><span class="term"><em class="replaceable">TMPL_SECRET</em></span></dt><dd><p>
       A secret string which is template-expanded before usage.
      </p></dd></dl></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.7" data-id-title="Global Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.1: </span><span class="title-name">Global Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.7">#</a></h6></div><div class="example-contents"><p>
     Parameters in the <code class="literal">global:</code> configuration are valid in
     all other configuration contexts. They also serve as defaults for other
     configuration sections.
    </p><div class="verbatim-wrap"><pre class="screen">global:
# the time after which an alert is declared resolved if it has not been updated
[ resolve_timeout: <em class="replaceable">DURATION</em> | default = 5m ]

# The default SMTP From header field.
[ smtp_from: <em class="replaceable">TMPL_STRING</em> ]
# The default SMTP smarthost used for sending emails, including port number.
# Port number usually is 25, or 587 for SMTP over TLS
# (sometimes referred to as STARTTLS).
# Example: smtp.example.org:587
[ smtp_smarthost: <em class="replaceable">STRING</em> ]
# The default host name to identify to the SMTP server.
[ smtp_hello: <em class="replaceable">STRING</em> | default = "localhost" ]
[ smtp_auth_username: <em class="replaceable">STRING</em> ]
# SMTP Auth using LOGIN and PLAIN.
[ smtp_auth_password: <em class="replaceable">SECRET</em> ]
# SMTP Auth using PLAIN.
[ smtp_auth_identity: <em class="replaceable">STRING</em> ]
# SMTP Auth using CRAM-MD5.
[ smtp_auth_secret: <em class="replaceable">SECRET</em> ]
# The default SMTP TLS requirement.
[ smtp_require_tls: <em class="replaceable">BOOL</em> | default = true ]

# The API URL to use for Slack notifications.
[ slack_api_url: <em class="replaceable">STRING</em> ]
[ victorops_api_key: <em class="replaceable">STRING</em> ]
[ victorops_api_url: <em class="replaceable">STRING</em> | default = "https://victorops.example.com/integrations/alert/" ]
[ pagerduty_url: <em class="replaceable">STRING</em> | default = "https://pagerduty.example.com/v2/enqueue" ]
[ opsgenie_api_key: <em class="replaceable">STRING</em> ]
[ opsgenie_api_url: <em class="replaceable">STRING</em> | default = "https://opsgenie.example.com/" ]
[ hipchat_api_url: <em class="replaceable">STRING</em> | default = "https://hipchat.example.com/" ]
[ hipchat_auth_token: <em class="replaceable">SECRET</em> ]
[ wechat_api_url: <em class="replaceable">STRING</em> | default = "https://wechat.example.com/cgi-bin/" ]
[ wechat_api_secret: <em class="replaceable">SECRET</em> ]
[ wechat_api_corp_id: <em class="replaceable">STRING</em> ]

# The default HTTP client configuration
[ http_config: <em class="replaceable">HTTP_CONFIG</em> ]

# Files from which custom notification template definitions are read.
# The last component may use a wildcard matcher, e.g. 'templates/*.tmpl'.
templates:
[ - <em class="replaceable">FILEPATH</em> ... ]

# The root node of the routing tree.
route: <em class="replaceable">ROUTE</em>

# A list of notification receivers.
receivers:
- <em class="replaceable">RECEIVER</em> ...

# A list of inhibition rules.
inhibit_rules:
[ - <em class="replaceable">INHIBIT_RULE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.8" data-id-title="ROUTE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.2: </span><span class="title-name"><em class="replaceable">ROUTE</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.8">#</a></h6></div><div class="example-contents"><p>
     A <em class="replaceable">ROUTE</em> block defines a node in a routing tree.
     Unspecified parameters are inherited from its parent node. Every alert
     enters the routing tree at the configured top-level route, which needs to
     match all alerts. It then traverses the child nodes. If the
     <code class="option">continue</code> option is set to 'false', the traversing stops
     after the first matched child. Setting the option to 'true' on a matched
     node, the alert will continue matching against subsequent siblings. If an
     alert does not match any children of a node, the alert is handled based on
     the configuration parameters of the current node.
    </p><div class="verbatim-wrap"><pre class="screen">[ receiver: <em class="replaceable">STRING</em> ]
[ group_by: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]

# If an alert should continue matching subsequent sibling nodes.
[ continue: <em class="replaceable">BOOLEAN</em> | default = false ]

# A set of equality matchers an alert has to fulfill to match a node.
match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]

# A set of regex-matchers an alert has to fulfill to match a node.
match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Time to wait before sending a notification for a group of alerts.
[ group_wait: <em class="replaceable">DURATION</em> | default = 30s ]

# Time to wait before sending a notification about new alerts
# added to a group of alerts for which an initial notification has
# already been sent.
[ group_interval: <em class="replaceable">DURATION</em> | default = 5m ]

# Time to wait before re-sending a notification
[ repeat_interval: <em class="replaceable">DURATION</em> | default = 4h ]

# Possible child routes.
routes:
 [ - <em class="replaceable">ROUTE</em> ... ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.9" data-id-title="INHIBIT_RULE"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.3: </span><span class="title-name"><em class="replaceable">INHIBIT_RULE</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.9">#</a></h6></div><div class="example-contents"><p>
     An inhibition rule mutes a target alert that matches a set of matchers
     when a source alert exists that matches another set of matchers. Both
     alerts need to share the same label values for the label names in the
     <code class="option">equal</code> list.
    </p><p>
     Alerts can match and therefore inhibit themselves. Do not write inhibition
     rules where an alert matches both source and target.
    </p><div class="verbatim-wrap"><pre class="screen"># Matchers that need to be fulfilled for the alerts to be muted.
target_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
target_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Matchers for which at least one alert needs to exist so that the
# inhibition occurs.
source_match:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">LABELVALUE</em>, ... ]
source_match_re:
 [ <em class="replaceable">LABELNAME</em>: <em class="replaceable">REGEX</em>, ... ]

# Labels with an equal value in the source and target
# alert for the inhibition to take effect.
[ equal: '[' <em class="replaceable">LABELNAME</em>, ... ']' ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.10" data-id-title="HTTP_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.4: </span><span class="title-name"><em class="replaceable">HTTP_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.10">#</a></h6></div><div class="example-contents"><p>
     <em class="replaceable">HTTP_CONFIG</em> configures the HTTP client used by
     the receiver to communicate with API services.
    </p><p>
     Note that <code class="option">basic_auth</code>, <code class="option">bearer_token</code> and
     <code class="option">bearer_token_file</code> options are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Sets the 'Authorization' header with the user name and password.
basic_auth:
[ username: <em class="replaceable">STRING</em> ]
[ password: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token.
[ bearer_token: <em class="replaceable">SECRET</em> ]

# Sets the 'Authorization' header with the bearer token read from a file.
[ bearer_token_file: <em class="replaceable">FILEPATH</em> ]

# TLS settings.
tls_config:
# CA certificate to validate the server certificate with.
[ ca_file: <em class="replaceable">FILEPATH</em> ]
# Certificate and key files for client cert authentication to the server.
[ cert_file: <em class="replaceable">FILEPATH</em> ]
[ key_file: <em class="replaceable">FILEPATH</em> ]
# ServerName extension to indicate the name of the server.
# http://tools.ietf.org/html/rfc4366#section-3.1
[ server_name: <em class="replaceable">STRING</em> ]
# Disable validation of the server certificate.
[ insecure_skip_verify: <em class="replaceable">BOOLEAN</em> | default = false]

# Optional proxy URL.
[ proxy_url: <em class="replaceable">STRING</em> ]</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.11" data-id-title="RECEIVER"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.5: </span><span class="title-name"><em class="replaceable">RECEIVER</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.11">#</a></h6></div><div class="example-contents"><p>
     Receiver is a named configuration for one or more notification
     integrations.
    </p><p>
     Instead of adding new receivers, we recommend implementing custom
     notification integrations using the webhook receiver (see
     <a class="xref" href="#alert-webhook" title="WEBHOOK_CONFIG">Example 18.15, “<em class="replaceable">WEBHOOK_CONFIG</em>”</a>).
    </p><div class="verbatim-wrap"><pre class="screen"># The unique name of the receiver.
name: <em class="replaceable">STRING</em>

# Configurations for several notification integrations.
email_configs:
[ - <em class="replaceable">EMAIL_CONFIG</em>, ... ]
hipchat_configs:
[ - <em class="replaceable">HIPCHAT_CONFIG</em>, ... ]
pagerduty_configs:
[ - <em class="replaceable">PAGERDUTY_CONFIG</em>, ... ]
pushover_configs:
[ - <em class="replaceable">PUSHOVER_CONFIG</em>, ... ]
slack_configs:
[ - <em class="replaceable">SLACK_CONFIG</em>, ... ]
opsgenie_configs:
[ - <em class="replaceable">OPSGENIE_CONFIG</em>, ... ]
webhook_configs:
[ - <em class="replaceable">WEBHOOK_CONFIG</em>, ... ]
victorops_configs:
[ - <em class="replaceable">VICTOROPS_CONFIG</em>, ... ]
wechat_configs:
[ - <em class="replaceable">WECHAT_CONFIG</em>, ... ]</pre></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.12" data-id-title="EMAIL_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.6: </span><span class="title-name"><em class="replaceable">EMAIL_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.12">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The email address to send notifications to.
to: <em class="replaceable">TMPL_STRING</em>

# The sender address.
[ from: <em class="replaceable">TMPL_STRING</em> | default = global.smtp_from ]

# The SMTP host through which emails are sent.
[ smarthost: <em class="replaceable">STRING</em> | default = global.smtp_smarthost ]

# The host name to identify to the SMTP server.
[ hello: <em class="replaceable">STRING</em> | default = global.smtp_hello ]

# SMTP authentication details.
[ auth_username: <em class="replaceable">STRING</em> | default = global.smtp_auth_username ]
[ auth_password: <em class="replaceable">SECRET</em> | default = global.smtp_auth_password ]
[ auth_secret: <em class="replaceable">SECRET</em> | default = global.smtp_auth_secret ]
[ auth_identity: <em class="replaceable">STRING</em> | default = global.smtp_auth_identity ]

# The SMTP TLS requirement.
[ require_tls: <em class="replaceable">BOOL</em> | default = global.smtp_require_tls ]

# The HTML body of the email notification.
[ html: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "email.default.html" . }}' ]
# The text body of the email notification.
[ text: <em class="replaceable">TMPL_STRING</em> ]

# Further headers email header key/value pairs. Overrides any headers
# previously set by the notification implementation.
[ headers: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.13" data-id-title="HIPCHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.7: </span><span class="title-name"><em class="replaceable">HIPCHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.13">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The HipChat Room ID.
room_id: <em class="replaceable">TMPL_STRING</em>
# The authentication token.
[ auth_token: <em class="replaceable">SECRET</em> | default = global.hipchat_auth_token ]
# The URL to send API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.hipchat_api_url ]

# A label to be shown in addition to the sender's name.
[ from:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.from" . }}' ]
# The message body.
[ message:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "hipchat.default.message" . }}' ]
# Whether this message will trigger a user notification.
[ notify:  <em class="replaceable">BOOLEAN</em> | default = false ]
# Determines how the message is treated by the alertmanager and rendered inside HipChat. Valid values are 'text' and 'html'.
[ message_format:  <em class="replaceable">STRING</em> | default = 'text' ]
# Background color for message.
[ color:  <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}red{{ else }}green{{ end }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.5.5.8.3.14" data-id-title="PAGERDUTY_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.8: </span><span class="title-name"><em class="replaceable">PAGERDUTY_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.14">#</a></h6></div><div class="example-contents"><p>
     The <code class="option">routing_key</code> and <code class="option">service_key</code> options
     are mutually exclusive.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The PagerDuty integration key (when using 'Events API v2').
routing_key: <em class="replaceable">TMPL_SECRET</em>
# The PagerDuty integration key (when using 'Prometheus').
service_key: <em class="replaceable">TMPL_SECRET</em>

# The URL to send API requests to.
[ url: <em class="replaceable">STRING</em> | default = global.pagerduty_url ]

# The client identification of the Alertmanager.
[ client:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.client" . }}' ]
# A backlink to the notification sender.
[ client_url:  <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.clientURL" . }}' ]

# The incident description.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pagerduty.default.description" .}}' ]

# Severity of the incident.
[ severity: <em class="replaceable">TMPL_STRING</em> | default = 'error' ]

# A set of arbitrary key/value pairs that provide further details.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } | default = {
 firing:       '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
 resolved:     '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
 num_firing:   '{{ .Alerts.Firing | len }}'
 num_resolved: '{{ .Alerts.Resolved | len }}'
} ]

# The HTTP client's configuration.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.15" data-id-title="PUSHOVER_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.9: </span><span class="title-name"><em class="replaceable">PUSHOVER_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.15">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The recipient user key.
user_key: <em class="replaceable">SECRET</em>

# Registered application’s API token.
token: <em class="replaceable">SECRET</em>

# Notification title.
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.title" . }}' ]

# Notification message.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.message" . }}' ]

# A supplementary URL displayed together with the message.
[ url: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "pushover.default.url" . }}' ]

# Priority.
[ priority: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}2{{ else }}0{{ end }}' ]

# How often the Pushover servers will send the same notification (at least 30 seconds).
[ retry: <em class="replaceable">DURATION</em> | default = 1m ]

# How long your notification will continue to be retried (unless the user
# acknowledges the notification).
[ expire: <em class="replaceable">DURATION</em> | default = 1h ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.16" data-id-title="SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.10: </span><span class="title-name"><em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.16">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The Slack webhook URL.
[ api_url: <em class="replaceable">SECRET</em> | default = global.slack_api_url ]

# The channel or user to send notifications to.
channel: <em class="replaceable">TMPL_STRING</em>

# API request data as defined by the Slack webhook API.
[ icon_emoji: <em class="replaceable">TMPL_STRING</em> ]
[ icon_url: <em class="replaceable">TMPL_STRING</em> ]
[ link_names: <em class="replaceable">BOOLEAN</em> | default = false ]
[ username: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.username" . }}' ]
# The following parameters define the attachment.
actions:
[ <em class="replaceable">ACTION_CONFIG</em> ... ]
[ color: <em class="replaceable">TMPL_STRING</em> | default = '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}' ]
[ fallback: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.fallback" . }}' ]
fields:
[ <em class="replaceable">FIELD_CONFIG</em> ... ]
[ footer: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.footer" . }}' ]
[ pretext: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.pretext" . }}' ]
[ short_fields: <em class="replaceable">BOOLEAN</em> | default = false ]
[ text: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.text" . }}' ]
[ title: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.title" . }}' ]
[ title_link: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "slack.default.titlelink" . }}' ]
[ image_url: <em class="replaceable">TMPL_STRING</em> ]
[ thumb_url: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.17" data-id-title="ACTION_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.11: </span><span class="title-name"><em class="replaceable">ACTION_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.17">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Provide a button to tell Slack you want to render a button.
type: <em class="replaceable">TMPL_STRING</em>
# Label for the button.
text: <em class="replaceable">TMPL_STRING</em>
# http or https URL to deliver users to. If you specify invalid URLs, the message will be posted with no button.
url: <em class="replaceable">TMPL_STRING</em>
#  If set to 'primary', the button will be green, indicating the best forward action to take
#  'danger' turns the button red, indicating a destructive action.
[ style: <em class="replaceable">TMPL_STRING</em> [ default = '' ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.18" data-id-title="FIELD_CONFIG for SLACK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.12: </span><span class="title-name"><em class="replaceable">FIELD_CONFIG</em> for <em class="replaceable">SLACK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.18">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># A bold heading without markup above the <code class="option">value</code> text.
title: <em class="replaceable">TMPL_STRING</em>
# The text of the field. It can span across several lines.
value: <em class="replaceable">TMPL_STRING</em>
# A flag indicating if <code class="option">value</code> is short enough to be displayed together with other values.
[ short: <em class="replaceable">BOOLEAN</em> | default = slack_config.short_fields ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.19" data-id-title="OPSGENIE_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.13: </span><span class="title-name"><em class="replaceable">OPSGENIE_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.19">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key to use with the OpsGenie API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.opsgenie_api_key ]

# The host to send OpsGenie API requests to.
[ api_url: <em class="replaceable">STRING</em> | default = global.opsgenie_api_url ]

# Alert text (maximum is 130 characters).
[ message: <em class="replaceable">TMPL_STRING</em> ]

# A description of the incident.
[ description: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.description" . }}' ]

# A backlink to the sender.
[ source: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "opsgenie.default.source" . }}' ]

# A set of arbitrary key/value pairs that provide further detail.
[ details: { <em class="replaceable">STRING</em>: <em class="replaceable">TMPL_STRING</em>, ... } ]

# Comma separated list of team responsible for notifications.
[ teams: <em class="replaceable">TMPL_STRING</em> ]

# Comma separated list of tags attached to the notifications.
[ tags: <em class="replaceable">TMPL_STRING</em> ]

# Additional alert note.
[ note: <em class="replaceable">TMPL_STRING</em> ]

# Priority level of alert, one of P1, P2, P3, P4, and P5.
[ priority: <em class="replaceable">TMPL_STRING</em> ]

# Configuration of the HTTP.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="example" id="id-1.3.5.5.8.3.20" data-id-title="VICTOROPS_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.14: </span><span class="title-name"><em class="replaceable">VICTOROPS_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.20">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The API key for talking to the VictorOps API.
[ api_key: <em class="replaceable">SECRET</em> | default = global.victorops_api_key ]

# The VictorOps API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.victorops_api_url ]

# A key used to map the alert to a team.
routing_key: <em class="replaceable">TMPL_STRING</em>

# Describes the behavior of the alert (one of 'CRITICAL', 'WARNING', 'INFO').
[ message_type: <em class="replaceable">TMPL_STRING</em> | default = 'CRITICAL' ]

# Summary of the alerted problem.
[ entity_display_name: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.entity_display_name" . }}' ]

# Long explanation of the alerted problem.
[ state_message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.state_message" . }}' ]

# The monitoring tool the state message is from.
[ monitoring_tool: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "victorops.default.monitoring_tool" . }}' ]

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div></div></div><div class="complex-example"><div class="example" id="alert-webhook" data-id-title="WEBHOOK_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.15: </span><span class="title-name"><em class="replaceable">WEBHOOK_CONFIG</em> </span><a title="Permalink" class="permalink" href="#alert-webhook">#</a></h6></div><div class="example-contents"><p>
     You can use the webhook receiver to configure a generic receiver.
    </p><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = true ]

# The endpoint for sending HTTP POST requests.
url: <em class="replaceable">STRING</em>

# Configuration of the HTTP client.
[ http_config: <em class="replaceable">HTTP_CONFIG</em> | default = global.http_config ]</pre></div><p>
     Alertmanager sends HTTP POST requests in the following JSON format:
    </p><div class="verbatim-wrap"><pre class="screen">{
 "version": "4",
 "groupKey": <em class="replaceable">STRING</em>, // identifycation of the group of alerts (to deduplicate)
 "status": "&lt;resolved|firing&gt;",
 "receiver": <em class="replaceable">STRING</em>,
 "groupLabels": <em class="replaceable">OBJECT</em>,
 "commonLabels": <em class="replaceable">OBJECT</em>,
 "commonAnnotations": <em class="replaceable">OBJECT</em>,
 "externalURL": <em class="replaceable">STRING</em>, // backlink to Alertmanager.
 "alerts": [
   {
     "status": "&lt;resolved|firing&gt;",
     "labels": <em class="replaceable">OBJECT</em>,
     "annotations": <em class="replaceable">OBJECT</em>,
     "startsAt": "&lt;rfc3339&gt;",
     "endsAt": "&lt;rfc3339&gt;",
     "generatorURL": <em class="replaceable">STRING</em> // identifies the entity that caused the alert
   },
   ...
 ]
}</pre></div><p>
     The webhook receiver allows for integration with the following
     notification mechanisms:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       DingTalk (https://github.com/timonwong/prometheus-webhook-dingtalk)
      </p></li><li class="listitem"><p>
       IRC Bot (https://github.com/multimfi/bot)
      </p></li><li class="listitem"><p>
       JIRAlert (https://github.com/free/jiralert)
      </p></li><li class="listitem"><p>
       Phabricator / Maniphest (https://github.com/knyar/phalerts)
      </p></li><li class="listitem"><p>
       prom2teams: forwards notifications to Microsoft Teams
       (https://github.com/idealista/prom2teams)
      </p></li><li class="listitem"><p>
       SMS: supports multiple providers (https://github.com/messagebird/sachet)
      </p></li><li class="listitem"><p>
       Telegram bot (https://github.com/inCaller/prometheus_bot)
      </p></li><li class="listitem"><p>
       SNMP trap (https://github.com/SUSE/prometheus-webhook-snmp)
      </p></li></ul></div></div></div></div><div class="example" id="id-1.3.5.5.8.3.22" data-id-title="WECHAT_CONFIG"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.16: </span><span class="title-name"><em class="replaceable">WECHAT_CONFIG</em> </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.3.22">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"># Whether or not to notify about resolved alerts.
[ send_resolved: <em class="replaceable">BOOLEAN</em> | default = false ]

# The API key to use for the WeChat API.
[ api_secret: <em class="replaceable">SECRET</em> | default = global.wechat_api_secret ]

# The WeChat API URL.
[ api_url: <em class="replaceable">STRING</em> | default = global.wechat_api_url ]

# The corp id used to authenticate.
[ corp_id: <em class="replaceable">STRING</em> | default = global.wechat_api_corp_id ]

# API request data as defined by the WeChat API.
[ message: <em class="replaceable">TMPL_STRING</em> | default = '{{ template "wechat.default.message" . }}' ]
[ agent_id: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.agent_id" . }}' ]
[ to_user: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_user" . }}' ]
[ to_party: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_party" . }}' ]
[ to_tag: <em class="replaceable">STRING</em> | default = '{{ template "wechat.default.to_tag" . }}' ]</pre></div></div></div></section><section class="sect2" id="id-1.3.5.5.8.4" data-id-title="Custom Alerts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.2 </span><span class="title-name">Custom Alerts</span> <a title="Permalink" class="permalink" href="#id-1.3.5.5.8.4">#</a></h3></div></div></div><p>
    You can define your custom alert conditions to send notifications to an
    external service. Prometheus uses its own expression language for
    defining custom alerts. Following is an example of a rule with an alert:
   </p><div class="verbatim-wrap"><pre class="screen">groups:
- name: example
 rules:
  # alert on high deviation from average PG count
  - alert: high pg count deviation
   expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
   for: 5m
   labels:
    severity: warning
    type: ses_default
   annotations:
   description: &gt;
    OSD {{ $labels.osd }} deviates by more then 30% from average PG count</pre></div><p>
    The optional <code class="literal">for</code> clause specifies the time Prometheus
    will wait between first encountering a new expression output vector element
    and counting an alert as firing. In this case, Prometheus will check that
    the alert continues to be active for 5 minutes before firing the alert.
    Elements in a pending state are active, but not firing yet.
   </p><p>
    The <code class="literal">labels</code> clause specifies a set of additional labels
    attached to the alert. Conflicting labels will be overwritten. Labels can
    be templated (see <a class="xref" href="#alertmanager-templates" title="18.4.2.1. Templates">Section 18.4.2.1, “Templates”</a> for more details
    on templating).
   </p><p>
    The <code class="literal">annotations</code> clause specifies informational labels.
    You can use them to store additional information, for example alert
    descriptions or runbook links. Annotations can be templated (see
    <a class="xref" href="#alertmanager-templates" title="18.4.2.1. Templates">Section 18.4.2.1, “Templates”</a> for more details on templating).
   </p><p>
    To add your custom alerts to SUSE Enterprise Storage 6, either
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      place your YAML files with custom alerts in the
      <code class="filename">/etc/prometheus/alerts</code> directory
     </p></li></ul></div><p>
    or
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      provide a list of paths to your custom alert files in the Pillar under
      the <code class="option">monitoring:custom_alerts</code> key. DeepSea Stage 2 or
      the <code class="command">salt <em class="replaceable">SALT_MASTER</em> state.apply
      ceph.monitoring.prometheus</code> command will add your alert files in
      the right place.
     </p><div class="complex-example"><div class="example" id="id-1.3.5.5.8.4.10.1.2" data-id-title="Adding Custom Alerts to SUSE Enterprise Storage"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.17: </span><span class="title-name">Adding Custom Alerts to SUSE Enterprise Storage </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.4.10.1.2">#</a></h6></div><div class="example-contents"><p>
       A file with custom alerts is in
       <code class="filename">/root/my_alerts/my_alerts.yml</code> on the Salt master.
       If you add
      </p><div class="verbatim-wrap"><pre class="screen">monitoring:
 custom_alerts:
   - /root/my_alerts/my_alerts.yml</pre></div><p>
       to the
       <code class="filename">/srv/pillar/ceph/cluster/<em class="replaceable">YOUR_SALT_MASTER_MINION_ID</em>.sls</code>
       file, DeepSea will create the
       <code class="filename">/etc/prometheus/alerts/my_alerts.yml</code> file and
       restart Prometheus.
      </p></div></div></div></li></ul></div><section class="sect3" id="alertmanager-templates" data-id-title="Templates"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">18.4.2.1 </span><span class="title-name">Templates</span> <a title="Permalink" class="permalink" href="#alertmanager-templates">#</a></h4></div></div></div><p>
     You can use templates for label and annotation values. The
     <code class="varname">$labels</code> variable includes the label key/value pairs of
     an alert instance, while <code class="varname">$value</code> holds the evaluated
     value of an alert instance.
    </p><p>
     The following example inserts a firing element label and value:
    </p><div class="verbatim-wrap"><pre class="screen">{{ $labels.<em class="replaceable">LABELNAME</em> }}
{{ $value }}</pre></div></section><section class="sect3" id="id-1.3.5.5.8.4.12" data-id-title="Inspecting Alerts at Runtime"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">18.4.2.2 </span><span class="title-name">Inspecting Alerts at Runtime</span> <a title="Permalink" class="permalink" href="#id-1.3.5.5.8.4.12">#</a></h4></div></div></div><p>
     If you need to verify which alerts are active, you have several options:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Navigate to the <span class="guimenu">Alerts</span> tab of Prometheus. It will
       show you the exact label sets for which defined alerts are active.
       Prometheus also stores synthetic time series for pending and firing
       alerts. They have the following form:
      </p><div class="verbatim-wrap"><pre class="screen">ALERTS{alertname="<em class="replaceable">ALERT_NAME</em>", alertstate="pending|firing", <em class="replaceable">ADDITIONAL_ALERT_LABELS</em>}</pre></div><p>
       The sample value is 1 if the alert is active (pending or firing). The
       series is marked 'stale' when the alert is inactive.
      </p></li><li class="listitem"><p>
       In the Prometheus Web interface at the URL address
       http://<em class="replaceable">PROMETHEUS_HOST_IP</em>:9090/alerts,
       inspect alerts and their state (INACTIVE, PENDING or FIRING).
      </p></li><li class="listitem"><p>
       In the Alertmanager Web interface at the URL address
       http://:<em class="replaceable">PROMETHEUS_HOST_IP</em>:9093/#/alerts,
       inspect alerts and silence them if desired.
      </p></li></ul></div></section></section><section class="sect2" id="snmp-trap-receiver" data-id-title="SNMP Trap Receiver"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.4.3 </span><span class="title-name">SNMP Trap Receiver</span> <a title="Permalink" class="permalink" href="#snmp-trap-receiver">#</a></h3></div></div></div><p>
    If you want to get notified about Prometheus alerts via SNMP traps, then
    you can install the Prometheus Alertmanager SNMP trap receiver via
    DeepSea. To do so you need to enable it in the pillar under the
    <code class="option">monitoring:alertmanager_receiver_snmp:enabled</code> key in your
    <code class="filename">global.yml</code> file. The configuration of the receiver
    must be set under the
    <code class="option">monitoring:alertmanager_receiver_snmp:config</code> key.
   </p><div class="complex-example"><div class="example" id="id-1.3.5.5.8.5.3" data-id-title="SNMP Trap Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 18.18: </span><span class="title-name">SNMP Trap Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.8.5.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">monitoring:
 alertmanager:
   receiver:
      snmp:
        enabled: True
        config:
          host: localhost
          port: 9099
          snmp_host: snmp.foo-bar.com
          snmp_community: private
          metrics: True</pre></div><p>
     Refer to the receiver manual at
     <a class="link" href="https://github.com/SUSE/prometheus-webhook-snmp#global-configuration-file" target="_blank">https://github.com/SUSE/prometheus-webhook-snmp#global-configuration-file</a>.
     for more details about the configuration options.
    </p></div></div></div><p>
    DeepSea Stage 2 or the <code class="command">salt
    <em class="replaceable">SALT_MASTER</em> state.apply
    ceph.monitoring.alertmanager</code> command will install and configure
    the receiver in the appropriate location. Verify your settings with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-call pillar.get 'monitoring:alertmanager_receiver_snmp:enabled'
<code class="prompt user">root@master # </code>salt-call pillar.get 'monitoring:alertmanager_receiver_snmp:config'</pre></div></section></section><section class="sect1" id="troubleshooting-alerts" data-id-title="Troubleshooting Alerts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.5 </span><span class="title-name">Troubleshooting Alerts</span> <a title="Permalink" class="permalink" href="#troubleshooting-alerts">#</a></h2></div></div></div><p>
   The following section details the alert that has been triggered and actions
   to take when the alert is displayed.
  </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MONITOR </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.3.2"><span class="term"><code class="option">MON_DOWN</code></span></dt><dd><p>
      One or more monitor daemons are down. The cluster requires a majority of
      the monitors in order to function. When one or more monitors are down,
      clients will initially have difficulty connecting to the cluster.
     </p><p>
      Restart the monitor daemon that is down as soon as possible to reduce the
      risk of a subsequent monitor failure.
     </p></dd><dt id="id-1.3.5.5.9.3.3"><span class="term"><code class="option">MON_CLOCK_SKEW</code></span></dt><dd><p>
      The clocks on the hosts running the
      <code class="systemitem">ceph-mon</code> monitor daemons are not
      well synchronized. This health alert is raised if the cluster detects a
      clock skew greater than <code class="option">mon_clock_drift_allowed</code>. Resolve
      this by synchronizing the clocks using either <code class="literal">ntpd</code> or
      <code class="literal">chrony</code>. If it is impractical to keep the clocks
      closely synchronized, the <code class="option">mon_clock_drift_allowed</code>
      threshold can be increased, but this value must stay well below the
      <code class="option">mon_lease</code> interval in order for monitor cluster to
      function properly.
     </p></dd><dt id="id-1.3.5.5.9.3.4"><span class="term"><code class="option">MON_MSGR2_NOT_ENABLED</code></span></dt><dd><p>
      The <code class="option">ms_bind_msgr2</code> option is enabled but one or more
      monitors is not configured to bind to a v2 port in the cluster’s monmap.
      This means that features specific to the msgr2 protocol (for example,
      encryption) are not available on some or all connections. In most cases
      this can be corrected by issuing the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mon enable-msgr2</pre></div><p>
      This command changes any monitor configured for the old default port 6789
      to continue to listen for v1 connections on 6789 and also listen for v2
      connections on the new default 3300 port. If a monitor is configured to
      listen for v1 connections on a non-standard port (not 6789), then the
      monmap needs to be modified manually.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MANAGER </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.4.2"><span class="term"><code class="option">MGR_MODULE_DEPENDENCY</code></span></dt><dd><p>
      An enabled manager module is failing its dependency check. This health
      check should come with a message from the module about the problem. For
      example, a module might report that a required package is not installed.
      In which case, the message will read: "Install the required package and
      restart your manager daemons." This health check only applies to enabled
      modules. If a module is not enabled, you can see whether it is reporting
      dependency issues in the output of <code class="command">ceph module ls</code>.
     </p></dd><dt id="id-1.3.5.5.9.4.3"><span class="term"><code class="option">MGR_MODULE_ERROR</code></span></dt><dd><p>
      A manager module has experienced an unexpected error. Typically, this
      means an unhandled exception was raised from the module’s serve function.
      The human readable description of the error may be obscurely worded if
      the exception did not provide a useful description of itself. This health
      check may indicate a bug. Open a bug report if you think you have
      encountered a bug. If you believe the error is transient, you may restart
      your manager daemon(s), or use <code class="command">ceph mgr fail</code> on the
      active daemon to prompt a failover to another daemon.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">OSDS </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.5.2"><span class="term"><code class="option">OSD_DOWN</code></span></dt><dd><p>
      One or more OSDs are marked down. The
      <code class="systemitem">ceph-osd</code> daemon may have been
      stopped, or peer OSDs may be unable to reach the OSD over the network.
      Common causes include a stopped or crashed daemon, a down host, or a
      network outage. Verify the host is healthy, the daemon is started, and
      network is functioning. If the daemon has crashed, the daemon log file
      (<code class="filename">/var/log/ceph/ceph-osd.*</code>) may contain debugging
      information.
     </p></dd><dt id="id-1.3.5.5.9.5.3"><span class="term"><code class="option">OSD_<em class="replaceable">CRUSH TYPE</em>_DOWN</code></span></dt><dd><p>
      For example, <code class="filename">OSD_HOST_DOWN</code> or
      <code class="filename">OSD_ROOT_DOWN</code>. All the OSDs within a particular
      CRUSH subtree are marked down, for example all OSDs on a host.
     </p></dd><dt id="id-1.3.5.5.9.5.4"><span class="term"><code class="option">OSD_ORPHAN</code></span></dt><dd><p>
      An OSD is referenced in the CRUSH Map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rm osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.5.5.9.5.5"><span class="term"><code class="option">OSD_OUT_OF_ORDER_FULL</code></span></dt><dd><p>
      The utilization thresholds for <code class="literal">backfillfull</code>,
      <code class="literal">nearfull</code>, <code class="literal">full</code>, and
      <code class="option">failsafe_full</code> are not ascending. The thresholds can be
      adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">RATIO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">RATIO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">RATIO</em></pre></div></dd><dt id="id-1.3.5.5.9.5.6"><span class="term"><code class="option">OSD_FULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">full</code> threshold and
      is preventing the cluster from servicing writes. Utilization by pool can
      be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div><p>
      The currently defined <code class="literal">full</code> ratio can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep full_ratio</pre></div><p>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">RATIO</em></pre></div><p>
      New storage should be added to the cluster by deploying more OSDs or
      existing data should be deleted in order to free up space.
     </p></dd><dt id="id-1.3.5.5.9.5.7"><span class="term"><code class="option">OSD_BACKFILLFULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">backfillfull</code>
      threshold, preventing data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching <code class="literal">full</code>.
      Utilization by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.5.9.5.8"><span class="term"><code class="option">OSD_NEARFULL</code></span></dt><dd><p>
      One or more OSDs have exceeded the <code class="literal">nearfull</code> threshold.
      This is an early warning that the cluster is approaching
      <code class="literal">full</code>. Utilization by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.3.5.5.9.5.9"><span class="term"><code class="option">OSDMAP_FLAGS</code></span></dt><dd><p>
      One or more cluster flags of interest has been set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.9.5.9.2.2.1"><span class="term">full</span></dt><dd><p>
         The cluster is flagged as <code class="literal">full</code> and cannot serve
         writes
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.2"><span class="term">pauserd, pausewr</span></dt><dd><p>
         Paused reads or writes
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.3"><span class="term">noup</span></dt><dd><p>
         OSDs are not allowed to start
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.4"><span class="term">nodown</span></dt><dd><p>
         OSD failure reports are being ignored and the monitors are not marking
         OSDs down
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.5"><span class="term">noin</span></dt><dd><p>
         OSDs that were previously marked out are not being marked back in when
         they start
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.6"><span class="term">noout</span></dt><dd><p>
         Down OSDs are not automatically marked out after the configured
         interval
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.7"><span class="term">nobackfill, norecover, norebalance</span></dt><dd><p>
         Recovery or data rebalancing is suspended
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.8"><span class="term">noscrub, nodeep_scrub</span></dt><dd><p>
         Scrubbing is disabled
        </p></dd><dt id="id-1.3.5.5.9.5.9.2.2.9"><span class="term">notieragent</span></dt><dd><p>
         Cache tiering activity is suspended
        </p></dd></dl></div><p>
      With the exception of <code class="literal">full</code>, these flags can be set or
      cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set <em class="replaceable">FLAG</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset <em class="replaceable">FLAG</em></pre></div></dd><dt id="id-1.3.5.5.9.5.10"><span class="term"><code class="option">OSD_FLAGS</code></span></dt><dd><p>
      One or more OSDs or CRUSH <code class="literal">{nodes,device classes}</code> has a
      flag of interest set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.5.9.5.10.2.2.1"><span class="term">noup</span></dt><dd><p>
         These OSDs are not allowed to start
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.2"><span class="term">nodown</span></dt><dd><p>
         Failure reports for these OSDs are ignored
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.3"><span class="term">noin</span></dt><dd><p>
         If these OSDs were previously marked out automatically after a
         failure, they are not to be marked in when they start
        </p></dd><dt id="id-1.3.5.5.9.5.10.2.2.4"><span class="term">noout</span></dt><dd><p>
         If these OSDs are down they are not automatically marked out after the
         configured interval
        </p></dd></dl></div><p>
      These flags can be set and cleared in batch with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group <em class="replaceable">FLAG</em> <em class="replaceable">WHO</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group <em class="replaceable">FLAG</em> <em class="replaceable">WHO</em></pre></div><p>
      For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout osd.0 osd.1
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout osd.0 osd.1
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout host-foo
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout host-foo
<code class="prompt user">cephadm@adm &gt; </code>ceph osd set-group noup,noout class-hdd
<code class="prompt user">cephadm@adm &gt; </code>ceph osd unset-group noup,noout class-hdd</pre></div></dd><dt id="id-1.3.5.5.9.5.11"><span class="term"><code class="option">OLD_CRUSH_TUNABLES</code></span></dt><dd><p>
      The CRUSH Map is using old settings and should be updated. The oldest
      tunables that can be used (for example, the oldest client version that
      can connect to the cluster) without triggering this health warning are
      determined by the <code class="option">mon_crush_min_required_version</code> config
      option.
     </p></dd><dt id="id-1.3.5.5.9.5.12"><span class="term"><code class="option">OLD_CRUSH_STRAW_CALC_VERSION</code></span></dt><dd><p>
      The CRUSH Map is using an older, sub-optimal method for calculating
      intermediate weight values for straw buckets. The CRUSH Map requires an
      update to use the newer method (<code class="literal">straw_calc_version=1</code>).
     </p></dd><dt id="id-1.3.5.5.9.5.13"><span class="term"><code class="option">CACHE_POOL_NO_HIT_SET</code></span></dt><dd><p>
      One or more cache pools are not configured with a hit set to track
      utilization. This prevents the tiering agent from identifying cold
      objects to flush and evict from the cache. Hit sets can be configured on
      the cache pool with the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_type <em class="replaceable">TYPE</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_period <em class="replaceable">PERIOD-IN-SECONDS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_count <em class="replaceable">NUMBER-OF-HITSETS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOLNAME</em> hit_set_fpp <em class="replaceable">TARGET-FALSE-POSITIVE-RATE</em></pre></div></dd><dt id="id-1.3.5.5.9.5.14"><span class="term"><code class="option">OSD_NO_SORTBITWISE</code></span></dt><dd><p>
      No SUSE Enterprise Storage 5.5 v12.y.z OSDs are running but the
      <code class="option">sortbitwise</code> flag has not been set. Set the
      <code class="option">sortbitwise</code> flag before v12.y.z or newer OSDs can start.
      You can safely set the flag with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd set sortbitwise</pre></div></dd><dt id="id-1.3.5.5.9.5.15"><span class="term"><code class="option">POOL_FULL</code></span></dt><dd><p>
      One or more pools have reached the quota and are no longer allowing
      writes. Pool quotas and utilization can be seen with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph df detail</pre></div><p>
      You can either raise the pool quota with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOLNAME</em> max_objects <em class="replaceable">NUM-OBJECTS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOLNAME</em> max_bytes <em class="replaceable">NUM-BYTES</em></pre></div><p>
      Or, you can delete existing data to reduce utilization.
     </p></dd><dt id="id-1.3.5.5.9.5.16"><span class="term"><code class="option">BLUEFS_SPILLOVER</code></span></dt><dd><p>
      One or more OSDs that use the BlueStore backend have been allocated db
      partitions (storage space for metadata, normally on a faster device) but
      that space has filled, such that metadata has overflowed onto the normal
      slow device. This is not necessarily an error condition or even
      unexpected, but if the administrator’s expectation was that all metadata
      would fit on the faster device, it indicates that not enough space was
      provided. This warning can be disabled on all OSDs with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd bluestore_warn_on_bluefs_spillover false</pre></div><p>
      Alternatively, it can be disabled on a specific OSD with the following
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd.123 bluestore_warn_on_bluefs_spillover false</pre></div><p>
      To provide more metadata space, the OSD in question can be destroyed and
      reprovisioned. This involves data migration and recovery. It is possible
      to expand the LVM logical volume backing the db storage. If the
      underlying LV has been expanded, the OSD daemon needs to be stopped and
      BlueFS informed of the device size change with the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool bluefs-bdev-expand --path /var/lib/ceph/osd/ceph-$ID</pre></div></dd><dt id="id-1.3.5.5.9.5.17"><span class="term"><code class="option">BLUEFS_AVAILABLE_SPACE</code></span></dt><dd><p>
      To check how much space is free for BlueFS, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore bluefs available</pre></div><p>
      This provides output for up to 3 values; <code class="literal">BDEV_DB free</code>,
      <code class="literal">BDEV_SLOW free</code> and
      <code class="option">available_from_bluestore</code>. <code class="literal">BDEV_DB</code> and
      <code class="literal">BDEV_SLOW</code> report the amount of space that has been
      acquired by BlueFS and is considered free. Value
      <code class="option">available_from_bluestore</code> denotes ability of BlueStore
      to leave more space to BlueFS. It is normal that this value is different
      from amount of BlueStore free space, as BlueFS allocation unit is
      typically larger than BlueStore allocation unit. This means that only
      part of BlueStore free space is acceptable for BlueFS.
     </p></dd><dt id="id-1.3.5.5.9.5.18"><span class="term"><code class="option">BLUEFS_LOW_SPACE</code></span></dt><dd><p>
      If BlueFS is running low on available free space and there is little
      <code class="option">available_from_bluestore</code>, consider reducing BlueFS'
      allocation unit size. To simulate available space when the allocation
      unit is different, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore bluefs available <em class="replaceable">ALLOC-UNIT-SIZE</em></pre></div></dd><dt id="id-1.3.5.5.9.5.19"><span class="term"><code class="option">BLUESTORE_FRAGMENTATION</code></span></dt><dd><p>
      As BlueStore works, free space on underlying storage becomes
      fragmented. This is normal and unavoidable, but excessive fragmentation
      can cause slowdown. To inspect BlueStore fragmentation, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore allocator score block</pre></div><p>
      Score is given in [0-1] range. [0.0 .. 0.4] tiny fragmentation [0.4 ..
      0.7] small, acceptable fragmentation [0.7 .. 0.9] considerable, but safe
      fragmentation [0.9 .. 1.0] severe fragmentation, can impact BlueFS'
      ability to get space from BlueStore. If detailed report of free
      fragments is required, execute:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.123 bluestore allocator dump block</pre></div><p>
      If the OSD process does not perform fragmentation, inspect with
      <code class="command">ceph-bluestore-tool</code>. Get the fragmentation score:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-score</pre></div><p>
      Dump detailed free chunks:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-dump</pre></div></dd><dt id="id-1.3.5.5.9.5.20"><span class="term"><code class="option">BLUESTORE_LEGACY_STATFS</code></span></dt><dd><p>
      As of SUSE Enterprise Storage 6, BlueStore tracks its internal usage statistics
      on a per-pool granular basis and one or more OSDs have BlueStore
      volumes that were created prior to SUSE Enterprise Storage 6. If all
      OSDs are older than SUSE Enterprise Storage 6, the per-pool metrics
      are not available. However, if there is a mix of pre-SUSE Enterprise Storage
      6 and post-SUSE Enterprise Storage 6 OSDs, the cluster
      usage statistics reported by <code class="command">ceph df</code> will not be
      accurate. The old OSDs can be updated to use the new usage tracking
      scheme by stopping each OSD, running a repair operation, and the
      restarting it. For example, if <code class="literal">osd.123</code> requires an
      update:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph-osd@123
<code class="prompt user">cephadm@adm &gt; </code>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-123
<code class="prompt user">root # </code>systemctl start ceph-osd@123</pre></div><p>
      This warning can be disabled with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global bluestore_warn_on_legacy_statfs false</pre></div></dd><dt id="id-1.3.5.5.9.5.21"><span class="term"><code class="option">BLUESTORE_DISK_SIZE_MISMATCH</code></span></dt><dd><p>
      One or more OSDs using BlueStore has an internal inconsistency between
      the size of the physical device and the metadata tracking its size. This
      can lead to the OSD crashing in the future. The OSDs in question should
      be destroyed and re-deployed. To avoid putting any data at risk,
      re-deploy only one OSD at a time. For example, if
      <em class="replaceable">OSD_ID</em> has the error:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd out osd.$N
while ! ceph osd safe-to-destroy osd.$N ; do sleep 1m ; done
ceph osd destroy osd.$N
ceph-volume lvm zap /path/to/device
ceph-volume lvm create --osd-id $N --data /path/to/device</pre></div></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">DEVICE HEALTH </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.6.2"><span class="term"><code class="option">DEVICE_HEALTH</code></span></dt><dd><p>
      One or more devices are expected to fail. The warning threshold is
      controlled by the <code class="literal">mgr/devicehealth/warn_threshold</code>
      configuration option. This warning only applies to OSDs that are
      currently marked <code class="option">in</code>. The expected response to this
      failure is to mark the device <code class="option">out</code>. The data is then
      migrated off of the device and the hardware is removed from the system.
      Marking out is normally done automatically if
      <code class="literal">mgr/devicehealth/self_heal</code> is enabled based on the
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>. Device health can
      be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph device info <em class="replaceable">DEVICE-ID</em></pre></div><p>
      Device life expectancy is set by a prediction model run by the Ceph Manager or
      by an external tool via the command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph device set-life-expectancy <em class="replaceable">DEVICE-ID</em> <em class="replaceable">FROM</em> <em class="replaceable">TO</em></pre></div><p>
      You can change the stored life expectancy manually, but that usually does
      not persist—the tool that originally set it reset and changing the
      stored value does not affect the actual health of the hardware device.
     </p></dd><dt id="id-1.3.5.5.9.6.3"><span class="term"><code class="option">DEVICE_HEALTH_IN_USE</code></span></dt><dd><p>
      One or more devices are expected to fail and has been marked
      <code class="option">out</code> of the cluster based on
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, but the devices
      are still participating in one more PGs. This may be because it was only
      recently marked as <code class="option">out</code> and the data is still migrating,
      or because the data cannot be migrated off for some reason (for example,
      the cluster is nearly full, or the CRUSH hierarchy is such that there is
      not another suitable OSD to migrate the data to). This message can be
      silenced by disabling the self heal behavior (setting
      <code class="literal">mgr/devicehealth/self_heal</code> to false), by adjusting the
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, or by addressing
      what is preventing data from being migrated off of the ailing device.
     </p></dd><dt id="id-1.3.5.5.9.6.4"><span class="term"><code class="option">DEVICE_HEALTH_TOOMANY</code></span></dt><dd><p>
      Too many devices are expected to fail and the
      <code class="literal">mgr/devicehealth/self_heal</code> behavior is enabled, such
      that marking <code class="option">out</code> all of the ailing devices would exceed
      the clusters <code class="option">mon_osd_min_in_ratio</code> ratio that prevents
      too many OSDs from being automatically marked <code class="option">out</code>. This
      can indicates that too many devices in the cluster are expected to fail
      and action is required to add newer (healthier) devices before too many
      devices fail and data is lost. The health message can also be silenced by
      adjusting parameters like <code class="option">mon_osd_min_in_ratio</code> or
      <code class="literal">mgr/devicehealth/mark_out_threshold</code>, but be warned
      that this increases the likelihood of unrecoverable data loss in the
      cluster.
     </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">DATA HEALTH (POOLS AND PLACEMENT GROUPS) </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.7">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.7.2"><span class="term"><code class="option">PG_AVAILABILITY</code></span></dt><dd><p>
      Data availability is reduced and the cluster is unable to service
      potential read or write requests for some data in the cluster.
      Specifically, if one or more PGs are in a state that does not allow IO
      requests to be serviced. Problematic PG states include peering, stale,
      incomplete, and in-active (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs are currently down;
      see the discussion for <code class="option">OSD_DOWN</code> above. The state of
      specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">PG_ID</em> query</pre></div></dd><dt id="id-1.3.5.5.9.7.3"><span class="term"><code class="option">PG_DEGRADED</code></span></dt><dd><p>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, if one
      or more PGs:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        have a degraded or undersized flag set, meaning there are not enough
        instances of that placement group in the cluster;
       </p></li><li class="listitem"><p>
        have not had the clean flag set for some time.
       </p></li></ul></div></dd><dt id="id-1.3.5.5.9.7.4"><span class="term"><code class="option">PG_RECOVERY_FULL</code></span></dt><dd><p>
      Data redundancy can be reduced or at risk for some data due to a lack of
      free space in the cluster. Specifically, one or more PGs have the
      <code class="option">recovery_toofull</code> flag set, meaning that the cluster is
      unable to migrate or recover data because one or more OSDs are above the
      full threshold. See the discussion for <code class="option">OSD_FULL</code> above
      for steps to resolve this condition.
     </p></dd><dt id="id-1.3.5.5.9.7.5"><span class="term"><code class="option">PG_BACKFILL_FULL</code></span></dt><dd><p>
      Data redundancy can be reduced or at risk for some data due to a lack of
      free space in the cluster. Specifically, one or more PGs have the
      <code class="option">backfill_toofull</code> flag set, meaning that the cluster is
      unable to migrate or recover data because one or more OSDs are above the
      backfillfull threshold. See the discussion for
      <code class="option">OSD_BACKFILLFULL</code> above for steps to resolve this
      condition.
     </p></dd><dt id="id-1.3.5.5.9.7.6"><span class="term"><code class="option">PG_DAMAGED</code></span></dt><dd><p>
      Data scrubbing has discovered some problems with data consistency in the
      cluster. Specifically, one or more PGs have the inconsistent or
      <code class="option">snaptrim_error</code> flag is set, indicating an earlier scrub
      operation found a problem, or that the repair flag is set and a repair
      for such an inconsistency is currently in progress.
     </p></dd><dt id="id-1.3.5.5.9.7.7"><span class="term"><code class="option">OSD_SCRUB_ERRORS</code></span></dt><dd><p>
      Recent OSD scrubs have uncovered inconsistencies. This error is generally
      paired with <code class="option">PG_DAMAGED</code>.
     </p></dd><dt id="id-1.3.5.5.9.7.8"><span class="term"><code class="option">LARGE_OMAP_OBJECTS</code></span></dt><dd><p>
      One or more pools contain large omap objects as determined by
      <code class="option">osd_deep_scrub_large_omap_object_key_threshold</code>
      (threshold for number of keys to determine a large omap object) or
      <code class="option">osd_deep_scrub_large_omap_object_value_sum_threshold</code>
      (the threshold for summed size (bytes) of all key values to determine a
      large omap object) or both. More information on the object name, key
      count, and size in bytes can be found by searching the cluster log for
      ‘Large omap object found’. Large omap objects can be caused by RGW bucket
      index objects that do not have automatic resharding enabled. The
      thresholds can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd osd_deep_scrub_large_omap_object_key_threshold <em class="replaceable">KEYS</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph config set osd osd_deep_scrub_large_omap_object_value_sum_threshold <em class="replaceable">BYTES</em></pre></div></dd><dt id="id-1.3.5.5.9.7.9"><span class="term"><code class="option">CACHE_POOL_NEAR_FULL</code></span></dt><dd><p>
      A cache tier pool is nearly full. Full is determined by the
      <code class="option">target_max_bytes</code> and <code class="option">target_max_objects</code>
      properties on the cache pool. Once the pool reaches the target threshold,
      write requests to the pool may block while data is flushed and evicted
      from the cache, a state that normally leads to very high latencies and
      poor performance. The cache pool target size can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">CACHE-POOL-NAME</em> target_max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">CACHE-POOL-NAME</em> target_max_objects <em class="replaceable">OBJECTS</em></pre></div><p>
      Normal cache flush and eviction activity can also be throttled due to
      reduced availability, performance of the base tier, or overall cluster
      load.
     </p></dd><dt id="id-1.3.5.5.9.7.10"><span class="term"><code class="option">POOL_TOO_FEW_PGS</code></span></dt><dd><p>
      One or more pools should probably have more PGs, based on the amount of
      data that is currently stored in the pool. This can lead to sub-optimal
      distribution and balance of data across the OSDs in the cluster, and
      similarly reduce overall performance. This warning is generated if the
      <code class="option">pg_autoscale_mode</code> property on the pool is set to warn.
      To disable the warning, you can disable auto-scaling of PGs for the pool
      entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode off</pre></div><p>
      To allow the cluster to automatically adjust the number of PGs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode on</pre></div><p>
      You can also manually set the number of PGs for the pool to the
      recommended amount with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div></dd><dt id="id-1.3.5.5.9.7.11"><span class="term"><code class="option">TOO_MANY_PGS</code></span></dt><dd><p>
      The number of PGs in use in the cluster is above the configurable
      threshold of <code class="option">mon_max_pg_per_osd</code> PGs per OSD. If this
      threshold is exceeded, the cluster does not allow new pools to be
      created, pool <code class="option">pg_num</code> to be increased, or pool
      replication to be increased (any of which would lead to more PGs in the
      cluster). A large number of PGs can lead to higher memory utilization for
      OSD daemons, slower peering after cluster state changes (like OSD
      restarts, additions, or removals), and higher load on the Ceph Manager and Ceph Monitor
      daemons. The simplest way to mitigate the problem is to increase the
      number of OSDs in the cluster by adding more hardware. The OSD count used
      for the purposes of this health check is the number of
      <code class="option">in</code> OSDs, marking <code class="option">out</code> OSDs
      <code class="option">in</code> (if there are any) can also help:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd in <em class="replaceable">OSD_IDs</em></pre></div></dd><dt id="id-1.3.5.5.9.7.12"><span class="term"><code class="option">POOL_TOO_MANY_PGS</code></span></dt><dd><p>
      One or more pools require more PGs based on the amount of data that is
      currently stored in the pool. This can lead to higher memory utilization
      for OSD daemons, slower peering after cluster state changes (like OSD
      restarts, additions, or removals), and higher load on the manager and
      monitor daemons. This warning is generated if the
      <code class="option">pg_autoscale_mode</code> property on the pool is set to warn.
      To disable the warning, you can disable auto-scaling of PGs for the pool
      entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode off</pre></div><p>
      To allow the cluster to automatically adjust the number of PGs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_autoscale_mode on</pre></div><p>
      You can also manually set the number of PGs for the pool to the
      recommended amount with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div></dd><dt id="id-1.3.5.5.9.7.13"><span class="term"><code class="option">POOL_TARGET_SIZE_RATIO_OVERCOMMITTED</code></span></dt><dd><p>
      One or more pools have a <code class="option">target_size_ratio</code> property set
      to estimate the expected size of the pool as a fraction of total storage,
      but the value(s) exceed the total available storage (either by themselves
      or in combination with other pools’ actual usage). This can indicate that
      the <code class="option">target_size_ratio</code> value for the pool is too large
      and should be reduced or set to zero with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> target_size_ratio 0</pre></div></dd><dt id="id-1.3.5.5.9.7.14"><span class="term"><code class="option">POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</code></span></dt><dd><p>
      One or more pools have a <code class="option">target_size_bytes</code> property set
      to estimate the expected size of the pool, but the value(s) exceed the
      total available storage (either by themselves or in combination with
      other pools’ actual usage). This indicates that the
      <code class="option">target_size_bytes</code> value for the pool is too large and
      should be reduced or set to zero with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> target_size_bytes 0</pre></div></dd><dt id="id-1.3.5.5.9.7.15"><span class="term"><code class="option">TOO_FEW_OSDS</code></span></dt><dd><p>
      The number of OSDs in the cluster is below the configurable threshold of
      <code class="option">osd_pool_default_size</code>.
     </p></dd><dt id="id-1.3.5.5.9.7.16"><span class="term"><code class="option">SMALLER_PGP_NUM</code></span></dt><dd><p>
      One or more pools have a <code class="option">pgp_num</code> value less than
      <code class="option">pg_num</code>, indicating that the PG count was increased
      without also increasing the placement behavior. To adjust the placement
      group number, adjust <code class="option">pgp_num</code> and
      <code class="option">pg_num</code>. Ensure that changing <code class="option">pgp_num</code> is
      performed first and does not trigger the rebalance. To resolve, set
      <code class="option">pgp_num</code> to match <code class="option">pg_num</code> and trigger the
      data migration with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL</em> pgp_num <em class="replaceable">PG-NUM-VALUE</em></pre></div></dd><dt id="id-1.3.5.5.9.7.17"><span class="term"><code class="option">MANY_OBJECTS_PER_PG</code></span></dt><dd><p>
      One or more pools has an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration value. This
      indicates that the pool(s) containing most of the data in the cluster
      have too few PGs, or that other pools that do not contain as much data
      have too many PGs. The threshold can be raised to silence the health
      warning by adjusting the <code class="option">mon_pg_warn_max_object_skew</code>
      configuration option on the monitors.
     </p></dd><dt id="id-1.3.5.5.9.7.18"><span class="term"><code class="option">POOL_APP_NOT_ENABLED</code></span></dt><dd><p>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd pool init <em class="replaceable">POOLNAME</em></pre></div><p>
      If the pool is being used by a custom application
      <em class="replaceable">FOO</em>, you can also label via the low-level
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable <em class="replaceable">FOO</em></pre></div></dd><dt id="id-1.3.5.5.9.7.19"><span class="term"><code class="option">POOL_FULL</code></span></dt><dd><p>
      One or more pools has reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <code class="option">mon_pool_quota_crit_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_objects <em class="replaceable">OBJECTS</em></pre></div><p>
      Setting the quota value to 0 disables the quota.
     </p></dd><dt id="id-1.3.5.5.9.7.20"><span class="term"><code class="option">POOL_NEAR_FULL</code></span></dt><dd><p>
      One or more pools are approaching its quota. The threshold to trigger
      this warning condition is controlled by the
      <code class="option">mon_pool_quota_warn_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_bytes <em class="replaceable">BYTES</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL</em> max_objects <em class="replaceable">OBJECTS</em></pre></div></dd><dt id="id-1.3.5.5.9.7.21"><span class="term"><code class="option">OBJECT_MISPLACED</code></span></dt><dd><p>
      One or more objects in the cluster is not stored on the node the cluster
      would like it to be stored on. This is an indication that data migration
      due to some recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in and of itself. Data consistency is not at
      risk and old copies of objects are not removed until the desired number
      of new copies (in the desired locations) are present.
     </p></dd><dt id="id-1.3.5.5.9.7.22"><span class="term"><code class="option">OBJECT_UNFOUND</code></span></dt><dd><p>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to unfound objects will block.
      Ideally, a down OSD can be brought back online that has the more recent
      copy of the unfound object. Candidate OSDs can be identified from the
      peering state for the PG(s) responsible for the unfound object:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell <em class="replaceable">PG_ID</em> query</pre></div><p>
      If the latest copy of the object is not available, the cluster can be
      told to roll back to a previous version of the object.
     </p></dd><dt id="id-1.3.5.5.9.7.23"><span class="term"><code class="option">SLOW_OPS</code></span></dt><dd><p>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. The
      request queue on the OSD(s) in question can be queried with the following
      command, executed from the OSD host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> ops</pre></div><p>
      A summary of the slowest recent requests can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_historic_ops</pre></div><p>
      The location of an OSD can be found with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd find osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.3.5.5.9.7.24"><span class="term"><code class="option">PG_NOT_SCRUBBED</code></span></dt><dd><p>
      One or more PGs have not been scrubbed recently. PGs are normally
      scrubbed every <code class="option">mon_scrub_interval</code> seconds and this
      warning triggers when
      <code class="option">mon_warn_pg_not_deep_scrubbed_ratio</code> percentage of
      interval has elapsed without a scrub since it was due. PGs do not scrub
      if they are not flagged as clean. This can happen if they are misplaced
      or degraded (see <code class="option">PG_AVAILABILITY</code> and
      <code class="option">PG_DEGRADED</code> above). You can manually initiate a scrub of
      a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg scrub <em class="replaceable">PG_ID</em></pre></div></dd><dt id="id-1.3.5.5.9.7.25"><span class="term"><code class="option">PG_NOT_DEEP_SCRUBBED</code></span></dt><dd><p>
      One or more PGs have not been deep scrubbed recently. PGs are normally
      scrubbed every <code class="option">osd_deep_scrub_interval</code> seconds and this
      warning triggers when
      <code class="option">mon_warn_pg_not_deep_scrubbed_ratio</code> percentage of
      interval has elapsed without a scrub since it was due. PGs do not (deep)
      scrub if they are not flagged as clean. This can happen if they are
      misplaced or degraded (see <code class="option">PG_AVAILABILITY</code> and
      <code class="option">PG_DEGRADED</code> above). You can manually initiate a scrub of
      a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph pg deep-scrub <em class="replaceable">PG_ID</em></pre></div></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">MISCELLANEOUS </span><a title="Permalink" class="permalink" href="#id-1.3.5.5.9.8">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.5.9.8.2"><span class="term"><code class="option">RECENT_CRASH</code></span></dt><dd><p>
      One or more Ceph daemons have crashed recently, and the crash has not
      yet been archived or acknowledged by the administrator. This may indicate
      a software bug, a hardware problem (for example, a failing disk), or some
      other problem.
     </p><div id="id-1.3.5.5.9.8.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       Encountering a crash is not normal, but can be observed on occasion.
       When a crash occurs, <code class="command">ceph crash</code> will alert the
       administrator. If <code class="command">ceph crash</code> is reporting an abnormal
       number of crashes, contact SUSE support for further assistance.
       <code class="command">supportconfig</code> reports from the affected nodes will
       help SUSE address the issue. Also consider patching the cluster at a
       regular interval.
      </p></div><p>
      New crashes can be listed with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash ls-new</pre></div><p>
      Information about a specific crash can be examined with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash info <em class="replaceable">CRASH-ID</em></pre></div><p>
      This warning can be silenced by archiving the crash (perhaps after being
      examined by an administrator) so that it does not generate this warning:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive <em class="replaceable">CRASH-ID</em></pre></div><p>
      Similarly, all new crashes can be archived with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive-all</pre></div><p>
      Archived crashes are still visible via <code class="command">ceph crash ls</code>
      but not <code class="command">ceph crash ls-new</code>. The time period for what
      recent means is controlled by the option
      <code class="literal">mgr/crash/warn_recent_interval</code> (default: two weeks).
      These warnings can be disabled entirely with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr/crash/warn_recent_interval 0</pre></div></dd><dt id="id-1.3.5.5.9.8.3"><span class="term"><code class="option">TELEMETRY_CHANGED</code></span></dt><dd><p>
      Telemetry has been enabled but the contents of the telemetry report have
      changed since that time, so telemetry reports are not sent. The Ceph
      developers periodically revise the telemetry feature to include new and
      useful information, or to remove information found to be useless or
      sensitive. If any new information is included in the report, Ceph
      requires the administrator to re-enable telemetry to ensure they have an
      opportunity to (re)review what information is shared. To review the
      contents of the telemetry report:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show</pre></div><p>
      The telemetry report consists of several optional channels that are
      independently enabled or disabled. To re-enable telemetry (and make this
      warning go away):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
      To disable telemetry (and make this warning go away):
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry off</pre></div></dd></dl></div><div class="verbatim-wrap"><pre class="screen"> groups:
  - name: cluster health
   rules:
    - alert: health error
     expr: ceph_health_status == 2
     for: 5m
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Ceph in error for &gt; 5m
    - alert: unhealthy
     expr: ceph_health_status != 0
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: Ceph not healthy for &gt; 5m
  - name: mon
   rules:
    - alert: low monitor quorum count
     expr: ceph_monitor_quorum_count &lt; 3
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Monitor count in quorum is low
  - name: osd
   rules:
    - alert: 10% OSDs down
     expr: sum(ceph_osd_down) / count(ceph_osd_in) &gt;= 0.1
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: More then 10% of OSDS are down
    - alert: OSD down
     expr: sum(ceph_osd_down) &gt; 1
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: One or more OSDS down for more then 15 minutes
    - alert: OSDs near full
     expr: (ceph_osd_utilization unless on(osd) ceph_osd_down) &gt; 80
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: OSD {{ $labels.osd }} is dangerously full, over 80%
    # alert on single OSDs flapping
    - alert: flap osd
     expr: rate(ceph_osd_up[5m])*60 &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
        OSD {{ $label.osd }} was marked down at back up at least once a
        minute for 5 minutes.
    # alert on high deviation from average PG count
    - alert: high pg count deviation
     expr: abs(((ceph_osd_pgs &gt; 0) - on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) / on (job) group_left avg(ceph_osd_pgs &gt; 0) by (job)) &gt; 0.35
     for: 5m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
        OSD {{ $labels.osd }} deviates by more then 30% from
        average PG count
    # alert on high commit latency...but how high is too high
  - name: mds
   rules:
   # no mds metrics are exported yet
  - name: mgr
   rules:
   # no mgr metrics are exported yet
  - name: pgs
   rules:
    - alert: pgs inactive
     expr: ceph_total_pgs - ceph_active_pgs &gt; 0
     for: 5m
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: One or more PGs are inactive for more then 5 minutes.
    - alert: pgs unclean
     expr: ceph_total_pgs - ceph_clean_pgs &gt; 0
     for: 15m
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: One or more PGs are not clean for more then 15 minutes.
  - name: nodes
   rules:
    - alert: root volume full
     expr: node_filesystem_avail{mountpoint="/"} / node_filesystem_size{mountpoint="/"} &lt; 0.1
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Root volume (OSD and MON store) is dangerously full (&lt; 10% free)
    # alert on nic packet errors and drops rates &gt; 1 packet/s
    - alert: network packets dropped
     expr: irate(node_network_receive_drop{device!="lo"}[5m]) + irate(node_network_transmit_drop{device!="lo"}[5m]) &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Node {{ $labels.instance }} experiences packet drop &gt; 1
       packet/s on interface {{ $lables.device }}
    - alert: network packet errors
     expr: irate(node_network_receive_errs{device!="lo"}[5m]) + irate(node_network_transmit_errs{device!="lo"}[5m]) &gt; 1
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Node {{ $labels.instance }} experiences packet errors &gt; 1
       packet/s on interface {{ $lables.device }}
    # predict fs fillup times
    - alert: storage filling
     expr: ((node_filesystem_free - node_filesystem_size) / deriv(node_filesystem_free[2d]) &lt;= 5) &gt; 0
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Mountpoint {{ $lables.mountpoint }} will be full in less then 5 days
       assuming the average fillup rate of the past 48 hours.
  - name: pools
   rules:
    - alert: pool full
     expr: ceph_pool_used_bytes / ceph_pool_available_bytes &gt; 0.9
     labels:
      severity: critical
      type: ses_default
     annotations:
      description: Pool {{ $labels.pool }} at 90% capacity or over
    - alert: pool filling up
     expr: (-ceph_pool_used_bytes / deriv(ceph_pool_available_bytes[2d]) &lt;= 5 ) &gt; 0
     labels:
      severity: warning
      type: ses_default
     annotations:
      description: &gt;
       Pool {{ $labels.pool }} will be full in less then 5 days
       assuming the average fillup rate of the past 48 hours.</pre></div></section></section><section class="chapter" id="cha-storage-cephx" data-id-title="Authentication with cephx"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span> <a title="Permalink" class="permalink" href="#cha-storage-cephx">#</a></h2></div></div></div><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p><div id="id-1.3.5.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   The <code class="systemitem">cephx</code> protocol does not address data encryption in transport, such as
   TLS/SSL.
  </p></div><section class="sect1" id="storage-cephx-arch" data-id-title="Authentication Architecture"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.1 </span><span class="title-name">Authentication Architecture</span> <a title="Permalink" class="permalink" href="#storage-cephx-arch">#</a></h2></div></div></div><p>
   <code class="systemitem">cephx</code> uses shared secret keys for authentication, meaning both the client
   and Ceph Monitors have a copy of the client’s secret key. The authentication
   protocol enables both parties to prove to each other that they have a copy
   of the key without actually revealing it. This provides mutual
   authentication, which means the cluster is sure the user possesses the
   secret key, and the user is sure that the cluster has a copy of the secret
   key as well.
  </p><p>
   A key scalability feature of Ceph is to avoid a centralized interface to
   the Ceph object store. This means that Ceph clients can interact with
   OSDs directly. To protect data, Ceph provides its <code class="systemitem">cephx</code> authentication
   system, which authenticates Ceph clients.
  </p><p>
   Each monitor can authenticate clients and distribute keys, so there is no
   single point of failure or bottleneck when using <code class="systemitem">cephx</code>. The monitor
   returns an authentication data structure that contains a session key for use
   in obtaining Ceph services. This session key is itself encrypted with the
   client’s permanent secret key, so that only the client can request services
   from the Ceph monitors. The client then uses the session key to request
   its desired services from the monitor, and the monitor provides the client
   with a ticket that will authenticate the client to the OSDs that actually
   handle data. Ceph monitors and OSDs share a secret, so the client can use
   the ticket provided by the monitor with any OSD or metadata server in the
   cluster. <code class="systemitem">cephx</code> tickets expire, so an attacker cannot use an expired ticket
   or session key obtained wrongfully.
  </p><p>
   To use <code class="systemitem">cephx</code>, an administrator must setup clients/users first. In the
   following diagram, the
   <code class="systemitem">client.admin</code> user invokes
   <code class="command">ceph auth get-or-create-key</code> from the command line to
   generate a user name and secret key. Ceph’s <code class="command">auth</code>
   subsystem generates the user name and key, stores a copy with the monitor(s)
   and transmits the user’s secret back to the
   <code class="systemitem">client.admin</code> user. This means that
   the client and the monitor share a secret key.
  </p><div class="figure" id="id-1.3.5.6.5.6"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring.png" target="_blank"><img src="images/cephx_keyring.png" width="" alt="Basic cephx Authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 19.1: </span><span class="title-name">Basic <code class="systemitem">cephx</code> Authentication </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.6">#</a></h6></div></div><p>
   To authenticate with the monitor, the client passes the user name to the
   monitor. The monitor generates a session key and encrypts it with the secret
   key associated with the user name and transmits the encrypted ticket back to
   the client. The client then decrypts the data with the shared secret key to
   retrieve the session key. The session key identifies the user for the
   current session. The client then requests a ticket related to the user,
   which is signed by the session key. The monitor generates a ticket, encrypts
   it with the user’s secret key and transmits it back to the client. The
   client decrypts the ticket and uses it to sign requests to OSDs and metadata
   servers throughout the cluster.
  </p><div class="figure" id="id-1.3.5.6.5.8"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring2.png" target="_blank"><img src="images/cephx_keyring2.png" width="" alt="cephx Authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 19.2: </span><span class="title-name"><code class="systemitem">cephx</code> Authentication </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.8">#</a></h6></div></div><p>
   The <code class="systemitem">cephx</code> protocol authenticates ongoing communications between the client
   machine and the Ceph servers. Each message sent between a client and a
   server after the initial authentication is signed using a ticket that the
   monitors, OSDs, and metadata servers can verify with their shared secret.
  </p><div class="figure" id="id-1.3.5.6.5.10"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring3.png" target="_blank"><img src="images/cephx_keyring3.png" width="" alt="cephx Authentication - MDS and OSD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 19.3: </span><span class="title-name"><code class="systemitem">cephx</code> Authentication - MDS and OSD </span><a title="Permalink" class="permalink" href="#id-1.3.5.6.5.10">#</a></h6></div></div><div id="id-1.3.5.6.5.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The protection offered by this authentication is between the Ceph client
    and the Ceph cluster hosts. The authentication is not extended beyond the
    Ceph client. If the user accesses the Ceph client from a remote host,
    Ceph authentication is not applied to the connection between the user’s
    host and the client host.
   </p></div></section><section class="sect1" id="storage-cephx-keymgmt" data-id-title="Key Management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.2 </span><span class="title-name">Key Management</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt">#</a></h2></div></div></div><p>
   This section describes Ceph client users and their authentication and
   authorization with the Ceph storage cluster. <span class="emphasis"><em>Users</em></span>
   are either individuals or system actors such as applications, which use
   Ceph clients to interact with the Ceph storage cluster daemons.
  </p><p>
   When Ceph runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user (usually via the command line). If you do not
   specify a user name, Ceph will use
   <code class="systemitem">client.admin</code> as the default user
   name. If you do not specify a keyring, Ceph will look for a keyring via
   the keyring setting in the Ceph configuration file. For example, if you
   execute the <code class="command">ceph health</code> command without specifying a user
   name or keyring, Ceph interprets the command like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</pre></div><p>
   Alternatively, you may use the <code class="literal">CEPH_ARGS</code> environment
   variable to avoid re-entering the user name and secret.
  </p><section class="sect2" id="storage-cephx-keymgmt-backgrnd" data-id-title="Background Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.2.1 </span><span class="title-name">Background Information</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-backgrnd">#</a></h3></div></div></div><p>
    Regardless of the type of Ceph client (for example, block device, object
    storage, file system, native API), Ceph stores all data as objects within
    <span class="emphasis"><em>pools</em></span>. Ceph users need to have access to pools in
    order to read and write data. Additionally, Ceph users must have execute
    permissions to use Ceph's administrative commands. The following concepts
    will help you understand Ceph user management.
   </p><section class="sect3" id="cephx-user" data-id-title="User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.1.1 </span><span class="title-name">User</span> <a title="Permalink" class="permalink" href="#cephx-user">#</a></h4></div></div></div><p>
     A user is either an individual or a system actor such as an application.
     Creating users allows you to control who (or what) can access your Ceph
     storage cluster, its pools, and the data within pools.
    </p><p>
     Ceph uses <span class="emphasis"><em>types</em></span> of users. For the purposes of user
     management, the type will always be <code class="literal">client</code>. Ceph
     identifies users in period (.) delimited form, consisting of the user type
     and the user ID. For example, <code class="literal">TYPE.ID</code>,
     <code class="literal">client.admin</code>, or <code class="literal">client.user1</code>. The
     reason for user typing is that Ceph monitors, OSDs, and metadata servers
     also use the cephx protocol, but they are not clients. Distinguishing the
     user type helps to distinguish between client users and other users,
     streamlining access control, user monitoring, and traceability.
    </p><p>
     Sometimes Ceph’s user type may seem confusing, because the Ceph
     command line allows you to specify a user with or without the type,
     depending upon your command line usage. If you specify
     <code class="option">--user</code> or <code class="option">--id</code>, you can omit the type.
     So <code class="literal">client.user1</code> can be entered simply as
     <code class="literal">user1</code>. If you specify <code class="option">--name</code> or
     <code class="option">-n</code>, you must specify the type and name, such as
     <code class="literal">client.user1</code>. We recommend using the type and name as a
     best practice wherever possible.
    </p><div id="id-1.3.5.6.6.6.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      A Ceph storage cluster user is not the same as a Ceph object storage
      user or a Ceph file system user. The Ceph Object Gateway uses a Ceph storage
      cluster user to communicate between the gateway daemon and the storage
      cluster, but the gateway has its own user management functionality for
      end users. The Ceph file system uses POSIX semantics. The user space
      associated with it is not the same as a Ceph storage cluster user.
     </p></div></section><section class="sect3" id="auth-capabilities" data-id-title="Authorization and Capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.1.2 </span><span class="title-name">Authorization and Capabilities</span> <a title="Permalink" class="permalink" href="#auth-capabilities">#</a></h4></div></div></div><p>
     Ceph uses the term <code class="literal">capabilities</code> to describe
     authorizing an authenticated user to exercise the functionality of the
     monitors, OSDs and metadata servers. Capabilities can also restrict access
     to data within a pool or pool namespace. A Ceph administrative user sets
     a user's capabilities when creating or updating a user.
    </p><p>
     Capability syntax follows the form:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">daemon-type</em> '<em class="replaceable">cap-spec</em>[, <em class="replaceable">cap-spec</em> ...]'</pre></div><p>
     Following is a list of capabilities for each service type:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.6.6.6.4.6.1"><span class="term">Monitor capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code> and <code class="literal">allow profile
        <em class="replaceable">cap</em></code>.
       </p><div class="verbatim-wrap"><pre class="screen">mon 'allow rwx'
mon 'allow profile osd'</pre></div></dd><dt id="id-1.3.5.6.6.6.4.6.2"><span class="term">OSD capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code>, <code class="literal">class-read</code>,
        <code class="literal">class-write</code> and <code class="literal">profile osd</code>.
        Additionally, OSD capabilities also allow for pool and namespace
        settings.
       </p><div class="verbatim-wrap"><pre class="screen">osd 'allow <em class="replaceable">capability</em>' [pool=<em class="replaceable">poolname</em>] [namespace=<em class="replaceable">namespace-name</em>]</pre></div></dd><dt id="id-1.3.5.6.6.6.4.6.3"><span class="term">MDS capability</span></dt><dd><p>
        simply requires <code class="literal">allow</code>, or blank.
       </p><div class="verbatim-wrap"><pre class="screen">mds 'allow'</pre></div></dd></dl></div><p>
     The following entries describe each capability:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.6.6.6.4.8.1"><span class="term">allow</span></dt><dd><p>
        Precedes access settings for a daemon. Implies <code class="literal">rw</code>
        for MDS only.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.2"><span class="term">r</span></dt><dd><p>
        Gives the user read access. Required with monitors to retrieve the
        CRUSH map.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.3"><span class="term">w</span></dt><dd><p>
        Gives the user write access to objects.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.4"><span class="term">x</span></dt><dd><p>
        Gives the user the capability to call class methods (both read and
        write) and to conduct <code class="literal">auth</code> operations on monitors.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.5"><span class="term">class-read</span></dt><dd><p>
        Gives the user the capability to call class read methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.6"><span class="term">class-write</span></dt><dd><p>
        Gives the user the capability to call class write methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.7"><span class="term">*</span></dt><dd><p>
        Gives the user read, write, and execute permissions for a particular
        daemon/pool, and the ability to execute admin commands.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.8"><span class="term">profile osd</span></dt><dd><p>
        Gives a user permissions to connect as an OSD to other OSDs or
        monitors. Conferred on OSDs to enable OSDs to handle replication
        heartbeat traffic and status reporting.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.9"><span class="term">profile mds</span></dt><dd><p>
        Gives a user permissions to connect as an MDS to other MDSs or
        monitors.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.10"><span class="term">profile bootstrap-osd</span></dt><dd><p>
        Gives a user permissions to bootstrap an OSD. Delegated to deployment
        tools so that they have permissions to add keys when bootstrapping an
        OSD.
       </p></dd><dt id="id-1.3.5.6.6.6.4.8.11"><span class="term">profile bootstrap-mds</span></dt><dd><p>
        Gives a user permissions to bootstrap a metadata server. Delegated to
        deployment tools so they have permissions to add keys when
        bootstrapping a metadata server.
       </p></dd></dl></div><p>
     For example, monitor capabilities include <code class="literal">r</code>,
     <code class="literal">w</code>, <code class="literal">x</code> access settings or
     <code class="literal">profile {name}</code>.
    </p><p>
     The syntax looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">mon 'allow {access-spec} [network {network/prefix}]'
mon 'profile {name}'</pre></div><p>
     The <code class="literal">{access-spec}</code> syntax is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">* | all | [r][w][x]</pre></div><p>
     The optional <code class="literal">{network/prefix}</code> is a standard network
     name and prefix length in CIDR notation (for exmaple, 10.3.0.0/16). If
     present, the use of this capability is restricted to clients connecting
     from this network.
    </p><p>
     Another example is the OSD capabilities that include <code class="literal">r</code>,
     <code class="literal">w</code>, <code class="literal">x</code>, <code class="literal">class-read</code>,
     <code class="literal">class-write</code> access settings or <code class="literal">profile
     {name}</code>. Additionally, OSD capabilities also allow for pool and
     namespace settings.
    </p><p>
     The syntax looks as follows:
    </p><div class="verbatim-wrap"><pre class="screen">osd 'allow {access-spec} [{match-spec}] [network {network/prefix}]'
osd 'profile {name} [pool={pool-name} [namespace={namespace-name}]] [network {network/prefix}]'</pre></div><p>
     Capabilities can also restrict access to data within a pool, a namespace
     within a pool, or a set of pools based on their application tags. A Ceph
     administrative user sets a user’s capabilities when creating or updating a
     user.
    </p></section><section class="sect3" id="id-1.3.5.6.6.6.5" data-id-title="Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.1.3 </span><span class="title-name">Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.6.5">#</a></h4></div></div></div><p>
     A pool is a logical partition where users store data. In Ceph
     deployments, it is common to create a pool as a logical partition for
     similar types of data. For example, when deploying Ceph as a back-end
     for OpenStack, a typical deployment would have pools for volumes, images,
     backups and virtual machines, and users such as
     <code class="systemitem">client.glance</code> or
     <code class="systemitem">client.cinder</code>.
    </p></section></section><section class="sect2" id="storage-cephx-keymgmt-usermgmt" data-id-title="Managing Users"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.2.2 </span><span class="title-name">Managing Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt">#</a></h3></div></div></div><p>
    User management functionality provides Ceph cluster administrators with
    the ability to create, update, and delete users directly in the Ceph
    cluster.
   </p><p>
    When you create or delete users in the Ceph cluster, you may need to
    distribute keys to clients so that they can be added to keyrings. See
    <a class="xref" href="#storage-cephx-keymgmt-keyringmgmt" title="19.2.3. Keyring Management">Section 19.2.3, “Keyring Management”</a> for details.
   </p><section class="sect3" id="id-1.3.5.6.6.7.4" data-id-title="Listing Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.1 </span><span class="title-name">Listing Users</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.7.4">#</a></h4></div></div></div><p>
     To list the users in your cluster, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth list</pre></div><p>
     Ceph will list all users in your cluster. For example, in a cluster with
     two nodes, <code class="command">ceph auth list</code> output looks similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd</pre></div><div id="id-1.3.5.6.6.7.4.6" data-id-title="TYPE.ID Notation" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: TYPE.ID Notation</h6><p>
      Note that the <code class="literal">TYPE.ID</code> notation for users applies such
      that <code class="literal">osd.0</code> specifies a user of type
      <code class="literal">osd</code> and its ID is <code class="literal">0</code>.
      <code class="literal">client.admin</code> is a user of type
      <code class="literal">client</code> and its ID is <code class="literal">admin</code>. Note
      also that each entry has a <code class="literal">key:
      <em class="replaceable">value</em></code> entry, and one or more
      <code class="literal">caps:</code> entries.
     </p><p>
      You may use the <code class="option">-o <em class="replaceable">filename</em></code>
      option with <code class="command">ceph auth list</code> to save the output to a
      file.
     </p></div></section><section class="sect3" id="id-1.3.5.6.6.7.5" data-id-title="Getting Information about Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.2 </span><span class="title-name">Getting Information about Users</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.7.5">#</a></h4></div></div></div><p>
     To retrieve a specific user, key, and capabilities, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get <em class="replaceable">TYPE.ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get client.admin
exported keyring for client.admin
[client.admin]
	key = AQA19uZUqIwkHxAAFuUwvq0eJD4S173oFRxe0g==
	caps mds = "allow"
	caps mon = "allow *"
 caps osd = "allow *"</pre></div><p>
     Developers may also execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth export <em class="replaceable">TYPE.ID</em></pre></div><p>
     The <code class="command">auth export</code> command is identical to <code class="command">auth
     get</code>, but also prints the internal authentication ID.
    </p></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-useradd" data-id-title="Adding Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.3 </span><span class="title-name">Adding Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-useradd">#</a></h4></div></div></div><p>
     Adding a user creates a user name (<code class="literal">TYPE.ID</code>), a secret
     key, and any capabilities included in the command you use to create the
     user.
    </p><p>
     A user's key enables the user to authenticate with the Ceph storage
     cluster. The user's capabilities authorize the user to read, write, or
     execute on Ceph monitors (mon), Ceph OSDs (osd), or Ceph metadata
     servers (mds).
    </p><p>
     There are a few commands available to add a user:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.6.6.7.6.5.1"><span class="term"><code class="command">ceph auth add</code></span></dt><dd><p>
        This command is the canonical way to add a user. It will create the
        user, generate a key, and add any specified capabilities.
       </p></dd><dt id="id-1.3.5.6.6.7.6.5.2"><span class="term"><code class="command">ceph auth get-or-create</code></span></dt><dd><p>
        This command is often the most convenient way to create a user, because
        it returns a keyfile format with the user name (in brackets) and the
        key. If the user already exists, this command simply returns the user
        name and key in the keyfile format. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd><dt id="id-1.3.5.6.6.7.6.5.3"><span class="term"><code class="command">ceph auth get-or-create-key</code></span></dt><dd><p>
        This command is a convenient way to create a user and return the user's
        key (only). This is useful for clients that need the key only (for
        example <code class="systemitem">libvirt</code>). If the user already exists, this command simply
        returns the key. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd></dl></div><p>
     When creating client users, you may create a user with no capabilities. A
     user with no capabilities can authenticate but nothing more. Such client
     cannot retrieve the cluster map from the monitor. However, you can create
     a user with no capabilities if you want to defer adding capabilities later
     using the <code class="command">ceph auth caps</code> command.
    </p><p>
     A typical user has at least read capabilities on the Ceph monitor and
     read and write capabilities on Ceph OSDs. Additionally, a user's OSD
     permissions are often restricted to accessing a particular pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth add client.john mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create client.paul mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create client.george mon 'allow r' osd \
 'allow rw pool=liverpool' -o george.keyring
<code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create-key client.ringo mon 'allow r' osd \
 'allow rw pool=liverpool' -o ringo.key</pre></div><div id="id-1.3.5.6.6.7.6.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If you provide a user with capabilities to OSDs, but you <span class="emphasis"><em>do
      not</em></span> restrict access to particular pools, the user will have
      access to <span class="emphasis"><em>all</em></span> pools in the cluster.
     </p></div></section><section class="sect3" id="id-1.3.5.6.6.7.7" data-id-title="Modifying User Capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.4 </span><span class="title-name">Modifying User Capabilities</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.7.7">#</a></h4></div></div></div><p>
     The <code class="command">ceph auth caps</code> command allows you to specify a user
     and change the user's capabilities. Setting new capabilities will
     overwrite current ones. To view current capabilities run <code class="command">ceph
     auth get
     <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em></code>.
     To add capabilities, you also need to specify the existing capabilities
     when using the following form:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth caps <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em> <em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]' [<em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]']</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get client.john
<code class="prompt user">cephadm@adm &gt; </code>ceph auth caps client.john mon 'allow r' osd 'allow rw pool=prague'
<code class="prompt user">cephadm@adm &gt; </code>ceph auth caps client.paul mon 'allow rw' osd 'allow r pool=prague'
<code class="prompt user">cephadm@adm &gt; </code>ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'</pre></div><p>
     To remove a capability, you may reset the capability. If you want the user
     to have no access to a particular daemon that was previously set, specify
     an empty string:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth caps client.ringo mon ' ' osd ' '</pre></div></section><section class="sect3" id="id-1.3.5.6.6.7.8" data-id-title="Deleting Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.5 </span><span class="title-name">Deleting Users</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.7.8">#</a></h4></div></div></div><p>
     To delete a user, use <code class="command">ceph auth del</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth del <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     If you created users with permissions strictly for a pool that no longer
     exists, you should consider deleting those users too.
    </p></section><section class="sect3" id="id-1.3.5.6.6.7.9" data-id-title="Printing a Users Key"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.6 </span><span class="title-name">Printing a User's Key</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.7.9">#</a></h4></div></div></div><p>
     To print a user’s authentication key to standard output, execute the
     following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth print-key <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     Printing a user's key is useful when you need to populate client software
     with a user's key (such as <code class="systemitem">libvirt</code>), as in the following example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph host:/ mount_point \
-o name=client.user,secret=`ceph auth print-key client.user`</pre></div></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-userimp" data-id-title="Importing Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.2.7 </span><span class="title-name">Importing Users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-userimp">#</a></h4></div></div></div><p>
     To import one or more users, use <code class="command">ceph auth import</code> and
     specify a keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><div id="id-1.3.5.6.6.7.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The Ceph storage cluster will add new users, their keys and their
      capabilities and will update existing users, their keys and their
      capabilities.
     </p></div></section></section><section class="sect2" id="storage-cephx-keymgmt-keyringmgmt" data-id-title="Keyring Management"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.2.3 </span><span class="title-name">Keyring Management</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-keyringmgmt">#</a></h3></div></div></div><p>
    When you access Ceph via a Ceph client, the client will look for a
    local keyring. Ceph presets the keyring setting with the following four
    keyring names by default so you do not need to set them in your Ceph
    configuration file unless you want to override the defaults:
   </p><div class="verbatim-wrap"><pre class="screen">/etc/ceph/<em class="replaceable">cluster</em>.<em class="replaceable">name</em>.keyring
/etc/ceph/<em class="replaceable">cluster</em>.keyring
/etc/ceph/keyring
/etc/ceph/keyring.bin</pre></div><p>
    The <em class="replaceable">cluster</em> metavariable is your Ceph cluster
    name as defined by the name of the Ceph configuration file.
    <code class="filename">ceph.conf</code> means that the cluster name is
    <code class="literal">ceph</code>, thus <code class="literal">ceph.keyring</code>. The
    <em class="replaceable">name</em> metavariable is the user type and user ID,
    for example <code class="literal">client.admin</code>, thus
    <code class="literal">ceph.client.admin.keyring</code>.
   </p><p>
    After you create a user (for example
    <code class="systemitem">client.ringo</code>), you must get the
    key and add it to a keyring on a Ceph client so that the user can access
    the Ceph storage cluster.
   </p><p>
    <a class="xref" href="#storage-cephx-keymgmt" title="19.2. Key Management">Section 19.2, “Key Management”</a> details how to list, get, add,
    modify and delete users directly in the Ceph storage cluster. However,
    Ceph also provides the <code class="command">ceph-authtool</code> utility to allow
    you to manage keyrings from a Ceph client.
   </p><section class="sect3" id="creating-keyring" data-id-title="Creating a Keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.3.1 </span><span class="title-name">Creating a Keyring</span> <a title="Permalink" class="permalink" href="#creating-keyring">#</a></h4></div></div></div><p>
     When you use the procedures in <a class="xref" href="#storage-cephx-keymgmt" title="19.2. Key Management">Section 19.2, “Key Management”</a> to
     create users, you need to provide user keys to the Ceph client(s) so
     that the client can retrieve the key for the specified user and
     authenticate with the Ceph storage cluster. Ceph clients access
     keyrings to look up a user name and retrieve the user's key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool --create-keyring /path/to/keyring</pre></div><p>
     When creating a keyring with multiple users, we recommend using the
     cluster name (for example <em class="replaceable">cluster</em>.keyring) for
     the keyring file name and saving it in the <code class="filename">/etc/ceph</code>
     directory so that the keyring configuration default setting will pick up
     the file name without requiring you to specify it in the local copy of
     your Ceph configuration file. For example, create
     <code class="filename">ceph.keyring</code> by executing the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring</pre></div><p>
     When creating a keyring with a single user, we recommend using the cluster
     name, the user type and the user name and saving it in the
     <code class="filename">/etc/ceph</code> directory. For example,
     <code class="filename">ceph.client.admin.keyring</code> for the
     <code class="systemitem">client.admin</code> user.
    </p></section><section class="sect3" id="id-1.3.5.6.6.8.8" data-id-title="Adding a User to a Keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.3.2 </span><span class="title-name">Adding a User to a Keyring</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.8.8">#</a></h4></div></div></div><p>
     When you add a user to the Ceph storage cluster (see
     <a class="xref" href="#storage-cephx-keymgmt-usermgmt-useradd" title="19.2.2.3. Adding Users">Section 19.2.2.3, “Adding Users”</a>), you can
     retrieve the user, key and capabilities, and save the user to a keyring.
    </p><p>
     If you only want to use one user per keyring, the <code class="command">ceph auth
     get</code> command with the <code class="option">-o</code> option will save the
     output in the keyring file format. For example, to create a keyring for
     the <code class="systemitem">client.admin</code> user, execute
     the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring</pre></div><p>
     When you want to import users to a keyring, you can use
     <code class="command">ceph-authtool</code> to specify the destination keyring and
     the source keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring \
  --import-keyring /etc/ceph/ceph.client.admin.keyring</pre></div><div id="id-1.3.5.6.6.8.8.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If your keyring is compromised, delete your key from the
      <code class="filename">/etc/ceph</code> directory and recreate a new key using the
      same instructions from <a class="xref" href="#creating-keyring" title="19.2.3.1. Creating a Keyring">Section 19.2.3.1, “Creating a Keyring”</a>.
     </p></div></section><section class="sect3" id="id-1.3.5.6.6.8.9" data-id-title="Creating a User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.3.3 </span><span class="title-name">Creating a User</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.8.9">#</a></h4></div></div></div><p>
     Ceph provides the <code class="command">ceph auth add</code> command to create a
     user directly in the Ceph storage cluster. However, you can also create
     a user, keys and capabilities directly on a Ceph client keyring. Then,
     you can import the user to the Ceph storage cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool -n client.ringo --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.keyring</pre></div><p>
     You can also create a keyring and add a new user to the keyring
     simultaneously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx' --gen-key</pre></div><p>
     In the previous scenarios, the new user
     <code class="systemitem">client.ringo</code> is only in the
     keyring. To add the new user to the Ceph storage cluster, you must still
     add the new user to the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth add client.ringo -i /etc/ceph/ceph.keyring</pre></div></section><section class="sect3" id="id-1.3.5.6.6.8.10" data-id-title="Modifying Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">19.2.3.4 </span><span class="title-name">Modifying Users</span> <a title="Permalink" class="permalink" href="#id-1.3.5.6.6.8.10">#</a></h4></div></div></div><p>
     To modify the capabilities of a user record in a keyring, specify the
     keyring and the user followed by the capabilities:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx'</pre></div><p>
     To update the modified user within the Ceph cluster environment, you
     must import the changes from the keyring to the user entry in the Ceph
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><p>
     See <a class="xref" href="#storage-cephx-keymgmt-usermgmt-userimp" title="19.2.2.7. Importing Users">Section 19.2.2.7, “Importing Users”</a> for details
     on updating a Ceph storage cluster user from a keyring.
    </p></section></section><section class="sect2" id="storage-cephx-keymgmt-cmdline" data-id-title="Command Line Usage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.2.4 </span><span class="title-name">Command Line Usage</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-cmdline">#</a></h3></div></div></div><p>
    The <code class="command">ceph</code> command supports the following options related
    to the user name and secret manipulation:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.6.6.9.3.1"><span class="term"><code class="option">--id</code> or <code class="option">--user</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">id</code>, <code class="option">name</code> and <code class="option">-n</code>
       options enable you to specify the ID portion of the user name (for
       example <code class="systemitem">admin</code> or
       <code class="systemitem">user1</code>). You can specify the
       user with the --id and omit the type. For example, to specify user
       client.foo enter the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph --id foo --keyring /path/to/keyring health
<code class="prompt user">cephadm@adm &gt; </code>ceph --user foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.3.5.6.6.9.3.2"><span class="term"><code class="option">--name</code> or <code class="option">-n</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">--name</code> and <code class="option">-n</code> options enable you to
       specify the fully qualified user name. You must specify the user type
       (typically <code class="literal">client</code>) with the user ID:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph --name client.foo --keyring /path/to/keyring health
<code class="prompt user">cephadm@adm &gt; </code>ceph -n client.foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.3.5.6.6.9.3.3"><span class="term"><code class="option">--keyring</code></span></dt><dd><p>
       The path to the keyring containing one or more user name and secret. The
       <code class="option">--secret</code> option provides the same functionality, but it
       does not work with Object Gateway, which uses <code class="option">--secret</code> for
       another purpose. You may retrieve a keyring with <code class="command">ceph auth
       get-or-create</code> and store it locally. This is a preferred
       approach, because you can switch user names without switching the
       keyring path:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --id foo --keyring /path/to/keyring mypool/myimage</pre></div></dd></dl></div></section></section></section><section class="chapter" id="cha-storage-datamgm" data-id-title="Stored Data Management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20 </span><span class="title-name">Stored Data Management</span> <a title="Permalink" class="permalink" href="#cha-storage-datamgm">#</a></h2></div></div></div><p>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers Ceph clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, Ceph
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </p><p>
  CRUSH requires a map of your cluster, and uses the CRUSH Map to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </p><p>
  CRUSH maps contain a list of OSDs, a list of 'buckets' for aggregating the
  devices into physical locations, and a list of rules that tell CRUSH how it
  should replicate data in a Ceph cluster’s pools. By reflecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </p><p>
  After you deploy a Ceph cluster, a default CRUSH Map is generated. It is
  fine for your Ceph sandbox environment. However, when you deploy a
  large-scale data cluster, you should give significant consideration to
  developing a custom CRUSH Map, because it will help you manage your Ceph
  cluster, improve performance and ensure data safety.
 </p><p>
  For example, if an OSD goes down, a CRUSH Map can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </p><p>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </p><p>
  A custom CRUSH Map can also help you identify the physical locations where
  Ceph stores redundant copies of data when the placement group(s) (refer to
  <a class="xref" href="#op-pgs" title="20.4. Placement Groups">Section 20.4, “Placement Groups”</a>) associated with a failed host are in a degraded
  state.
 </p><p>
  There are three main sections to a CRUSH Map.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#datamgm-devices" title="20.1. Devices">Devices</a> consist of any
    object storage device corresponding to a <code class="systemitem">ceph-osd</code>
    daemon.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-buckets" title="20.2. Buckets">Buckets</a> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-rules" title="20.3. Rule Sets">Rule Sets</a> consist of the
    manner of selecting buckets.
   </p></li></ul></div><section class="sect1" id="datamgm-devices" data-id-title="Devices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.1 </span><span class="title-name">Devices</span> <a title="Permalink" class="permalink" href="#datamgm-devices">#</a></h2></div></div></div><p>
   To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   CRUSH Map.
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device <em class="replaceable">NUM</em> osd.<em class="replaceable">OSD_NAME</em> class <em class="replaceable">CLASS_NAME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3class ssd</pre></div><p>
   As a general rule, an OSD daemon maps to a single disk.
  </p><section class="sect2" id="crush-devclasses" data-id-title="Device Classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.1 </span><span class="title-name">Device Classes</span> <a title="Permalink" class="permalink" href="#crush-devclasses">#</a></h3></div></div></div><p>
    The flexibility of the CRUSH Map in controlling data placement is one of
    the Ceph's strengths. It is also one of the most difficult parts of the
    cluster to manage. <span class="emphasis"><em>Device classes</em></span> automate the most
    common changes to CRUSH Maps that the administrator had to do manually
    previously.
   </p><section class="sect3" id="id-1.3.5.7.12.7.3" data-id-title="The CRUSH Management Problem"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.1 </span><span class="title-name">The CRUSH Management Problem</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.3">#</a></h4></div></div></div><p>
     Ceph clusters are frequently built with multiple types of storage
     devices: HDD, SSD, NVMe, or even mixed classes of the above. We call these
     different types of storage devices <span class="emphasis"><em>device classes</em></span> to
     avoid confusion between the <span class="emphasis"><em>type</em></span> property of CRUSH
     buckets (for example, host, rack, row, see
     <a class="xref" href="#datamgm-buckets" title="20.2. Buckets">Section 20.2, “Buckets”</a> for more details). Ceph OSDs backed
     by SSDs are much faster than those backed by spinning disks, making them
     better suited for certain workloads. Ceph makes it easy to create RADOS
     pools for different data sets or workloads and to assign different CRUSH
     rules to control data placement for those pools.
    </p><div class="figure" id="id-1.3.5.7.12.7.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/device_classes.png" target="_blank"><img src="images/device_classes.png" width="" alt="OSDs with Mixed Device Classes"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.1: </span><span class="title-name">OSDs with Mixed Device Classes </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.3.3">#</a></h6></div></div><p>
     However, setting up the CRUSH rules to place data only on a certain class
     of device is tedious. Rules work in terms of the CRUSH hierarchy, but if
     the devices are mixed into the same hosts or racks (as in the sample
     hierarchy above), they will (by default) be mixed together and appear in
     the same sub-trees of the hierarchy. Manually separating them out into
     separate trees involved creating multiple versions of each intermediate
     node for each device class in previous versions of SUSE Enterprise Storage.
    </p></section><section class="sect3" id="id-1.3.5.7.12.7.4" data-id-title="Device Classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.2 </span><span class="title-name">Device Classes</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.4">#</a></h4></div></div></div><p>
     An elegant solution that Ceph offers is to add a property called
     <span class="emphasis"><em>device class</em></span> to each OSD. By default, OSDs will
     automatically set their device classes to either 'hdd', 'ssd', or 'nvme'
     based on the hardware properties exposed by the Linux kernel. These device
     classes are reported in a new column of the <code class="command">ceph osd
     tree</code> command output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000</pre></div><p>
     If the automatic device class detection fails, for example because the
     device driver is not properly exposing information about the device via
     <code class="filename">/sys/block</code>, you can adjust device classes from the
     command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush set-device-class ssd osd.2 osd.3
set osd(s) 2,3 to class 'ssd'</pre></div></section><section class="sect3" id="crush-placement-rules" data-id-title="CRUSH Placement Rules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.3 </span><span class="title-name">CRUSH Placement Rules</span> <a title="Permalink" class="permalink" href="#crush-placement-rules">#</a></h4></div></div></div><p>
     CRUSH rules can restrict placement to a specific device class. For
     example, you can create a 'fast'
     <span class="bold"><strong>replicated</strong></span> pool that distributes data
     only over SSD disks by running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule create-replicated <em class="replaceable">RULE_NAME</em> <em class="replaceable">ROOT</em> <em class="replaceable">FAILURE_DOMAIN_TYPE</em> <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule create-replicated fast default host ssd</pre></div><p>
     Create a pool named 'fast_pool' and assign it to the 'fast' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create fast_pool 128 128 replicated fast</pre></div><p>
     The process for creating <span class="bold"><strong>erasure code</strong></span>
     rules is slightly different. First, you create an erasure code profile
     that includes a property for your desired device class. Then, use that
     profile when creating the erasure coded pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd erasure-code-profile set myprofile \
 k=4 m=2 crush-device-class=ssd crush-failure-domain=host
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create mypool 64 erasure myprofile</pre></div><p>
     In case you need to manually edit the CRUSH Map to customize your rule,
     the syntax has been extended to allow the device class to be specified.
     For example, the CRUSH rule generated by the above commands looks as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rule ecpool {
  id 2
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default <span class="bold"><strong>class ssd</strong></span>
  step chooseleaf indep 0 type host
  step emit
}</pre></div><p>
     The important difference here is that the 'take' command includes the
     additional 'class <em class="replaceable">CLASS_NAME</em>' suffix.
    </p></section><section class="sect3" id="crush-additional-commands" data-id-title="Additional Commands"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.4 </span><span class="title-name">Additional Commands</span> <a title="Permalink" class="permalink" href="#crush-additional-commands">#</a></h4></div></div></div><p>
     To list device classes used in a CRUSH Map, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush class ls
[
  "hdd",
  "ssd"
]</pre></div><p>
     To list existing CRUSH rules, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule ls
replicated_rule
fast</pre></div><p>
     To view details of the CRUSH rule named 'fast', run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule dump fast
{
		"rule_id": 1,
		"rule_name": "fast",
		"ruleset": 1,
		"type": 1,
		"min_size": 1,
		"max_size": 10,
		"steps": [
						{
										"op": "take",
										"item": -21,
										"item_name": "default~ssd"
						},
						{
										"op": "chooseleaf_firstn",
										"num": 0,
										"type": "host"
						},
						{
										"op": "emit"
						}
		]
}</pre></div><p>
     To list OSDs that belong to an 'ssd' class, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush class ls-osd ssd
0
1</pre></div></section><section class="sect3" id="device-classes-reclassify" data-id-title="Migrating from a Legacy SSD Rule to Device Classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.5 </span><span class="title-name">Migrating from a Legacy SSD Rule to Device Classes</span> <a title="Permalink" class="permalink" href="#device-classes-reclassify">#</a></h4></div></div></div><p>
     In SUSE Enterprise Storage prior to version 5, you needed to manually edit the
     CRUSH Map and maintain a parallel hierarchy for each specialized device
     type (such as SSD) in order to write rules that apply to these devices.
     Since SUSE Enterprise Storage 5, the device class feature has enabled this
     transparently.
    </p><p>
     You can transform a legacy rule and hierarchy to the new class-based rules
     by using the <code class="command">crushtool</code> command. There are several types
     of transformation possible:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.12.7.7.4.1"><span class="term"><code class="command">crushtool --reclassify-root <em class="replaceable">ROOT_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This command takes everything in the hierarchy beneath
        <em class="replaceable">ROOT_NAME</em> and adjusts any rules that
        reference that root via
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em></pre></div><p>
        to instead
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em> class <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
        It renumbers the buckets so that the old IDs are used for the specified
        class’s 'shadow tree'. As a consequence, no data movement occurs.
       </p><div class="complex-example"><div class="example" id="id-1.3.5.7.12.7.7.4.1.2.6" data-id-title="crushtool --reclassify-root"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 20.1: </span><span class="title-name"><code class="command">crushtool --reclassify-root</code> </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.7.4.1.2.6">#</a></h6></div><div class="example-contents"><p>
         Consider the following existing rule:
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div><p>
         If you reclassify the root 'default' as class 'hdd', the rule will
         become
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default class hdd
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div></div></div></div></dd><dt id="id-1.3.5.7.12.7.7.4.2"><span class="term"><code class="command">crushtool --set-subtree-class <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This method marks every device in the subtree rooted at
        <em class="replaceable">BUCKET_NAME</em> with the specified device class.
       </p><p>
        <code class="option">--set-subtree-class</code> is normally used in conjunction
        with the <code class="option">--reclassify-root</code> option to ensure that all
        devices in that root are labeled with the correct class. However, some
        of those devices may intentionally have a different class, and
        therefore you do not want to relabel them. In such cases, exclude the
        <code class="option">--set-subtree-class</code> option. Keep in mind that such
        remapping will not be perfect, because the previous rule is distributed
        across devices of multiple classes but the adjusted rules will only map
        to devices of the specified device class.
       </p></dd><dt id="id-1.3.5.7.12.7.7.4.3"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">MATCH_PATTERN</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">DEFAULT_PATTERN</em></code></span></dt><dd><p>
        This method allows merging a parallel type-specific hierarchy with the
        normal hierarchy. For example, many users have CRUSH Maps similar to
        the following one:
       </p><div class="example" id="id-1.3.5.7.12.7.7.4.3.2.2" data-id-title="crushtool --reclassify-bucket"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 20.2: </span><span class="title-name"><code class="command">crushtool --reclassify-bucket</code> </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.7.4.3.2.2">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">host node1 {
   id -2           # do not change unnecessarily
   # weight 109.152
   alg straw
   hash 0  # rjenkins1
   item osd.0 weight 9.096
   item osd.1 weight 9.096
   item osd.2 weight 9.096
   item osd.3 weight 9.096
   item osd.4 weight 9.096
   item osd.5 weight 9.096
   [...]
}

host node1-ssd {
   id -10          # do not change unnecessarily
   # weight 2.000
   alg straw
   hash 0  # rjenkins1
   item osd.80 weight 2.000
   [...]
}

root default {
   id -1           # do not change unnecessarily
   alg straw
   hash 0  # rjenkins1
   item node1 weight 110.967
   [...]
}

root ssd {
   id -18          # do not change unnecessarily
   # weight 16.000
   alg straw
   hash 0  # rjenkins1
   item node1-ssd weight 2.000
   [...]
}</pre></div></div></div><p>
        This function reclassifies each bucket that matches a given pattern.
        The pattern can look like <code class="literal">%suffix</code> or
        <code class="literal">prefix%</code>. In the above example, you would use the
        pattern <code class="literal">%-ssd</code>. For each matched bucket, the
        remaining portion of the name that matches the '%' wild card specifies
        the base bucket. All devices in the matched bucket are labeled with the
        specified device class and then moved to the base bucket. If the base
        bucket does not exist (for example, if 'node12-ssd' exists but 'node12'
        does not), then it is created and linked underneath the specified
        default parent bucket. The old bucket IDs are preserved for the new
        shadow buckets to prevent data movement. Rules with the
        <code class="literal">take</code> steps that reference the old buckets are
        adjusted.
       </p></dd><dt id="id-1.3.5.7.12.7.7.4.4"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">BASE_BUCKET</em></code></span></dt><dd><p>
        You can use the <code class="option">--reclassify-bucket</code> option without a
        wild card to map a single bucket. For example, in the previous example,
        we want the 'ssd' bucket to be mapped to the default bucket.
       </p><p>
        The final command to convert the map comprised of the above fragments
        would be as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap -o original
<code class="prompt user">cephadm@adm &gt; </code>crushtool -i original --reclassify \
  --set-subtree-class default hdd \
  --reclassify-root default hdd \
  --reclassify-bucket %-ssd ssd default \
  --reclassify-bucket ssd ssd default \
  -o adjusted</pre></div><p>
        In order to verify that the conversion is correct, there is a
        <code class="option">--compare</code> option that tests a large sample of inputs
        to the CRUSH Map and compares if the same result comes back out. These
        inputs are controlled by the same options that apply to the
        <code class="option">--test</code>. For the above example, the command would be as
        follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent</pre></div><div id="id-1.3.5.7.12.7.7.4.4.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
         If there were differences, you would see what ratio of inputs are
         remapped in the parentheses.
        </p></div><p>
        If you are satisfied with the adjusted CRUSH Map, you can apply it to
        the cluster:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd setcrushmap -i adjusted</pre></div></dd></dl></div></section><section class="sect3" id="id-1.3.5.7.12.7.8" data-id-title="For More Information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.1.1.6 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.12.7.8">#</a></h4></div></div></div><p>
     Find more details on CRUSH Maps in <a class="xref" href="#op-crush" title="20.5. CRUSH Map Manipulation">Section 20.5, “CRUSH Map Manipulation”</a>.
    </p><p>
     Find more details on Ceph pools in general in
     <a class="xref" href="#ceph-pools" title="Chapter 22. Managing Storage Pools">Chapter 22, <em>Managing Storage Pools</em></a>.
    </p><p>
     Find more details about erasure coded pools in
     <a class="xref" href="#cha-ceph-erasure" title="Chapter 24. Erasure Coded Pools">Chapter 24, <em>Erasure Coded Pools</em></a>.
    </p></section></section></section><section class="sect1" id="datamgm-buckets" data-id-title="Buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.2 </span><span class="title-name">Buckets</span> <a title="Permalink" class="permalink" href="#datamgm-buckets">#</a></h2></div></div></div><p>
   CRUSH maps contain a list of OSDs, which can be organized into 'buckets' for
   aggregating the devices into physical locations.
  </p><div class="informaltable"><table style="border: none;"><colgroup><col/><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        0
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        osd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        An OSD daemon (osd.1, osd.2, etc.).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        host
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A host name containing one or more OSDs.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        2
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        chassis
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Chassis of which the rack is composed.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        rack
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A computer rack. The default is <code class="literal">unknownrack</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        row
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A row in a series of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        5
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pdu
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Power distribution unit.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        6
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pod
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Abbreviation for "Point of Delivery": in this context, a group of PDUs,
        or a group of rows of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        7
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        room
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A room containing racks and rows of hosts.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        datacenter
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A physical data center containing rooms.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        9
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        region
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        10
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        root
       </p>
      </td><td>
       
      </td></tr></tbody></table></div><div id="id-1.3.5.7.13.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    You can modify the existing types and create your own bucket types.
   </p></div><p>
   Ceph’s deployment tools generate a CRUSH Map that contains a bucket for
   each host, and a root named 'default', which is useful for the default
   <code class="literal">rbd</code> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </p><p>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <code class="literal">straw2</code> by default), and
   the hash (<code class="literal">0</code> by default, reflecting CRUSH Hash
   <code class="literal">rjenkins1</code>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </p><div class="verbatim-wrap"><pre class="screen">[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</pre></div><p>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </p><div class="verbatim-wrap"><pre class="screen">host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</pre></div></section><section class="sect1" id="datamgm-rules" data-id-title="Rule Sets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.3 </span><span class="title-name">Rule Sets</span> <a title="Permalink" class="permalink" href="#datamgm-rules">#</a></h2></div></div></div><p>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default CRUSH Map has a rule for the default root. If you want more
   roots and more rules, you need to create them later or they will be created
   automatically when new pools are created.
  </p><div id="id-1.3.5.7.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </p></div><p>
   A rule takes the following form:
  </p><div class="verbatim-wrap"><pre class="screen">rule <em class="replaceable">rulename</em> {

        ruleset <em class="replaceable">ruleset</em>
        type <em class="replaceable">type</em>
        min_size <em class="replaceable">min-size</em>
        max_size <em class="replaceable">max-size</em>
        step <em class="replaceable">step</em>

}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.14.6.1"><span class="term">ruleset</span></dt><dd><p>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <code class="literal">0</code>.
     </p></dd><dt id="id-1.3.5.7.14.6.2"><span class="term">type</span></dt><dd><p>
      A string. Describes a rule for either a 'replicated' or 'erasure' coded
      pool. This option is required. Default is <code class="literal">replicated</code>.
     </p></dd><dt id="id-1.3.5.7.14.6.3"><span class="term">min_size</span></dt><dd><p>
      An integer. If a pool group makes fewer replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">2</code>.
     </p></dd><dt id="id-1.3.5.7.14.6.4"><span class="term">max_size</span></dt><dd><p>
      An integer. If a pool group makes more replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">10</code>.
     </p></dd><dt id="id-1.3.5.7.14.6.5"><span class="term">step take <em class="replaceable">bucket</em></span></dt><dd><p>
      Takes a bucket specified by a name, and begins iterating down the tree.
      This option is required. For an explanation about iterating through the
      tree, see <a class="xref" href="#datamgm-rules-step-iterate" title="20.3.1. Iterating Through the Node Tree">Section 20.3.1, “Iterating Through the Node Tree”</a>.
     </p></dd><dt id="id-1.3.5.7.14.6.6"><span class="term">step <em class="replaceable">target</em><em class="replaceable">mode</em><em class="replaceable">num</em> type <em class="replaceable">bucket-type</em></span></dt><dd><p>
      <em class="replaceable">target</em> can either be <code class="literal">choose</code>
      or <code class="literal">chooseleaf</code>. When set to <code class="literal">choose</code>,
      a number of buckets is selected. <code class="literal">chooseleaf</code> directly
      selects the OSDs (leaf nodes) from the sub-tree of each bucket in the set
      of buckets.
     </p><p>
      <em class="replaceable">mode</em> can either be <code class="literal">firstn</code>
      or <code class="literal">indep</code>. See
      <a class="xref" href="#datamgm-rules-step-mode" title="20.3.2. firstn and indep">Section 20.3.2, “firstn and indep”</a>.
     </p><p>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <em class="replaceable">num</em> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <em class="replaceable">num</em> &lt;
      0, it means N - <em class="replaceable">num</em>; and, if
      <em class="replaceable">num</em> == 0, choose N buckets (all available).
      Follows <code class="literal">step take</code> or <code class="literal">step choose</code>.
     </p></dd><dt id="id-1.3.5.7.14.6.7"><span class="term">step emit</span></dt><dd><p>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <code class="literal">step choose</code>.
     </p></dd></dl></div><section class="sect2" id="datamgm-rules-step-iterate" data-id-title="Iterating Through the Node Tree"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.3.1 </span><span class="title-name">Iterating Through the Node Tree</span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate">#</a></h3></div></div></div><p>
    The structure defined with the buckets can be viewed as a node tree.
    Buckets are nodes and OSDs are leafs in this tree.
   </p><p>
    Rules in the CRUSH Map define how OSDs are selected from this tree. A rule
    starts with a node and then iterates down the tree to return a set of OSDs.
    It is not possible to define which branch needs to be selected. Instead the
    CRUSH algorithm assures that the set of OSDs fulfills the replication
    requirements and evenly distributes the data.
   </p><p>
    With <code class="literal">step take</code> <em class="replaceable">bucket</em> the
    iteration through the node tree begins at the given bucket (not bucket
    type). If OSDs from all branches in the tree are to be returned, the bucket
    must be the root bucket. Otherwise the following steps are only iterating
    through a sub-tree.
   </p><p>
    After <code class="literal">step take</code> one or more <code class="literal">step
    choose</code> entries follow in the rule definition. Each <code class="literal">step
    choose</code> chooses a defined number of nodes (or branches) from the
    previously selected upper node.
   </p><p>
    In the end the selected OSDs are returned with <code class="literal">step
    emit</code>.
   </p><p>
    <code class="literal">step chooseleaf</code> is a convenience function that directly
    selects OSDs from branches of the given bucket.
   </p><p>
    <a class="xref" href="#datamgm-rules-step-iterate-figure" title="Example Tree">Figure 20.2, “Example Tree”</a> provides an example of
    how <code class="literal">step</code> is used to iterate through a tree. The orange
    arrows and numbers correspond to <code class="literal">example1a</code> and
    <code class="literal">example1b</code>, while blue corresponds to
    <code class="literal">example2</code> in the following rule definitions.
   </p><div class="figure" id="datamgm-rules-step-iterate-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-step.png" target="_blank"><img src="images/crush-step.png" width="" alt="Example Tree"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.2: </span><span class="title-name">Example Tree </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate-figure">#</a></h6></div></div><div class="verbatim-wrap"><pre class="screen"># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</pre></div></section><section class="sect2" id="datamgm-rules-step-mode" data-id-title="firstn and indep"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.3.2 </span><span class="title-name">firstn and indep</span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-mode">#</a></h3></div></div></div><p>
    A CRUSH rule defines replacements for failed nodes or OSDs (see
    <a class="xref" href="#datamgm-rules" title="20.3. Rule Sets">Section 20.3, “Rule Sets”</a>). The keyword <code class="literal">step</code>
    requires either <code class="literal">firstn</code> or <code class="literal">indep</code> as
    parameter. <a class="xref" href="#datamgm-rules-step-mode-indep-figure" title="Node Replacement Methods">Figure 20.3, “Node Replacement Methods”</a> provides
    an example.
   </p><p>
    <code class="literal">firstn</code> adds replacement nodes to the end of the list of
    active nodes. In case of a failed node, the following healthy nodes are
    shifted to the left to fill the gap of the failed node. This is the default
    and desired method for <span class="emphasis"><em>replicated pools</em></span>, because a
    secondary node already has all data and therefore can take over the duties
    of the primary node immediately.
   </p><p>
    <code class="literal">indep</code> selects fixed replacement nodes for each active
    node. The replacement of a failed node does not change the order of the
    remaining nodes. This is desired for <span class="emphasis"><em>erasure coded
    pools</em></span>. In erasure coded pools the data stored on a node depends
    on its position in the node selection. When the order of nodes changes, all
    data on affected nodes needs to be relocated.
   </p><div class="figure" id="datamgm-rules-step-mode-indep-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-firstn-indep.png" target="_blank"><img src="images/crush-firstn-indep.png" width="" alt="Node Replacement Methods"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.3: </span><span class="title-name">Node Replacement Methods </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-mode-indep-figure">#</a></h6></div></div></section></section><section class="sect1" id="op-pgs" data-id-title="Placement Groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.4 </span><span class="title-name">Placement Groups</span> <a title="Permalink" class="permalink" href="#op-pgs">#</a></h2></div></div></div><p>
   Ceph maps objects to placement groups (PGs). Placement groups are shards
   or fragments of a logical object pool that place objects as a group into
   OSDs. Placement groups reduce the amount of per-object metadata when Ceph
   stores the data in OSDs. A larger number of placement groups—for
   example, 100 per OSD—leads to better balancing.
  </p><section class="sect2" id="op-pgs-usage" data-id-title="How Are Placement Groups Used?"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.1 </span><span class="title-name">How Are Placement Groups Used?</span> <a title="Permalink" class="permalink" href="#op-pgs-usage">#</a></h3></div></div></div><p>
    A placement group (PG) aggregates objects within a pool. The main reason is
    that tracking object placement and metadata on a per-object basis is
    computationally expensive. For example, a system with millions of objects
    cannot track placement of each of its objects directly.
   </p><div class="figure" id="id-1.3.5.7.15.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_schema.png" target="_blank"><img src="images/ceph_pgs_schema.png" width="" alt="Placement Groups in a Pool"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.4: </span><span class="title-name">Placement Groups in a Pool </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.15.3.3">#</a></h6></div></div><p>
    The Ceph client will calculate to which placement group an object will
    belong to. It does this by hashing the object ID and applying an operation
    based on the number of PGs in the defined pool and the ID of the pool.
   </p><p>
    The object’s contents within a placement group are stored in a set of OSDs.
    For example, in a replicated pool of size two, each placement group will
    store objects on two OSDs:
   </p><div class="figure" id="id-1.3.5.7.15.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_osds.png" target="_blank"><img src="images/ceph_pgs_osds.png" width="" alt="Placement Groups and OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.5: </span><span class="title-name">Placement Groups and OSDs </span><a title="Permalink" class="permalink" href="#id-1.3.5.7.15.3.6">#</a></h6></div></div><p>
    If OSD #2 fails, another OSD will be assigned to placement group #1 and
    will be filled with copies of all objects in OSD #1. If the pool size is
    changed from two to three, an additional OSD will be assigned to the
    placement group and will receive copies of all objects in the placement
    group.
   </p><p>
    Placement groups do not own the OSD, they share it with other placement
    groups from the same pool or even other pools. If OSD #2 fails, the
    placement group #2 will also need to restore copies of objects, using OSD
    #3.
   </p><p>
    When the number of placement groups increases, the new placement groups
    will be assigned OSDs. The result of the CRUSH function will also change
    and some objects from the former placement groups will be copied over to
    the new placement groups and removed from the old ones.
   </p></section><section class="sect2" id="op-pgs-pg-num" data-id-title="Determining the Value of PG_NUM"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.2 </span><span class="title-name">Determining the Value of <em class="replaceable">PG_NUM</em></span> <a title="Permalink" class="permalink" href="#op-pgs-pg-num">#</a></h3></div></div></div><p>
    When creating a new pool, it is mandatory to choose the value of
    <em class="replaceable">PG_NUM</em>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd pool create <em class="replaceable">POOL_NAME</em> <em class="replaceable">PG_NUM</em></pre></div><p>
    <em class="replaceable">PG_NUM</em> cannot be calculated automatically.
    Following are a few commonly used values, depending on the number of OSDs
    in the cluster:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.15.4.5.1"><span class="term">Less than 5 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 128.
      </p></dd><dt id="id-1.3.5.7.15.4.5.2"><span class="term">Between 5 and 10 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 512.
      </p></dd><dt id="id-1.3.5.7.15.4.5.3"><span class="term">Between 10 and 50 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 1024.
      </p></dd></dl></div><p>
    As the number of OSDs increases, choosing the right value for
    <em class="replaceable">PG_NUM</em> becomes more important.
    <em class="replaceable">PG_NUM</em> strongly affects the behavior of the
    cluster as well as the durability of the data in case of OSD failure.
   </p><section class="sect3" id="op-pgs-choosing" data-id-title="Number of Placement Groups for More Than 50 OSDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.1 </span><span class="title-name">Number of Placement Groups for More Than 50 OSDs</span> <a title="Permalink" class="permalink" href="#op-pgs-choosing">#</a></h4></div></div></div><p>
     If you have less than 50 OSDs, use the preselection described in
     <a class="xref" href="#op-pgs-pg-num" title="20.4.2. Determining the Value of PG_NUM">Section 20.4.2, “Determining the Value of <em class="replaceable">PG_NUM</em>”</a>. If you have more than 50 OSDs, we
     recommend approximately 50-100 placement groups per OSD to balance out
     resource usage, data durability, and distribution. For a single pool of
     objects, you can use the following formula to get a baseline:
    </p><div class="verbatim-wrap"><pre class="screen">          total PGs = (OSDs * 100) / <em class="replaceable">POOL_SIZE</em></pre></div><p>
     Where <em class="replaceable">POOL_SIZE</em> is either the number of
     replicas for replicated pools, or the 'k'+'m' sum for erasure coded pools
     as returned by the <code class="command">ceph osd erasure-code-profile get</code>
     command. You should round the result up to the nearest power of 2.
     Rounding up is recommended for the CRUSH algorithm to evenly balance the
     number of objects among placement groups.
    </p><p>
     As an example, for a cluster with 200 OSDs and a pool size of 3 replicas,
     you would estimate the number of PGs as follows:
    </p><div class="verbatim-wrap"><pre class="screen">          (200 * 100) / 3 = 6667</pre></div><p>
     The nearest power of 2 is <span class="bold"><strong>8192</strong></span>.
    </p><p>
     When using multiple data pools for storing objects, you need to ensure
     that you balance the number of placement groups per pool with the number
     of placement groups per OSD. You need to reach a reasonable total number
     of placement groups that provides reasonably low variance per OSD without
     taxing system resources or making the peering process too slow.
    </p><p>
     For example, a cluster of 10 pools, each with 512 placement groups on 10
     OSDs, is a total of 5,120 placement groups spread over 10 OSDs, that is
     512 placement groups per OSD. Such a setup does not use too many
     resources. However, if 1000 pools were created with 512 placement groups
     each, the OSDs would handle approximately 50,000 placement groups each and
     it would require significantly more resources and time for peering.
    </p></section></section><section class="sect2" id="op-pg-set" data-id-title="Setting the Number of Placement Groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.3 </span><span class="title-name">Setting the Number of Placement Groups</span> <a title="Permalink" class="permalink" href="#op-pg-set">#</a></h3></div></div></div><p>
    To set the number of placement groups in a pool, you need to specify the
    number of placement groups at the time you create the pool (see
    <a class="xref" href="#ceph-pools-operate-add-pool" title="22.2.2. Create a Pool">Section 22.2.2, “Create a Pool”</a>). Once you have set placement
    groups for a pool, you may increase the number of placement groups, but you
    cannot decrease them. To increase the number of placement groups, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">PG_NUM</em></pre></div><p>
    After you increase the number of placement groups, you also need to
    increase the number of placement groups for placement
    (<code class="option">PGP_NUM</code>) before your cluster will rebalance.
    <code class="option">PGP_NUM</code> will be the number of placement groups that will
    be considered for placement by the CRUSH algorithm. Increasing
    <code class="option">PG_NUM</code> splits the placement groups but data will not be
    migrated to the newer placement groups until <code class="option">PGP_NUM</code> is
    increased. <code class="option">PGP_NUM</code> should be equal to
    <code class="option">PG_NUM</code>. To increase the number of placement groups for
    placement, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pgp_num <em class="replaceable">PGP_NUM</em></pre></div></section><section class="sect2" id="op-pg-get" data-id-title="Getting the Number of Placement Groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.4 </span><span class="title-name">Getting the Number of Placement Groups</span> <a title="Permalink" class="permalink" href="#op-pg-get">#</a></h3></div></div></div><p>
    To get the number of placement groups in a pool, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> pg_num</pre></div></section><section class="sect2" id="op-pg-getpgstat" data-id-title="Getting a Clusters PG Statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.5 </span><span class="title-name">Getting a Cluster's PG Statistics</span> <a title="Permalink" class="permalink" href="#op-pg-getpgstat">#</a></h3></div></div></div><p>
    To get the statistics for the placement groups in your cluster, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg dump [--format <em class="replaceable">FORMAT</em>]</pre></div><p>
    Valid formats are 'plain' (default) and 'json'.
   </p></section><section class="sect2" id="op-pg-getstuckstat" data-id-title="Getting Statistics for Stuck PGs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.6 </span><span class="title-name">Getting Statistics for Stuck PGs</span> <a title="Permalink" class="permalink" href="#op-pg-getstuckstat">#</a></h3></div></div></div><p>
    To get the statistics for all placement groups stuck in a specified state,
    run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg dump_stuck <em class="replaceable">STATE</em> \
 [--format <em class="replaceable">FORMAT</em>] [--threshold <em class="replaceable">THRESHOLD</em>]</pre></div><p>
    <em class="replaceable">STATE</em> is one of 'inactive' (PGs cannot process
    reads or writes because they are waiting for an OSD with the most
    up-to-date data to come up), 'unclean' (PGs contain objects that are not
    replicated the desired number of times), 'stale' (PGs are in an unknown
    state—the OSDs that host them have not reported to the monitor
    cluster in a time interval specified by the
    <code class="option">mon_osd_report_timeout</code> option), 'undersized', or
    'degraded'.
   </p><p>
    Valid formats are 'plain' (default) and 'json'.
   </p><p>
    The threshold defines the minimum number of seconds the placement group is
    stuck before including it in the returned statistics (300 seconds by
    default).
   </p></section><section class="sect2" id="op-pgs-pgmap" data-id-title="Getting a Placement Group Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.7 </span><span class="title-name">Getting a Placement Group Map</span> <a title="Permalink" class="permalink" href="#op-pgs-pgmap">#</a></h3></div></div></div><p>
    To get the placement group map for a particular placement group, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg map <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph will return the placement group map, the placement group, and the
    OSD status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg map 1.6c
osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</pre></div></section><section class="sect2" id="op-pg-pgstats" data-id-title="Getting a Placement Groups Statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.8 </span><span class="title-name">Getting a Placement Groups Statistics</span> <a title="Permalink" class="permalink" href="#op-pg-pgstats">#</a></h3></div></div></div><p>
    To retrieve statistics for a particular placement group, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg <em class="replaceable">PG_ID</em> query</pre></div></section><section class="sect2" id="op-pg-scrubpg" data-id-title="Scrubbing a Placement Group"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.9 </span><span class="title-name">Scrubbing a Placement Group</span> <a title="Permalink" class="permalink" href="#op-pg-scrubpg">#</a></h3></div></div></div><p>
    To scrub (<a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) a placement group, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg scrub <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph checks the primary and replica nodes, generates a catalog of all
    objects in the placement group, and compares them to ensure that no objects
    are missing or mismatched and their contents are consistent. Assuming the
    replicas all match, a final semantic sweep ensures that all of the
    snapshot-related object metadata is consistent. Errors are reported via
    logs.
   </p></section><section class="sect2" id="op-pg-backfill" data-id-title="Prioritizing Backfill and Recovery of Placement Groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.10 </span><span class="title-name">Prioritizing Backfill and Recovery of Placement Groups</span> <a title="Permalink" class="permalink" href="#op-pg-backfill">#</a></h3></div></div></div><p>
    You may run into a situation where several placement groups require
    recovery and/or back-fill, while some groups hold data more important than
    others. For example, those PGs may hold data for images used by running
    machines and other PGs may be used by inactive machines or less relevant
    data. In that case, you may want to prioritize recovery of those groups so
    that performance and availability of data stored on those groups is
    restored earlier. To mark particular placement groups as prioritized during
    backfill or recovery, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt user">root # </code>ceph pg force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    This will cause Ceph to perform recovery or backfill on specified
    placement groups first, before other placement groups. This does not
    interrupt currently ongoing backfills or recovery, but causes specified PGs
    to be processed as soon as possible. If you change your mind or prioritize
    wrong groups, cancel the prioritization:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph pg cancel-force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt user">root # </code>ceph pg cancel-force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    The <code class="command">cancel-*</code> commands remove the 'force' flag from the
    PGs so that they are processed in default order. Again, this does not
    affect placement groups currently being processed, only those that are
    still queued. The 'force' flag is cleared automatically after recovery or
    backfill of the group is done.
   </p></section><section class="sect2" id="op-pgs-revert" data-id-title="Reverting Lost Objects"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.11 </span><span class="title-name">Reverting Lost Objects</span> <a title="Permalink" class="permalink" href="#op-pgs-revert">#</a></h3></div></div></div><p>
    If the cluster has lost one or more objects and you have decided to abandon
    the search for the lost data, you need to mark the unfound objects as
    'lost'.
   </p><p>
    If the objects are still lost after having queried all possible locations,
    you may need to give up on the lost objects. This is possible given unusual
    combinations of failures that allow the cluster to learn about writes that
    were performed before the writes themselves are recovered.
   </p><p>
    Currently the only supported option is 'revert', which will either roll
    back to a previous version of the object, or forget about it entirely in
    case of a new object. To mark the 'unfound' objects as 'lost', run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">  <code class="prompt user">cephadm@adm &gt; </code>ceph pg <em class="replaceable">PG_ID</em> mark_unfound_lost revert|delete</pre></div></section><section class="sect2" id="op-pgs-autoscaler" data-id-title="PG Auto-scaler"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.12 </span><span class="title-name">PG Auto-scaler</span> <a title="Permalink" class="permalink" href="#op-pgs-autoscaler">#</a></h3></div></div></div><p>
    As of the Nautilus release, Ceph includes a new manager module called
    <code class="literal">pg_autoscaler</code> that allows the cluster to consider the
    amount of data actually stored (or expected to be stored) in each pool and
    choose appropriate <code class="option">pg_num</code> values automatically.
   </p><p>
    To enable the auto-scaler:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module enable pg_autoscaler</pre></div><p>
    The autoscaler is configured on a per-pool basis, and can run in three
    modes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.15.14.6.1"><span class="term">warn</span></dt><dd><p>
       In <span class="emphasis"><em>warn</em></span> mode, a health warning is issued if the
       suggested <code class="option">pg_num</code> value is too different from the
       current value. This is the default for new and existing pools.
      </p></dd><dt id="id-1.3.5.7.15.14.6.2"><span class="term">on</span></dt><dd><p>
       In autoscaler <span class="emphasis"><em>on</em></span> mode, the pool
       <code class="option">pg_num</code> is adjusted automatically with no need for any
       administrator interaction.
      </p></dd><dt id="id-1.3.5.7.15.14.6.3"><span class="term">off</span></dt><dd><p>
       The autoscaler can also be turned <span class="emphasis"><em>off</em></span> for any given
       pool, leaving the administrator to manage <code class="option">pg_num</code>
       manually as before.
      </p></dd></dl></div><div id="id-1.3.5.7.15.14.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     We recommend using the <span class="emphasis"><em>on</em></span> mode, but administrators
     who want to exercise more manual control should use the
     <span class="emphasis"><em>warn</em></span> mode.
    </p></div><p>
    To enable autoscaling for a particular pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set foo pg_autoscale_mode on</pre></div><p>
    Once enabled, the current state of all pools and the proposed adjustments
    can be queried on the command line:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool autoscale-status</pre></div></section></section><section class="sect1" id="op-crush" data-id-title="CRUSH Map Manipulation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.5 </span><span class="title-name">CRUSH Map Manipulation</span> <a title="Permalink" class="permalink" href="#op-crush">#</a></h2></div></div></div><p>
   This section introduces ways to basic CRUSH Map manipulation, such as
   editing a CRUSH Map, changing CRUSH Map parameters, and
   adding/moving/removing an OSD.
  </p><section class="sect2" id="id-1.3.5.7.16.3" data-id-title="Editing a CRUSH Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.1 </span><span class="title-name">Editing a CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.16.3">#</a></h3></div></div></div><p>
    To edit an existing CRUSH map, do the following:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Get a CRUSH Map. To get the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap -o <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will output (<code class="option">-o</code>) a compiled CRUSH Map to the
      file name you specified. Since the CRUSH Map is in a compiled form, you
      must decompile it first before you can edit it.
     </p></li><li class="step"><p>
      Decompile a CRUSH Map. To decompile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -d <em class="replaceable">compiled-crushmap-filename</em> \
 -o <em class="replaceable">decompiled-crushmap-filename</em></pre></div><p>
      Ceph will decompile (<code class="option">-d</code>) the compiled CRUSH Mapand
      output (<code class="option">-o</code>) it to the file name you specified.
     </p></li><li class="step"><p>
      Edit at least one of Devices, Buckets and Rules parameters.
     </p></li><li class="step"><p>
      Compile a CRUSH Map. To compile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -c <em class="replaceable">decompiled-crush-map-filename</em> \
 -o <em class="replaceable">compiled-crush-map-filename</em></pre></div><p>
      Ceph will store a compiled CRUSH Mapto the file name you specified.
     </p></li><li class="step"><p>
      Set a CRUSH Map. To set the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd setcrushmap -i <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will input the compiled CRUSH Map of the file name you specified
      as the CRUSH Map for the cluster.
     </p></li></ol></div></div><div id="id-1.3.5.7.16.3.4" data-id-title="Use Versioning System" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use Versioning System</h6><p>
     Use a versioning system—such as git or svn—for the exported
     and modified CRUSH Map files. It makes a possible rollback simple.
    </p></div><div id="id-1.3.5.7.16.3.5" data-id-title="Test the New CRUSH Map" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Test the New CRUSH Map</h6><p>
     Test the new adjusted CRUSH Map using the <code class="command">crushtool
     --test</code> command, and compare to the state before applying the new
     CRUSH Map. You may find the following command switches useful:
     <code class="option">--show-statistics</code>, <code class="option">--show-mappings</code>,
     <code class="option">--show-bad-mappings</code>, <code class="option">--show-utilization</code>,
     <code class="option">--show-utilization-all</code>,
     <code class="option">--show-choose-tries</code>
    </p></div></section><section class="sect2" id="op-crush-addosd" data-id-title="Add/Move an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.2 </span><span class="title-name">Add/Move an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-addosd">#</a></h3></div></div></div><p>
    To add or move an OSD in the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush set <em class="replaceable">id_or_name</em> <em class="replaceable">weight</em> root=<em class="replaceable">pool-name</em>
<em class="replaceable">bucket-type</em>=<em class="replaceable">bucket-name</em> ...</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.16.4.4.1"><span class="term">id</span></dt><dd><p>
       An integer. The numeric ID of the OSD. This option is required.
      </p></dd><dt id="id-1.3.5.7.16.4.4.2"><span class="term">name</span></dt><dd><p>
       A string. The full name of the OSD. This option is required.
      </p></dd><dt id="id-1.3.5.7.16.4.4.3"><span class="term">weight</span></dt><dd><p>
       A double. The CRUSH weight for the OSD. This option is required.
      </p></dd><dt id="id-1.3.5.7.16.4.4.4"><span class="term">root</span></dt><dd><p>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </p></dd><dt id="id-1.3.5.7.16.4.4.5"><span class="term">bucket-type</span></dt><dd><p>
       Key/value pairs. You may specify the OSD’s location in the CRUSH
       hierarchy.
      </p></dd></dl></div><p>
    The following example adds <code class="literal">osd.0</code> to the hierarchy, or
    moves the OSD from a previous location.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-osdweight" data-id-title="Difference between ceph osd reweight and ceph osd crush reweight"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.3 </span><span class="title-name">Difference between <code class="command">ceph osd reweight</code> and <code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#op-crush-osdweight">#</a></h3></div></div></div><p>
    There are two similar commands that change the 'weight' of a Ceph OSD. The
    context of their usage is different and may cause confusion.
   </p><section class="sect3" id="id-1.3.5.7.16.5.3" data-id-title="ceph osd reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.5.3.1 </span><span class="title-name"><code class="command">ceph osd reweight</code></span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.16.5.3">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd reweight</code> sets an override weight on the Ceph OSD.
     This value is in the range of 0 to 1, and forces CRUSH to reposition the
     data that would otherwise live on this drive. It does
     <span class="bold"><strong>not</strong></span> change the weights assigned to the
     buckets above the OSD, and is a corrective measure in case the normal
     CRUSH distribution is not working out quite right. For example, if one of
     your OSDs is at 90% and the others are at 40%, you could reduce this
     weight to try and compensate for it.
    </p><div id="id-1.3.5.7.16.5.3.5" data-id-title="OSD Weight Is Temporary" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: OSD Weight Is Temporary</h6><p>
      Note that <code class="command">ceph osd reweight</code> is not a persistent
      setting. When an OSD gets marked out, its weight will be set to 0 and
      when it gets marked in again, the weight will be changed to 1.
     </p></div></section><section class="sect3" id="id-1.3.5.7.16.5.4" data-id-title="ceph osd crush reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.5.3.2 </span><span class="title-name"><code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#id-1.3.5.7.16.5.4">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd crush reweight</code> sets the
     <span class="bold"><strong>CRUSH</strong></span> weight of the OSD. This weight is
     an arbitrary value—generally the size of the disk in TB—and
     controls how much data the system tries to allocate to the OSD.
    </p></section></section><section class="sect2" id="op-crush-osdremove" data-id-title="Remove an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.4 </span><span class="title-name">Remove an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-osdremove">#</a></h3></div></div></div><p>
    To remove an OSD from the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush remove <em class="replaceable">OSD_NAME</em></pre></div></section><section class="sect2" id="op-crush-addbaucket" data-id-title="Add a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.5 </span><span class="title-name">Add a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-addbaucket">#</a></h3></div></div></div><p>
    To add a bucket to the CRUSH Map of a running cluster, execute the
    <code class="command">ceph osd crush add-bucket</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush add-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em></pre></div></section><section class="sect2" id="op-crush-movebucket" data-id-title="Move a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.6 </span><span class="title-name">Move a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-movebucket">#</a></h3></div></div></div><p>
    To move a bucket to a different location or position in the CRUSH Map
    hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush move <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em>=<em class="replaceable">BUCKET_NAME</em> [...]</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-rmbucket" data-id-title="Remove a Bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.5.7 </span><span class="title-name">Remove a Bucket</span> <a title="Permalink" class="permalink" href="#op-crush-rmbucket">#</a></h3></div></div></div><p>
    To remove a bucket from the CRUSH Map hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush remove <em class="replaceable">BUCKET_NAME</em></pre></div><div id="id-1.3.5.7.16.9.4" data-id-title="Empty Bucket Only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Empty Bucket Only</h6><p>
     A bucket must be empty before removing it from the CRUSH hierarchy.
    </p></div></section></section><section class="sect1" id="scrubbing" data-id-title="Scrubbing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.6 </span><span class="title-name">Scrubbing</span> <a title="Permalink" class="permalink" href="#scrubbing">#</a></h2></div></div></div><p>
   In addition to making multiple copies of objects, Ceph ensures data
   integrity by <span class="emphasis"><em>scrubbing</em></span> placement groups (find more
   information about placement groups in
   <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.3.2 “Placement Group”</span>). Ceph scrubbing is analogous
   to running <code class="command">fsck</code> on the object storage layer. For each
   placement group, Ceph generates a catalog of all objects and compares each
   primary object and its replicas to ensure that no objects are missing or
   mismatched. Daily light scrubbing checks the object size and attributes,
   while weekly deep scrubbing reads the data and uses checksums to ensure data
   integrity.
  </p><p>
   Scrubbing is important for maintaining data integrity, but it can reduce
   performance. You can adjust the following settings to increase or decrease
   scrubbing operations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.7.17.4.1"><span class="term"><code class="option">osd max scrubs</code></span></dt><dd><p>
      The maximum number of simultaneous scrub operations for a Ceph OSD. Default
      is 1.
     </p></dd><dt id="id-1.3.5.7.17.4.2"><span class="term"><code class="option">osd scrub begin hour</code>, <code class="option">osd scrub end hour</code></span></dt><dd><p>
      The hours of day (0 to 24) that define a time window during which the
      scrubbing can happen. By default, begins at 0 and ends at 24.
     </p><div id="id-1.3.5.7.17.4.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       If the placement group’s scrub interval exceeds the <code class="option">osd scrub
       max interval</code> setting, the scrub will happen no matter what time
       window you define for scrubbing.
      </p></div></dd><dt id="id-1.3.5.7.17.4.3"><span class="term"><code class="option">osd scrub during recovery</code></span></dt><dd><p>
      Allows scrubs during recovery. Setting this to 'false' will disable
      scheduling new scrubs while there is an active recovery. Already running
      scrubs will continue. This option is useful for reducing load on busy
      clusters. Default is 'true'.
     </p></dd><dt id="id-1.3.5.7.17.4.4"><span class="term"><code class="option">osd scrub thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub thread times out. Default is
      60.
     </p></dd><dt id="id-1.3.5.7.17.4.5"><span class="term"><code class="option">osd scrub finalize thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub finalize thread times out.
      Default is 60*10.
     </p></dd><dt id="id-1.3.5.7.17.4.6"><span class="term"><code class="option">osd scrub load threshold</code></span></dt><dd><p>
      The normalized maximum load. Ceph will not scrub when the system load
      (as defined by the ratio of <code class="literal">getloadavg()</code> / number of
      <code class="literal">online cpus</code>) is higher than this number. Default is
      0.5.
     </p></dd><dt id="id-1.3.5.7.17.4.7"><span class="term"><code class="option">osd scrub min interval</code></span></dt><dd><p>
      The minimal interval in seconds for scrubbing Ceph OSD when the Ceph
      cluster load is low. Default is 60*60*24 (once a day).
     </p></dd><dt id="id-1.3.5.7.17.4.8"><span class="term"><code class="option">osd scrub max interval</code></span></dt><dd><p>
      The maximum interval in seconds for scrubbing Ceph OSD, irrespective of
      cluster load. Default is 7*60*60*24 (once a week).
     </p></dd><dt id="id-1.3.5.7.17.4.9"><span class="term"><code class="option">osd scrub chunk min</code></span></dt><dd><p>
      The minimum number of object store chunks to scrub during a single
      operation. Ceph blocks writes to a single chunk during a scrub. Default
      is 5.
     </p></dd><dt id="id-1.3.5.7.17.4.10"><span class="term"><code class="option">osd scrub chunk max</code></span></dt><dd><p>
      The maximum number of object store chunks to scrub during a single
      operation. Default is 25.
     </p></dd><dt id="id-1.3.5.7.17.4.11"><span class="term"><code class="option">osd scrub sleep</code></span></dt><dd><p>
      Time to sleep before scrubbing the next group of chunks. Increasing this
      value slows down the whole scrub operation, while client operations are
      less impacted. Default is 0.
     </p></dd><dt id="id-1.3.5.7.17.4.12"><span class="term"><code class="option">osd deep scrub interval</code></span></dt><dd><p>
      The interval for 'deep' scrubbing (fully reading all data). The
      <code class="option">osd scrub load threshold</code> option does not affect this
      setting. Default is 60*60*24*7 (once a week).
     </p></dd><dt id="id-1.3.5.7.17.4.13"><span class="term"><code class="option">osd scrub interval randomize ratio</code></span></dt><dd><p>
      Add a random delay to the <code class="option">osd scrub min interval</code> value
      when scheduling the next scrub job for a placement group. The delay is a
      random value smaller than the result of <code class="option">osd scrub min
      interval</code> * <code class="option">osd scrub interval randomized ratio</code>.
      Therefore, the default setting practically randomly spreads the scrubs
      out in the allowed time window of [1, 1.5] * <code class="option">osd scrub min
      interval</code>. Default is 0.5.
     </p></dd><dt id="id-1.3.5.7.17.4.14"><span class="term"><code class="option">osd deep scrub stride</code></span></dt><dd><p>
      Read size when doing a deep scrub. Default is 524288 (512 kB).
     </p></dd></dl></div></section></section><section class="chapter" id="cha-mgr-modules" data-id-title="Ceph Manager Modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21 </span><span class="title-name">Ceph Manager Modules</span> <a title="Permalink" class="permalink" href="#cha-mgr-modules">#</a></h2></div></div></div><p>
  The architecture of the Ceph Manager (refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.2.3 “Ceph Nodes and Daemons”</span> for a brief introduction) allows
  extending its functionality via <span class="emphasis"><em>modules</em></span>, such as
  'dashboard' (see <a class="xref" href="#part-dashboard" title="Part II. Ceph Dashboard">Part II, “Ceph Dashboard”</a>), 'prometheus' (see
  <a class="xref" href="#monitoring-alerting" title="Chapter 18. Monitoring and Alerting">Chapter 18, <em>Monitoring and Alerting</em></a>), or 'balancer'.
 </p><p>
  To list all available modules, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module ls
{
        "enabled_modules": [
                "restful",
                "status"
        ],
        "disabled_modules": [
                "dashboard"
        ]
}</pre></div><p>
  To enable or disable a specific module, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module enable <em class="replaceable">MODULE-NAME</em></pre></div><p>
  For example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module disable dashboard</pre></div><p>
  To list the services that the enabled modules provide, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr services
{
        "dashboard": "http://myserver.com:7789/",
        "restful": "https://myserver.com:8789/"
}</pre></div><section class="sect1" id="mgr-modules-balancer" data-id-title="Balancer"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.1 </span><span class="title-name">Balancer</span> <a title="Permalink" class="permalink" href="#mgr-modules-balancer">#</a></h2></div></div></div><p>
   The balancer module optimizes the placement group (PG) distribution across
   OSDs for a more balanced deployment. Although the module is activated by
   default, it is inactive. It supports the following two modes: 'crush-compat'
   and 'upmap'.
  </p><div id="id-1.3.5.8.12.3" data-id-title="Current Balancer Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Current Balancer Configuration</h6><p>
    To view the current balancer configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer status</pre></div></div><div id="id-1.3.5.8.12.4" data-id-title="Supported Mode" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Supported Mode</h6><p>
    We currently only support the 'crush-compat' mode because the 'upmap' mode
    requires an OSD feature that prevents any pre-Luminous OSDs from connecting
    to the cluster.
   </p></div><section class="sect2" id="mgr-balancer-crush-compat" data-id-title="The crush-compat Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.1.1 </span><span class="title-name">The 'crush-compat' Mode</span> <a title="Permalink" class="permalink" href="#mgr-balancer-crush-compat">#</a></h3></div></div></div><p>
    In 'crush-compat' mode, the balancer adjusts the OSDs' reweight-sets to
    achieve improved distribution of the data. It moves PGs between OSDs,
    temporarily causing a HEALTH_WARN cluster state resulting from misplaced
    PGs.
   </p><div id="id-1.3.5.8.12.5.3" data-id-title="Mode Activation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mode Activation</h6><p>
     Although 'crush-compat' is the default mode, we recommend activating it
     explicitly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer mode crush-compat</pre></div></div></section><section class="sect2" id="mgr-balancer-planning-executing" data-id-title="Planning and Executing of Data Balancing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.1.2 </span><span class="title-name">Planning and Executing of Data Balancing</span> <a title="Permalink" class="permalink" href="#mgr-balancer-planning-executing">#</a></h3></div></div></div><p>
    Using the balancer module, you can create a plan for data balancing. You
    can then execute the plan manually, or let the balancer balance PGs
    continuously.
   </p><p>
    The decision whether to run the balancer in manual or automatic mode
    depends on several factors, such as the current data imbalance, cluster
    size, PG count, or I/O activity. We recommend creating an initial plan and
    executing it at a time of low I/O load in the cluster. The reason for this
    is that the initial imbalance will probably be considerable and it is a
    good practice to keep the impact on clients low. After an initial manual
    run, consider activating the automatic mode and monitor the rebalance
    traffic under normal I/O load. The improvements in PG distribution need to
    be weighed against the rebalance traffic caused by the balancer.
   </p><div id="id-1.3.5.8.12.6.4" data-id-title="Movable Fraction of Placement Groups (PGs)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Movable Fraction of Placement Groups (PGs)</h6><p>
     During the process of balancing, the balancer module throttles PG
     movements so that only a configurable fraction of PGs is moved. The
     default is 5% and you can adjust the fraction, to 9% for example, by
     running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr target_max_misplaced_ratio .09</pre></div></div><p>
    To create and execute a balancing plan, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check the current cluster score:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer eval</pre></div></li><li class="step"><p>
      Create a plan. For example, 'great_plan':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer optimize great_plan</pre></div></li><li class="step"><p>
      See what changes the 'great_plan' will entail:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer show great_plan</pre></div></li><li class="step"><p>
      Check the potential cluster score if you decide to apply the
      'great_plan':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer eval great_plan</pre></div></li><li class="step"><p>
      Execute the 'great_plan' for one time only:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer execute great_plan</pre></div></li><li class="step"><p>
      Observe the cluster balancing with the <code class="command">ceph -s</code>
      command. If you are satisfied with the result, activate automatic
      balancing:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer on</pre></div><p>
      If you later decide to deactivate automatic balancing, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph balancer off</pre></div></li></ol></div></div><div id="id-1.3.5.8.12.6.7" data-id-title="Automatic Balancing without Initial Plan" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Automatic Balancing without Initial Plan</h6><p>
     You can activate automatic balancing without executing an initial plan. In
     such case, expect a potentially long running rebalancing of placement
     groups.
    </p></div></section></section><section class="sect1" id="mgr-modules-telemetry" data-id-title="Telemetry Module"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.2 </span><span class="title-name">Telemetry Module</span> <a title="Permalink" class="permalink" href="#mgr-modules-telemetry">#</a></h2></div></div></div><p>
   The telemetry plugin sends the Ceph project anonymous data about the
   cluster in which the plugin is running.
  </p><p>
   This (opt-in) component contains counters and statistics on how the cluster
   has been deployed, the version of Ceph, the distribution of the hosts and
   other parameters which help the project to gain a better understanding of
   the way Ceph is used. It does not contain any sensitive data like pool
   names, object names, object contents, or host names.
  </p><p>
   The purpose of the telemetry module is to provide an automated feedback loop
   for the developers to help quantify adoption rates, tracking, or point out
   things that need to be better explained or validated during configuration to
   prevent undesirable outcomes.
  </p><div id="id-1.3.5.8.13.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The telemetry module requires the Ceph Manager nodes to have the ability to push
    data over HTTPS to the upstream servers. Ensure your corporate firewalls
    permit this action.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     To enable the telemetry module:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph mgr module enable telemetry</pre></div><div id="id-1.3.5.8.13.6.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This command only enables you to view your data locally. This command
      does not share your data with the Ceph community.
     </p></div></li><li class="step"><p>
     To allow the telemetry module to start sharing data:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div></li><li class="step"><p>
     To disable telemetry data sharing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry off</pre></div></li><li class="step"><p>
     To generate a JSON report that can be printed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show</pre></div></li><li class="step"><p>
     To add a contact and description to the report:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/contact ‘John Doe john.doe@example.com’
<code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/description ‘My first Ceph cluster’</pre></div></li><li class="step"><p>
     The module compiles and sends a new report every 24 hours by default. To
     adjust this interval:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/interval HOURS</pre></div></li></ol></div></div></section></section><section class="chapter" id="ceph-pools" data-id-title="Managing Storage Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22 </span><span class="title-name">Managing Storage Pools</span> <a title="Permalink" class="permalink" href="#ceph-pools">#</a></h2></div></div></div><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Resilience</em></span>: You can set how many OSDs, buckets, or
    leaves are allowed to fail without losing data. For replicated pools, it is
    the desired number of copies/replicas of an object. New pools are created
    with a default count of replicas set to 3. For erasure coded pools, it is
    the number of coding chunks (that is <span class="emphasis"><em>m=2</em></span> in the
    erasure code profile).
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Placement Groups</em></span>: are internal data structures for
    storing data in a pool across OSDs. The way Ceph stores data into PGs is
    defined in a CRUSH Map. You can set the number of placement groups for a
    pool at its creation. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>CRUSH Rules</em></span>: When you store data in a pool, objects
    and its replicas (or chunks in case of erasure coded pools) are placed
    according to the CRUSH ruleset mapped to the pool. You can create a custom
    CRUSH rule for your pool.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Snapshots</em></span>: When you create snapshots with
    <code class="command">ceph osd pool mksnap</code>, you effectively take a snapshot of
    a particular pool.
   </p></li></ul></div><p>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </p><section class="sect1" id="ceph-pools-associate" data-id-title="Associate Pools with an Application"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.1 </span><span class="title-name">Associate Pools with an Application</span> <a title="Permalink" class="permalink" href="#ceph-pools-associate">#</a></h2></div></div></div><p>
   Before using pools, you need to associate them with an application. Pools
   that will be used with CephFS, or pools that are automatically created by
   Object Gateway are automatically associated.
  </p><p>
   For other cases, you can manually associate a free-form application name
   with a pool:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable <em class="replaceable">pool_name</em> <em class="replaceable">application_name</em></pre></div><div id="id-1.3.5.9.6.5" data-id-title="Default Application Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Default Application Names</h6><p>
    CephFS uses the application name <code class="literal">cephfs</code>, RADOS Block Device uses
    <code class="literal">rbd</code>, and Object Gateway uses <code class="literal">rgw</code>.
   </p></div><p>
   A pool can be associated with multiple applications, and each application
   can have its own metadata. You can display the application metadata for a
   given pool using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application get <em class="replaceable">pool_name</em></pre></div></section><section class="sect1" id="ceph-pools-operate" data-id-title="Operating Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.2 </span><span class="title-name">Operating Pools</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate">#</a></h2></div></div></div><p>
   This section introduces practical information to perform basic tasks with
   pools. You can find out how to list, create, and delete pools, as well as
   show pool statistics or manage snapshots of a pool.
  </p><section class="sect2" id="id-1.3.5.9.7.3" data-id-title="List Pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.1 </span><span class="title-name">List Pools</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.7.3">#</a></h3></div></div></div><p>
    To list your cluster’s pools, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool ls</pre></div></section><section class="sect2" id="ceph-pools-operate-add-pool" data-id-title="Create a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.2 </span><span class="title-name">Create a Pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-add-pool">#</a></h3></div></div></div><p>
    A pool can be created as either 'replicated' to recover from lost OSDs by
    keeping multiple copies of the objects or 'erasure' to get a kind of
    generalized RAID5/6 capability. Replicated pools require more raw storage,
    while erasure coded pools require less raw storage. Default is
    'replicated'.
   </p><p>
    To create a replicated pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create <em class="replaceable">pool_name</em> <em class="replaceable">pg_num</em> <em class="replaceable">pgp_num</em> replicated <em class="replaceable">crush_ruleset_name</em> \
<em class="replaceable">expected_num_objects</em></pre></div><p>
    To create an erasure coded pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create <em class="replaceable">pool_name</em> <em class="replaceable">pg_num</em> <em class="replaceable">pgp_num</em> erasure <em class="replaceable">erasure_code_profile</em> \
 <em class="replaceable">crush_ruleset_name</em> <em class="replaceable">expected_num_objects</em></pre></div><p>
    The <code class="command">ceph osd pool create</code> can fail if you exceed the
    limit of placement groups per OSD. The limit is set with the option
    <code class="option">mon_max_pg_per_osd</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.9.7.4.8.1"><span class="term">pool_name</span></dt><dd><p>
       The name of the pool. It must be unique. This option is required.
      </p></dd><dt id="id-1.3.5.9.7.4.8.2"><span class="term">pg_num</span></dt><dd><p>
       The total number of placement groups for the pool. This option is
       required. Default value is 8.
      </p></dd><dt id="id-1.3.5.9.7.4.8.3"><span class="term">pgp_num</span></dt><dd><p>
       The total number of placement groups for placement purposes. This should
       be equal to the total number of placement groups, except for placement
       group splitting scenarios. This option is required. Default value is 8.
      </p></dd><dt id="id-1.3.5.9.7.4.8.4"><span class="term">crush_ruleset_name</span></dt><dd><p>
       The name of the crush ruleset for this pool. If the specified ruleset
       does not exist, the creation of replicated pools will fail with -ENOENT.
       For replicated pools it is the ruleset specified by the <code class="varname">osd
       pool default crush replicated ruleset</code> configuration variable.
       This ruleset must exist. For erasure pools it is 'erasure-code' if the
       default erasure code profile is used or
       <em class="replaceable">POOL_NAME</em> otherwise. This ruleset will be
       created implicitly if it does not exist already.
      </p></dd><dt id="id-1.3.5.9.7.4.8.5"><span class="term">erasure_code_profile=profile</span></dt><dd><p>
       For erasure coded pools only. Use the erasure code profile. It must be
       an existing profile as defined by <code class="command">osd erasure-code-profile
       set</code>.
      </p><p>
       When you create a pool, set the number of placement groups to a
       reasonable value. Consider the total number of placement groups per OSD
       too. Placement groups are computationally expensive, so performance will
       degrade when you have many pools with many placement groups (for example
       50 pools with 100 placement groups each).
      </p><p>
       See <a class="xref" href="#op-pgs" title="20.4. Placement Groups">Section 20.4, “Placement Groups”</a> for details on calculating an appropriate
       number of placement groups for your pool.
      </p></dd><dt id="id-1.3.5.9.7.4.8.6"><span class="term">expected_num_objects</span></dt><dd><p>
       The expected number of objects for this pool. By setting this value
       (together with a negative <code class="option">filestore merge threshold</code>),
       the PG folder splitting happens at the pool creation time. This avoids
       the latency impact with a runtime folder splitting.
      </p></dd></dl></div></section><section class="sect2" id="id-1.3.5.9.7.5" data-id-title="Set Pool Quotas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.3 </span><span class="title-name">Set Pool Quotas</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.7.5">#</a></h3></div></div></div><p>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool-name</em> <em class="replaceable">max_objects</em> <em class="replaceable">obj-count</em> <em class="replaceable">max_bytes</em> <em class="replaceable">bytes</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set-quota data max_objects 10000</pre></div><p>
    To remove a quota, set its value to 0.
   </p></section><section class="sect2" id="ceph-pools-operate-del-pool" data-id-title="Delete a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.4 </span><span class="title-name">Delete a Pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-del-pool">#</a></h3></div></div></div><div id="id-1.3.5.9.7.6.2" data-id-title="Pool Deletion is Not Reversible" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Pool Deletion is Not Reversible</h6><p>
     Pools may contain important data. Deleting a pool causes all data in the
     pool to disappear, and there is no way to recover it.
    </p></div><p>
    Because inadvertent pool deletion is a real danger, Ceph implements two
    mechanisms that prevent pools from being deleted. Both mechanisms must be
    disabled before a pool can be deleted.
   </p><p>
    The first mechanism is the <code class="literal">NODELETE</code> flag. Each pool has
    this flag, and its default value is 'false'. To find out the value of this
    flag on a pool, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool get <em class="replaceable">pool_name</em> nodelete</pre></div><p>
    If it outputs <code class="literal">nodelete: true</code>, it is not possible to
    delete the pool until you change the flag using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool_name</em> nodelete false</pre></div><p>
    The second mechanism is the cluster-wide configuration parameter
    <code class="option">mon allow pool delete</code>, which defaults to 'false'. This
    means that, by default, it is not possible to delete a pool. The error
    message displayed is:
   </p><div class="verbatim-wrap"><pre class="screen">Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</pre></div><p>
    To delete the pool in spite of this safety setting, you can temporarily set
    <code class="option">mon allow pool delete</code> to 'true', delete the pool, and then
    return the parameter to 'false':
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=false</pre></div><p>
    The <code class="command">injectargs</code> command displays the following message:
   </p><div class="verbatim-wrap"><pre class="screen">injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</pre></div><p>
    This is merely confirming that the command was executed successfully. It is
    not an error.
   </p><p>
    If you created your own rulesets and rules for a pool you created, you
    should consider removing them when you no longer need your pool.
   </p></section><section class="sect2" id="id-1.3.5.9.7.7" data-id-title="Rename a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.5 </span><span class="title-name">Rename a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.7.7">#</a></h3></div></div></div><p>
    To rename a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rename <em class="replaceable">current-pool-name</em> <em class="replaceable">new-pool-name</em></pre></div><p>
    If you rename a pool and you have per-pool capabilities for an
    authenticated user, you must update the user’s capabilities with the new
    pool name.
   </p></section><section class="sect2" id="id-1.3.5.9.7.8" data-id-title="Show Pool Statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.6 </span><span class="title-name">Show Pool Statistics</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.7.8">#</a></h3></div></div></div><p>
    To show a pool’s usage statistics, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados df
POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
.rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B</pre></div><p>
    A description of individual columns follow:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.9.7.8.5.1"><span class="term">USED</span></dt><dd><p>
       Number of bytes used by the pool.
      </p></dd><dt id="id-1.3.5.9.7.8.5.2"><span class="term">OBJECTS</span></dt><dd><p>
       Number of objects stored in the pool.
      </p></dd><dt id="id-1.3.5.9.7.8.5.3"><span class="term">CLONES</span></dt><dd><p>
       Number of clones stored in the pool. When a snapshot is created and one
       writes to an object, instead of modifying the original object its clone
       is created so the original snapshotted object content is not modified.
      </p></dd><dt id="id-1.3.5.9.7.8.5.4"><span class="term">COPIES</span></dt><dd><p>
       Number of object replicas. For example, if a replicated pool with the
       replication factor 3 has 'x' objects, it will normally have 3 * x
       copies.
      </p></dd><dt id="id-1.3.5.9.7.8.5.5"><span class="term">MISSING_ON_PRIMARY</span></dt><dd><p>
       Number of objects in the degraded state (not all copies exist) while the
       copy is missing on the primary OSD.
      </p></dd><dt id="id-1.3.5.9.7.8.5.6"><span class="term">UNFOUND</span></dt><dd><p>
       Number of unfound objects.
      </p></dd><dt id="id-1.3.5.9.7.8.5.7"><span class="term">DEGRADED</span></dt><dd><p>
       Number of degraded objects.
      </p></dd><dt id="id-1.3.5.9.7.8.5.8"><span class="term">RD_OPS</span></dt><dd><p>
       Total number of read operations requested for this pool.
      </p></dd><dt id="id-1.3.5.9.7.8.5.9"><span class="term">RD</span></dt><dd><p>
       Total number of bytes read from this pool.
      </p></dd><dt id="id-1.3.5.9.7.8.5.10"><span class="term">WR_OPS</span></dt><dd><p>
       Total number of write operations requested for this pool.
      </p></dd><dt id="id-1.3.5.9.7.8.5.11"><span class="term">WR</span></dt><dd><p>
       Total number of bytes written to the pool. Note that it is not the same
       as the pool's usage because you can write to the same object many times.
       The result is that the pool's usage will remain the same but the number
       of bytes written to the pool will grow.
      </p></dd><dt id="id-1.3.5.9.7.8.5.12"><span class="term">USED COMPR</span></dt><dd><p>
       Number of bytes allocated for compressed data.
      </p></dd><dt id="id-1.3.5.9.7.8.5.13"><span class="term">UNDER COMPR</span></dt><dd><p>
       Number of bytes that the compressed data occupy when it is not
       compressed.
      </p></dd></dl></div></section><section class="sect2" id="id-1.3.5.9.7.9" data-id-title="Get Pool Values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.7 </span><span class="title-name">Get Pool Values</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.7.9">#</a></h3></div></div></div><p>
    To get a value from a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool get <em class="replaceable">pool-name</em> <em class="replaceable">key</em></pre></div><p>
    You can get values for keys listed in <a class="xref" href="#ceph-pools-values" title="22.2.8. Set Pool Values">Section 22.2.8, “Set Pool Values”</a>
    plus the following keys:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.9.7.9.5.1"><span class="term">pg_num</span></dt><dd><p>
       The number of placement groups for the pool.
      </p></dd><dt id="id-1.3.5.9.7.9.5.2"><span class="term">pgp_num</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than
       <code class="literal">pg_num</code>.
      </p></dd></dl></div><div id="id-1.3.5.9.7.9.6" data-id-title="All of a Pools Values" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: All of a Pool's Values</h6><p>
     To list all values related to a specific pool, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> all</pre></div></div></section><section class="sect2" id="ceph-pools-values" data-id-title="Set Pool Values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.8 </span><span class="title-name">Set Pool Values</span> <a title="Permalink" class="permalink" href="#ceph-pools-values">#</a></h3></div></div></div><p>
    To set a value to a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool-name</em> <em class="replaceable">KEY</em> <em class="replaceable">VALUE</em></pre></div><p>
    The following is a list of pool values sorted by a pool type:
   </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Common Pool Values </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.7.10.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.9.7.10.5.2"><span class="term">crash_replay_interval</span></dt><dd><p>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </p></dd><dt id="id-1.3.5.9.7.10.5.3"><span class="term">pg_num</span></dt><dd><p>
       The number of placement groups for the pool. If you add new OSDs to the
       cluster, verify the value for placement groups on all pools targeted for
       the new OSDs.
      </p></dd><dt id="id-1.3.5.9.7.10.5.4"><span class="term">pgp_num</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement.
      </p></dd><dt id="id-1.3.5.9.7.10.5.5"><span class="term">crush_ruleset</span></dt><dd><p>
       The ruleset to use for mapping object placement in the cluster.
      </p></dd><dt id="id-1.3.5.9.7.10.5.6"><span class="term">hashpspool</span></dt><dd><p>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to the
       default 0, the cluster starts backfilling to have a correct placement of
       all PGs again. Be aware that this can create quite substantial I/O load
       on a cluster, therefore do not enable the flag from 0 to 1 on highly
       loaded production clusters.
      </p></dd><dt id="id-1.3.5.9.7.10.5.7"><span class="term">nodelete</span></dt><dd><p>
       Prevents the pool from being removed.
      </p></dd><dt id="id-1.3.5.9.7.10.5.8"><span class="term">nopgchange</span></dt><dd><p>
       Prevents the pool's <code class="option">pg_num</code> and <code class="option">pgp_num</code>
       from being changed.
      </p></dd><dt id="id-1.3.5.9.7.10.5.9"><span class="term">noscrub,nodeep-scrub</span></dt><dd><p>
       Disables (deep) scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </p></dd><dt id="id-1.3.5.9.7.10.5.10"><span class="term">write_fadvise_dontneed</span></dt><dd><p>
       Set or unset the <code class="literal">WRITE_FADVISE_DONTNEED</code> flag on a
       given pool's read/write requests to bypass putting data into cache.
       Default is <code class="literal">false</code>. Applies to both replicated and EC
       pools.
      </p></dd><dt id="id-1.3.5.9.7.10.5.11"><span class="term">scrub_min_interval</span></dt><dd><p>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_min_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.3.5.9.7.10.5.12"><span class="term">scrub_max_interval</span></dt><dd><p>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_max_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.3.5.9.7.10.5.13"><span class="term">deep_scrub_interval</span></dt><dd><p>
       The interval in seconds for the pool <span class="emphasis"><em>deep</em></span>
       scrubbing. The default <code class="literal">0</code> means that the
       <code class="option">osd_deep_scrub</code> value from the Ceph configuration file
       is used.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Replicated Pool Values </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.7.10.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.9.7.10.6.2"><span class="term">size</span></dt><dd><p>
       Sets the number of replicas for objects in the pool. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="22.2.9. Set the Number of Object Replicas">Section 22.2.9, “Set the Number of Object Replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.3.5.9.7.10.6.3"><span class="term">min_size</span></dt><dd><p>
       Sets the minimum number of replicas required for I/O. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="22.2.9. Set the Number of Object Replicas">Section 22.2.9, “Set the Number of Object Replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.3.5.9.7.10.6.4"><span class="term">nosizechange</span></dt><dd><p>
       Prevents the pool's size from being changed. When a pool is created, the
       default value is taken from the value of the
       <code class="option">osd_pool_default_flag_nosizechange</code> parameter which is
       <code class="literal">false</code> by default. Applies to replicated pools only
       because you cannot change size for EC pools.
      </p></dd><dt id="id-1.3.5.9.7.10.6.5"><span class="term">hit_set_type</span></dt><dd><p>
       Enables hit set tracking for cache pools. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. This option can have the
       following values: <code class="literal">bloom</code>,
       <code class="literal">explicit_hash</code>, <code class="literal">explicit_object</code>.
       Default is <code class="literal">bloom</code>, other values are for testing only.
      </p></dd><dt id="id-1.3.5.9.7.10.6.6"><span class="term">hit_set_count</span></dt><dd><p>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <code class="systemitem">ceph-osd</code> daemon.
       Default is <code class="literal">0</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.7"><span class="term">hit_set_period</span></dt><dd><p>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <code class="systemitem">ceph-osd</code> daemon. When a pool is created, the
       default value is taken from the value of the
       <code class="option">osd_tier_default_cache_hit_set_period</code> parameter, which
       is <code class="literal">1200</code> by default. Applies to replicated pools only
       because EC pools cannot be used as a cache tier.
      </p></dd><dt id="id-1.3.5.9.7.10.6.8"><span class="term">hit_set_fpp</span></dt><dd><p>
       The false positive probability for the bloom hit set type. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. Valid range is 0.0 - 1.0
       Default is <code class="literal">0.05</code>
      </p></dd><dt id="id-1.3.5.9.7.10.6.9"><span class="term">use_gmt_hitset</span></dt><dd><p>
       Force OSDs to use GMT (Greenwich Mean Time) time stamps when creating a
       hit set for cache tiering. This ensures that nodes in different time
       zones return the same result. Default is <code class="literal">1</code>. This
       value should not be changed.
      </p></dd><dt id="id-1.3.5.9.7.10.6.10"><span class="term">cache_target_dirty_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <code class="literal">0.4</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.11"><span class="term">cache_target_dirty_high_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <code class="literal">0.6</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.12"><span class="term">cache_target_full_ratio</span></dt><dd><p>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <code class="literal">0.8</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.13"><span class="term">target_max_bytes</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_bytes</code> threshold is triggered.
      </p></dd><dt id="id-1.3.5.9.7.10.6.14"><span class="term">target_max_objects</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_objects</code> threshold is triggered.
      </p></dd><dt id="id-1.3.5.9.7.10.6.15"><span class="term">hit_set_grade_decay_rate</span></dt><dd><p>
       Temperature decay rate between two successive
       <code class="literal">hit_set</code>s. Default is <code class="literal">20</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.16"><span class="term">hit_set_search_last_n</span></dt><dd><p>
       Count at most <code class="literal">N</code> appearances in
       <code class="literal">hit_set</code>s for temperature calculation. Default is
       <code class="literal">1</code>.
      </p></dd><dt id="id-1.3.5.9.7.10.6.17"><span class="term">cache_min_flush_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </p></dd><dt id="id-1.3.5.9.7.10.6.18"><span class="term">cache_min_evict_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </p></dd></dl></div><div class="variablelist" id="pool-values-ec" data-id-title="Erasure Coded Pool Values"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Erasure Coded Pool Values </span><a title="Permalink" class="permalink" href="#pool-values-ec">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.9.7.10.7.2"><span class="term">fast_read</span></dt><dd><p>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <span class="emphasis"><em>jerasure</em></span> and <span class="emphasis"><em>isa</em></span> erasure
       plug-ins, when the first <code class="literal">K</code> replies return, then the
       client’s request is served immediately using the data decoded from these
       replies. This approach causes more CPU load and less disk/network load.
       Currently, this flag is only supported for erasure coding pools. Default
       is <code class="literal">0</code>.
      </p></dd></dl></div></section><section class="sect2" id="ceph-pools-options-num-of-replicas" data-id-title="Set the Number of Object Replicas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.2.9 </span><span class="title-name">Set the Number of Object Replicas</span> <a title="Permalink" class="permalink" href="#ceph-pools-options-num-of-replicas">#</a></h3></div></div></div><p>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The <em class="replaceable">num-replicas</em> includes the object itself. For
    example if you want the object and two copies of the object for a total of
    three instances of the object, specify 3.
   </p><div id="id-1.3.5.9.7.11.5" data-id-title="Do Not Set Less Than 3 Replicas" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Do Not Set Less Than 3 Replicas</h6><p>
     If you set the <em class="replaceable">num-replicas</em> to 2, there will be
     only <span class="emphasis"><em>one</em></span> copy of your data. If you lose one object
     instance, you need to trust that the other copy has not been corrupted,
     for example since the last scrubbing during recovery (refer to
     <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a> for details).
    </p><p>
     Setting a pool to one replica means that there is exactly
     <span class="emphasis"><em>one</em></span> instance of the data object in the pool. If the
     OSD fails, you lose the data. A possible usage for a pool with one replica
     is storing temporary data for a short time.
    </p></div><div id="id-1.3.5.9.7.11.6" data-id-title="Setting More Than 3 Replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Setting More Than 3 Replicas</h6><p>
     Setting 4 replicas for a pool increases the reliability by 25%.
    </p><p>
     In case of two data centers, you need to set at least 4 replicas for a
     pool to have two copies in each data center so that if one data center is
     lost, two copies still exist and you can still lose one disk without
     losing data.
    </p></div><div id="id-1.3.5.9.7.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     An object might accept I/Os in degraded mode with fewer than <code class="literal">pool
     size</code> replicas. To set a minimum number of required replicas for
     I/O, you should use the <code class="literal">min_size</code> setting. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set data min_size 2</pre></div><p>
     This ensures that no object in the data pool will receive I/O with fewer
     than <code class="literal">min_size</code> replicas.
    </p></div><div id="id-1.3.5.9.7.11.8" data-id-title="Get the Number of Object Replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Get the Number of Object Replicas</h6><p>
     To get the number of object replicas, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep 'replicated size'</pre></div><p>
     Ceph will list the pools, with the <code class="literal">replicated size</code>
     attribute highlighted. By default, Ceph creates two replicas of an
     object (a total of three copies, or a size of 3).
    </p></div></section></section><section class="sect1" id="pools-migration" data-id-title="Pool Migration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.3 </span><span class="title-name">Pool Migration</span> <a title="Permalink" class="permalink" href="#pools-migration">#</a></h2></div></div></div><p>
   When creating a pool (see <a class="xref" href="#ceph-pools-operate-add-pool" title="22.2.2. Create a Pool">Section 22.2.2, “Create a Pool”</a>) you
   need to specify its initial parameters, such as the pool type or the number
   of placement groups. If you later decide to change any of these
   parameters—for example when converting a replicated pool into an
   erasure coded one, or decreasing the number of placement groups—you
   need to migrate the pool data to another one whose parameters suit your
   deployment.
  </p><p>
   This section describes two migration methods—a <span class="emphasis"><em>cache
   tier</em></span> method for general pool data migration, and a method using
   <code class="command">rbd migrate</code> sub-commands to migrate RBD images to a new
   pool. Each method has its specifics and limitations.
  </p><section class="sect2" id="pool-migrate-limits" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.3.1 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#pool-migrate-limits">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can use the <span class="emphasis"><em>cache tier</em></span> method to migrate from a
      replicated pool to either an EC pool or another replicated pool.
      Migrating from an EC pool is not supported.
     </p></li><li class="listitem"><p>
      You cannot migrate RBD images and CephFS exports from a replicated pool
      to an EC pool. The reason is that EC pools do not support
      <code class="literal">omap</code>, while RBD and CephFS use
      <code class="literal">omap</code> to store its metadata. For example, the header
      object of the RBD will fail to be flushed. But you can migrate data to EC
      pool, leaving metadata in replicated pool.
     </p></li><li class="listitem"><p>
      The <code class="command">rbd migration</code> method allows migrating images with
      minimal client downtime. You only need to stop the client before the
      <code class="option">prepare</code> step and start it afterward. Note that only a
      <code class="systemitem">librbd</code> client that supports this feature (Ceph
      Nautilus or newer) will be able to open the image just after the
      <code class="option">prepare</code> step, while older
      <code class="systemitem">librbd</code> clients or the
      <code class="systemitem">krbd</code> clients will not be able to open the image
      until the <code class="option">commit</code> step is executed.
     </p></li></ul></div></section><section class="sect2" id="pool-migrate-cache-tier" data-id-title="Migrate Using Cache Tier"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.3.2 </span><span class="title-name">Migrate Using Cache Tier</span> <a title="Permalink" class="permalink" href="#pool-migrate-cache-tier">#</a></h3></div></div></div><p>
    The principle is simple—include the pool that you need to migrate
    into a cache tier in reverse order. Find more details on cache tiers in
    <span class="intraxref">Book “Tuning Guide”, Chapter 7 “Cache Tiering”</span>. The following example migrates a
    replicated pool named 'testpool' to an erasure coded pool:
   </p><div class="procedure" id="id-1.3.5.9.8.5.3" data-id-title="Migrating Replicated to Erasure Coded Pool"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 22.1: </span><span class="title-name">Migrating Replicated to Erasure Coded Pool </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a new erasure coded pool named 'newpool'. Refer to
      <a class="xref" href="#ceph-pools-operate-add-pool" title="22.2.2. Create a Pool">Section 22.2.2, “Create a Pool”</a> for a detailed explanation
      of pool creation parameters.
     </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create newpool <em class="replaceable">PG_NUM</em> <em class="replaceable">PGP_NUM</em> erasure default</pre></div><p>
      Verify that the used client keyring provides at least the same
      capabilities for 'newpool' as it does for 'testpool'.
     </p><p>
      Now you have two pools: the original replicated 'testpool' filled with
      data, and the new empty erasure coded 'newpool':
     </p><div class="figure" id="id-1.3.5.9.8.5.3.2.5"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate1.png" target="_blank"><img src="images/pool_migrate1.png" width="" alt="Pools before Migration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.1: </span><span class="title-name">Pools before Migration </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3.2.5">#</a></h6></div></div></li><li class="step"><p>
      Set up the cache tier and configure the replicated pool 'testpool' as a
      cache pool. The <code class="option">-force-nonempty</code> option allows adding a
      cache tier even if the pool already has data:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<code class="prompt user">cephadm@adm &gt; </code>ceph osd tier add newpool testpool --force-nonempty
<code class="prompt user">cephadm@adm &gt; </code>ceph osd tier cache-mode testpool proxy</pre></div><div class="figure" id="id-1.3.5.9.8.5.3.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate2.png" target="_blank"><img src="images/pool_migrate2.png" width="" alt="Cache Tier Setup"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.2: </span><span class="title-name">Cache Tier Setup </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3.3.3">#</a></h6></div></div></li><li class="step"><p>
      Force the cache pool to move all objects to the new pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p testpool cache-flush-evict-all</pre></div><div class="figure" id="id-1.3.5.9.8.5.3.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate3.png" target="_blank"><img src="images/pool_migrate3.png" width="" alt="Data Flushing"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.3: </span><span class="title-name">Data Flushing </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3.4.3">#</a></h6></div></div></li><li class="step"><p>
      Until all the data has been flushed to the new erasure coded pool, you
      need to specify an overlay so that objects are searched on the old pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier set-overlay newpool testpool</pre></div><p>
      With the overlay, all operations are forwarded to the old replicated
      'testpool':
     </p><div class="figure" id="id-1.3.5.9.8.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate4.png" target="_blank"><img src="images/pool_migrate4.png" width="" alt="Setting Overlay"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.4: </span><span class="title-name">Setting Overlay </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3.5.4">#</a></h6></div></div><p>
      Now you can switch all the clients to access objects on the new pool.
     </p></li><li class="step"><p>
      After all data is migrated to the erasure coded 'newpool', remove the
      overlay and the old cache pool 'testpool':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier remove-overlay newpool
<code class="prompt user">cephadm@adm &gt; </code>ceph osd tier remove newpool testpool</pre></div><div class="figure" id="id-1.3.5.9.8.5.3.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate5.png" target="_blank"><img src="images/pool_migrate5.png" width="" alt="Migration Complete"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.5: </span><span class="title-name">Migration Complete </span><a title="Permalink" class="permalink" href="#id-1.3.5.9.8.5.3.6.3">#</a></h6></div></div></li><li class="step"><p>
      Run
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'</pre></div></li></ol></div></div></section><section class="sect2" id="migrate-rbd-image" data-id-title="Migrating RBD Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.3.3 </span><span class="title-name">Migrating RBD Images</span> <a title="Permalink" class="permalink" href="#migrate-rbd-image">#</a></h3></div></div></div><p>
    The following is the recommended way to migrate RBD images from one
    replicated pool to another replicated pool.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop clients (such as a virtual machine) from accessing the RBD image.
     </p></li><li class="step"><p>
      Create a new image in the target pool, with the parent set to the source
      image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd migration prepare <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em> <em class="replaceable">TARGET_POOL</em>/<em class="replaceable">IMAGE</em></pre></div><div id="id-1.3.5.9.8.6.3.2.3" data-id-title="Migrate Only Data to an EC Pool" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Migrate Only Data to an EC Pool</h6><p>
       If you need to migrate only the image data to a new EC pool and leave
       the metadata in the original replicated pool, run the following command
       instead:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd migration prepare <em class="replaceable">SRC_METADATA_POOL</em>/<em class="replaceable">IMAGE</em> <em class="replaceable">TARGET_METADATA_POOL</em>/<em class="replaceable">IMAGE</em> \
 --data-pool <em class="replaceable">TARGET_DATA_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></div></li><li class="step"><p>
      Let clients access the image in the target pool.
     </p></li><li class="step"><p>
      Migrate data to the target pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd migration execute <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li><li class="step"><p>
      Remove the old image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd migration commit <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li></ol></div></div></section></section><section class="sect1" id="cha-ceph-snapshots-pool" data-id-title="Pool Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.4 </span><span class="title-name">Pool Snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-pool">#</a></h2></div></div></div><p>
   Pool snapshots are snapshots of the state of the whole Ceph pool. With
   pool snapshots, you can retain the history of the pool's state. Creating
   pool snapshots consumes storage space proportional to the pool size. Always
   check the related storage for enough disk space before creating a snapshot
   of a pool.
  </p><section class="sect2" id="id-1.3.5.9.9.3" data-id-title="Make a Snapshot of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.4.1 </span><span class="title-name">Make a Snapshot of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.9.3">#</a></h3></div></div></div><p>
    To make a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool mksnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</pre></div></section><section class="sect2" id="id-1.3.5.9.9.4" data-id-title="List Snapshots of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.4.2 </span><span class="title-name">List Snapshots of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.9.4">#</a></h3></div></div></div><p>
    To list existing snapshots of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados lssnap -p <em class="replaceable">POOL_NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps</pre></div></section><section class="sect2" id="id-1.3.5.9.9.5" data-id-title="Remove a Snapshot of a Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.4.3 </span><span class="title-name">Remove a Snapshot of a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.5.9.9.5">#</a></h3></div></div></div><p>
    To remove a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rmsnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div></section></section><section class="sect1" id="sec-ceph-pool-compression" data-id-title="Data Compression"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.5 </span><span class="title-name">Data Compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression">#</a></h2></div></div></div><p>
   BlueStore (find more details in <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.4 “BlueStore”</span>)
   provides on-the-fly data compression to save disk space. The compression
   ratio depends on the data stored in the system. Note that
   compression/decompression requires additional CPU power.
  </p><p>
   You can configure data compression globally (see
   <a class="xref" href="#sec-ceph-pool-bluestore-compression-options" title="22.5.3. Global Compression Options">Section 22.5.3, “Global Compression Options”</a>) and then
   override specific compression settings for each individual pool.
  </p><p>
   You can enable or disable pool data compression, or change the compression
   algorithm and mode at any time, regardless of whether the pool contains data
   or not.
  </p><p>
   No compression will be applied to existing data after enabling the pool
   compression.
  </p><p>
   After disabling the compression of a pool, all its data will be
   decompressed.
  </p><section class="sect2" id="sec-ceph-pool-compression-enable" data-id-title="Enable Compression"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.5.1 </span><span class="title-name">Enable Compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-enable">#</a></h3></div></div></div><p>
    To enable data compression for a pool named
    <em class="replaceable">POOL_NAME</em>, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm <em class="replaceable">COMPRESSION_ALGORITHM</em>
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_mode <em class="replaceable">COMPRESSION_MODE</em></pre></div><div id="id-1.3.5.9.10.7.4" data-id-title="Disabling Pool Compression" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling Pool Compression</h6><p>
     To disable data compression for a pool, use 'none' as the compression
     algorithm:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm none</pre></div></div></section><section class="sect2" id="sec-ceph-pool-compression-options" data-id-title="Pool Compression Options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.5.2 </span><span class="title-name">Pool Compression Options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-options">#</a></h3></div></div></div><p>
    A full list of compression settings:
   </p><div class="variablelist"><dl class="variablelist"><dt id="compr-algorithm"><span class="term">compression_algorithm</span></dt><dd><p>
       Possible values are <code class="literal">none</code>, <code class="literal">zstd</code>,
       <code class="literal">snappy</code>. Default is <code class="literal">snappy</code>.
      </p><p>
       Which compression algorithm to use depends on the specific use case.
       Several recommendations follow:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Use the default <code class="literal">snappy</code> as long as you do not have a
         good reason to change it.
        </p></li><li class="listitem"><p>
         <code class="literal">zstd</code> offers a good compression ratio, but causes
         high CPU overhead when compressing small amounts of data.
        </p></li><li class="listitem"><p>
         Run a benchmark of these algorithms on a sample of your actual data
         while keeping an eye on the CPU and memory usage of your cluster.
        </p></li></ul></div></dd><dt id="compr-mode"><span class="term">compression_mode</span></dt><dd><p>
       Possible values are <code class="literal">none</code>,
       <code class="literal">aggressive</code>, <code class="literal">passive</code>,
       <code class="literal">force</code>. Default is <code class="literal">none</code>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">none</code>: compress never
        </p></li><li class="listitem"><p>
         <code class="literal">passive</code>: compress if hinted
         <code class="literal">COMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">aggressive</code>: compress unless hinted
         <code class="literal">INCOMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">force</code>: compress always
        </p></li></ul></div></dd><dt id="compr-ratio"><span class="term">compression_required_ratio</span></dt><dd><p>
       Value: Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Default is
       <code class="literal">0.875</code>, which means that if the compression does not
       reduce the occupied space by at least 12.5%, the object will not be
       compressed.
      </p><p>
       Objects above this ratio will not be stored compressed because of the
       low net gain.
      </p></dd><dt id="id-1.3.5.9.10.8.3.4"><span class="term">compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed.
      </p></dd><dt id="id-1.3.5.9.10.8.3.5"><span class="term">compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed.
      </p></dd></dl></div></section><section class="sect2" id="sec-ceph-pool-bluestore-compression-options" data-id-title="Global Compression Options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.5.3 </span><span class="title-name">Global Compression Options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-bluestore-compression-options">#</a></h3></div></div></div><p>
    The following configuration options can be set in the Ceph configuration
    and apply to all OSDs and not only a single pool. The pool specific
    configuration listed in <a class="xref" href="#sec-ceph-pool-compression-options" title="22.5.2. Pool Compression Options">Section 22.5.2, “Pool Compression Options”</a>
    takes precedence.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.9.10.9.3.1"><span class="term">bluestore_compression_algorithm</span></dt><dd><p>
       See <a class="xref" href="#compr-algorithm">compression_algorithm</a>
      </p></dd><dt id="id-1.3.5.9.10.9.3.2"><span class="term">bluestore_compression_mode</span></dt><dd><p>
       See <a class="xref" href="#compr-mode">compression_mode</a>
      </p></dd><dt id="id-1.3.5.9.10.9.3.3"><span class="term">bluestore_compression_required_ratio</span></dt><dd><p>
       See <a class="xref" href="#compr-ratio">compression_required_ratio</a>
      </p></dd><dt id="id-1.3.5.9.10.9.3.4"><span class="term">bluestore_compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed. The setting is ignored by
       default in favor of
       <code class="option">bluestore_compression_min_blob_size_hdd</code> and
       <code class="option">bluestore_compression_min_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.3.5.9.10.9.3.5"><span class="term">bluestore_compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed before they will be split
       into smaller chunks. The setting is ignored by default in favor of
       <code class="option">bluestore_compression_max_blob_size_hdd</code> and
       <code class="option">bluestore_compression_max_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.3.5.9.10.9.3.6"><span class="term">bluestore_compression_min_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">8K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on solid-state
       drive.
      </p></dd><dt id="id-1.3.5.9.10.9.3.7"><span class="term">bluestore_compression_max_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">64K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on solid-state
       drive before they will be split into smaller chunks.
      </p></dd><dt id="id-1.3.5.9.10.9.3.8"><span class="term">bluestore_compression_min_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">128K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on hard disks.
      </p></dd><dt id="id-1.3.5.9.10.9.3.9"><span class="term">bluestore_compression_max_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">512K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on hard disks
       before they will be split into smaller chunks.
      </p></dd></dl></div></section></section></section><section class="chapter" id="ceph-rbd" data-id-title="RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23 </span><span class="title-name">RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ceph-rbd">#</a></h2></div></div></div><p>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like Ceph.
 </p><p>
  Ceph block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a Ceph cluster. Ceph block
  devices leverage RADOS capabilities such as snapshotting, replication, and
  consistency. Ceph's RADOS Block Devices (RBD) interact with OSDs using kernel modules or
  the <code class="systemitem">librbd</code> library.
 </p><div class="figure" id="id-1.3.5.10.5"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_rbd_schema.png" target="_blank"><img src="images/ceph_rbd_schema.png" width="" alt="RADOS Protocol"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 23.1: </span><span class="title-name">RADOS Protocol </span><a title="Permalink" class="permalink" href="#id-1.3.5.10.5">#</a></h6></div></div><p>
  Ceph's block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as QEMU, or
  cloud-based computing systems such as OpenStack that rely on <code class="systemitem">libvirt</code>. You
  can use the same cluster to operate the Object Gateway, CephFS, and RADOS Block Devices
  simultaneously.
 </p><section class="sect1" id="ceph-rbd-commands" data-id-title="Block Device Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.1 </span><span class="title-name">Block Device Commands</span> <a title="Permalink" class="permalink" href="#ceph-rbd-commands">#</a></h2></div></div></div><p>
   The <code class="command">rbd</code> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </p><section class="sect2" id="ceph-rbd-cmds-create" data-id-title="Creating a Block Device Image in a Replicated Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.1 </span><span class="title-name">Creating a Block Device Image in a Replicated Pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create">#</a></h3></div></div></div><p>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <a class="xref" href="#ceph-pools" title="Chapter 22. Managing Storage Pools">Chapter 22, <em>Managing Storage Pools</em></a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create --size <em class="replaceable">MEGABYTES</em> <em class="replaceable">POOL-NAME</em>/<em class="replaceable">IMAGE-NAME</em></pre></div><p>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create --size 1024 mypool/myimage</pre></div><div id="id-1.3.5.10.7.3.6" data-id-title="Image Size Units" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Image Size Units</h6><p>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </p></div></section><section class="sect2" id="ceph-rbd-cmds-create-ec" data-id-title="Creating a Block Device Image in an Erasure Coded Pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.2 </span><span class="title-name">Creating a Block Device Image in an Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create-ec">#</a></h3></div></div></div><p>
    As of SUSE Enterprise Storage 5, it is possible to store data of a block device image
    directly in erasure coded (EC) pools. A RADOS Block Device image consists of
    <span class="emphasis"><em>data</em></span> and <span class="emphasis"><em>metadata</em></span> parts. You can
    store only the 'data' part of a RADOS Block Device image in an EC pool. The pool needs
    to have the 'overwrite' flag set to <span class="emphasis"><em>true</em></span>, and that is
    only possible if all OSDs where the pool is stored use BlueStore.
   </p><p>
    You cannot store the image's 'metadata' part in an EC pool. You need to
    specify the replicated pool for storing the image's metadata with the
    <code class="option">--pool=</code> option of the <code class="command">rbd create</code>
    command.
   </p><p>
    Use the following steps to create an RBD image in a newly created EC pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool create <em class="replaceable">POOL_NAME</em> 12 12 erasure
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> allow_ec_overwrites true

#Metadata will reside in pool "<em class="replaceable">OTHER_POOL</em>", and data in pool "<em class="replaceable">POOL_NAME</em>"
<code class="prompt user">cephadm@adm &gt; </code><code class="command">rbd</code> create <em class="replaceable">IMAGE_NAME</em> --size=1G --data-pool <em class="replaceable">POOL_NAME</em> --pool=<em class="replaceable">OTHER_POOL</em></pre></div></section><section class="sect2" id="ceph-rbd-cmds-list" data-id-title="Listing Block Device Images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.3 </span><span class="title-name">Listing Block Device Images</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-list">#</a></h3></div></div></div><p>
    To list block devices in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd ls mypool</pre></div></section><section class="sect2" id="ceph-rbd-cmds-info" data-id-title="Retrieving Image Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.4 </span><span class="title-name">Retrieving Image Information</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-info">#</a></h3></div></div></div><p>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd info mypool/myimage</pre></div></section><section class="sect2" id="ceph-rbd-cmds-resize" data-id-title="Resizing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.5 </span><span class="title-name">Resizing a Block Device Image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-resize">#</a></h3></div></div></div><p>
    RADOS Block Device images are thin provisioned—they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <code class="option">--size</code> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> # to increase
<code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --allow-shrink # to decrease</pre></div></section><section class="sect2" id="ceph-rbd-cmds-rm" data-id-title="Removing a Block Device Image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.6 </span><span class="title-name">Removing a Block Device Image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-rm">#</a></h3></div></div></div><p>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd rm mypool/myimage</pre></div></section></section><section class="sect1" id="storage-bp-integration-mount-rbd" data-id-title="Mounting and Unmounting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.2 </span><span class="title-name">Mounting and Unmounting</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-mount-rbd">#</a></h2></div></div></div><p>
   After you create a RADOS Block Device, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make sure your Ceph cluster includes a pool with the disk image you want
     to map. Assume the pool is called <code class="literal">mypool</code> and the image
     is <code class="literal">myimage</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
     Map the image to a new block device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.5.10.8.3.2.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
      To specify a user name, use <code class="option">--id
      <em class="replaceable">user-name</em></code>. If you use
      <code class="systemitem">cephx</code> authentication, you also need to specify a
      secret. It may come from a keyring or a file containing the secret:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
      or
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
     List all mapped devices:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div><p>
     The device we want to work on is <code class="filename">/dev/rbd0</code>.
    </p><div id="id-1.3.5.10.8.3.3.4" data-id-title="RBD Device Path" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: RBD Device Path</h6><p>
      Instead of
      <code class="filename">/dev/rbd<em class="replaceable">DEVICE_NUMBER</em></code>,
      you can use
      <code class="filename">/dev/rbd/<em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></code>
      as a persistent device path. For example:
     </p><div class="verbatim-wrap"><pre class="screen">/dev/rbd/mypool/myimage</pre></div></div></li><li class="step"><p>
     Make an XFS file system on the <code class="filename">/dev/rbd0</code> device.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div></li><li class="step"><p>
     Mount the device and check it is correctly mounted. Replace
     <code class="filename">/mnt</code> with your mount point.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount /dev/rbd0 /mnt
<code class="prompt user">root # </code>mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</pre></div><p>
     Now you can move data to and from the device as if it was a local
     directory.
    </p><div id="id-1.3.5.10.8.3.5.4" data-id-title="Increasing the Size of RBD Device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increasing the Size of RBD Device</h6><p>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
        Increase the size of the RBD image, for example up to 10 GB.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
        Grow the file system to fill up the new size of the device.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></div></li><li class="step"><p>
     After you finish accessing the device, you can unmap and unmount it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd unmap /dev/rbd0
<code class="prompt user">root # </code>unmount /mnt</pre></div></li></ol></div></div><div id="id-1.3.5.10.8.4" data-id-title="Manual (Un)mounting" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Manual (Un)mounting</h6><p>
    Since manually mapping and mounting RBD images after boot and unmounting
    and unmapping them before shutdown can be tedious, an
    <code class="command">rbdmap</code> script and <code class="systemitem">systemd</code> unit is provided. Refer to
    <a class="xref" href="#ceph-rbd-rbdmap" title="23.2.1. rbdmap: Map RBD Devices at Boot Time">Section 23.2.1, “rbdmap: Map RBD Devices at Boot Time”</a>.
   </p></div><section class="sect2" id="ceph-rbd-rbdmap" data-id-title="rbdmap: Map RBD Devices at Boot Time"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.2.1 </span><span class="title-name">rbdmap: Map RBD Devices at Boot Time</span> <a title="Permalink" class="permalink" href="#ceph-rbd-rbdmap">#</a></h3></div></div></div><p>
    <code class="command">rbdmap</code> is a shell script that automates <code class="command">rbd
    map</code> and <code class="command">rbd unmap</code> operations on one or more
    RBD images. Although you can run the script manually at any time, the main
    advantage is automatic mapping and mounting of RBD images at boot time (and
    unmounting and unmapping at shutdown), as triggered by the Init system. A
    <code class="systemitem">systemd</code> unit file, <code class="filename">rbdmap.service</code> is included with
    the <code class="systemitem">ceph-common</code> package for this purpose.
   </p><p>
    The script takes a single argument, which can be either
    <code class="option">map</code> or <code class="option">unmap</code>. In either case, the script
    parses a configuration file. It defaults to
    <code class="filename">/etc/ceph/rbdmap</code>, but can be overridden via an
    environment variable <code class="literal">RBDMAPFILE</code>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </p><p>
    The configuration file has the following format:
   </p><div class="verbatim-wrap"><pre class="screen">image_specification rbd_options</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.8.5.6.1"><span class="term"><code class="option">image_specification</code></span></dt><dd><p>
       Path to an image within a pool. Specify as
       <em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>.
      </p></dd><dt id="id-1.3.5.10.8.5.6.2"><span class="term"><code class="option">rbd_options</code></span></dt><dd><p>
       An optional list of parameters to be passed to the underlying
       <code class="command">rbd map</code> command. These parameters and their values
       should be specified as a comma-separated string, for example:
      </p><div class="verbatim-wrap"><pre class="screen">PARAM1=VAL1,PARAM2=VAL2,...</pre></div><p>
       The example makes the <code class="command">rbdmap</code> script run the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --PARAM1 VAL1 --PARAM2 VAL2</pre></div><p>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbdmap map mypool/myimage id=rbd_user,keyring=/etc/ceph/ceph.client.rbd.keyring</pre></div></dd></dl></div><p>
    When run as <code class="command">rbdmap map</code>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using <code class="command">the rbd map</code> command) and then mount
    the image.
   </p><p>
    When run as <code class="command">rbdmap unmap</code>, images listed in the
    configuration file will be unmounted and unmapped.
   </p><p>
    <code class="command">rbdmap unmap-all</code> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </p><p>
    If successful, the rbd map operation maps the image to a /dev/rbdX device,
    at which point a udev rule is triggered to create a friendly device name
    symbolic link
    <code class="filename">/dev/rbd/<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em></code>
    pointing to the real mapped device.
   </p><p>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <code class="filename">/etc/fstab</code>.
    When writing <code class="filename">/etc/fstab</code> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early—before the device in
    question even exists, as <code class="filename">rbdmap.service</code> is typically
    triggered quite late in the boot sequence.
   </p><p>
    For a complete list of <code class="command">rbd</code> options, see the
    <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>).
   </p><p>
    For examples of the <code class="command">rbdmap</code> usage, see the
    <code class="command">rbdmap</code> manual page (<code class="command">man 8 rbdmap</code>).
   </p></section><section class="sect2" id="id-1.3.5.10.8.6" data-id-title="Increasing the Size of RBD Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.2.2 </span><span class="title-name">Increasing the Size of RBD Device</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.8.6">#</a></h3></div></div></div><p>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Increase the size of the RBD image, for example up to 10GB.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
      Grow the file system to fill up the new size of the device.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></section></section><section class="sect1" id="cha-ceph-snapshots-rbd" data-id-title="Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.3 </span><span class="title-name">Snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-rbd">#</a></h2></div></div></div><p>
   An RBD snapshot is a snapshot of a RADOS Block Device image. With snapshots, you retain a
   history of the image's state. Ceph also supports snapshot layering, which
   allows you to clone VM images quickly and easily. Ceph supports block
   device snapshots using the <code class="command">rbd</code> command and many
   higher-level interfaces, including QEMU, <code class="systemitem">libvirt</code>,
   OpenStack, and CloudStack.
  </p><div id="id-1.3.5.10.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </p></div><section class="sect2" id="id-1.3.5.10.9.4" data-id-title="Cephx Notes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.1 </span><span class="title-name">Cephx Notes</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.4">#</a></h3></div></div></div><p>
    When <code class="systemitem">cephx</code> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <a class="xref" href="#cha-storage-cephx" title="Chapter 19. Authentication with cephx">Chapter 19, <em>Authentication with <code class="systemitem">cephx</code></em></a> for more details. You may
    also add the <code class="systemitem">CEPH_ARGS</code> environment variable to
    avoid re-entry of the following parameters.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --id <em class="replaceable">user-ID</em> --keyring=/path/to/secret <em class="replaceable">commands</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --name <em class="replaceable">username</em> --keyring=/path/to/secret <em class="replaceable">commands</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --id admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em></pre></div><div id="id-1.3.5.10.9.4.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Add the user and secret to the <code class="systemitem">CEPH_ARGS</code>
     environment variable so that you do not need to enter them each time.
    </p></div></section><section class="sect2" id="id-1.3.5.10.9.5" data-id-title="Snapshot Basics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.2 </span><span class="title-name">Snapshot Basics</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <code class="command">rbd</code> command on the command line.
   </p><section class="sect3" id="id-1.3.5.10.9.5.3" data-id-title="Create Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.1 </span><span class="title-name">Create Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5.3">#</a></h4></div></div></div><p>
     To create a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     create</code> option, the pool name, and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap create --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap create <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool rbd snap create --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap create rbd/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.5.4" data-id-title="List Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.2 </span><span class="title-name">List Snapshots</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5.4">#</a></h4></div></div></div><p>
     To list snapshots of an image, specify the pool name and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap ls <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap ls <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool rbd snap ls image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap ls rbd/image1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.5.5" data-id-title="Rollback Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.3 </span><span class="title-name">Rollback Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5.5">#</a></h4></div></div></div><p>
     To rollback to a snapshot with <code class="command">rbd</code>, specify the
     <code class="option">snap rollback</code> option, the pool name, the image name, and
     the snapshot name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rollback --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rollback <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap rollback --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rollback pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.5.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <span class="emphasis"><em>faster to
      clone</em></span> from a snapshot <span class="emphasis"><em>than to rollback</em></span> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.5.6" data-id-title="Delete a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.4 </span><span class="title-name">Delete a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5.6">#</a></h4></div></div></div><p>
     To delete a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     rm</code> option, the pool name, the image name, and the user name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rm --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rm <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap rm --snap snapshot1 image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap rm pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.5.6.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.5.7" data-id-title="Purge Snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.2.5 </span><span class="title-name">Purge Snapshots</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.5.7">#</a></h4></div></div></div><p>
     To delete all snapshots for an image with <code class="command">rbd</code>, specify
     the <code class="option">snap purge</code> option and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap purge <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap purge <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap purge image1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap purge pool1/image1</pre></div></section></section><section class="sect2" id="ceph-snapshoti-layering" data-id-title="Layering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.3.3 </span><span class="title-name">Layering</span> <a title="Permalink" class="permalink" href="#ceph-snapshoti-layering">#</a></h3></div></div></div><p>
    Ceph supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables Ceph block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics—making it possible to create clones rapidly.
   </p><div id="id-1.3.5.10.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a Ceph block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </p></div><p>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </p><p>
    A COW clone of a snapshot behaves exactly like any other Ceph block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <span class="emphasis"><em>must</em></span> protect the snapshot before you clone it.
   </p><div id="id-1.3.5.10.9.6.6" data-id-title="--image-format 1 Not Supported" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">--image-format 1</code> Not Supported</h6><p>
     You cannot create snapshots of images created with the deprecated
     <code class="command">rbd create --image-format 1</code> option. Ceph only
     supports cloning of the default <span class="emphasis"><em>format 2</em></span> images.
    </p></div><section class="sect3" id="id-1.3.5.10.9.6.7" data-id-title="Getting Started with Layering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.1 </span><span class="title-name">Getting Started with Layering</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.6.7">#</a></h4></div></div></div><p>
     Ceph block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </p><p>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Image Template</em></span>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, SUSE Linux Enterprise Server), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <code class="command">zypper ref &amp;&amp; zypper patch</code>
       followed by <code class="command">rbd snap create</code>). As the image matures,
       the user can clone any one of the snapshots.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Extended Template</em></span>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Template Pool</em></span>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Image Migration/Recovery</em></span>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </p></li></ul></div></section><section class="sect3" id="id-1.3.5.10.9.6.8" data-id-title="Protecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.2 </span><span class="title-name">Protecting a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.6.8">#</a></h4></div></div></div><p>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap protect \
 --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap protect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap protect pool1/image1@snapshot1</pre></div><div id="id-1.3.5.10.9.6.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You cannot delete a protected snapshot.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.6.9" data-id-title="Cloning a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.3 </span><span class="title-name">Cloning a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.6.9">#</a></h4></div></div></div><p>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd clone --pool <em class="replaceable">pool-name</em> --image <em class="replaceable">parent-image</em> \
 --snap <em class="replaceable">snap-name</em> --dest-pool <em class="replaceable">pool-name</em> \
 --dest <em class="replaceable">child-image</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd clone <em class="replaceable">pool-name</em>/<em class="replaceable">parent-image</em>@<em class="replaceable">snap-name</em> \
<em class="replaceable">pool-name</em>/<em class="replaceable">child-image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd clone pool1/image1@snapshot1 pool1/image2</pre></div><div id="id-1.3.5.10.9.6.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </p></div></section><section class="sect3" id="id-1.3.5.10.9.6.10" data-id-title="Unprotecting a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.4 </span><span class="title-name">Unprotecting a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.6.10">#</a></h4></div></div></div><p>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <span class="emphasis"><em>not</em></span> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap unprotect --image <em class="replaceable">image-name</em> \
 --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd snap unprotect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd snap unprotect pool1/image1@snapshot1</pre></div></section><section class="sect3" id="id-1.3.5.10.9.6.11" data-id-title="Listing Children of a Snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.5 </span><span class="title-name">Listing Children of a Snapshot</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.9.6.11">#</a></h4></div></div></div><p>
     To list the children of a snapshot, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> children --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snap-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd children <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 children --image image1 --snap snapshot1
<code class="prompt user">cephadm@adm &gt; </code>rbd children pool1/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-flatten" data-id-title="Flattening a Cloned Image"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.3.3.6 </span><span class="title-name">Flattening a Cloned Image</span> <a title="Permalink" class="permalink" href="#rbd-flatten">#</a></h4></div></div></div><p>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> flatten --image <em class="replaceable">image-name</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd flatten <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool pool1 flatten --image image1
<code class="prompt user">cephadm@adm &gt; </code>rbd flatten pool1/image1</pre></div><div id="id-1.3.5.10.9.6.12.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </p></div></section></section></section><section class="sect1" id="ceph-rbd-mirror" data-id-title="Mirroring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.4 </span><span class="title-name">Mirroring</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror">#</a></h2></div></div></div><p>
   RBD images can be asynchronously mirrored between two Ceph clusters. This
   capability uses the RBD journaling image feature to ensure crash-consistent
   replication between clusters. Mirroring is configured on a per-pool basis
   within peer clusters and can be configured to automatically mirror all
   images within a pool or only a specific subset of images. Mirroring is
   configured using the <code class="command">rbd</code> command. The
   <code class="systemitem">rbd-mirror</code> daemon is responsible for pulling image
   updates from the remote peer cluster and applying them to the image within
   the local cluster.
  </p><div id="id-1.3.5.10.10.3" data-id-title="rbd-mirror Daemon" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: rbd-mirror Daemon</h6><p>
    To use RBD mirroring, you need to have two Ceph clusters, each running
    the <code class="systemitem">rbd-mirror</code> daemon.
   </p></div><div id="id-1.3.5.10.10.4" data-id-title="RADOS Block Devices Exported via iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: RADOS Block Devices Exported via iSCSI</h6><p>
    You cannot mirror RBD devices that are exported via iSCSI using
    kernel-based iSCSI Gateway.
   </p><p>
    Refer to <a class="xref" href="#cha-ceph-iscsi" title="Chapter 27. Ceph iSCSI Gateway">Chapter 27, <em>Ceph iSCSI Gateway</em></a> for more details on iSCSI.
   </p></div><section class="sect2" id="rbd-mirror-daemon" data-id-title="rbd-mirror Daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.1 </span><span class="title-name">rbd-mirror Daemon</span> <a title="Permalink" class="permalink" href="#rbd-mirror-daemon">#</a></h3></div></div></div><p>
    The two <code class="systemitem">rbd-mirror</code> daemons are responsible for
    watching image journals on the remote, peer cluster and replaying the
    journal events against the local cluster. The RBD image journaling feature
    records all modifications to the image in the order they occur. This
    ensures that a crash-consistent mirror of the remote image is available
    locally.
   </p><p>
    The <code class="systemitem">rbd-mirror</code> daemon is available in the
    <span class="package">rbd-mirror</span> package. You can install the package on OSD
    nodes, gateway nodes, or even on dedicated nodes. We do not recommend
    installing the <span class="package">rbd-mirror</span> on the Admin Node. Install, enable,
    and start <span class="package">rbd-mirror</span>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper install rbd-mirror
<code class="prompt user">root@minion &gt; </code>systemctl enable ceph-rbd-mirror@<em class="replaceable">server_name</em>.service
<code class="prompt user">root@minion &gt; </code>systemctl start ceph-rbd-mirror@<em class="replaceable">server_name</em>.service</pre></div><div id="id-1.3.5.10.10.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Each <code class="systemitem">rbd-mirror</code> daemon requires the ability to
     connect to both clusters simultaneously.
    </p></div></section><section class="sect2" id="ceph-rbd-mirror-poolconfig" data-id-title="Pool Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.2 </span><span class="title-name">Pool Configuration</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror-poolconfig">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <code class="command">rbd</code> command. Mirroring is configured on a per-pool basis
    within the Ceph clusters.
   </p><p>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named 'local' and 'remote', are
    accessible from a single host for clarity.
   </p><p>
    See the <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>)
    for additional details on how to connect to different Ceph clusters.
   </p><div id="id-1.3.5.10.10.6.5" data-id-title="Multiple Clusters" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple Clusters</h6><p>
     The cluster name in the following examples corresponds to a Ceph
     configuration file of the same name
     <code class="filename">/etc/ceph/remote.conf</code>.
    </p></div><section class="sect3" id="id-1.3.5.10.10.6.6" data-id-title="Enable Mirroring on a Pool"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.1 </span><span class="title-name">Enable Mirroring on a Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.6.6">#</a></h4></div></div></div><p>
     To enable mirroring on a pool, specify the <code class="command">mirror pool
     enable</code> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.10.6.6.3.1"><span class="term">pool</span></dt><dd><p>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </p></dd><dt id="id-1.3.5.10.10.6.6.3.2"><span class="term">image</span></dt><dd><p>
        Mirroring needs to be explicitly enabled on each image. See
        <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="23.4.3.2. Enable Image Mirroring">Section 23.4.3.2, “Enable Image Mirroring”</a> for more
        information.
       </p></dd></dl></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool enable <em class="replaceable">POOL_NAME</em> pool
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool enable <em class="replaceable">POOL_NAME</em> pool</pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.7" data-id-title="Disable Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.2 </span><span class="title-name">Disable Mirroring</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.6.7">#</a></h4></div></div></div><p>
     To disable mirroring on a pool, specify the <code class="command">mirror pool
     disable</code> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool disable <em class="replaceable">POOL_NAME</em>
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool disable <em class="replaceable">POOL_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.8" data-id-title="Add Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.3 </span><span class="title-name">Add Cluster Peer</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.6.8">#</a></h4></div></div></div><p>
     In order for the <code class="systemitem">rbd-mirror</code> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <code class="command">mirror pool peer
     add</code> subcommand, the pool name, and a cluster specification:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool peer add <em class="replaceable">POOL_NAME</em> client.remote@remote
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool peer add <em class="replaceable">POOL_NAME</em> client.local@local</pre></div></section><section class="sect3" id="id-1.3.5.10.10.6.9" data-id-title="Remove Cluster Peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.2.4 </span><span class="title-name">Remove Cluster Peer</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.6.9">#</a></h4></div></div></div><p>
     To remove a mirroring peer cluster, specify the <code class="command">mirror pool peer
     remove</code> subcommand, the pool name, and the peer UUID (available
     from the <code class="command">rbd mirror pool info</code> command):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 55672766-c02b-4729-8567-f13a66893445
<code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 60c0e299-b38f-4234-91f6-eed0a367be08</pre></div></section></section><section class="sect2" id="rbd-mirror-imageconfig" data-id-title="Image Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.3 </span><span class="title-name">Image Configuration</span> <a title="Permalink" class="permalink" href="#rbd-mirror-imageconfig">#</a></h3></div></div></div><p>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer Ceph cluster.
   </p><p>
    Mirrored RBD images are designated as either <span class="emphasis"><em>primary</em></span>
    or <span class="emphasis"><em>non-primary</em></span>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </p><p>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="23.4.3.2. Enable Image Mirroring">Section 23.4.3.2, “Enable Image Mirroring”</a>) by the
    <code class="command">rbd</code> command).
   </p><section class="sect3" id="id-1.3.5.10.10.7.5" data-id-title="Image Journaling Support"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.1 </span><span class="title-name">Image Journaling Support</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.7.5">#</a></h4></div></div></div><p>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. Before an image can be
     mirrored to a peer cluster, the journaling feature must be enabled. The
     feature can be enabled at the time of image creation by providing the
     <code class="option">--image-feature exclusive-lock,journaling</code> option to the
     <code class="command">rbd</code> command.
    </p><p>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <code class="command">feature enable</code> subcommand, the pool and image name, and
     the feature name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local feature enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> journaling</pre></div><div id="id-1.3.5.10.10.7.5.5" data-id-title="Option Dependency" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Option Dependency</h6><p>
      The <code class="option">journaling</code> feature is dependent on the
      <code class="option">exclusive-lock</code> feature. If the
      <code class="option">exclusive-lock</code> feature is not already enabled, you need
      to enable it prior to enabling the <code class="option">journaling</code> feature.
     </p></div><div id="id-1.3.5.10.10.7.5.6" data-id-title="Journaling on All New Images" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Journaling on All New Images</h6><p>
      You can enable journaling on all new images by default by appending the
      <code class="literal">journaling</code> value to the <code class="option">rbd default
      features</code> option in the Ceph configuration file. For example:
     </p><div class="verbatim-wrap"><pre class="screen">rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</pre></div><p>
      Before applying such a change, carefully consider if enabling journaling
      on all new images is good for your deployment, because it can have a
      negative performance impact.
     </p></div></section><section class="sect3" id="rbd-mirror-enable-image-mirroring" data-id-title="Enable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.2 </span><span class="title-name">Enable Image Mirroring</span> <a title="Permalink" class="permalink" href="#rbd-mirror-enable-image-mirroring">#</a></h4></div></div></div><p>
     If mirroring is configured in the 'image' mode, then it is necessary to
     explicitly enable mirroring for each image within the pool. To enable
     mirroring for a specific image, specify the <code class="command">mirror image
     enable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.7.7" data-id-title="Disable Image Mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.3 </span><span class="title-name">Disable Image Mirroring</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.7.7">#</a></h4></div></div></div><p>
     To disable mirroring for a specific image, specify the <code class="command">mirror
     image disable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image disable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="id-1.3.5.10.10.7.8" data-id-title="Image Promotion and Demotion"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.4 </span><span class="title-name">Image Promotion and Demotion</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.7.8">#</a></h4></div></div></div><p>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </p><div id="id-1.3.5.10.10.7.8.3" data-id-title="Forced Promotion" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Forced Promotion</h6><p>
      Promotion can be forced using the <code class="option">--force</code> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <code class="command">resync</code> subcommand is issued.
     </p></div><p>
     To demote a specific image to non-primary, specify the <code class="command">mirror
     image demote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror image demote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To demote all primary images within a pool to non-primary, specify the
     <code class="command">mirror pool demote</code> subcommand along with the pool name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool demote <em class="replaceable">POOL_NAME</em></pre></div><p>
     To promote a specific image to primary, specify the <code class="command">mirror image
     promote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster remote mirror image promote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To promote all non-primary images within a pool to primary, specify the
     <code class="command">mirror pool promote</code> subcommand along with the pool
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --cluster local mirror pool promote <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.5.10.10.7.8.12" data-id-title="Split I/O Load" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Split I/O Load</h6><p>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </p></div></section><section class="sect3" id="id-1.3.5.10.10.7.9" data-id-title="Force Image Resync"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">23.4.3.5 </span><span class="title-name">Force Image Resync</span> <a title="Permalink" class="permalink" href="#id-1.3.5.10.10.7.9">#</a></h4></div></div></div><p>
     If a split-brain event is detected by the
     <code class="systemitem">rbd-mirror</code> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out of date and then request a
     resync to the primary image. To request an image resync, specify the
     <code class="command">mirror image resync</code> subcommand along with the pool and
     image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror image resync <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section></section><section class="sect2" id="rbd-mirror-status" data-id-title="Mirror Status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.4 </span><span class="title-name">Mirror Status</span> <a title="Permalink" class="permalink" href="#rbd-mirror-status">#</a></h3></div></div></div><p>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <code class="command">mirror image
    status</code> and <code class="command">mirror pool status</code> subcommands:
   </p><p>
    To request the mirror image status, specify the <code class="command">mirror image
    status</code> subcommand along with the pool and image name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror image status <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
    To request the mirror pool summary status, specify the <code class="command">mirror pool
    status</code> subcommand along with the pool name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd mirror pool status <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.3.5.10.10.8.7" data-id-title="" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: </h6><p>
     Adding the <code class="option">--verbose</code> option to the <code class="command">mirror pool
     status</code> subcommand will additionally output status details for
     every mirroring image in the pool.
    </p></div></section></section><section class="sect1" id="rbd-cache-settings" data-id-title="Cache Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.5 </span><span class="title-name">Cache Settings</span> <a title="Permalink" class="permalink" href="#rbd-cache-settings">#</a></h2></div></div></div><p>
   The user space implementation of the Ceph block device
   (<code class="systemitem">librbd</code>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <span class="emphasis"><em>Least
   Recently Used</em></span> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </p><p>
   Ceph supports write-back caching for RBD. To enable it, add
  </p><div class="verbatim-wrap"><pre class="screen">[client]
...
rbd cache = true</pre></div><p>
   to the <code class="literal">[client]</code> section of your
   <code class="filename">ceph.conf</code> file. By default,
   <code class="systemitem">librbd</code> does not perform any caching. Writes and
   reads go directly to the storage cluster, and writes return only when the
   data is on disk on all replicas. With caching enabled, writes return
   immediately, unless there are more unflushed bytes than set in the
   <code class="option">rbd cache max dirty</code> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </p><p>
   Ceph supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, set
  </p><div class="verbatim-wrap"><pre class="screen">rbd cache max dirty = 0</pre></div><p>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </p><p>
   The <code class="filename">ceph.conf</code> file settings for RBD should be set in
   the <code class="literal">[client]</code> section of your configuration file. The
   settings include:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.11.10.1"><span class="term"><code class="option">rbd cache</code></span></dt><dd><p>
      Enable caching for RADOS Block Device (RBD). Default is 'true'.
     </p></dd><dt id="id-1.3.5.10.11.10.2"><span class="term"><code class="option">rbd cache size</code></span></dt><dd><p>
      The RBD cache size in bytes. Default is 32 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.3"><span class="term"><code class="option">rbd cache max dirty</code></span></dt><dd><p>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <code class="option">rbd cache max dirty</code> needs to be less than <code class="option">rbd
      cache size</code>. If set to 0, uses write-through caching. Default is
      24 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.4"><span class="term"><code class="option">rbd cache target dirty</code></span></dt><dd><p>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </p></dd><dt id="id-1.3.5.10.11.10.5"><span class="term"><code class="option">rbd cache max dirty age</code></span></dt><dd><p>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </p></dd><dt id="id-1.3.5.10.11.10.6"><span class="term"><code class="option">rbd cache writethrough until flush</code></span></dt><dd><p>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <code class="systemitem">rbd</code>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-qos" data-id-title="QoS Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.6 </span><span class="title-name">QoS Settings</span> <a title="Permalink" class="permalink" href="#rbd-qos">#</a></h2></div></div></div><p>
   Generallyi, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </p><div id="id-1.3.5.10.12.3" data-id-title="Not Supported by iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Not Supported by iSCSI</h6><p>
    The following QoS settings are used only by the userspace RBD
    implementation <code class="systemitem">librbd</code> and
    <span class="emphasis"><em>not</em></span> used by the <code class="systemitem">kRBD</code>
    implementation. Because iSCSI uses <code class="systemitem">kRBD</code>, it does
    not use the QoS settings. However, for iSCSI you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.12.4.1"><span class="term"><code class="option">rbd qos iops limit</code></span></dt><dd><p>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.2"><span class="term"><code class="option">rbd qos bps limit</code></span></dt><dd><p>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.3"><span class="term"><code class="option">rbd qos read iops limit</code></span></dt><dd><p>
      The desired limit of read operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.4"><span class="term"><code class="option">rbd qos write iops limit</code></span></dt><dd><p>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </p></dd><dt id="id-1.3.5.10.12.4.5"><span class="term"><code class="option">rbd qos read bps limit</code></span></dt><dd><p>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.6"><span class="term"><code class="option">rbd qos write bps limit</code></span></dt><dd><p>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.7"><span class="term"><code class="option">rbd qos iops burst</code></span></dt><dd><p>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.8"><span class="term"><code class="option">rbd qos bps burst</code></span></dt><dd><p>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.9"><span class="term"><code class="option">rbd qos read iops burst</code></span></dt><dd><p>
      The desired burst limit of read operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.10"><span class="term"><code class="option">rbd qos write iops burst</code></span></dt><dd><p>
      The desired burst limit of write operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.11"><span class="term"><code class="option">rbd qos read bps burst</code></span></dt><dd><p>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.12"><span class="term"><code class="option">rbd qos write bps burst</code></span></dt><dd><p>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.3.5.10.12.4.13"><span class="term"><code class="option">rbd qos schedule tick min</code></span></dt><dd><p>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </p></dd></dl></div></section><section class="sect1" id="rbd-readahead-settings" data-id-title="Read-ahead Settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.7 </span><span class="title-name">Read-ahead Settings</span> <a title="Permalink" class="permalink" href="#rbd-readahead-settings">#</a></h2></div></div></div><p>
   RADOS Block Device supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.13.3.1"><span class="term"><code class="option">rbd readahead trigger requests</code></span></dt><dd><p>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </p></dd><dt id="id-1.3.5.10.13.3.2"><span class="term"><code class="option">rbd readahead max bytes</code></span></dt><dd><p>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </p></dd><dt id="id-1.3.5.10.13.3.3"><span class="term"><code class="option">rbd readahead disable after bytes</code></span></dt><dd><p>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </p></dd></dl></div></section><section class="sect1" id="rbd-features" data-id-title="Advanced Features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.8 </span><span class="title-name">Advanced Features</span> <a title="Permalink" class="permalink" href="#rbd-features">#</a></h2></div></div></div><p>
   RADOS Block Device supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the Ceph configuration file by using the
   <code class="option">rbd_default_features</code> option.
  </p><p>
   You can specify the values of the <code class="option">rbd_default_features</code>
   option in two ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     As a sum of features' internal values. Each feature has its own internal
     value—for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = 17</pre></div></li><li class="listitem"><p>
     As a comma-separated list of features. The previous example will look as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = layering,fast-diff</pre></div></li></ul></div><div id="id-1.3.5.10.14.5" data-id-title="Features Not Supported by iSCSI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Features Not Supported by iSCSI</h6><p>
    RBD images with the following features will not be supported by iSCSI:
    <code class="option">deep-flatten</code>, <code class="option">object-map</code>,
    <code class="option">journaling</code>, <code class="option">fast-diff</code>,
    <code class="option">striping</code>
   </p></div><p>
   A list of advanced RBD features follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.10.14.7.1"><span class="term"><code class="option">layering</code></span></dt><dd><p>
      Layering enables you to use cloning.
     </p><p>
      Internal value is 1, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.2"><span class="term"><code class="option">striping</code></span></dt><dd><p>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy RADOS Block Devices.
     </p><p>
      Internal value is 2, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.3"><span class="term"><code class="option">exclusive-lock</code></span></dt><dd><p>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.4"><span class="term"><code class="option">object-map</code></span></dt><dd><p>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </p><p>
      Internal value is 8, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.5"><span class="term"><code class="option">fast-diff</code></span></dt><dd><p>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </p><p>
      Internal value is 16, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.6"><span class="term"><code class="option">deep-flatten</code></span></dt><dd><p>
      Deep-flatten makes the <code class="command">rbd flatten</code> (see
      <a class="xref" href="#rbd-flatten" title="23.3.3.6. Flattening a Cloned Image">Section 23.3.3.6, “Flattening a Cloned Image”</a>) work on all the snapshots of an image, in
      addition to the image itself. Without it, snapshots of an image will
      still rely on the parent, therefore you will not be able to delete the
      parent image until the snapshots are deleted. Deep-flatten makes a parent
      independent of its clones, even if they have snapshots.
     </p><p>
      Internal value is 32, default is 'yes'.
     </p></dd><dt id="id-1.3.5.10.14.7.7"><span class="term"><code class="option">journaling</code></span></dt><dd><p>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <a class="xref" href="#ceph-rbd-mirror" title="23.4. Mirroring">Section 23.4, “Mirroring”</a>) uses the journal to replicate a crash
      consistent image to a remote cluster.
     </p><p>
      Internal value is 64, default is 'no'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-old-clients-map" data-id-title="Mapping RBD Using Old Kernel Clients"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.9 </span><span class="title-name">Mapping RBD Using Old Kernel Clients</span> <a title="Permalink" class="permalink" href="#rbd-old-clients-map">#</a></h2></div></div></div><p>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with SUSE Enterprise Storage 6 forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000</pre></div><div id="id-1.3.5.10.15.4" data-id-title="Changing CRUSH Map Bucket Types Causes Massive Rebalancing" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Changing CRUSH Map Bucket Types Causes Massive Rebalancing</h6><p>
    If you intend to switch the CRUSH Map bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Disable any RBD image features that are not supported. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd feature disable pool1/image1 object-map
<code class="prompt user">cephadm@adm &gt; </code>rbd feature disable pool1/image1 exclusive-lock</pre></div></li><li class="step"><p>
     Change the CRUSH Map bucket types from 'straw2' to 'straw':
    </p><ol type="a" class="substeps"><li class="step"><p>
       Save the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd getcrushmap -o crushmap.original</pre></div></li><li class="step"><p>
       Decompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -d crushmap.original -o crushmap.txt</pre></div></li><li class="step"><p>
       Edit the CRUSH Map and replace 'straw2' with 'straw'.
      </p></li><li class="step"><p>
       Recompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>crushtool -c crushmap.txt -o crushmap.new</pre></div></li><li class="step"><p>
       Set the new CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd setcrushmap -i crushmap.new</pre></div></li></ol></li></ol></div></div></section></section><section class="chapter" id="cha-ceph-erasure" data-id-title="Erasure Coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24 </span><span class="title-name">Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure">#</a></h2></div></div></div><p>
  Ceph provides an alternative to the normal replication of data in pools,
  called <span class="emphasis"><em>erasure</em></span> or <span class="emphasis"><em>erasure coded</em></span>
  pool. Erasure pools do not provide all functionality of
  <span class="emphasis"><em>replicated</em></span> pools (for example, they cannot store
  metadata for RBD pools), but require less raw storage. A default erasure pool
  capable of storing 1 TB of data requires 1.5 TB of raw storage, allowing a
  single disk failure. This compares favorably to a replicated pool, which
  needs 2 TB of raw storage for the same purpose.
 </p><p>
  For background information on Erasure Code, see
  <a class="link" href="https://en.wikipedia.org/wiki/Erasure_code" target="_blank">https://en.wikipedia.org/wiki/Erasure_code</a>.
 </p><p>
  For a list of pool values related to EC pools, refer to
  <a class="xref" href="#pool-values-ec" title="Erasure Coded Pool Values">Erasure Coded Pool Values</a>.
 </p><div id="id-1.3.5.11.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   When using FileStore, you cannot access erasure coded pools with the RBD
   interface unless you have a cache tier configured. Refer to
   <span class="intraxref">Book “Tuning Guide”, Chapter 7 “Cache Tiering”, Section 7.5 “Erasure Coded Pool and Cache Tiering”</span> for more details, or use the default
   BlueStore (see <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SUSE Enterprise Storage 6 and Ceph”, Section 1.4 “BlueStore”</span>).
  </p></div><section class="sect1" id="ec-prerequisite" data-id-title="Prerequisite for Erasure Coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.1 </span><span class="title-name">Prerequisite for Erasure Coded Pools</span> <a title="Permalink" class="permalink" href="#ec-prerequisite">#</a></h2></div></div></div><p>
   To make use of erasure coding, you need to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Define an erasure rule in the CRUSH Map.
    </p></li><li class="listitem"><p>
     Define an erasure code profile that specifies the coding algorithm to be
     used.
    </p></li><li class="listitem"><p>
     Create a pool using the previously mentioned rule and profile.
    </p></li></ul></div><p>
   Keep in mind that changing the profile and the details in the profile will
   not be possible after the pool is created and has data.
  </p><p>
   Ensure that the CRUSH rules for <span class="emphasis"><em>erasure pools</em></span> use
   <code class="literal">indep</code> for <code class="literal">step</code>. For details see
   <a class="xref" href="#datamgm-rules-step-mode" title="20.3.2. firstn and indep">Section 20.3.2, “firstn and indep”</a>.
  </p></section><section class="sect1" id="cha-ceph-erasure-default-profile" data-id-title="Creating a Sample Erasure Coded Pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.2 </span><span class="title-name">Creating a Sample Erasure Coded Pool</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-default-profile">#</a></h2></div></div></div><p>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     The command <code class="command">ceph osd pool create</code> is used to create a
     pool with type <span class="emphasis"><em>erasure</em></span>. The <code class="literal">12</code>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</pre></div></li><li class="step"><p>
     The string <code class="literal">ABCDEFGHI</code> is written into an object called
     <code class="literal">NYAN</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -</pre></div></li><li class="step"><p>
     For testing purposes OSDs can now be disabled, for example by
     disconnecting them from the network.
    </p></li><li class="step"><p>
     To test whether the pool can handle the failure of devices, the content of
     the file can be accessed with the <code class="command">rados</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div></li></ol></div></div></section><section class="sect1" id="cha-ceph-erasure-erasure-profiles" data-id-title="Erasure Code Profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.3 </span><span class="title-name">Erasure Code Profiles</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-erasure-profiles">#</a></h2></div></div></div><p>
   When the <code class="command">ceph osd pool create</code> command is invoked to
   create an <span class="emphasis"><em>erasure pool</em></span>, the default profile is used,
   unless another profile is specified. Profiles define the redundancy of data.
   This is done by setting two parameters, arbitrarily named
   <code class="literal">k</code> and <code class="literal">m</code>. k and m define in how many
   <code class="literal">chunks</code> a piece of data is split and how many coding
   chunks are created. Redundant chunks are then stored on different OSDs.
  </p><p>
   Definitions required for erasure pool profiles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.11.9.4.1"><span class="term">chunk</span></dt><dd><p>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </p></dd><dt id="id-1.3.5.11.9.4.2"><span class="term">k</span></dt><dd><p>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example, if <code class="literal">k = 2</code> a 10
      kB object will be divided into <code class="literal">k</code> objects of 5 kB each.
      The default <code class="literal">min_size</code> on erasure coded pools is
      <code class="literal">k + 1</code>. However, we recommend
      <code class="literal">min_size</code> to be <code class="literal">k + 2</code> or more to
      prevent loss of writes and data.
     </p></dd><dt id="id-1.3.5.11.9.4.3"><span class="term">m</span></dt><dd><p>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </p></dd><dt id="id-1.3.5.11.9.4.4"><span class="term">crush-failure-domain</span></dt><dd><p>
      defines to which devices the chunks are distributed. A bucket type needs
      to be set as value. For all bucket types, see
      <a class="xref" href="#datamgm-buckets" title="20.2. Buckets">Section 20.2, “Buckets”</a>. If the failure domain is
      <code class="literal">rack</code>, the chunks will be stored on different racks to
      increase the resilience in case of rack failures. Keep in mind that this
      requires k+m racks.
     </p></dd></dl></div><p>
   With the default erasure code profile used in
   <a class="xref" href="#cha-ceph-erasure-default-profile" title="24.2. Creating a Sample Erasure Coded Pool">Section 24.2, “Creating a Sample Erasure Coded Pool”</a>, you will not lose
   cluster data if a single OSD or host fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage is
   required for 1 TB of data (because of k=2, m=1). This is equivalent to a
   common RAID 5 configuration. For comparison, a replicated pool needs 2 TB of
   raw storage to store 1 TB of data.
  </p><p>
   The settings of the default profile can be displayed with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</pre></div><p>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new one (see
   <a class="xref" href="#pools-migration" title="22.3. Pool Migration">Section 22.3, “Pool Migration”</a>).
  </p><p>
   The most important parameters of the profile are <code class="literal">k</code>,
   <code class="literal">m</code> and <code class="literal">crush-failure-domain</code> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66%, the following profile can be defined. Note that this is
   only valid with a CRUSH Map that has buckets of type 'rack':
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd erasure-code-profile set <em class="replaceable">myprofile</em> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</pre></div><p>
   The example <a class="xref" href="#cha-ceph-erasure-default-profile" title="24.2. Creating a Sample Erasure Coded Pool">Section 24.2, “Creating a Sample Erasure Coded Pool”</a> can be
   repeated with this new profile:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create ecpool 12 12 erasure <em class="replaceable">myprofile</em>
<code class="prompt user">cephadm@adm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<code class="prompt user">cephadm@adm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div><p>
   The NYAN object will be divided in three (<code class="literal">k=3</code>) and two
   additional chunks will be created (<code class="literal">m=2</code>). The value of
   <code class="literal">m</code> defines how many OSDs can be lost simultaneously
   without losing any data. The <code class="literal">crush-failure-domain=rack</code>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/ceph_erasure_obj.png" target="_blank"><img src="images/ceph_erasure_obj.png" width=""/></a></div></div><section class="sect2" id="ec-create" data-id-title="Creating a New Erasure Code Profile"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.3.1 </span><span class="title-name">Creating a New Erasure Code Profile</span> <a title="Permalink" class="permalink" href="#ec-create">#</a></h3></div></div></div><p>
    The following command creates a new erasure code profile:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd erasure-code-profile set <em class="replaceable">NAME</em> \
 directory=<em class="replaceable">DIRECTORY</em> \
 plugin=<em class="replaceable">PLUGIN</em> \
 stripe_unit=<em class="replaceable">STRIPE_UNIT</em> \
 <em class="replaceable">KEY</em>=<em class="replaceable">VALUE</em> ... \
 --force</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.11.9.15.4.1"><span class="term">DIRECTORY</span></dt><dd><p>
       Optional. Set the directory name from which the erasure code plugin is
       loaded. Default is <code class="filename">/usr/lib/ceph/erasure-code</code>.
      </p></dd><dt id="id-1.3.5.11.9.15.4.2"><span class="term">PLUGIN</span></dt><dd><p>
       Optional. Use the erasure code plugin to compute coding chunks and
       recover missing chunks. Available plugins are 'jerasure', 'isa', 'lrc',
       and 'shes'. Default is 'jerasure'.
      </p></dd><dt id="id-1.3.5.11.9.15.4.3"><span class="term">STRIPE_UNIT</span></dt><dd><p>
       Optional. The amount of data in a data chunk, per stripe. For example, a
       profile with 2 data chunks and stripe_unit=4K would put the range 0-4K
       in chunk 0, 4K-8K in chunk 1, then 8K-12K in chunk 0 again. This should
       be a multiple of 4K for best performance. The default value is taken
       from the monitor configuration option
       <code class="option">osd_pool_erasure_code_stripe_unit</code> when a pool is
       created. The 'stripe_width' of a pool using this profile will be the
       number of data chunks multiplied by this 'stripe_unit'.
      </p></dd><dt id="id-1.3.5.11.9.15.4.4"><span class="term">KEY=VALUE</span></dt><dd><p>
       Key/value pairs of options specific to the selected erasure code plugin.
      </p></dd><dt id="id-1.3.5.11.9.15.4.5"><span class="term">--force</span></dt><dd><p>
       Optional. Override an existing profile by the same name, and allow
       setting a non-4K-aligned stripe_unit.
      </p></dd></dl></div><div id="id-1.3.5.11.9.15.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     We strongly recommend that profiles are never modified. Instead, a new
     profile should be created and used when creating a new pool or creating a
     new rule for an existing pool. Seek expert advice before performing this
     action in specific circumstances.
    </p></div></section><section class="sect2" id="ec-rm" data-id-title="Removing an Erasure Code Profile"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.3.2 </span><span class="title-name">Removing an Erasure Code Profile</span> <a title="Permalink" class="permalink" href="#ec-rm">#</a></h3></div></div></div><p>
    The following command removes an erasure code profile as identified by its
    <em class="replaceable">NAME</em>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd erasure-code-profile rm <em class="replaceable">NAME</em></pre></div><div id="id-1.3.5.11.9.16.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     If the profile is referenced by a pool, the deletion will fail.
    </p></div></section><section class="sect2" id="ec-get" data-id-title="Displaying an Erasure Code Profiles Details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.3.3 </span><span class="title-name">Displaying an Erasure Code Profile's Details</span> <a title="Permalink" class="permalink" href="#ec-get">#</a></h3></div></div></div><p>
    The following command displays details of an erasure code profile as
    identified by its <em class="replaceable">NAME</em>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd erasure-code-profile get <em class="replaceable">NAME</em></pre></div></section><section class="sect2" id="ec-ls" data-id-title="Listing Erasure Code Profiles"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.3.4 </span><span class="title-name">Listing Erasure Code Profiles</span> <a title="Permalink" class="permalink" href="#ec-ls">#</a></h3></div></div></div><p>
    The following command lists the names of all erasure code profiles:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>ceph osd erasure-code-profile ls</pre></div></section></section><section class="sect1" id="ec-rbd" data-id-title="Erasure Coded Pools with RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.4 </span><span class="title-name">Erasure Coded Pools with RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ec-rbd">#</a></h2></div></div></div><p>
   To mark an EC pool as an RBD pool, tag it accordingly:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable <em class="replaceable">ec_pool_name</em> rbd</pre></div><p>
   RBD can store image <span class="emphasis"><em>data</em></span> in EC pools. However, the
   image header and metadata still need to be stored in a replicated pool.
   Assuming you have the pool named 'rbd' for this purpose:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create rbd/<em class="replaceable">image_name</em> --size 1T --data-pool <em class="replaceable">ec_pool_name</em></pre></div><p>
   You can use the image normally like any other image, except that all of the
   data will be stored in the <em class="replaceable">ec_pool_name</em> pool
   instead of 'rbd' pool.
  </p></section></section><section class="chapter" id="cha-ceph-configuration" data-id-title="Ceph Cluster Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25 </span><span class="title-name">Ceph Cluster Configuration</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration">#</a></h2></div></div></div><p>
  This chapter provides a list of important Ceph cluster settings and their
  description. The settings are sorted by topic.
 </p><section class="sect1" id="ceph-config-runtime" data-id-title="Runtime Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.1 </span><span class="title-name">Runtime Configuration</span> <a title="Permalink" class="permalink" href="#ceph-config-runtime">#</a></h2></div></div></div><p>
   <a class="xref" href="#ds-custom-cephconf" title="2.14. Adjusting ceph.conf with Custom Settings">Section 2.14, “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</a> describes how to make changes to the
   Ceph configuration file <code class="filename">ceph.conf</code>. However, the
   actual cluster behavior is determined not by the current state of the
   <code class="filename">ceph.conf</code> file but by the configuration of the running
   Ceph daemons, which is stored in memory.
  </p><p>
   You can query an individual Ceph daemon for a particular configuration
   setting using the <span class="emphasis"><em>admin socket</em></span> on the node where the
   daemon is running. For example, the following command gets the value of the
   <code class="option">osd_max_write_size</code> configuration parameter from the daemon
   named <code class="literal">osd.0</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</pre></div><p>
   You can also <span class="emphasis"><em>change</em></span> the daemons' settings at runtime.
   Remember that this change is temporary and will be lost after the next
   daemon restart. For example, the following command changes the
   <code class="option">osd_max_write_size</code> parameter to '50' for all OSDs in the
   cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell osd.* injectargs --osd_max_write_size 50</pre></div><div id="id-1.3.5.12.4.7" data-id-title="injectargs Is Not Reliable" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: <code class="command">injectargs</code> Is Not Reliable</h6><p>
    Unfortunately, changing the cluster settings with the
    <code class="command">injectargs</code> command is not 100% reliable. If you need to
    be sure that the changed parameter is active, change it in the
    configuration files on all cluster nodes and restart all daemons in the
    cluster.
   </p></div></section><section class="sect1" id="config-osd-and-bluestore" data-id-title="Ceph OSD and BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.2 </span><span class="title-name">Ceph OSD and BlueStore</span> <a title="Permalink" class="permalink" href="#config-osd-and-bluestore">#</a></h2></div></div></div><section class="sect2" id="config-auto-cache-sizing" data-id-title="Automatic Cache Sizing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">25.2.1 </span><span class="title-name">Automatic Cache Sizing</span> <a title="Permalink" class="permalink" href="#config-auto-cache-sizing">#</a></h3></div></div></div><p>
    BlueStore can be configured to automatically resize its caches when
    <code class="option">tc_malloc</code> is configured as the memory allocator and the
    <code class="option">bluestore_cache_autotune</code> setting is enabled. This option
    is currently enabled by default. BlueStore will attempt to keep OSD heap
    memory usage under a designated target size via the
    <code class="option">osd_memory_target</code> configuration option. This is a best
    effort algorithm and caches will not shrink smaller than the amount
    specified by <code class="option">osd_memory_cache_min</code>. Cache ratios will be
    chosen based on a hierarchy of priorities. If priority information is not
    available, the <code class="option">bluestore_cache_meta_ratio</code> and
    <code class="option">bluestore_cache_kv_ratio</code> options are used as fallbacks.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.5.2.3.1"><span class="term">bluestore_cache_autotune</span></dt><dd><p>
       Automatically tunes the ratios assigned to different BlueStore caches
       while respecting minimum values. Default is <code class="option">True</code>.
      </p></dd><dt id="id-1.3.5.12.5.2.3.2"><span class="term">osd_memory_target</span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, try to keep this
       many bytes mapped in memory.
      </p><div id="id-1.3.5.12.5.2.3.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        This may not exactly match the RSS memory usage of the process. While
        the total amount of heap memory mapped by the process should generally
        stay close to this target, there is no guarantee that the kernel will
        actually reclaim memory that has been unmapped.
       </p></div></dd><dt id="id-1.3.5.12.5.2.3.3"><span class="term">osd_memory_cache_min</span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, set the minimum
       amount of memory used for caches.
      </p><div id="id-1.3.5.12.5.2.3.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        Setting this value too low can result in significant cache thrashing.
       </p></div></dd></dl></div></section></section><section class="sect1" id="config-ogw" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.3 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#config-ogw">#</a></h2></div></div></div><p>
   You can influence the Object Gateway behavior by a number of options in the
   <code class="filename">/etc/ceph/ceph.conf</code> file under sections named
  </p><div class="verbatim-wrap"><pre class="screen">[client.radosgw.<em class="replaceable">INSTANCE_NAME</em>]</pre></div><p>
   If an option is not specified, its default value is used. A complete list of
   the Object Gateway options follows:
  </p><section class="sect2" id="id-1.3.5.12.6.5" data-id-title="General Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">25.3.1 </span><span class="title-name">General Settings</span> <a title="Permalink" class="permalink" href="#id-1.3.5.12.6.5">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.6.5.2.1"><span class="term">rgw_frontends</span></dt><dd><p>
       Configures the HTTP front-end(s). Specify multiple front-ends in a
       comma-delimited list. Each front-end configuration may include a list of
       options separated by spaces, where each option is in the form
       “key=value” or “key”. Default is
      </p><div class="verbatim-wrap"><pre class="screen">rgw_frontends = beast port=7480</pre></div></dd><dt id="id-1.3.5.12.6.5.2.2"><span class="term">rgw_data</span></dt><dd><p>
       Sets the location of the data files for the Object Gateway. Default is
       <code class="filename">/var/lib/ceph/radosgw/<em class="replaceable">CLUSTER_ID</em></code>.
      </p></dd><dt id="id-1.3.5.12.6.5.2.3"><span class="term">rgw_enable_apis</span></dt><dd><p>
       Enables the specified APIs. Default is 's3, swift, swift_auth, admin All
       APIs'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.4"><span class="term">rgw_cache_enabled</span></dt><dd><p>
       Enables or disables the Object Gateway cache. Default is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.5"><span class="term">rgw_cache_lru_size</span></dt><dd><p>
       The number of entries in the Object Gateway cache. Default is 10000.
      </p></dd><dt id="id-1.3.5.12.6.5.2.6"><span class="term">rgw_socket_path</span></dt><dd><p>
       The socket path for the domain socket.
       <code class="option">FastCgiExternalServer</code> uses this socket. If you do not
       specify a socket path, the Object Gateway will not run as an external server. The
       path you specify here needs to be the same as the path specified in the
       <code class="filename">rgw.conf</code> file.
      </p></dd><dt id="id-1.3.5.12.6.5.2.7"><span class="term">rgw_fcgi_socket_backlog</span></dt><dd><p>
       The socket backlog for fcgi. Default is 1024.
      </p></dd><dt id="id-1.3.5.12.6.5.2.8"><span class="term">rgw_host</span></dt><dd><p>
       The host for the Object Gateway instance. It can be an IP address or a host name.
       Default is 0.0.0.0
      </p></dd><dt id="id-1.3.5.12.6.5.2.9"><span class="term">rgw_port</span></dt><dd><p>
       The port number where the instance listens for requests. If not
       specified, the Object Gateway runs external FastCGI.
      </p></dd><dt id="id-1.3.5.12.6.5.2.10"><span class="term">rgw_dns_name</span></dt><dd><p>
       The DNS name of the served domain.
      </p></dd><dt id="id-1.3.5.12.6.5.2.11"><span class="term">rgw_script_uri</span></dt><dd><p>
       The alternative value for the SCRIPT_URI if not set in the request.
      </p></dd><dt id="id-1.3.5.12.6.5.2.12"><span class="term">rgw_request_uri</span></dt><dd><p>
       The alternative value for the REQUEST_URI if not set in the request.
      </p></dd><dt id="id-1.3.5.12.6.5.2.13"><span class="term">rgw_print_continue</span></dt><dd><p>
       Enable 100-continue if it is operational. Default is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.14"><span class="term">rgw_remote_addr_param</span></dt><dd><p>
       The remote address parameter. For example, the HTTP field containing the
       remote address, or the X-Forwarded-For address if a reverse proxy is
       operational. Default is REMOTE_ADDR.
      </p></dd><dt id="id-1.3.5.12.6.5.2.15"><span class="term">rgw_op_thread_timeout</span></dt><dd><p>
       The timeout in seconds for open threads. Default is 600.
      </p></dd><dt id="id-1.3.5.12.6.5.2.16"><span class="term">rgw_op_thread_suicide_timeout</span></dt><dd><p>
       The time timeout in seconds before the Object Gateway process dies. Disabled if
       set to 0 (default).
      </p></dd><dt id="id-1.3.5.12.6.5.2.17"><span class="term">rgw_thread_pool_size</span></dt><dd><p>
       Number of threads for the Beast server. Increase to a higher value if
       you need to serve more requests. Defaults to 100 threads.
      </p></dd><dt id="id-1.3.5.12.6.5.2.18"><span class="term">rgw_num_rados_handles</span></dt><dd><p>
       The number of RADOS cluster handles for Object Gateway. Each Object Gateway worker thread
       now gets to pick a RADOS handle for its lifetime. This option may be
       deprecated and removed in future releases. Default is 1.
      </p></dd><dt id="id-1.3.5.12.6.5.2.19"><span class="term">rgw_num_control_oids</span></dt><dd><p>
       The number of notification objects used for cache synchronization
       between different rgw instances. Default is 8.
      </p></dd><dt id="id-1.3.5.12.6.5.2.20"><span class="term">rgw_init_timeout</span></dt><dd><p>
       The number of seconds before the Object Gateway gives up on initialization.
       Default is 30.
      </p></dd><dt id="id-1.3.5.12.6.5.2.21"><span class="term">rgw_mime_types_file</span></dt><dd><p>
       The path and location of the MIME types. Used for Swift auto-detection
       of object types. Default is <code class="filename">/etc/mime.types</code>.
      </p></dd><dt id="id-1.3.5.12.6.5.2.22"><span class="term">rgw_gc_max_objs</span></dt><dd><p>
       The maximum number of objects that may be handled by garbage collection
       in one garbage collection processing cycle. Default is 32.
      </p></dd><dt id="id-1.3.5.12.6.5.2.23"><span class="term">rgw_gc_obj_min_wait</span></dt><dd><p>
       The minimum wait time before the object may be removed and handled by
       garbage collection processing. Default is 2 * 3600.
      </p></dd><dt id="id-1.3.5.12.6.5.2.24"><span class="term">rgw_gc_processor_max_time</span></dt><dd><p>
       The maximum time between the beginning of two consecutive garbage
       collection processing cycles. Default is 3600.
      </p></dd><dt id="id-1.3.5.12.6.5.2.25"><span class="term">rgw_gc_processor_period</span></dt><dd><p>
       The cycle time for garbage collection processing. Default is 3600.
      </p></dd><dt id="id-1.3.5.12.6.5.2.26"><span class="term">rgw_s3_success_create_obj_status</span></dt><dd><p>
       The alternate success status response for <code class="literal">create-obj</code>.
       Default is 0.
      </p></dd><dt id="id-1.3.5.12.6.5.2.27"><span class="term">rgw_resolve_cname</span></dt><dd><p>
       Whether the Object Gateway should use DNS CNAME record of the request host name
       field (if host name is not equal to the Object Gateway DNS name). Default is
       'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.28"><span class="term">rgw_obj_stripe_size</span></dt><dd><p>
       The size of an object stripe for Object Gateway objects. Default is 4 &lt;&lt;
       20.
      </p></dd><dt id="id-1.3.5.12.6.5.2.29"><span class="term">rgw_extended_http_attrs</span></dt><dd><p>
       Add a new set of attributes that can be set on an entity (for example, a
       user, a bucket, or an object). These extra attributes can be set through
       HTTP header fields when putting the entity or modifying it using the
       POST method. If set, these attributes will return as HTTP fields when
       requesting GET/HEAD on the entity. Default is 'content_foo, content_bar,
       x-foo-bar'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.30"><span class="term">rgw_exit_timeout_secs</span></dt><dd><p>
       Number of seconds to wait for a process before exiting unconditionally.
       Default is 120.
      </p></dd><dt id="id-1.3.5.12.6.5.2.31"><span class="term">rgw_get_obj_window_size</span></dt><dd><p>
       The window size in bytes for a single object request. Default is '16
       &lt;&lt; 20'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.32"><span class="term">rgw_get_obj_max_req_size</span></dt><dd><p>
       The maximum request size of a single GET operation sent to the Ceph
       Storage Cluster. Default is 4 &lt;&lt; 20.
      </p></dd><dt id="id-1.3.5.12.6.5.2.33"><span class="term">rgw_relaxed_s3_bucket_names</span></dt><dd><p>
       Enables relaxed S3 bucket name rules for US region buckets. Default is
       'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.34"><span class="term">rgw_list_buckets_max_chunk</span></dt><dd><p>
       The maximum number of buckets to retrieve in a single operation when
       listing user buckets. Default is 1000.
      </p></dd><dt id="id-1.3.5.12.6.5.2.35"><span class="term">rgw_override_bucket_index_max_shards</span></dt><dd><p>
       Represents the number of shards for the bucket index object. Setting 0
       (default) indicates there is no sharding. It is not recommended to set a
       value too large (for example 1000) as it increases the cost for bucket
       listing. This variable should be set in the client or global sections so
       that it is automatically applied to <code class="command">radosgw-admin</code>
       commands.
      </p></dd><dt id="id-1.3.5.12.6.5.2.36"><span class="term">rgw_curl_wait_timeout_ms</span></dt><dd><p>
       The timeout in milliseconds for certain <code class="command">curl</code> calls.
       Default is 1000.
      </p></dd><dt id="id-1.3.5.12.6.5.2.37"><span class="term">rgw_copy_obj_progress</span></dt><dd><p>
       Enables output of object progress during long copy operations. Default
       is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.38"><span class="term">rgw_copy_obj_progress_every_bytes</span></dt><dd><p>
       The minimum bytes between copy progress output. Default is 1024 * 1024.
      </p></dd><dt id="id-1.3.5.12.6.5.2.39"><span class="term">rgw_admin_entry</span></dt><dd><p>
       The entry point for an admin request URL. Default is 'admin'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.40"><span class="term">rgw_content_length_compat</span></dt><dd><p>
       Enable compatibility handling of FCGI requests with both CONTENT_LENGTH
       AND HTTP_CONTENT_LENGTH set. Default is 'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.41"><span class="term">rgw_bucket_quota_ttl</span></dt><dd><p>
       The amount of time in seconds for which cached quota information is
       trusted. After this timeout, the quota information will be re-fetched
       from the cluster. Default is 600.
      </p></dd><dt id="id-1.3.5.12.6.5.2.42"><span class="term">rgw_user_quota_bucket_sync_interval</span></dt><dd><p>
       The amount of time in seconds for which the bucket quota information is
       accumulated before synchronizing to the cluster. During this time, other
       Object Gateway instances will not see the changes in the bucket quota stats
       related to operations on this instance. Default is 180.
      </p></dd><dt id="id-1.3.5.12.6.5.2.43"><span class="term">rgw_user_quota_sync_interval</span></dt><dd><p>
       The amount of time in seconds for which user quota information is
       accumulated before synchronizing to the cluster. During this time, other
       Object Gateway instances will not see the changes in the user quota stats related
       to operations on this instance. Default is 180.
      </p></dd><dt id="id-1.3.5.12.6.5.2.44"><span class="term">rgw_bucket_default_quota_max_objects</span></dt><dd><p>
       Default maximum number of objects per bucket. It is set on new users if
       no other quota is specified, and has no effect on existing users. This
       variable should be set in the client or global sections so that it is
       automatically applied to <code class="command">radosgw-admin</code> commands.
       Default is -1.
      </p></dd><dt id="id-1.3.5.12.6.5.2.45"><span class="term">rgw_bucket_default_quota_max_size</span></dt><dd><p>
       Default maximum capacity per bucket in bytes. It is set on new users if
       no other quota is specified, and has no effect on existing users.
       Default is -1.
      </p></dd><dt id="id-1.3.5.12.6.5.2.46"><span class="term">rgw_user_default_quota_max_objects</span></dt><dd><p>
       Default maximum number of objects for a user. This includes all objects
       in all buckets owned by the user. It is set on new users if no other
       quota is specified, and has no effect on existing users. Default is -1.
      </p></dd><dt id="id-1.3.5.12.6.5.2.47"><span class="term">rgw_user_default_quota_max_size</span></dt><dd><p>
       The value for user maximum size quota in bytes set on new users if no
       other quota is specified. It has no effect on existing users. Default is
       -1.
      </p></dd><dt id="id-1.3.5.12.6.5.2.48"><span class="term">rgw_verify_ssl</span></dt><dd><p>
       Verify SSL certificates while making requests. Default is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.2.49"><span class="term">rgw_max_chunk_size</span></dt><dd><p>
       Maximum size of a chunk of data that will be read in a single operation.
       Increasing the value to 4 MB (4194304) will provide better performance
       when processing large objects. Default is 128 kB (131072).
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Multisite Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.5.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.12.6.5.3.2"><span class="term">rgw_zone</span></dt><dd><p>
       The name of the zone for the gateway instance. If no zone is set, a
       cluster-wide default can be configured with the <code class="command">radosgw-admin
       zone default</code> command.
      </p></dd><dt id="id-1.3.5.12.6.5.3.3"><span class="term">rgw_zonegroup</span></dt><dd><p>
       The name of the zonegroup for the gateway instance. If no zonegroup is
       set, a cluster-wide default can be configured with the
       <code class="command">radosgw-admin zonegroup default</code> command.
      </p></dd><dt id="id-1.3.5.12.6.5.3.4"><span class="term">rgw_realm</span></dt><dd><p>
       The name of the realm for the gateway instance. If no realm is set, a
       cluster-wide default can be configured with the<code class="command">radosgw-admin
       realm default</code> command.
      </p></dd><dt id="id-1.3.5.12.6.5.3.5"><span class="term">rgw_run_sync_thread</span></dt><dd><p>
       If there are other zones in the realm to synchronize from, spawn threads
       to handle the synchronization of data and metadata. Default is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.3.6"><span class="term">rgw_data_log_window</span></dt><dd><p>
       The data log entries window in seconds. Default is 30.
      </p></dd><dt id="id-1.3.5.12.6.5.3.7"><span class="term">rgw_data_log_changes_size</span></dt><dd><p>
       The number of in-memory entries to hold for the data changes log.
       Default is 1000.
      </p></dd><dt id="id-1.3.5.12.6.5.3.8"><span class="term">rgw_data_log_obj_prefix</span></dt><dd><p>
       The object name prefix for the data log. Default is 'data_log'.
      </p></dd><dt id="id-1.3.5.12.6.5.3.9"><span class="term">rgw_data_log_num_shards</span></dt><dd><p>
       The number of shards (objects) on which to keep the data changes log.
       Default is 128.
      </p></dd><dt id="id-1.3.5.12.6.5.3.10"><span class="term">rgw_md_log_max_shards</span></dt><dd><p>
       The maximum number of shards for the metadata log. Default is 64.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Swift Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.5.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.12.6.5.4.2"><span class="term">rgw_enforce_swift_acls</span></dt><dd><p>
       Enforces the Swift Access Control List (ACL) settings. Default is
       'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.4.3"><span class="term">rgw_swift_token_expiration</span></dt><dd><p>
       The time in seconds for expiring a Swift token. Default is 24 * 3600.
      </p></dd><dt id="id-1.3.5.12.6.5.4.4"><span class="term">rgw_swift_url</span></dt><dd><p>
       The URL for the Ceph Object Gateway Swift API.
      </p></dd><dt id="id-1.3.5.12.6.5.4.5"><span class="term">rgw_swift_url_prefix</span></dt><dd><p>
       The URL prefix for the Swift StorageURL that goes in front of the
       “/v1” part. This allows to run several Gateway instances on the same
       host. For compatibility, setting this configuration variable to empty
       causes the default “/swift” to be used. Use explicit prefix “/”
       to start StorageURL at the root.
      </p><div id="id-1.3.5.12.6.5.4.5.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
        Setting this option to “/” will not work if S3 API is enabled. Keep
        in mind that disabling S3 will make it impossible to deploy the Object Gateway
        in the multisite configuration!
       </p></div></dd><dt id="id-1.3.5.12.6.5.4.6"><span class="term">rgw_swift_auth_url</span></dt><dd><p>
       Default URL for verifying v1 authentication tokens when the internal
       Swift authentication is not used.
      </p></dd><dt id="id-1.3.5.12.6.5.4.7"><span class="term">rgw_swift_auth_entry</span></dt><dd><p>
       The entry point for a Swift authentication URL. Default is 'auth'.
      </p></dd><dt id="id-1.3.5.12.6.5.4.8"><span class="term">rgw_swift_versioning_enabled</span></dt><dd><p>
       Enables the Object Versioning of OpenStack Object Storage API. This
       allows clients to put the <code class="literal">X-Versions-Location</code>
       attribute on containers that should be versioned. The attribute
       specifies the name of container storing archived versions. It must be
       owned by the same user as the versioned container for reasons of access
       control verification—ACLs are <span class="emphasis"><em>not</em></span> taken into
       consideration. Those containers cannot be versioned by the S3 object
       versioning mechanism. Default is 'false'.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Logging Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.5.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.12.6.5.5.2"><span class="term">rgw_log_nonexistent_bucket</span></dt><dd><p>
       Enables the Object Gateway to log a request for a non-existent bucket. Default is
       'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.3"><span class="term">rgw_log_object_name</span></dt><dd><p>
       The logging format for an object name. See the manual page <code class="command">man
       1 date</code> for details about format specifiers. Default is
       '%Y-%m-%d-%H-%i-%n'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.4"><span class="term">rgw_log_object_name_utc</span></dt><dd><p>
       Whether a logged object name includes a UTC time. If set to 'false'
       (default), it uses the local time.
      </p></dd><dt id="id-1.3.5.12.6.5.5.5"><span class="term">rgw_usage_max_shards</span></dt><dd><p>
       The maximum number of shards for usage logging. Default is 32.
      </p></dd><dt id="id-1.3.5.12.6.5.5.6"><span class="term">rgw_usage_max_user_shards</span></dt><dd><p>
       The maximum number of shards used for a single user’s usage logging.
       Default is 1.
      </p></dd><dt id="id-1.3.5.12.6.5.5.7"><span class="term">rgw_enable_ops_log</span></dt><dd><p>
       Enable logging for each successful Object Gateway operation. Default is 'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.8"><span class="term">rgw_enable_usage_log</span></dt><dd><p>
       Enable the usage log. Default is 'false'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.9"><span class="term">rgw_ops_log_rados</span></dt><dd><p>
       Whether the operations log should be written to the Ceph Storage Cluster
       back-end. Default is 'true'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.10"><span class="term">rgw_ops_log_socket_path</span></dt><dd><p>
       The Unix domain socket for writing operations logs.
      </p></dd><dt id="id-1.3.5.12.6.5.5.11"><span class="term">rgw_ops_log_data_backlog</span></dt><dd><p>
       The maximum data backlog data size for operations logs written to a Unix
       domain socket. Default is 5 &lt;&lt; 20.
      </p></dd><dt id="id-1.3.5.12.6.5.5.12"><span class="term">rgw_usage_log_flush_threshold</span></dt><dd><p>
       The number of dirty merged entries in the usage log before flushing
       synchronously. Default is 1024.
      </p></dd><dt id="id-1.3.5.12.6.5.5.13"><span class="term">rgw_usage_log_tick_interval</span></dt><dd><p>
       Flush pending usage log data every 'n' seconds. Default is 30.
      </p></dd><dt id="id-1.3.5.12.6.5.5.14"><span class="term">rgw_log_http_headers</span></dt><dd><p>
       Comma-delimited list of HTTP headers to include in log entries. Header
       names are case-insensitive, and use the full header name with words
       separated by underscores. For example, 'http_x_forwarded_for',
       'http_x_special_k'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.15"><span class="term">rgw_intent_log_object_name</span></dt><dd><p>
       The logging format for the intent log object name. See the manual page
       <code class="command">man 1 date</code> for details about format specifiers.
       Default is '%Y-%m-%d-%i-%n'.
      </p></dd><dt id="id-1.3.5.12.6.5.5.16"><span class="term">rgw_intent_log_object_name_utc</span></dt><dd><p>
       Whether the intent log object name includes a UTC time. If set to
       'false' (default), it uses the local time.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Keystone Settings </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.5.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.5.12.6.5.6.2"><span class="term">rgw_keystone_url</span></dt><dd><p>
       The URL for the Keystone server.
      </p></dd><dt id="id-1.3.5.12.6.5.6.3"><span class="term">rgw_keystone_api_version</span></dt><dd><p>
       The version (2 or 3) of OpenStack Identity API that should be used for
       communication with the Keystone server. Default is 2.
      </p></dd><dt id="id-1.3.5.12.6.5.6.4"><span class="term">rgw_keystone_admin_domain</span></dt><dd><p>
       The name of the OpenStack domain with the administrator privilege when
       using OpenStack Identity API v3.
      </p></dd><dt id="id-1.3.5.12.6.5.6.5"><span class="term">rgw_keystone_admin_project</span></dt><dd><p>
       The name of the OpenStack project with the administrator privilege when
       using OpenStack Identity API v3. If not set, the value of the
       <code class="command">rgw keystone admin tenant</code> will be used instead.
      </p></dd><dt id="id-1.3.5.12.6.5.6.6"><span class="term">rgw_keystone_admin_token</span></dt><dd><p>
       The Keystone administrator token (shared secret). In the Object Gateway,
       authentication with the administrator token has priority over
       authentication with the administrator credentials (options <code class="option">rgw
       keystone admin user</code>, <code class="option">rgw keystone admin
       password</code>, <code class="option">rgw keystone admin tenant</code>,
       <code class="option">rgw keystone admin project</code>, and <code class="option">rgw keystone
       admin domain</code>). The administrator token feature is considered as
       deprecated.
      </p></dd><dt id="id-1.3.5.12.6.5.6.7"><span class="term">rgw_keystone_admin_tenant</span></dt><dd><p>
       The name of the OpenStack tenant with the administrator privilege
       (Service Tenant) when using OpenStack Identity API v2.
      </p></dd><dt id="id-1.3.5.12.6.5.6.8"><span class="term">rgw_keystone_admin_user</span></dt><dd><p>
       The name of the OpenStack user with the administrator privilege for
       Keystone authentication (Service User) when using OpenStack Identity API
       v2.
      </p></dd><dt id="id-1.3.5.12.6.5.6.9"><span class="term">rgw_keystone_admin_password</span></dt><dd><p>
       The password for the OpenStack administrator user when using OpenStack
       Identity API v2.
      </p></dd><dt id="id-1.3.5.12.6.5.6.10"><span class="term">rgw_keystone_accepted_roles</span></dt><dd><p>
       The roles required to serve requests. Default is 'Member, admin'.
      </p></dd><dt id="id-1.3.5.12.6.5.6.11"><span class="term">rgw_keystone_token_cache_size</span></dt><dd><p>
       The maximum number of entries in each Keystone token cache. Default is
       10000.
      </p></dd><dt id="id-1.3.5.12.6.5.6.12"><span class="term">rgw_keystone_revocation_interval</span></dt><dd><p>
       The number of seconds between token revocation checks. Default is 15 *
       60.
      </p></dd><dt id="id-1.3.5.12.6.5.6.13"><span class="term">rgw_keystone_verify_ssl</span></dt><dd><p>
       Verify SSL certificates while making token requests to Keystone. Default
       is 'true'.
      </p></dd></dl></div><section class="sect3" id="sec-ceph-rgw-configuration-notes" data-id-title="Additional Notes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">25.3.1.1 </span><span class="title-name">Additional Notes</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-configuration-notes">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.6.5.7.2.1"><span class="term">rgw_dns_name</span></dt><dd><p>
        If the parameter <code class="literal">rgw dns name</code> is added to the
        <code class="filename">ceph.conf</code>, make sure that the S3 client is
        configured to direct requests at the endpoint specified by <code class="literal">rgw
        dns name</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="config-ogw-http-frontends" data-id-title="HTTP Front-ends"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">25.3.2 </span><span class="title-name">HTTP Front-ends</span> <a title="Permalink" class="permalink" href="#config-ogw-http-frontends">#</a></h3></div></div></div><section class="sect3" id="id-1.3.5.12.6.6.2" data-id-title="Beast"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">25.3.2.1 </span><span class="title-name">Beast</span> <a title="Permalink" class="permalink" href="#id-1.3.5.12.6.6.2">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.6.6.2.2.1"><span class="term">port, ssl_port</span></dt><dd><p>
        IPv4 &amp; IPv6 listening port numbers. You can specify multiple port
        numbers:
       </p><div class="verbatim-wrap"><pre class="screen">port=80 port=8000 ssl_port=8080</pre></div><p>
        Default is 80.
       </p></dd><dt id="id-1.3.5.12.6.6.2.2.2"><span class="term">endpoint, ssl_endpoint</span></dt><dd><p>
        The listening addresses in the form 'address[:port]', where the address
        is an IPv4 address string in dotted decimal form, or an IPv6 address in
        hexadecimal notation surrounded by square brackets. Specifying an IPv6
        endpoint would listen to IPv6 only. The optional port number defaults
        to 80 for <code class="option">endpoint</code> and 443 for
        <code class="option">ssl_endpoint</code>. You can specify multiple addresses:
       </p><div class="verbatim-wrap"><pre class="screen">endpoint=[::1] endpoint=192.168.0.100:8000 ssl_endpoint=192.168.0.100:8080</pre></div></dd><dt id="id-1.3.5.12.6.6.2.2.3"><span class="term">ssl_private_key</span></dt><dd><p>
        Optional path to the private key file used for SSL-enabled endpoints.
        If not specified, the <code class="option">ssl_certificate</code> file is used as
        a private key.
       </p></dd><dt id="id-1.3.5.12.6.6.2.2.4"><span class="term">tcp_nodelay</span></dt><dd><p>
        If specified, the socket option will disable Nagle's algorithm on the
        connection. It means that packets will be sent as soon as possible
        instead of waiting for a full buffer or timeout to occur.
       </p><p>
        '1' disables Nagle's algorithm for all sockets.
       </p><p>
        '0' keeps Nagle's algorithm enabled (default).
       </p></dd></dl></div><div class="example" id="id-1.3.5.12.6.6.2.3" data-id-title="Example Beast Configuration in /etc/ceph/ceph.conf"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 25.1: </span><span class="title-name">Example Beast Configuration in <code class="filename">/etc/ceph/ceph.conf</code> </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.6.2.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">rgw_frontends = beast port=8000 ssl_port=443 ssl_certificate=/etc/ssl/ssl.crt error_log_file=/var/log/radosgw/civetweb.error.log</pre></div></div></div></section><section class="sect3" id="id-1.3.5.12.6.6.3" data-id-title="CivetWeb"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">25.3.2.2 </span><span class="title-name">CivetWeb</span> <a title="Permalink" class="permalink" href="#id-1.3.5.12.6.6.3">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.6.6.3.2.1"><span class="term">port</span></dt><dd><p>
        The listening port number. For SSL-enabled ports, add an 's' suffix
        (for example, '443s'). To bind a specific IPv4 or IPv6 address, use the
        form 'address:port'. You can specify multiple endpoints either by
        joining them with '+' or by providing multiple options:
       </p><div class="verbatim-wrap"><pre class="screen">port=127.0.0.1:8000+443s
port=8000 port=443s</pre></div><p>
        Default is 7480.
       </p></dd><dt id="id-1.3.5.12.6.6.3.2.2"><span class="term">num_threads</span></dt><dd><p>
        The number of threads spawned by Civetweb to handle incoming HTTP
        connections. This effectively limits the number of concurrent
        connections that the front-end can service.
       </p><p>
        Default is the value specified by the
        <code class="option">rgw_thread_pool_size</code> option.
       </p></dd><dt id="id-1.3.5.12.6.6.3.2.3"><span class="term">request_timeout_ms</span></dt><dd><p>
        The amount of time in milliseconds that Civetweb will wait for more
        incoming data before giving up.
       </p><p>
        Default is 30000 milliseconds.
       </p></dd><dt id="id-1.3.5.12.6.6.3.2.4"><span class="term">access_log_file</span></dt><dd><p>
        Path to the access log file. You can specify either a full path, or a
        path relative to the current working directory. If not specified
        (default), then accesses are not logged.
       </p></dd><dt id="id-1.3.5.12.6.6.3.2.5"><span class="term">error_log_file</span></dt><dd><p>
        Path to the error log file. You can specify either a full path, or a
        path relative to the current working directory. If not specified
        (default), then errors are not logged.
       </p></dd></dl></div><div class="example" id="id-1.3.5.12.6.6.3.3" data-id-title="Example Civetweb Configuration in /etc/ceph/ceph.conf"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 25.2: </span><span class="title-name">Example Civetweb Configuration in <code class="filename">/etc/ceph/ceph.conf</code> </span><a title="Permalink" class="permalink" href="#id-1.3.5.12.6.6.3.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">rgw_frontends = civetweb port=8000+443s request_timeout_ms=30000 error_log_file=/var/log/radosgw/civetweb.error.log</pre></div></div></div></section><section class="sect3" id="id-1.3.5.12.6.6.4" data-id-title="Common Options"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">25.3.2.3 </span><span class="title-name">Common Options</span> <a title="Permalink" class="permalink" href="#id-1.3.5.12.6.6.4">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.5.12.6.6.4.2.1"><span class="term">ssl_certificate</span></dt><dd><p>
        Path to the SSL certificate file used for SSL-enabled endpoints.
       </p></dd><dt id="id-1.3.5.12.6.6.4.2.2"><span class="term">prefix</span></dt><dd><p>
        A prefix string that is inserted into the URI of all requests. For
        example, a Swift-only front-end could supply a URI prefix of
        '/swift'.
       </p></dd></dl></div></section></section></section></section></div><div class="part" id="part-dataccess" data-id-title="Accessing Cluster Data"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part IV </span><span class="title-name">Accessing Cluster Data </span><a title="Permalink" class="permalink" href="#part-dataccess">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span></a></span></li><dd class="toc-abstract"><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span>.
  
 </p></dd><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">28 </span><span class="title-name">Clustered File System</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
 </p></dd><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span></a></span></li><dd class="toc-abstract"><p>This chapter describes how to export data stored in a Ceph cluster via a Samba/CIFS share so that you can easily access them from Windows* client machines. It also includes information that will help you configure a Ceph Samba gateway to join Active Directory in the Windows* domain to authenticate a…</p></dd><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span></a></span></li><dd class="toc-abstract"><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-gw" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-gw">#</a></h2></div></div></div><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p><section class="sect1" id="sec-ceph-rgw-limits" data-id-title="Object Gateway Restrictions and Naming Limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.1 </span><span class="title-name">Object Gateway Restrictions and Naming Limitations</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-limits">#</a></h2></div></div></div><p>
   Following is a list of important Object Gateway limits:
  </p><section class="sect2" id="ogw-limits-bucket" data-id-title="Bucket Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.1.1 </span><span class="title-name">Bucket Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-bucket">#</a></h3></div></div></div><p>
    When approaching Object Gateway via the S3 API, bucket names are limited to
    DNS-compliant names with a dash character '-' allowed. When approaching
    Object Gateway via the Swift API, you may use any combination of UTF-8 supported
    characters except for a slash character '/'. The maximum length of a bucket
    name is 255 characters. Bucket names must be unique.
   </p><div id="id-1.3.6.2.4.3.3" data-id-title="Use DNS-compliant Bucket Names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use DNS-compliant Bucket Names</h6><p>
     Although you may use any UTF-8 based bucket name via the Swift API, it
     is recommended to name buckets with regard to the S3 naming limitations to
     avoid problems accessing the same bucket via the S3 API.
    </p></div></section><section class="sect2" id="ogw-limits-object" data-id-title="Stored Object Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.1.2 </span><span class="title-name">Stored Object Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-object">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.4.4.2.1"><span class="term">Maximum number of objects per user</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.3.6.2.4.4.2.2"><span class="term">Maximum number of objects per bucket</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.3.6.2.4.4.2.3"><span class="term">Maximum size of an object to upload/store</span></dt><dd><p>
       Single uploads are restricted to 5 GB. Use multipart for larger object
       sizes. The maximum number of multipart chunks is 10000.
      </p></dd></dl></div></section><section class="sect2" id="ogw-limits-http" data-id-title="HTTP Header Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.1.3 </span><span class="title-name">HTTP Header Limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-http">#</a></h3></div></div></div><p>
    HTTP header and request limitation depend on the Web front-end used. The
    default Beast restricts the size of the HTTP header to 16 kB.
   </p></section></section><section class="sect1" id="ogw-deploy" data-id-title="Deploying the Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.2 </span><span class="title-name">Deploying the Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-deploy">#</a></h2></div></div></div><p>
   The recommended way of deploying the Ceph Object Gateway is via the DeepSea
   infrastructure by adding the relevant <code class="literal">role-rgw [...]</code>
   line(s) into the <code class="filename">policy.cfg</code> file on the Salt master, and
   running the required DeepSea stages.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     To include the Object Gateway during the Ceph cluster deployment process, refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.3 “Cluster Deployment”</span> and
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.5.1 “The <code class="filename">policy.cfg</code> File”</span>.
    </p></li><li class="listitem"><p>
     To add the Object Gateway role to an already deployed cluster, refer to
     <a class="xref" href="#salt-adding-services" title="2.2. Adding New Roles to Nodes">Section 2.2, “Adding New Roles to Nodes”</a>.
    </p></li></ul></div></section><section class="sect1" id="ceph-rgw-operating" data-id-title="Operating the Object Gateway Service"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.3 </span><span class="title-name">Operating the Object Gateway Service</span> <a title="Permalink" class="permalink" href="#ceph-rgw-operating">#</a></h2></div></div></div><p>
   The Object Gateway service is operated with the <code class="command">systemctl</code> command.
   You need to have <code class="systemitem">root</code> privileges to operate the Object Gateway service. Note
   that <em class="replaceable">GATEWAY_HOST</em> is the host name of the server
   whose Object Gateway instance you need to operate.
  </p><p>
   The following subcommands are supported for the Object Gateway service:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.6.4.1"><span class="term">systemctl status ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Prints the status information of the service.
     </p></dd><dt id="id-1.3.6.2.6.4.2"><span class="term">systemctl start ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Starts the service if it is not already running.
     </p></dd><dt id="id-1.3.6.2.6.4.3"><span class="term">systemctl restart ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Restarts the service.
     </p></dd><dt id="id-1.3.6.2.6.4.4"><span class="term">systemctl stop ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Stops the running service.
     </p></dd><dt id="id-1.3.6.2.6.4.5"><span class="term">systemctl enable ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Enables the service so that it is automatically started on system
      start-up.
     </p></dd><dt id="id-1.3.6.2.6.4.6"><span class="term">systemctl disable ceph-radosgw@rgw.<em class="replaceable">GATEWAY_HOST</em></span></dt><dd><p>
      Disables the service so that it is not automatically started on system
      start-up.
     </p></dd></dl></div></section><section class="sect1" id="ogw-config-parameters" data-id-title="Configuration Options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.4 </span><span class="title-name">Configuration Options</span> <a title="Permalink" class="permalink" href="#ogw-config-parameters">#</a></h2></div></div></div><p>
   Refer to <a class="xref" href="#config-ogw" title="25.3. Ceph Object Gateway">Section 25.3, “Ceph Object Gateway”</a> for a list of Object Gateway configuration
   options.
  </p></section><section class="sect1" id="ceph-rgw-access" data-id-title="Managing Object Gateway Access"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.5 </span><span class="title-name">Managing Object Gateway Access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-access">#</a></h2></div></div></div><p>
   You can communicate with Object Gateway using either S3- or Swift-compatible
   interface. S3 interface is compatible with a large subset of the Amazon S3
   RESTful API. Swift interface is compatible with a large subset of the
   OpenStack Swift API.
  </p><p>
   Both interfaces require you to create a specific user, and install the
   relevant client software to communicate with the gateway using the user's
   secret key.
  </p><section class="sect2" id="accessing-ragos-gateway" data-id-title="Accessing Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.5.1 </span><span class="title-name">Accessing Object Gateway</span> <a title="Permalink" class="permalink" href="#accessing-ragos-gateway">#</a></h3></div></div></div><section class="sect3" id="id-1.3.6.2.8.4.2" data-id-title="S3 Interface Access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.1.1 </span><span class="title-name">S3 Interface Access</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.4.2">#</a></h4></div></div></div><p>
     To access the S3 interface, you need a REST client.
     <code class="command">S3cmd</code> is a command line S3 client. You can find it in
     the
     <a class="link" href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd" target="_blank">OpenSUSE
     Build Service</a>. The repository contains versions for both SUSE Linux Enterprise and
     openSUSE based distributions.
    </p><p>
     If you want to test your access to the S3 interface, you can also write a
     small a Python script. The script will connect to Object Gateway, create a new
     bucket, and list all buckets. The values for
     <code class="option">aws_access_key_id</code> and
     <code class="option">aws_secret_access_key</code> are taken from the values of
     <code class="option">access_key</code> and <code class="option">secret_key</code> returned by
     the <code class="command">radosgw_admin</code> command from
     <a class="xref" href="#adding-s3-swift-users" title="26.5.2.1. Adding S3 and Swift Users">Section 26.5.2.1, “Adding S3 and Swift Users”</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install the <code class="systemitem">python-boto</code> package:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in python-boto</pre></div></li><li class="step"><p>
       Create a new Python script called <code class="filename">s3test.py</code> with
       the following content:
       
      </p><div class="verbatim-wrap"><pre class="screen">import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '<em class="replaceable">HOSTNAME</em>',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
  print "<em class="replaceable">NAME</em>\t<em class="replaceable">CREATED</em>".format(
  name = bucket.name,
  created = bucket.creation_date,
  )</pre></div><p>
       Replace <code class="literal"><em class="replaceable">HOSTNAME</em></code> with the
       host name of the host where you configured the Object Gateway service, for
       example <code class="literal">gateway_host</code>.
      </p></li><li class="step"><p>
       Run the script:
      </p><div class="verbatim-wrap"><pre class="screen">python s3test.py</pre></div><p>
       The script outputs something like the following:
      </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket 2015-07-22T15:37:42.000Z</pre></div></li></ol></div></div></section><section class="sect3" id="id-1.3.6.2.8.4.3" data-id-title="Swift Interface Access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.1.2 </span><span class="title-name">Swift Interface Access</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.8.4.3">#</a></h4></div></div></div><p>
     To access Object Gateway via Swift interface, you need the <code class="command">swift</code>
     command line client. Its manual page <code class="command">man 1 swift</code> tells
     you more about its command line options.
    </p><p>
     The package is included in the 'Public Cloud' module for SUSE Linux Enterprise 12 from SP3
     and SUSE Linux Enterprise 15. Before installing the package, you need to activate the
     module and refresh the software repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>SUSEConnect -p sle-module-public-cloud/12/<em class="replaceable">SYSTEM-ARCH</em>
sudo zypper refresh</pre></div><p>
     Or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>SUSEConnect -p sle-module-public-cloud/15/<em class="replaceable">SYSTEM-ARCH</em>
<code class="prompt user">root # </code>zypper refresh</pre></div><p>
     To install the <code class="command">swift</code> command, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in python-swiftclient</pre></div><p>
     The swift access uses the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>swift -A http://<em class="replaceable">IP_ADDRESS</em>/auth/1.0 \
-U example_user:swift -K '<em class="replaceable">SWIFT_SECRET_KEY</em>' list</pre></div><p>
     Replace <em class="replaceable">IP_ADDRESS</em> with the IP address of the
     gateway server, and <em class="replaceable">SWIFT_SECRET_KEY</em> with its
     value from the output of the <code class="command">radosgw-admin key create</code>
     command executed for the <code class="systemitem">swift</code> user in
     <a class="xref" href="#adding-s3-swift-users" title="26.5.2.1. Adding S3 and Swift Users">Section 26.5.2.1, “Adding S3 and Swift Users”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</pre></div><p>
     The output is:
    </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket</pre></div></section></section><section class="sect2" id="s3-swift-accounts-managment" data-id-title="Managing S3 and Swift Accounts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.5.2 </span><span class="title-name">Managing S3 and Swift Accounts</span> <a title="Permalink" class="permalink" href="#s3-swift-accounts-managment">#</a></h3></div></div></div><section class="sect3" id="adding-s3-swift-users" data-id-title="Adding S3 and Swift Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.2.1 </span><span class="title-name">Adding S3 and Swift Users</span> <a title="Permalink" class="permalink" href="#adding-s3-swift-users">#</a></h4></div></div></div><p>
     You need to create a user, access key and secret to enable end users to
     interact with the gateway. There are two types of users: a
     <span class="emphasis"><em>user</em></span> and <span class="emphasis"><em>subuser</em></span>. While
     <span class="emphasis"><em>users</em></span> are used when interacting with the S3
     interface, <span class="emphasis"><em>subusers</em></span> are users of the Swift
     interface. Each subuser is associated to a user.
    </p><p>
     Users can also be added via the DeepSea file
     <code class="filename">rgw.sls</code>. For an example, see
     <a class="xref" href="#ceph-nfsganesha-customrole-rgw-multiusers" title="30.3.1. Different Object Gateway Users for NFS Ganesha">Section 30.3.1, “Different Object Gateway Users for NFS Ganesha”</a>.
    </p><p>
     To create a Swift user, follow the steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To create a Swift user—which is a <span class="emphasis"><em>subuser</em></span>
       in our terminology—you need to create the associated
       <span class="emphasis"><em>user</em></span> first.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USERNAME</em> \
 --display-name="<em class="replaceable">DISPLAY-NAME</em>" --email=<em class="replaceable">EMAIL</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div></li><li class="step"><p>
       To create a subuser (Swift interface) for the user, you must specify
       the user ID (--uid=<em class="replaceable">USERNAME</em>), a subuser ID,
       and the access level for the subuser.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin subuser create --uid=<em class="replaceable">UID</em> \
 --subuser=<em class="replaceable">UID</em> \
 --access=[ <em class="replaceable">read | write | readwrite | full</em> ]</pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</pre></div></li><li class="step"><p>
       Generate a secret key for the user.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</pre></div></li><li class="step"><p>
       Both commands will output JSON-formatted data showing the user state.
       Notice the following lines, and remember the
       <code class="literal">secret_key</code> value:
      </p><div class="verbatim-wrap"><pre class="screen">"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</pre></div></li></ol></div></div><p>
     When accessing Object Gateway through the S3 interface you need to create an S3
     user by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USERNAME</em> \
 --display-name="<em class="replaceable">DISPLAY-NAME</em>" --email=<em class="replaceable">EMAIL</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div><p>
     The command also creates the user's access and secret key. Check its
     output for <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     keywords and their values:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</pre></div></section><section class="sect3" id="removing-s3-swift-users" data-id-title="Removing S3 and Swift Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.2.2 </span><span class="title-name">Removing S3 and Swift Users</span> <a title="Permalink" class="permalink" href="#removing-s3-swift-users">#</a></h4></div></div></div><p>
     The procedure for deleting users is similar for S3 and Swift users. But
     in case of Swift users you may need to delete the user including its
     subusers.
    </p><p>
     To remove a S3 or Swift user (including all its subusers), specify
     <code class="option">user rm</code> and the user ID in the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user rm --uid=example_user</pre></div><p>
     To remove a subuser, specify <code class="option">subuser rm</code> and the subuser
     ID.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin subuser rm --uid=example_user:swift</pre></div><p>
     You can make use of the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.8.5.3.8.1"><span class="term">--purge-data</span></dt><dd><p>
        Purges all data associated to the user ID.
       </p></dd><dt id="id-1.3.6.2.8.5.3.8.2"><span class="term">--purge-keys</span></dt><dd><p>
        Purges all keys associated to the user ID.
       </p></dd></dl></div><div id="id-1.3.6.2.8.5.3.9" data-id-title="Removing a Subuser" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a Subuser</h6><p>
      When you remove a subuser, you are removing access to the Swift
      interface. The user will remain in the system.
     </p></div></section><section class="sect3" id="changing-s3-swift-users-password" data-id-title="Changing S3 and Swift User Access and Secret Keys"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.2.3 </span><span class="title-name">Changing S3 and Swift User Access and Secret Keys</span> <a title="Permalink" class="permalink" href="#changing-s3-swift-users-password">#</a></h4></div></div></div><p>
     The <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     parameters identify the Object Gateway user when accessing the gateway. Changing
     the existing user keys is the same as creating new ones, as the old keys
     get overwritten.
    </p><p>
     For S3 users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin key create --uid=<em class="replaceable">EXAMPLE_USER</em> --key-type=s3 --gen-access-key --gen-secret</pre></div><p>
     For Swift users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin key create --subuser=<em class="replaceable">EXAMPLE_USER</em>:swift --key-type=swift --gen-secret</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.8.5.4.7.1"><span class="term"><code class="option">--key-type=<em class="replaceable">TYPE</em></code></span></dt><dd><p>
        Specifies the type of key. Either <code class="literal">swift</code> or
        <code class="literal">s3</code>.
       </p></dd><dt id="id-1.3.6.2.8.5.4.7.2"><span class="term"><code class="option">--gen-access-key</code></span></dt><dd><p>
        Generates a random access key (for S3 user by default).
       </p></dd><dt id="id-1.3.6.2.8.5.4.7.3"><span class="term"><code class="option">--gen-secret</code></span></dt><dd><p>
        Generates a random secret key.
       </p></dd><dt id="id-1.3.6.2.8.5.4.7.4"><span class="term"><code class="option">--secret=<em class="replaceable">KEY</em></code></span></dt><dd><p>
        Specifies a secret key, for example manually generated.
       </p></dd></dl></div></section><section class="sect3" id="user-quota-managment" data-id-title="User Quota Management"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.5.2.4 </span><span class="title-name">User Quota Management</span> <a title="Permalink" class="permalink" href="#user-quota-managment">#</a></h4></div></div></div><p>
     The Ceph Object Gateway enables you to set quotas on users and buckets owned by users.
     Quotas include the maximum number of objects in a bucket and the maximum
     storage size in megabytes.
    </p><p>
     Before you enable a user quota, you first need to set its parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin quota set --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em> \
 --max-objects=1024 --max-size=1024</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.8.5.5.5.1"><span class="term"><code class="option">--max-objects</code></span></dt><dd><p>
        Specifies the maximum number of objects. A negative value disables the
        check.
       </p></dd><dt id="id-1.3.6.2.8.5.5.5.2"><span class="term"><code class="option">--max-size</code></span></dt><dd><p>
        Specifies the maximum number of bytes. A negative value disables the
        check.
       </p></dd><dt id="id-1.3.6.2.8.5.5.5.3"><span class="term"><code class="option">--quota-scope</code></span></dt><dd><p>
        Sets the scope for the quota. The options are <code class="literal">bucket</code>
        and <code class="literal">user</code>. Bucket quotas apply to buckets a user
        owns. User quotas apply to a user.
       </p></dd></dl></div><p>
     Once you set a user quota, you may enable it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin quota enable --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To disable a quota:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin quota disable --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To list quota settings:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user info --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To update quota statistics:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user stats --uid=<em class="replaceable">EXAMPLE_USER</em> --sync-stats</pre></div></section></section></section><section class="sect1" id="ogw-http-frontends" data-id-title="HTTP Front-ends"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.6 </span><span class="title-name">HTTP Front-ends</span> <a title="Permalink" class="permalink" href="#ogw-http-frontends">#</a></h2></div></div></div><p>
   The Ceph Object Gateway supports two embedded HTTP front-ends: <span class="emphasis"><em>Beast</em></span>
   and <span class="emphasis"><em>Civetweb</em></span>.
  </p><p>
   The Beast front-end uses the Boost.Beast library for HTTP parsing and the
   Boost.Asio library for asynchronous network I/O.
  </p><p>
   The Civetweb front-end uses the Civetweb HTTP library, which is a fork of
   Mongoose.
  </p><p>
   You can configure them with the <code class="option">rgw_frontends</code> option in the
   <code class="filename">/etc/ceph/ceph.conf</code> file. Refer to
   <a class="xref" href="#config-ogw" title="25.3. Ceph Object Gateway">Section 25.3, “Ceph Object Gateway”</a> for a list of configuration options.
  </p></section><section class="sect1" id="ceph-rgw-https" data-id-title="Enabling HTTPS/SSL for Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.7 </span><span class="title-name">Enabling HTTPS/SSL for Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-https">#</a></h2></div></div></div><p>
   To enable the default Object Gateway role to communicate securely using SSL, you need
   to either have a CA issued certificate or create a self-signed one. There
   are two ways to configure Object Gateway with HTTPS enabled—a simple way that
   makes use of the default settings, and an advanced way that lets you fine
   tune HTTPS related settings.
  </p><section class="sect2" id="ogw-selfcert" data-id-title="Create a Self-Signed Certificate"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.7.1 </span><span class="title-name">Create a Self-Signed Certificate</span> <a title="Permalink" class="permalink" href="#ogw-selfcert">#</a></h3></div></div></div><div id="id-1.3.6.2.10.3.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Skip this section if you already have a valid certificate signed by CA.
    </p></div><p>
    By default, DeepSea expects the certificate file in
    <code class="filename">/srv/salt/ceph/rgw/cert/rgw.pem</code> on the Salt master. It
    will then distribute the certificate to
    <code class="filename">/etc/ceph/rgw.pem</code> on the Salt minion with the Object Gateway
    role, where Ceph reads it.
   </p><p>
    The following procedure describes how to generate a self-signed SSL
    certificate on the Salt master.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you need your Object Gateway to be known by additional subject identities, add
      them to the <code class="option">subjectAltName</code> option in the
      <code class="literal">[v3_req]</code> section of the
      <code class="filename">/etc/ssl/openssl.cnf</code> file:
     </p><div class="verbatim-wrap"><pre class="screen">[...]
[ v3_req ]
subjectAltName = DNS:server1.example.com DNS:server2.example.com
[...]</pre></div><div id="id-1.3.6.2.10.3.5.1.3" data-id-title="IP Addresses in subjectAltName" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: IP Addresses in <code class="option">subjectAltName</code></h6><p>
       To use IP addresses instead of domain names in the
       <code class="option">subjectAltName</code> option, replace the example line with
       the following:
      </p><div class="verbatim-wrap"><pre class="screen">subjectAltName = IP:10.0.0.10 IP:10.0.0.11</pre></div></div></li><li class="step"><p>
      Create the key and the certificate using <code class="command">openssl</code>.
      Enter all data you need to include in your certificate. We recommend
      entering the FQDN as the common name. Before signing the certificate,
      verify that 'X509v3 Subject Alternative Name:' is included in requested
      extensions, and that the resulting certificate has "X509v3 Subject
      Alternative Name:" set.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>openssl req -x509 -nodes -days 1095 \
 -newkey rsa:4096 -keyout rgw.key -out /srv/salt/ceph/rgw/cert/rgw.pem</pre></div></li><li class="step"><p>
      Append the key to the certificate file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cat rgw.key &gt;&gt; /srv/salt/ceph/rgw/cert/rgw.pem</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-ssl-simple" data-id-title="Simple HTTPS Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.7.2 </span><span class="title-name">Simple HTTPS Configuration</span> <a title="Permalink" class="permalink" href="#ogw-ssl-simple">#</a></h3></div></div></div><p>
    By default, Ceph on the Object Gateway node reads the
    <code class="filename">/etc/ceph/rgw.pem</code> certificate, and uses port 443 for
    secure SSL communication. If you do not need to change these values, follow
    these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_init: default-ssl</pre></div></li><li class="step"><p>
      Copy the default Object Gateway SSL configuration to the
      <code class="filename">ceph.conf.d</code> subdirectory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</pre></div></li><li class="step"><p>
      Run DeepSea stages 2, 3, and 4 to apply the changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li><li class="step"><p>
      Finally, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph dashboard set-rgw-api-ssl-verify False</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-ssl-advanced" data-id-title="Advanced HTTPS Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.7.3 </span><span class="title-name">Advanced HTTPS Configuration</span> <a title="Permalink" class="permalink" href="#ogw-ssl-advanced">#</a></h3></div></div></div><p>
    If you need to change the default values for SSL settings of the Object Gateway,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> and add the
      following line:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_init: default-ssl</pre></div></li><li class="step"><p>
      Copy the default Object Gateway SSL configuration to the
      <code class="filename">ceph.conf.d</code> subdirectory:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</pre></div></li><li class="step"><p>
      Edit
      <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</code>
      and change the default options, such as port number or path to the SSL
      certificate, to reflect your setup.
     </p></li><li class="step"><p>
      Run DeepSea stage 3 and 4 to apply the changes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.2
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.3
<code class="prompt user">root@master # </code>salt-run state.orch ceph.stage.4</pre></div></li></ol></div></div><div id="rgw-webserver-multiport" data-id-title="Binding to Multiple Ports" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Binding to Multiple Ports</h6><p>
     The Beast server can bind to multiple ports. This is useful if you need to
     access a single Object Gateway instance with both SSL and non-SSL connections. A
     two-port configuration line example follows:
    </p><div class="verbatim-wrap"><pre class="screen">[client.{{ client }}]
rgw_frontends = beast port=80 ssl_port=443 ssl_certificate=/etc/ceph/rgw.pem</pre></div></div></section></section><section class="sect1" id="ceph-rgw-sync" data-id-title="Synchronization Modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.8 </span><span class="title-name">Synchronization Modules</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync">#</a></h2></div></div></div><p>
   The <span class="emphasis"><em>multisite</em></span> functionality of Object Gateway allows you to
   create multiple zones and mirror data and metadata between them.
   <span class="emphasis"><em>Synchronization modules</em></span> are built atop of the multisite
   framework that allows for forwarding data and metadata to a different
   external tier. A synchronization module allows for a set of actions to be
   performed whenever a change in data occurs (for example, metadata operations
   such as bucket or user creation). As the Object Gateway multisite changes are
   eventually consistent at remote sites, changes are propagated
   asynchronously. This covers use cases such as backing up the object storage
   to an external cloud cluster, a custom backup solution using tape drives, or
   indexing metadata in ElasticSearch.
  </p><section class="sect2" id="ogw-sync-general-config" data-id-title="General Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.8.1 </span><span class="title-name">General Configuration</span> <a title="Permalink" class="permalink" href="#ogw-sync-general-config">#</a></h3></div></div></div><p>
    All synchronization modules are configured in a similar way. You need to
    create a new zone (refer to <a class="xref" href="#ceph-rgw-fed" title="26.13. Multisite Object Gateways">Section 26.13, “Multisite Object Gateways”</a> for more
    details) and set its <code class="option">--tier_type</code> option, for example
    <code class="option">--tier-type=cloud</code> for the cloud synchronization module:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --endpoints=http://endpoint1.example.com,http://endpoint2.example.com, [...] \
 --tier-type=cloud</pre></div><p>
    You can configure the specific tier by using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=<em class="replaceable">KEY1</em>=<em class="replaceable">VALUE1</em>,<em class="replaceable">KEY2</em>=<em class="replaceable">VALUE2</em></pre></div><p>
    The <em class="replaceable">KEY</em> in the configuration specifies the
    configuration variable that you want to update, and the
    <em class="replaceable">VALUE</em> specifies its new value. Nested values can
    be accessed using period. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=connection.access_key=<em class="replaceable">KEY</em>,connection.secret=<em class="replaceable">SECRET</em></pre></div><p>
    You can access array entries by appending square brackets '[]' with the
    referenced entry. You can add a new array entry by using square brackets
    '[]'. Index value of -1 references the last entry in the array. It is not
    possible to create a new entry and reference it again in the same command.
    For example, a command to create a new profile for buckets starting with
    <em class="replaceable">PREFIX</em> follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=profiles[].source_bucket=<em class="replaceable">PREFIX</em>'*'
<code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=profiles[-1].connection_id=<em class="replaceable">CONNECTION_ID</em>,profiles[-1].acls_id=<em class="replaceable">ACLS_ID</em></pre></div><div id="id-1.3.6.2.11.3.10" data-id-title="Adding and Removing Configuration Entries" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Adding and Removing Configuration Entries</h6><p>
     You can add a new tier configuration entry by using the
     <code class="option">--tier-config-add=<em class="replaceable">KEY</em>=<em class="replaceable">VALUE</em></code>
     parameter.
    </p><p>
     You can remove an existing entry by using
     <code class="option">--tier-config-rm=<em class="replaceable">KEY</em></code>.
    </p></div></section><section class="sect2" id="ceph-rgw-sync-zones" data-id-title="Synchronizing Zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.8.2 </span><span class="title-name">Synchronizing Zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones">#</a></h3></div></div></div><p>
    A synchronization module configuration is local to a zone. The
    synchronization module determines whether the zone exports data or can only
    consume data that was modified in another zone. As of Luminous the
    supported synchronization plug-ins are <code class="literal">ElasticSearch</code>,
    <code class="literal">rgw</code>, which is the default synchronization plug-in that
    synchronizes data between the zones and <code class="literal">log</code> which is a
    trivial synchronization plug-in that logs the metadata operation that
    happens in the remote zones. The following sections are written with the
    example of a zone using <code class="literal">ElasticSearch</code> synchronization
    module. The process would be similar for configuring any other
    synchronization plug-in.
   </p><div id="id-1.3.6.2.11.4.3" data-id-title="Default Synchronization Plug-in" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Default Synchronization Plug-in</h6><p>
     <code class="literal">rgw</code> is the default synchronization plug-in and there is
     no need to explicitly configure this.
    </p></div><section class="sect3" id="ceph-rgw-sync-zones-req" data-id-title="Requirements and Assumptions"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.2.1 </span><span class="title-name">Requirements and Assumptions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-req">#</a></h4></div></div></div><p>
     Let us assume a simple multisite configuration as described in
     <a class="xref" href="#ceph-rgw-fed" title="26.13. Multisite Object Gateways">Section 26.13, “Multisite Object Gateways”</a> consists of 2 zones:
     <code class="literal">us-east</code> and <code class="literal">us-west</code>. Now we add a
     third zone <code class="literal">us-east-es</code> which is a zone that only
     processes metadata from the other sites. This zone can be in the same or a
     different Ceph cluster than <code class="literal">us-east</code>. This zone would
     only consume metadata from other zones and Object Gateways in this zone will not
     serve any end user requests directly.
    </p></section><section class="sect3" id="ceph-rgw-sync-zones-configure" data-id-title="Configuring Synchronization Modules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.2.2 </span><span class="title-name">Configuring Synchronization Modules</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-configure">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the third zone similar to the ones described in
       <a class="xref" href="#ceph-rgw-fed" title="26.13. Multisite Object Gateways">Section 26.13, “Multisite Object Gateways”</a>, for example
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">radosgw-admin</code> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key=<em class="replaceable">SYSTEM-KEY</em> --secret=<em class="replaceable">SECRET</em> --endpoints=http://rgw-es:80</pre></div></li><li class="step"><p>
       A synchronization module can be configured for this zone via the
       following
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --tier-type=<em class="replaceable">TIER-TYPE</em> \
--tier-config={set of key=value pairs}</pre></div></li><li class="step"><p>
       For example in the <code class="literal">ElasticSearch</code> synchronization
       module
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1</pre></div><p>
       For the various supported tier-config options refer to
       <a class="xref" href="#ceph-rgw-sync-elastic" title="26.8.3. ElasticSearch Synchronization Module">Section 26.8.3, “ElasticSearch Synchronization Module”</a>.
      </p></li><li class="step"><p>
       Finally update the period
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
       Now start the Object Gateway in the zone
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">systemctl</code> start ceph-radosgw@rgw.`hostname -s`
<code class="prompt user">root # </code><code class="command">systemctl</code> enable ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div></section></section><section class="sect2" id="ceph-rgw-sync-elastic" data-id-title="ElasticSearch Synchronization Module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.8.3 </span><span class="title-name">ElasticSearch Synchronization Module</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic">#</a></h3></div></div></div><p>
    This synchronization module writes the metadata from other zones to
    ElasticSearch. As of Luminous this is JSON of data fields we currently
    store in ElasticSearch.
   </p><div class="verbatim-wrap"><pre class="screen">{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}</pre></div><section class="sect3" id="ceph-rgw-sync-elastic-config" data-id-title="ElasticSearch Tier Type Configuration Parameters"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.3.1 </span><span class="title-name">ElasticSearch Tier Type Configuration Parameters</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-config">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.5.4.2.1"><span class="term">endpoint</span></dt><dd><p>
        Specifies the ElasticSearch server endpoint to access.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.2"><span class="term">num_shards</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of shards that ElasticSearch
        will be configured with on data synchronization initialization. Note
        that this cannot be changed after initialization. Any change here
        requires rebuild of the ElasticSearch index and reinitialization of the
        data synchronization process.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.3"><span class="term">num_replicas</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of replicas that
        ElasticSearch will be configured with on data synchronization
        initialization.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.4"><span class="term">explicit_custom_meta</span></dt><dd><p>
        <span class="emphasis"><em>(true | false)</em></span> Specifies whether all user custom
        metadata will be indexed, or whether user will need to configure (at
        the bucket level) what customer metadata entries should be indexed.
        This is false by default
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.5"><span class="term">index_buckets_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty, all
        buckets will be indexed. Otherwise, only buckets specified here will be
        indexed. It is possible to provide bucket prefixes (for example
        'foo*'), or bucket suffixes (for example '*bar').
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.6"><span class="term">approved_owners_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty,
        buckets of all owners will be indexed (subject to other restrictions),
        otherwise, only buckets owned by specified owners will be indexed.
        Suffixes and prefixes can also be provided.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.7"><span class="term">override_index_path</span></dt><dd><p>
        <span class="emphasis"><em>(string)</em></span> if not empty, this string will be used as
        the ElasticSearch index path. Otherwise the index path will be
        determined and generated on synchronization initialization.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.8"><span class="term">username</span></dt><dd><p>
        Specifies a user name for ElasticSearch if authentication is required.
       </p></dd><dt id="id-1.3.6.2.11.5.4.2.9"><span class="term">password</span></dt><dd><p>
        Specifies a password for ElasticSearch if authentication is required.
       </p></dd></dl></div></section><section class="sect3" id="ceph-rgw-sync-elastic-query" data-id-title="Metadata Queries"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.3.2 </span><span class="title-name">Metadata Queries</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-query">#</a></h4></div></div></div><p>
     Since the ElasticSearch cluster now stores object metadata, it is
     important that the ElasticSearch endpoint is not exposed to the public and
     only accessible to the cluster administrators. For exposing metadata
     queries to the end user itself this poses a problem since we'd want the
     user to only query their metadata and not of any other users, this would
     require the ElasticSearch cluster to authenticate users in a way similar
     to RGW does which poses a problem.
    </p><p>
     As of Luminous RGW in the metadata master zone can now service end user
     requests. This allows for not exposing the ElasticSearch endpoint in
     public and also solves the authentication and authorization problem since
     RGW itself can authenticate the end user requests. For this purpose RGW
     introduces a new query in the bucket APIs that can service ElasticSearch
     requests. All these requests must be sent to the metadata master zone.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.5.5.4.1"><span class="term">Get an ElasticSearch Query</span></dt><dd><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?query=<em class="replaceable">QUERY-EXPR</em></pre></div><p>
        request params:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          max-keys: max number of entries to return
         </p></li><li class="listitem"><p>
          marker: pagination marker
         </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]</pre></div><p>
        op is one of the following: &lt;, &lt;=, ==, &gt;=, &gt;
       </p><p>
        For example:
       </p><div class="verbatim-wrap"><pre class="screen">GET /?query=name==foo</pre></div><p>
        Will return all the indexed keys that user has read permission to, and
        are named 'foo'. The output will be a list of keys in XML that is
        similar to the S3 list buckets response.
       </p></dd><dt id="id-1.3.6.2.11.5.5.4.2"><span class="term">Configure custom metadata fields</span></dt><dd><p>
        Define which custom metadata entries should be indexed (under the
        specified bucket), and what are the types of these keys. If explicit
        custom metadata indexing is configured, this is needed so that rgw will
        index the specified custom metadata values. Otherwise it is needed in
        cases where the indexed metadata keys are of a type other than string.
       </p><div class="verbatim-wrap"><pre class="screen">POST /<em class="replaceable">BUCKET</em>?mdsearch
x-amz-meta-search: &lt;key [; type]&gt; [, ...]</pre></div><p>
        Multiple metadata fields must be comma separated, a type can be forced
        for a field with a `;`. The currently allowed types are
        string(default), integer and date, for example, if you want to index a
        custom object metadata x-amz-meta-year as int, x-amz-meta-date as type
        date and x-amz-meta-title as string, you would do
       </p><div class="verbatim-wrap"><pre class="screen">POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string</pre></div></dd><dt id="id-1.3.6.2.11.5.5.4.3"><span class="term">Delete custom metadata configuration</span></dt><dd><p>
        Delete custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">DELETE /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd><dt id="id-1.3.6.2.11.5.5.4.4"><span class="term">Get custom metadata configuration</span></dt><dd><p>
        Retrieve custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd></dl></div></section></section><section class="sect2" id="ogw-cloud-sync" data-id-title="Cloud Synchronization Module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.8.4 </span><span class="title-name">Cloud Synchronization Module</span> <a title="Permalink" class="permalink" href="#ogw-cloud-sync">#</a></h3></div></div></div><p>
    This section introduces a module that synchronizes the zone data to a
    remote cloud service. The synchronization is only unidirectional—the
    date is not synchronized back from the remote zone. The main goal of this
    module is to enable synchronizing data to multiple cloud service providers.
    Currently it supports cloud providers that are compatible with AWS (S3).
   </p><p>
    To synchronize data to a remote cloud service, you need to configure user
    credentials. Because many cloud services introduce limits on the number of
    buckets that each user can create, you can configure the mapping of source
    objects and buckets, different targets to different buckets and bucket
    prefixes. Note that source access lists (ACLs) will not be preserved. It is
    possible to map permissions of specific source users to specific
    destination users.
   </p><p>
    Because of API limitations, there is no way to preserve original object
    modification time and HTTP entity tag (ETag). The cloud synchronization
    module stores these as metadata attributes on the destination objects.
   </p><section class="sect3" id="id-1.3.6.2.11.6.5" data-id-title="General Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.4.1 </span><span class="title-name">General Configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.11.6.5">#</a></h4></div></div></div><p>
     Following are examples of a trivial and non-trivial configuration for the
     cloud synchronization module. Note that the trivial configuration can
     collide with the non-trivial one.
    </p><div class="example" id="id-1.3.6.2.11.6.5.3" data-id-title="Trivial Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 26.1: </span><span class="title-name">Trivial Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.11.6.5.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "connection": {
    "access_key": <em class="replaceable">ACCESS</em>,
    "secret": <em class="replaceable">SECRET</em>,
    "endpoint": <em class="replaceable">ENDPOINT</em>,
    "host_style": <em class="replaceable">path | virtual</em>,
  },
  "acls": [ { "type": <em class="replaceable">id | email | uri</em>,
    "source_id": <em class="replaceable">SOURCE_ID</em>,
    "dest_id": <em class="replaceable">DEST_ID</em> } ... ],
  "target_path": <em class="replaceable">TARGET_PATH</em>,
}</pre></div></div></div><div class="example" id="id-1.3.6.2.11.6.5.4" data-id-title="Non-trivial Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 26.2: </span><span class="title-name">Non-trivial Configuration </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.11.6.5.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "default": {
    "connection": {
      "access_key": <em class="replaceable">ACCESS</em>,
      "secret": <em class="replaceable">SECRET</em>,
      "endpoint": <em class="replaceable">ENDPOINT</em>,
      "host_style" <em class="replaceable">path | virtual</em>,
    },
    "acls": [
    {
      "type": <em class="replaceable">id | email | uri</em>,   #  optional, default is id
      "source_id": <em class="replaceable">ID</em>,
      "dest_id": <em class="replaceable">ID</em>
    } ... ]
    "target_path": <em class="replaceable">PATH</em> # optional
  },
  "connections": [
  {
    "connection_id": <em class="replaceable">ID</em>,
    "access_key": <em class="replaceable">ACCESS</em>,
    "secret": <em class="replaceable">SECRET</em>,
    "endpoint": <em class="replaceable">ENDPOINT</em>,
    "host_style": <em class="replaceable">path | virtual</em>,  # optional
  } ... ],
  "acl_profiles": [
  {
    "acls_id": <em class="replaceable">ID</em>, # acl mappings
    "acls": [ {
      "type": <em class="replaceable">id | email | uri</em>,
      "source_id": <em class="replaceable">ID</em>,
      "dest_id": <em class="replaceable">ID</em>
    } ... ]
  }
  ],
  "profiles": [
  {
   "source_bucket": <em class="replaceable">SOURCE</em>,
   "connection_id": <em class="replaceable">CONNECTION_ID</em>,
   "acls_id": <em class="replaceable">MAPPINGS_ID</em>,
   "target_path": <em class="replaceable">DEST</em>,          # optional
  } ... ],
}</pre></div></div></div><p>
     Explanation of used configuration terms follows:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.6.5.6.1"><span class="term">connection</span></dt><dd><p>
        Represents a connection to the remote cloud service. Contains
        'connection_id', 'access_key', 'secret', 'endpoint', and 'host_style'.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.2"><span class="term">access_key</span></dt><dd><p>
        The remote cloud access key that will be used for the specific
        connection.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.3"><span class="term">secret</span></dt><dd><p>
        The secret key for the remote cloud service.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.4"><span class="term">endpoint</span></dt><dd><p>
        URL of remote cloud service endpoint.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.5"><span class="term">host_style</span></dt><dd><p>
        Type of host style ('path' or 'virtual') to be used when accessing
        remote cloud endpoint. Default is 'path'.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.6"><span class="term">acls</span></dt><dd><p>
        Array of access list mappings.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.7"><span class="term">acl_mapping</span></dt><dd><p>
        Each 'acl_mapping' structure contains 'type', 'source_id', and
        'dest_id'. These will define the ACL mutation for each object. An ACL
        mutation allows converting source user ID to a destination ID.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.8"><span class="term">type</span></dt><dd><p>
        ACL type: 'id' defines user ID, 'email' defines user by email, and
        'uri' defines user by uri (group).
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.9"><span class="term">source_id</span></dt><dd><p>
        ID of user in the source zone.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.10"><span class="term">dest_id</span></dt><dd><p>
        ID of user in the destination.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.11"><span class="term">target_path</span></dt><dd><p>
        A string that defines how the target path is created. The target path
        specifies a prefix to which the source object name is appended. The
        target path configurable can include any of the following variables:
       </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.6.5.6.11.2.2.1"><span class="term">SID</span></dt><dd><p>
           A unique string that represents the synchronization instance ID.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.2"><span class="term">ZONEGROUP</span></dt><dd><p>
           Zonegroup name.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.3"><span class="term">ZONEGROUP_ID</span></dt><dd><p>
           Zonegroup ID.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.4"><span class="term">ZONE</span></dt><dd><p>
           Zone name.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.5"><span class="term">ZONE_ID</span></dt><dd><p>
           Zone ID.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.6"><span class="term">BUCKET</span></dt><dd><p>
           Source bucket name.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.11.2.2.7"><span class="term">OWNER</span></dt><dd><p>
           Source bucket owner ID.
          </p></dd></dl></div><p>
        For example: target_path =
        rgwx-<em class="replaceable">ZONE</em>-<em class="replaceable">SID</em>/<em class="replaceable">OWNER</em>/<em class="replaceable">BUCKET</em>
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.12"><span class="term">acl_profiles</span></dt><dd><p>
        An array of access list profiles.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.13"><span class="term">acl_profile</span></dt><dd><p>
        Each profile contains 'acls_id' that represents the profile, and an
        'acls' array that holds a list of 'acl_mappings'.
       </p></dd><dt id="id-1.3.6.2.11.6.5.6.14"><span class="term">profiles</span></dt><dd><p>
        A list of profiles. Each profile contains the following:
       </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.6.5.6.14.2.2.1"><span class="term">source_bucket</span></dt><dd><p>
           Either a bucket name, or a bucket prefix (if ends with *) that
           defines the source bucket(s) for this profile.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.14.2.2.2"><span class="term">target_path</span></dt><dd><p>
           See above for the explanation.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.14.2.2.3"><span class="term">connection_id</span></dt><dd><p>
           ID of the connection that will be used for this profile.
          </p></dd><dt id="id-1.3.6.2.11.6.5.6.14.2.2.4"><span class="term">acls_id</span></dt><dd><p>
           ID of ACL's profile that will be used for this profile.
          </p></dd></dl></div></dd></dl></div></section><section class="sect3" id="id-1.3.6.2.11.6.6" data-id-title="S3 Specific Configurables"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.4.2 </span><span class="title-name">S3 Specific Configurables</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.11.6.6">#</a></h4></div></div></div><p>
     The cloud synchronization module will only work with back-ends that are
     compatible with AWS S3. There are a few configurables that can be used to
     tweak its behavior when accessing S3 cloud services:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "multipart_sync_threshold": <em class="replaceable">OBJECT_SIZE</em>,
  "multipart_min_part_size": <em class="replaceable">PART_SIZE</em>
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.11.6.6.4.1"><span class="term">multipart_sync_threshold</span></dt><dd><p>
        Objects whose size is equal to or larger than this value will be
        synchronized with the cloud service using multipart upload.
       </p></dd><dt id="id-1.3.6.2.11.6.6.4.2"><span class="term">multipart_min_part_size</span></dt><dd><p>
        Minimum parts size to use when synchronizing objects using multipart
        upload.
       </p></dd></dl></div></section></section><section class="sect2" id="archive-sync-module" data-id-title="Archive Synchronization Module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.8.5 </span><span class="title-name">Archive Synchronization Module</span> <a title="Permalink" class="permalink" href="#archive-sync-module">#</a></h3></div></div></div><p>
    The <span class="emphasis"><em>archive sync module</em></span> utilizes the versioning
    feature of S3 objects in Object Gateway. You can configure an <span class="emphasis"><em>archive
    zone</em></span> that captures the different versions of S3 objects as they
    occur over time in other zones. The history of versions that the archive
    zone keeps can only be eliminated via gateways associated with the archive
    zone.
   </p><p>
    With such an architecture, several non-versioned zones can mirror their
    data and metadata via their zone gateways providing high availability to
    the end users, while the archive zone captures all the data updates to
    consolidate them as versions of S3 objects.
   </p><p>
    By including the archive zone in a multi-zone configuration, you gain the
    flexibility of an S3 object history in one zone while saving the space that
    the replicas of the versioned S3 objects would consume in the remaining
    zones.
   </p><section class="sect3" id="archive-sync-module-configuration" data-id-title="Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.8.5.1 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#archive-sync-module-configuration">#</a></h4></div></div></div><div id="id-1.3.6.2.11.7.5.2" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
      Refer to <a class="xref" href="#ceph-rgw-fed" title="26.13. Multisite Object Gateways">Section 26.13, “Multisite Object Gateways”</a> for details on configuring
      multisite gateways.
     </p><p>
      Refer to <a class="xref" href="#ceph-rgw-sync" title="26.8. Synchronization Modules">Section 26.8, “Synchronization Modules”</a> for details on configuring
      synchronization modules.
     </p></div><p>
     To use the archive module, you need to create a new zone whose tier type
     is set to <code class="literal">archive</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE_GROUP_NAME</em> \
 --rgw-zone=<em class="replaceable">OGW_ZONE_NAME</em> \
 --endpoints=<em class="replaceable">http://OGW_ENDPOINT1_URL[,http://OGW_ENDPOINT2_URL,...]</em>
 --tier-type=archive</pre></div></section></section></section><section class="sect1" id="ceph-rgw-ldap" data-id-title="LDAP Authentication"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.9 </span><span class="title-name">LDAP Authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap">#</a></h2></div></div></div><p>
   Apart from the default local user authentication, Object Gateway can use LDAP server
   services to authenticate users as well.
  </p><section class="sect2" id="ceph-rgw-ldap-how-works" data-id-title="Authentication Mechanism"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.9.1 </span><span class="title-name">Authentication Mechanism</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-how-works">#</a></h3></div></div></div><p>
    The Object Gateway extracts the user's LDAP credentials from a token. A search
    filter is constructed from the user name. The Object Gateway uses the configured
    service account to search the directory for a matching entry. If an entry
    is found, the Object Gateway attempts to bind to the found distinguished name with
    the password from the token. If the credentials are valid, the bind will
    succeed, and the Object Gateway grants access.
   </p><p>
    You can limit the allowed users by setting the base for the search to a
    specific organizational unit or by specifying a custom search filter, for
    example requiring specific group membership, custom object classes, or
    attributes.
   </p></section><section class="sect2" id="ceph-rgw-ldap-reqs" data-id-title="Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.9.2 </span><span class="title-name">Requirements</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-reqs">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>LDAP or Active Directory</em></span>: A running LDAP instance
      accessible by the Object Gateway.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Service account</em></span>: LDAP credentials to be used by the
      Object Gateway with search permissions.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>User account</em></span>: At least one user account in the LDAP
      directory.
     </p></li></ul></div><div id="id-1.3.6.2.12.4.3" data-id-title="Do Not Overlap LDAP and Local Users" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Overlap LDAP and Local Users</h6><p>
     You should not use the same user names for local users and for users being
     authenticated by using LDAP. The Object Gateway cannot distinguish them and it
     treats them as the same user.
    </p></div><div id="id-1.3.6.2.12.4.4" data-id-title="Sanity Checks" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sanity Checks</h6><p>
     Use the <code class="command">ldapsearch</code> utility to verify the service
     account or the LDAP connection. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</pre></div><p>
     Make sure to use the same LDAP parameters as in the Ceph configuration
     file to eliminate possible problems.
    </p></div></section><section class="sect2" id="ceph-rgw-ldap-config" data-id-title="Configure Object Gateway to Use LDAP Authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.9.3 </span><span class="title-name">Configure Object Gateway to Use LDAP Authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-config">#</a></h3></div></div></div><p>
    The following parameters in the <code class="filename">/etc/ceph/ceph.conf</code>
    configuration file are related to the LDAP authentication:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.12.5.3.1"><span class="term"><code class="option">rgw_s3_auth_use_ldap</code></span></dt><dd><p>
       Set this option to <code class="literal">true</code> to enable S3 authentication
       with LDAP.
      </p></dd><dt id="id-1.3.6.2.12.5.3.2"><span class="term"><code class="option">rgw_ldap_uri</code></span></dt><dd><p>
       Specifies the LDAP server to use. Make sure to use the
       <code class="literal">ldaps://<em class="replaceable">FQDN</em>:<em class="replaceable">PORT</em></code>
       parameter to avoid transmitting the plain text credentials openly.
      </p></dd><dt id="id-1.3.6.2.12.5.3.3"><span class="term"><code class="option">rgw_ldap_binddn</code></span></dt><dd><p>
       The Distinguished Name (DN) of the service account used by the Object Gateway.
      </p></dd><dt id="id-1.3.6.2.12.5.3.4"><span class="term"><code class="option">rgw_ldap_secret</code></span></dt><dd><p>
       The password for the service account.
      </p></dd><dt id="id-1.3.6.2.12.5.3.5"><span class="term">rgw_ldap_searchdn</span></dt><dd><p>
       Specifies the base in the directory information tree for searching
       users. This might be your users organizational unit or some more
       specific Organizational Unit (OU).
      </p></dd><dt id="id-1.3.6.2.12.5.3.6"><span class="term"><code class="option">rgw_ldap_dnattr</code></span></dt><dd><p>
       The attribute being used in the constructed search filter to match a
       user name. Depending on your Directory Information Tree (DIT) this would
       probably be <code class="literal">uid</code> or <code class="literal">cn</code>.
      </p></dd><dt id="id-1.3.6.2.12.5.3.7"><span class="term"><code class="option">rgw_search_filter</code></span></dt><dd><p>
       If not specified, the Object Gateway automatically constructs the search filter
       with the <code class="option">rgw_ldap_dnattr</code> setting. Use this parameter to
       narrow the list of allowed users in very flexible ways. Consult
       <a class="xref" href="#ceph-rgw-ldap-filter" title="26.9.4. Using a Custom Search Filter to Limit User Access">Section 26.9.4, “Using a Custom Search Filter to Limit User Access”</a> for details.
      </p></dd></dl></div></section><section class="sect2" id="ceph-rgw-ldap-filter" data-id-title="Using a Custom Search Filter to Limit User Access"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.9.4 </span><span class="title-name">Using a Custom Search Filter to Limit User Access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-filter">#</a></h3></div></div></div><p>
    There are two ways you can use the <code class="option">rgw_search_filter</code>
    parameter.
   </p><section class="sect3" id="id-1.3.6.2.12.6.3" data-id-title="Partial Filter to Further Limit the Constructed Search Filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.9.4.1 </span><span class="title-name">Partial Filter to Further Limit the Constructed Search Filter</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.12.6.3">#</a></h4></div></div></div><p>
     An example of a partial filter:
    </p><div class="verbatim-wrap"><pre class="screen">"objectclass=inetorgperson"</pre></div><p>
     The Object Gateway will generate the search filter as usual with the user name from
     the token and the value of <code class="option">rgw_ldap_dnattr</code>. The
     constructed filter is then combined with the partial filter from the
     <code class="option">rgw_search_filter</code> attribute. Depending on the user name
     and the settings the final search filter may become:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=hari)(objectclass=inetorgperson))"</pre></div><p>
     In that case, user 'hari' will only be granted access if he is found in
     the LDAP directory, has an object class of 'inetorgperson', and did
     specify a valid password.
    </p></section><section class="sect3" id="id-1.3.6.2.12.6.4" data-id-title="Complete Filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.9.4.2 </span><span class="title-name">Complete Filter</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.12.6.4">#</a></h4></div></div></div><p>
     A complete filter must contain a <code class="option">USERNAME</code> token which
     will be substituted with the user name during the authentication attempt.
     The <code class="option">rgw_ldap_dnattr</code> parameter is not used anymore in this
     case. For example, to limit valid users to a specific group, use the
     following filter:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</pre></div><div id="id-1.3.6.2.12.6.4.4" data-id-title="memberOf Attribute" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="literal">memberOf</code> Attribute</h6><p>
      Using the <code class="literal">memberOf</code> attribute in LDAP searches requires
      server side support from you specific LDAP server implementation.
     </p></div></section></section><section class="sect2" id="ceph-rgw-ldap-token" data-id-title="Generating an Access Token for LDAP authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.9.5 </span><span class="title-name">Generating an Access Token for LDAP authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-token">#</a></h3></div></div></div><p>
    The <code class="command">radosgw-token</code> utility generates the access token
    based on the LDAP user name and password. It outputs a base-64 encoded
    string which is the actual access token. Use your favorite S3 client (refer
    to <a class="xref" href="#accessing-ragos-gateway" title="26.5.1. Accessing Object Gateway">Section 26.5.1, “Accessing Object Gateway”</a>) and specify the token as the
    access key and use an empty secret key.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>export RGW_ACCESS_KEY_ID="<em class="replaceable">USERNAME</em>"
<code class="prompt user">tux &gt; </code>export RGW_SECRET_ACCESS_KEY="<em class="replaceable">PASSWORD</em>"
<code class="prompt user">cephadm@adm &gt; </code>radosgw-token --encode --ttype=ldap</pre></div><div id="id-1.3.6.2.12.7.4" data-id-title="Clear Text Credentials" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Clear Text Credentials</h6><p>
     The access token is a base-64 encoded JSON structure and contains the LDAP
     credentials as a clear text.
    </p></div><div id="id-1.3.6.2.12.7.5" data-id-title="Active Directory" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Active Directory</h6><p>
     For Active Directory, use the <code class="option">--ttype=ad</code> parameter.
    </p></div></section></section><section class="sect1" id="ogw-bucket-sharding" data-id-title="Bucket Index Sharding"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.10 </span><span class="title-name">Bucket Index Sharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding">#</a></h2></div></div></div><p>
   The Object Gateway stores bucket index data in an index pool, which defaults to
   <code class="literal">.rgw.buckets.index</code>. If you put too many (hundreds of
   thousands) objects into a single bucket and the quota for maximum number of
   objects per bucket (<code class="option">rgw bucket default quota max objects</code>)
   is not set, the performance of the index pool may degrade. <span class="emphasis"><em>Bucket
   index sharding</em></span> prevents such performance decreases and allows a
   high number of objects per bucket.
  </p><section class="sect2" id="ogw-bucket-reshard" data-id-title="Bucket Index Resharding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.10.1 </span><span class="title-name">Bucket Index Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-reshard">#</a></h3></div></div></div><p>
    If a bucket has grown large and its initial configuration is not sufficient
    anymore, the bucket's index pool needs to be resharded. You can either use
    automatic online bucket index resharding (refer to
    <a class="xref" href="#ogw-bucket-sharding-dyn" title="26.10.1.1. Dynamic Resharding">Section 26.10.1.1, “Dynamic Resharding”</a>), or reshard the bucket index
    offline manually (refer to <a class="xref" href="#ogw-bucket-sharding-re" title="26.10.1.2. Manual Resharding">Section 26.10.1.2, “Manual Resharding”</a>).
   </p><section class="sect3" id="ogw-bucket-sharding-dyn" data-id-title="Dynamic Resharding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.10.1.1 </span><span class="title-name">Dynamic Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-dyn">#</a></h4></div></div></div><p>
     From SUSE Enterprise Storage 5, we support online bucket resharding. This detects if
     the number of objects per bucket reaches a certain threshold, and
     automatically increases the number of shards used by the bucket index.
     This process reduces the number of entries in each bucket index shard.
    </p><p>
     The detection process runs:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       When new objects are added to the bucket.
      </p></li><li class="listitem"><p>
       In a background process that periodically scans all the buckets. This is
       needed in order to deal with existing buckets that are not being
       updated.
      </p></li></ul></div><p>
     A bucket that requires resharding is added to the
     <code class="option">reshard_log</code> queue and will be scheduled to be resharded
     later. The reshard threads run in the background and execute the scheduled
     resharding, one at a time.
    </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Configuring Dynamic Resharding </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.13.3.3.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.6.2.13.3.3.6.2"><span class="term"><code class="option">rgw_dynamic_resharding</code></span></dt><dd><p>
        Enables or disables dynamic bucket index resharding. Possible values
        are 'true' or 'false'. Defaults to 'true'.
       </p></dd><dt id="id-1.3.6.2.13.3.3.6.3"><span class="term"><code class="option">rgw_reshard_num_logs</code></span></dt><dd><p>
        Number of shards for the resharding log. Defaults to 16.
       </p></dd><dt id="id-1.3.6.2.13.3.3.6.4"><span class="term"><code class="option">rgw_reshard_bucket_lock_duration</code></span></dt><dd><p>
        Duration of lock on the bucket object during resharding. Defaults to
        120 seconds.
       </p></dd><dt id="id-1.3.6.2.13.3.3.6.5"><span class="term"><code class="option">rgw_max_objs_per_shard</code></span></dt><dd><p>
        Maximum number of objects per bucket index shard. Defaults to 100000
        objects.
       </p></dd><dt id="id-1.3.6.2.13.3.3.6.6"><span class="term"><code class="option">rgw_reshard_thread_interval</code></span></dt><dd><p>
        Maximum time between rounds of reshard thread processing. Defaults to
        600 seconds.
       </p></dd></dl></div><div id="id-1.3.6.2.13.3.3.7" data-id-title="Multisite Configurations" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Multisite Configurations</h6><p>
      Dynamic resharding is not supported in multisite environments. It is
      disabled by default from Ceph 12.2.2 onward, but we recommend you to
      double check the setting.
     </p></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Commands to Administer the Resharding Process </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.13.3.3.8">#</a></h6></div><dl class="variablelist"><dt id="id-1.3.6.2.13.3.3.8.2"><span class="term">Add a bucket to the resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin reshard add \
 --bucket <em class="replaceable">BUCKET_NAME</em> \
 --num-shards <em class="replaceable">NEW_NUMBER_OF_SHARDS</em></pre></div></dd><dt id="id-1.3.6.2.13.3.3.8.3"><span class="term">List resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin reshard list</pre></div></dd><dt id="id-1.3.6.2.13.3.3.8.4"><span class="term">Process/schedule a bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin reshard process</pre></div></dd><dt id="id-1.3.6.2.13.3.3.8.5"><span class="term">Display the bucket resharding status:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin reshard status --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd><dt id="id-1.3.6.2.13.3.3.8.6"><span class="term">Cancel pending bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin reshard cancel --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd></dl></div></section><section class="sect3" id="ogw-bucket-sharding-re" data-id-title="Manual Resharding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.10.1.2 </span><span class="title-name">Manual Resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-re">#</a></h4></div></div></div><p>
     Dynamic resharding as mentioned in
     <a class="xref" href="#ogw-bucket-sharding-dyn" title="26.10.1.1. Dynamic Resharding">Section 26.10.1.1, “Dynamic Resharding”</a> is supported only for simple
     Object Gateway configurations. For multisite configurations, use manual resharding
     as described in this section.
    </p><p>
     To reshard the bucket index manually offline, use the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket reshard</pre></div><p>
     The <code class="command">bucket reshard</code> command performs the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creates a new set of bucket index objects for the specified object.
      </p></li><li class="listitem"><p>
       Spreads all entries of these index objects.
      </p></li><li class="listitem"><p>
       Creates a new bucket instance.
      </p></li><li class="listitem"><p>
       Links the new bucket instance with the bucket so that all new index
       operations go through the new bucket indexes.
      </p></li><li class="listitem"><p>
       Prints the old and the new bucket ID to the standard output.
      </p></li></ul></div><div id="id-1.3.6.2.13.3.4.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      When choosing a number of shards, note the following: aim for no more
      than 100000 entries per shard. Bucket index shards that are prime numbers
      tend to work better in evenly distributing bucket index entries across
      the shards. For example, 503 bucket index shards is better than 500 since
      the former is prime.
     </p></div><div id="id-1.3.6.2.13.3.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Multi-site configurations do not support resharding a bucket index.
     </p><p>
      For multi-site configurations, resharding a bucket index requires
      resynchronizing all data from the master zone to all slave zones.
      Depending on the bucket size, this can take a considerable amount of time
      and resources.
     </p></div><div class="procedure" id="id-1.3.6.2.13.3.4.9" data-id-title="Resharding the Bucket Index"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 26.1: </span><span class="title-name">Resharding the Bucket Index </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.13.3.4.9">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Make sure that all operations to the bucket are stopped.
      </p></li><li class="step"><p>
       Back up the original bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bi list \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 &gt; <em class="replaceable">BUCKET_NAME</em>.list.backup</pre></div></li><li class="step"><p>
       Reshard the bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket reshard \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 --num-shards=<em class="replaceable">NEW_SHARDS_NUMBER</em></pre></div><div id="id-1.3.6.2.13.3.4.9.4.3" data-id-title="Old Bucket ID" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Old Bucket ID</h6><p>
        As part of its output, this command also prints the new and the old
        bucket ID.
       </p></div></li></ol></div></div></section></section><section class="sect2" id="ogw-bucket-sharding-new" data-id-title="Bucket Index Sharding for New Buckets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.10.2 </span><span class="title-name">Bucket Index Sharding for New Buckets</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-new">#</a></h3></div></div></div><p>
    There are two options that affect bucket index sharding:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Use the <code class="option">rgw_override_bucket_index_max_shards</code> option for
      simple configurations.
     </p></li><li class="listitem"><p>
      Use the <code class="option">bucket_index_max_shards</code> option for multisite
      configurations.
     </p></li></ul></div><p>
    Setting the options to <code class="literal">0</code> disables bucket index sharding.
    A value greater than <code class="literal">0</code> enables bucket index sharding and
    sets the maximum number of shards.
   </p><p>
    The following formula helps you calculate the recommended number of shards:
   </p><div class="verbatim-wrap"><pre class="screen">number_of_objects_expected_in_a_bucket / 100000</pre></div><p>
    Be aware that the maximum number of shards is 7877.
   </p><section class="sect3" id="id-1.3.6.2.13.4.8" data-id-title="Simple Configurations"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.10.2.1 </span><span class="title-name">Simple Configurations</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.13.4.8">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Open the Ceph configuration file and add or modify the following
       option:
      </p><div class="verbatim-wrap"><pre class="screen">rgw_override_bucket_index_max_shards = 12</pre></div><div id="id-1.3.6.2.13.4.8.2.1.3" data-id-title="All or One Object Gateway Instances" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: All or One Object Gateway Instances</h6><p>
        To configure bucket index sharding for all instances of the Object Gateway,
        include <code class="option">rgw_override_bucket_index_max_shards</code> in the
        <code class="literal">[global]</code> section.
       </p><p>
        To configure bucket index sharding only for a particular instance of
        the Object Gateway, include
        <code class="option">rgw_override_bucket_index_max_shards</code> in the related
        instance section.
       </p></div></li><li class="step"><p>
       Restart the Object Gateway. See <a class="xref" href="#ceph-rgw-operating" title="26.3. Operating the Object Gateway Service">Section 26.3, “Operating the Object Gateway Service”</a> for more
       details.
      </p></li></ol></div></div></section><section class="sect3" id="id-1.3.6.2.13.4.9" data-id-title="Multisite Configurations"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.10.2.2 </span><span class="title-name">Multisite Configurations</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.13.4.9">#</a></h4></div></div></div><p>
     Multisite configurations can have a different index pool to manage
     failover. To configure a consistent shard count for zones in one zone
     group, set the <code class="option">bucket_index_max_shards</code> option in the zone
     group's configuration:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Export the zonegroup configuration to the
       <code class="filename">zonegroup.json</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup get &gt; zonegroup.json</pre></div></li><li class="step"><p>
       Edit the <code class="filename">zonegroup.json</code> file and set the
       <code class="option">bucket_index_max_shards</code> option for each named zone.
      </p></li><li class="step"><p>
       Reset the zonegroup:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup set &lt; zonegroup.json</pre></div></li><li class="step"><p>
       Update the period. See
       <a class="xref" href="#ceph-rgw-fed-masterzone-updateperiod" title="26.13.3.6. Update the Period">Section 26.13.3.6, “Update the Period”</a>.
      </p></li></ol></div></div></section></section></section><section class="sect1" id="ogw-keystone" data-id-title="Integrating OpenStack Keystone"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.11 </span><span class="title-name">Integrating OpenStack Keystone</span> <a title="Permalink" class="permalink" href="#ogw-keystone">#</a></h2></div></div></div><p>
   OpenStack Keystone is an identity service for the OpenStack product. You can
   integrate the Object Gateway with Keystone to set up a gateway that accepts a
   Keystone authentication token. A user authorized by Keystone to access
   the gateway will be verified on the Ceph Object Gateway side and automatically created if
   needed. The Object Gateway queries Keystone periodically for a list of revoked
   tokens.
  </p><section class="sect2" id="ogw-keystone-ostack" data-id-title="Configuring OpenStack"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.11.1 </span><span class="title-name">Configuring OpenStack</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ostack">#</a></h3></div></div></div><p>
    Before configuring the Ceph Object Gateway, you need to configure the OpenStack Keystone to
    enable the Swift service and point it to the Ceph Object Gateway:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <span class="emphasis"><em>Set the Swift service.</em></span> To use OpenStack to validate
      Swift users, first create the Swift service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack service create \
 --name=swift \
 --description="Swift Service" \
 object-store</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Set the endpoints.</em></span> After you create the Swift
      service, point to the Ceph Object Gateway. Replace
      <em class="replaceable">REGION_NAME</em> with the name of the gateway’s
      zonegroup name or region name.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack endpoint create --region <em class="replaceable">REGION_NAME</em> \
 --publicurl   "http://radosgw.example.com:8080/swift/v1" \
 --adminurl    "http://radosgw.example.com:8080/swift/v1" \
 --internalurl "http://radosgw.example.com:8080/swift/v1" \
 swift</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Verify the settings.</em></span> After you create the Swift
      service and set the endpoints, show the endpoints to verify that all the
      settings are correct.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>openstack endpoint show object-store</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-keystone-ogw" data-id-title="Configuring the Ceph Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.11.2 </span><span class="title-name">Configuring the Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ogw">#</a></h3></div></div></div><section class="sect3" id="id-1.3.6.2.14.4.2" data-id-title="Configure SSL Certificates"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.11.2.1 </span><span class="title-name">Configure SSL Certificates</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.14.4.2">#</a></h4></div></div></div><p>
     The Ceph Object Gateway queries Keystone periodically for a list of revoked tokens.
     These requests are encoded and signed. Keystone may be also configured
     to provide self-signed tokens, which are also encoded and signed. You need
     to configure the gateway so that it can decode and verify these signed
     messages. Therefore, the OpenSSL certificates that Keystone uses to
     create the requests need to be converted to the 'nss db' format:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /var/ceph/nss
<code class="prompt user">root # </code>openssl x509 -in /etc/keystone/ssl/certs/ca.pem \
 -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
<code class="systemitem">root</code>openssl x509 -in /etc/keystone/ssl/certs/signing_cert.pem \
 -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"</pre></div><p>
     To allow Ceph Object Gateway to interact with OpenStack Keystone, OpenStack Keystone can use a
     self-signed SSL certificate. Either install Keystone’s SSL certificate
     on the node running the Ceph Object Gateway, or alternatively set the value of the
     option <code class="option">rgw keystone verify ssl</code> to 'false'. Setting
     <code class="option">rgw keystone verify ssl</code> to 'false' means that the gateway
     will not attempt to verify the certificate.
    </p></section><section class="sect3" id="id-1.3.6.2.14.4.3" data-id-title="Configure the Object Gateways Options"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.11.2.2 </span><span class="title-name">Configure the Object Gateway's Options</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.14.4.3">#</a></h4></div></div></div><p>
     You can configure Keystone integration using the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.14.4.3.3.1"><span class="term"><code class="option">rgw keystone api version</code></span></dt><dd><p>
        Version of the Keystone API. Valid options are 2 or 3. Defaults to 2.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.2"><span class="term"><code class="option">rgw keystone url</code></span></dt><dd><p>
        The URL and port number of the administrative RESTful API on the
        Keystone server. Follows the pattern
        <em class="replaceable">SERVER_URL:PORT_NUMBER</em>.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.3"><span class="term"><code class="option">rgw keystone admin token</code></span></dt><dd><p>
        The token or shared secret that is configured internally in Keystone
        for administrative requests.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.4"><span class="term"><code class="option">rgw keystone accepted roles</code></span></dt><dd><p>
        The roles required to serve requests. Defaults to 'Member, admin'.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.5"><span class="term"><code class="option">rgw keystone accepted admin roles</code></span></dt><dd><p>
        The list of roles allowing a user to gain administrative privileges.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.6"><span class="term"><code class="option">rgw keystone token cache size</code></span></dt><dd><p>
        The maximum number of entries in the Keystone token cache.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.7"><span class="term"><code class="option">rgw keystone revocation interval</code></span></dt><dd><p>
        The number of seconds before checking revoked tokens. Defaults to 15 *
        60.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.8"><span class="term"><code class="option">rgw keystone implicit tenants</code></span></dt><dd><p>
        Create new users in their own tenants of the same name. Defaults to
        'false'.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.9"><span class="term"><code class="option">rgw s3 auth use keystone</code></span></dt><dd><p>
        If set to 'true', the Ceph Object Gateway will authenticate users using Keystone.
        Defaults to 'false'.
       </p></dd><dt id="id-1.3.6.2.14.4.3.3.10"><span class="term"><code class="option">nss db path</code></span></dt><dd><p>
        The path to the NSS database.
       </p></dd></dl></div><p>
     It is also possible to configure the Keystone service tenant, user, and
     password for Keystone (for version 2.0 of the OpenStack Identity API),
     similar to the way OpenStack services tend to be configured. This way you
     can avoid setting the shared secret <code class="option">rgw keystone admin
     token</code> in the configuration file, which should be disabled in
     production environments. The service tenant credentials should have admin
     privileges. For more details refer to the
     <a class="link" href="https://docs.openstack.org/keystone/latest/#setting-up-projects-users-and-roles" target="_blank">official
     OpenStack Keystone documentation</a>. The related configuration options
     follow:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.14.4.3.5.1"><span class="term"><code class="option">rgw keystone admin user</code></span></dt><dd><p>
        The Keystone administrator user name.
       </p></dd><dt id="id-1.3.6.2.14.4.3.5.2"><span class="term"><code class="option">rgw keystone admin password</code></span></dt><dd><p>
        The keystone administrator user password.
       </p></dd><dt id="id-1.3.6.2.14.4.3.5.3"><span class="term"><code class="option">rgw keystone admin tenant</code></span></dt><dd><p>
        The Keystone version 2.0 administrator user tenant.
       </p></dd></dl></div><p>
     A Ceph Object Gateway user is mapped to a Keystone tenant. A Keystone user has
     different roles assigned to it, possibly on more than one tenant. When the
     Ceph Object Gateway gets the ticket, it looks at the tenant and the user roles that are
     assigned to that ticket, and accepts or rejects the request according to
     the setting of the <code class="option">rgw keystone accepted roles</code> option.
    </p><div id="id-1.3.6.2.14.4.3.7" data-id-title="Mapping to OpenStack Tenants" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mapping to OpenStack Tenants</h6><p>
      Although Swift tenants are mapped to the Object Gateway user by default, they
      can be also mapped to OpenStack tenants via the <code class="option">rgw keystone
      implicit tenants</code> option. This will make containers use the
      tenant namespace instead of the S3 like global namespace that the Object Gateway
      defaults to. We recommend deciding on the mapping method at the planning
      stage to avoid confusion. The reason for this is that toggling the option
      later affects only newer requests which get mapped under a tenant, while
      older buckets created before still continue to be in a global namespace.
     </p></div><p>
     For version 3 of the OpenStack Identity API, you should replace the
     <code class="option">rgw keystone admin tenant</code> option with:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.14.4.3.9.1"><span class="term"><code class="option">rgw keystone admin domain</code></span></dt><dd><p>
        The Keystone administrator user domain.
       </p></dd><dt id="id-1.3.6.2.14.4.3.9.2"><span class="term"><code class="option">rgw keystone admin project</code></span></dt><dd><p>
        The Keystone administrator user project.
       </p></dd></dl></div></section></section></section><section class="sect1" id="ogw-storage-classes" data-id-title="Pool Placement and Storage Classes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.12 </span><span class="title-name">Pool Placement and Storage Classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes">#</a></h2></div></div></div><section class="sect2" id="ogw-storage-classes-placement-targets" data-id-title="Placement Targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.12.1 </span><span class="title-name">Placement Targets</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-placement-targets">#</a></h3></div></div></div><p>
    Placement targets control which pools are associated with a particular
    bucket. A bucket’s placement target is selected on creation, and cannot be
    modified. You can display its 'placement_rule' by running the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket stats</pre></div><p>
    The zonegroup configuration contains a list of placement targets with an
    initial target named 'default-placement'. The zone configuration then maps
    each zonegroup placement target name onto its local storage. This zone
    placement information includes the 'index_pool' name for the bucket index,
    the 'data_extra_pool' name for metadata about incomplete multipart uploads,
    and a 'data_pool' name for each storage class.
   </p></section><section class="sect2" id="ogw-storage-classes-itself" data-id-title="Storage Classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.12.2 </span><span class="title-name">Storage Classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-itself">#</a></h3></div></div></div><p>
    Storage classes help customizing the placement of object data. S3 Bucket
    Lifecycle rules can automate the transition of objects between storage
    classes.
   </p><p>
    Storage classes are defined in terms of placement targets. Each zonegroup
    placement target lists its available storage classes with an initial class
    named 'STANDARD'. The zone configuration is responsible for providing a
    'data_pool' pool name for each of the zonegroup’s storage classes.
   </p></section><section class="sect2" id="ogw-storage-classes-zone-config" data-id-title="Zonegroup/Zone Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.12.3 </span><span class="title-name">Zonegroup/Zone Configuration</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-zone-config">#</a></h3></div></div></div><p>
    Use the <code class="command">radosgw-admin</code> command on the zonegroups and zones
    to configure their placement. You can query the zonegroup placement
    configuration using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup get
{
    "id": "ab01123f-e0df-4f29-9d71-b44888d67cd5",
    "name": "default",
    "api_name": "default",
    ...
    "placement_targets": [
        {
            "name": "default-placement",
            "tags": [],
            "storage_classes": [
                "STANDARD"
            ]
        }
    ],
    "default_placement": "default-placement",
    ...
}</pre></div><p>
    To query the zone placement configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone get
{
    "id": "557cdcee-3aae-4e9e-85c7-2f86f5eddb1f",
    "name": "default",
    "domain_root": "default.rgw.meta:root",
    ...
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "default.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "default.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "default.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    ...
}</pre></div><div id="id-1.3.6.2.15.4.6" data-id-title="No Previous Multisite Configuration" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: No Previous Multisite Configuration</h6><p>
     If you have not done any previous multisite configuration, a 'default'
     zone and zonegroup are created for you, and changes to the zone/zonegroup
     will not take effect until you restart the Ceph Object Gateways. If you have created a
     realm for multisite, the zone/zonegroup changes will take effect after you
     commit the changes with the <code class="command">radosgw-admin period update
     --commit</code> command.
    </p></div><section class="sect3" id="id-1.3.6.2.15.4.7" data-id-title="Adding a Placement Target"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.3.1 </span><span class="title-name">Adding a Placement Target</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.4.7">#</a></h4></div></div></div><p>
     To create a new placement target named 'temporary', start by adding it to
     the zonegroup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id temporary</pre></div><p>
     Then provide the zone placement info for that target:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id temporary \
      --data-pool default.rgw.temporary.data \
      --index-pool default.rgw.temporary.index \
      --data-extra-pool default.rgw.temporary.non-ec</pre></div></section><section class="sect3" id="id-1.3.6.2.15.4.8" data-id-title="Adding a Storage Class"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.3.2 </span><span class="title-name">Adding a Storage Class</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.4.8">#</a></h4></div></div></div><p>
     To add a new storage class named 'COLD' to the 'default-placement' target,
     start by adding it to the zonegroup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id default-placement \
      --storage-class COLD</pre></div><p>
     Then provide the zone placement info for that storage class:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id default-placement \
      --storage-class COLD \
      --data-pool default.rgw.cold.data \
      --compression lz4</pre></div></section></section><section class="sect2" id="ogw-storage-classes-customizing-placement" data-id-title="Customizing Placement"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.12.4 </span><span class="title-name">Customizing Placement</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-customizing-placement">#</a></h3></div></div></div><section class="sect3" id="id-1.3.6.2.15.5.2" data-id-title="Default Placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.4.1 </span><span class="title-name">Default Placement</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.5.2">#</a></h4></div></div></div><p>
     By default, new buckets will use the zonegroup’s 'default_placement'
     target. You can change this zonegroup setting with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup placement default \
      --rgw-zonegroup default \
      --placement-id new-placement</pre></div></section><section class="sect3" id="id-1.3.6.2.15.5.3" data-id-title="User Placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.4.2 </span><span class="title-name">User Placement</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.5.3">#</a></h4></div></div></div><p>
     A Ceph Object Gateway user can override the zonegroup’s default placement target by
     setting a non-empty 'default_placement' field in the user info. Similarly,
     the 'default_storage_class' can override the 'STANDARD' storage class
     applied to objects by default.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user info --uid testid
{
    ...
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    ...
}</pre></div><p>
     If a zonegroup’s placement target contains any tags, users will be unable
     to create buckets with that placement target unless their user info
     contains at least one matching tag in its 'placement_tags' field. This can
     be useful to restrict access to certain types of storage.
    </p><p>
     The <code class="command">radosgw-admin</code> command cannot modify these fields
     directly, therefore you need to edit the JSON format manually:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin metadata get user:<em class="replaceable">USER-ID</em> &gt; user.json
<code class="prompt user">tux &gt; </code>vi user.json     # edit the file as required
<code class="prompt user">cephadm@adm &gt; </code>radosgw-admin metadata put user:<em class="replaceable">USER-ID</em> &lt; user.json</pre></div></section><section class="sect3" id="id-1.3.6.2.15.5.4" data-id-title="S3 Bucket Placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.4.3 </span><span class="title-name">S3 Bucket Placement</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.5.4">#</a></h4></div></div></div><p>
     When creating a bucket with the S3 protocol, a placement target can be
     provided as part of the <code class="option">LocationConstraint</code> to override
     the default placement targets from the user and zonegroup.
    </p><p>
     Normally, the <code class="option">LocationConstraint</code> needs to match the
     zonegroup’s <code class="option">api_name</code>:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;LocationConstraint&gt;default&lt;/LocationConstraint&gt;</pre></div><p>
     You can add a custom placement target to the <code class="option">api_name</code>
     following a colon:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;LocationConstraint&gt;default:new-placement&lt;/LocationConstraint&gt;</pre></div></section><section class="sect3" id="id-1.3.6.2.15.5.5" data-id-title="Swift Bucket Placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.12.4.4 </span><span class="title-name">Swift Bucket Placement</span> <a title="Permalink" class="permalink" href="#id-1.3.6.2.15.5.5">#</a></h4></div></div></div><p>
     When creating a bucket with the Swift protocol, you can provide a
     placement target in the HTTP header's 'X-Storage-Policy':
    </p><div class="verbatim-wrap"><pre class="screen"> X-Storage-Policy: <em class="replaceable">NEW-PLACEMENT</em></pre></div></section></section><section class="sect2" id="ogw-storage-classes-usage" data-id-title="Using Storage Classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.12.5 </span><span class="title-name">Using Storage Classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-usage">#</a></h3></div></div></div><p>
    All placement targets have a 'STANDARD' storage class which is applied to
    new objects by default. You can override this default with its
    'default_storage_class'.
   </p><p>
    To create an object in a non-default storage class, provide that storage
    class name in an HTTP header with the request. The S3 protocol uses the
    'X-Amz-Storage-Class' header, while the Swift protocol uses the
    'X-Object-Storage-Class' header.
   </p><p>
    You can use <span class="emphasis"><em>S3 Object Lifecycle Management</em></span> to move
    object data between storage classes using 'Transition' actions.
   </p></section></section><section class="sect1" id="ceph-rgw-fed" data-id-title="Multisite Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.13 </span><span class="title-name">Multisite Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed">#</a></h2></div></div></div><p>
   Ceph supports several multi-site configuration options for the Ceph Object Gateway:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.2.16.3.1"><span class="term">Multi-zone</span></dt><dd><p>
      A configuration consisting of one zonegroup and multiple zones, each zone
      with one or more <code class="systemitem">ceph-radosgw</code>
      instances. Each zone is backed by its own Ceph Storage Cluster.
      Multiple zones in a zone group provide disaster recovery for the zonegroup
      should one of the zones experience a significant failure. Each zone is
      active and may receive write operations. In addition to disaster
      recovery, multiple active zones may also serve as a foundation for
      content delivery networks.
     </p></dd><dt id="id-1.3.6.2.16.3.2"><span class="term">Multi-zone-group</span></dt><dd><p>
      Ceph Object Gateway supports multiple zonegroups, each zonegroup with one or more zones.
      Objects stored to zones in one zonegroup within the same realm as another
      zonegroup share a global object namespace, ensuring unique object IDs
      across zonegroups and zones.
     </p><div id="id-1.3.6.2.16.3.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       It is important to note that zonegroups <span class="emphasis"><em>only</em></span> sync
       metadata amongst themselves. Data and metadata are replicated between
       the zones within the zonegroup. No data or metadata is shared across a
       realm.
      </p></div></dd><dt id="id-1.3.6.2.16.3.3"><span class="term">Multiple Realms</span></dt><dd><p>
      Ceph Object Gateway supports the notion of realms; a globally unique namespace.
      Multiple realms are supported which may encompass single or multiple
      zonegroups.
     </p></dd></dl></div><p>
   You can configure each Object Gateway to work in an active-active zone configuration,
   allowing for writes to non-master zones. The multi-site configuration is
   stored within a container called a realm. The realm stores zonegroups, zones,
   and a time period with multiple epochs for tracking changes to the
   configuration. The <code class="systemitem">ceph-radosgw</code>
   daemons handle the synchronization, eliminating the need for a separate
   synchronization agent. This approach to synchronization allows the Ceph Object Gateway to
   operate with an active-active configuration instead of active-passive.
  </p><section class="sect2" id="ceph-rgw-multi-req-assump" data-id-title="Requirements and Assumptions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.1 </span><span class="title-name">Requirements and Assumptions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multi-req-assump">#</a></h3></div></div></div><p>
    A multi-site configuration requires at least two Ceph storage clusters,
    and at least two Ceph Object Gateway instances, one for each Ceph storage cluster. The
    following configuration assumes at least two Ceph storage clusters are in
    geographically separate locations. However, the configuration can work on
    the same site. For example, named <code class="literal">rgw1</code> and
    <code class="literal">rgw2</code>.
   </p><p>
    A multi-site configuration requires a master zonegroup and a master zone. A
    master zone is the source of truth with respect to all metadata operations
    in a multisite cluster. Additionally, each zonegroup requires a master zone.
    zonegroups may have one or more secondary or non-master zones. In this
    guide, the <code class="literal">rgw1</code> host serves as the master zone of the
    master zonegroup and the <code class="literal">rgw2</code> host serves as the
    secondary zone of the master zonegroup.
   </p></section><section class="sect2" id="ceph-rgw-multi-limitations" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.2 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multi-limitations">#</a></h3></div></div></div><p>
    Multi-site configurations do not support resharding a bucket index.
   </p><p>
    As a workaround, the bucket can be purged from the slave zones, resharded
    on the master zone, and then resynchronized. Depending on the contents of
    the bucket, this can be a time- and resource-intensive operation.
   </p></section><section class="sect2" id="ceph-rgw-config-master-zone" data-id-title="Configuring a Master Zone"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.3 </span><span class="title-name">Configuring a Master Zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-config-master-zone">#</a></h3></div></div></div><p>
    All gateways in a multi-site configuration retrieve their configuration
    from a <code class="systemitem">ceph-radosgw</code> daemon on a
    host within the master zonegroup and master zone. To configure your gateways
    in a multi-site configuration, choose a
    <code class="systemitem">ceph-radosgw</code> instance to configure
    the master zonegroup and master zone.
   </p><section class="sect3" id="ceph-rgw-fed-realm" data-id-title="Creating a Realm"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.1 </span><span class="title-name">Creating a Realm</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-realm">#</a></h4></div></div></div><p>
     A realm represents a globally unique namespace consisting of one or more
     zonegroups containing one or more zones. Zones contain buckets, which in
     turn contain objects. A realm enables the Ceph Object Gateway to support multiple
     namespaces and their configuration on the same hardware. A realm contains
     the notion of periods. Each period represents the state of the zonegroup
     and zone configuration in time. Each time you make a change to a zonegroup
     or zone, update the period and commit it. By default, the Ceph Object Gateway does not
     create a realm for backward compatibility. As a best practice, we
     recommend creating realms for new clusters.
    </p><p>
     Create a new realm called <code class="literal">gold</code> for the multi-site
     configuration by opening a command line interface on a host identified to
     serve in the master zonegroup and zone. Then, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin realm create --rgw-realm=gold --default</pre></div><p>
     If the cluster has a single realm, specify the <code class="option">--default</code>
     flag. If <code class="option">--default</code> is specified,
     <code class="command">radosgw-admin</code> uses this realm by default. If
     <code class="option">--default</code> is not specified, adding zone-groups and zones
     requires specifying either the <code class="option">--rgw-realm</code> flag or the
     <code class="option">--realm-id</code> flag to identify the realm when adding
     zonegroups and zones.
    </p><p>
     After creating the realm, <code class="command">radosgw-admin</code> returns the
     realm configuration:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</pre></div><div id="id-1.3.6.2.16.7.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph generates a unique ID for the realm, which allows the renaming of
      a realm if the need arises.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-createmasterzonegrp" data-id-title="Creating a Master zonegroup"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.2 </span><span class="title-name">Creating a Master zonegroup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-createmasterzonegrp">#</a></h4></div></div></div><p>
     A realm must have at least one zonegroup to serve as the master zonegroup
     for the realm. Create a new master zonegroup for the multi-site
     configuration by opening a command line interface on a host identified to
     serve in the master zonegroup and zone. Create a master zonegroup called
     <code class="literal">us</code> by executing the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default</pre></div><p>
     If the realm only has a single zonegroup, specify the
     <code class="option">--default</code> flag. If <code class="option">--default</code> is
     specified, <code class="command">radosgw-admin</code> uses this zonegroup by default
     when adding new zones. If <code class="option">--default</code> is not specified,
     adding zones requires either the <code class="option">--rgw-zonegroup</code> flag or
     the <code class="option">--zonegroup-id</code> flag to identify the zonegroup when
     adding or modifying zones.
    </p><p>
     After creating the master zonegroup, <code class="command">radosgw-admin</code>
     returns the zonegroup configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">{
 "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
 "name": "us",
 "api_name": "us",
 "is_master": "true",
 "endpoints": [
     "http:\/\/rgw1:80"
 ],
 "hostnames": [],
 "hostnames_s3website": [],
 "master_zone": "",
 "zones": [],
 "placement_targets": [],
 "default_placement": "",
 "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone" data-id-title="Creating a Master Zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.3 </span><span class="title-name">Creating a Master Zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone">#</a></h4></div></div></div><div id="id-1.3.6.2.16.7.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Zones need to be created on a Ceph Object Gateway node that will be within the zone.
     </p></div><p>
     Create a new master zone for the multi-site configuration by opening a
     command line interface on a host identified to serve in the master
     zonegroup and zone. Execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">SYSTEM_SECRET_KEY</em></pre></div><div id="id-1.3.6.2.16.7.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The <code class="option">--access-key</code> and <code class="option">--secret</code> options
      are not specified in the above example. These settings are added to the
      zone once the user is created in the next section.
     </p></div><p>
     After creating the master zone, <code class="command">radosgw-admin</code> returns
     the zone configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">  {
      "id": "56dfabbb-2f4e-4223-925e-de3c72de3866",
      "name": "us-east-1",
      "domain_root": "us-east-1.rgw.meta:root",
      "control_pool": "us-east-1.rgw.control",
      "gc_pool": "us-east-1.rgw.log:gc",
      "lc_pool": "us-east-1.rgw.log:lc",
      "log_pool": "us-east-1.rgw.log",
      "intent_log_pool": "us-east-1.rgw.log:intent",
      "usage_log_pool": "us-east-1.rgw.log:usage",
      "reshard_pool": "us-east-1.rgw.log:reshard",
      "user_keys_pool": "us-east-1.rgw.meta:users.keys",
      "user_email_pool": "us-east-1.rgw.meta:users.email",
      "user_swift_pool": "us-east-1.rgw.meta:users.swift",
      "user_uid_pool": "us-east-1.rgw.meta:users.uid",
      "otp_pool": "us-east-1.rgw.otp",
      "system_key": {
          "access_key": "1555b35654ad1656d804",
          "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
      },
      "placement_pools": [
          {
              "key": "us-east-1-placement",
              "val": {
                  "index_pool": "us-east-1.rgw.buckets.index",
                  "storage_classes": {
                      "STANDARD": {
                          "data_pool": "us-east-1.rgw.buckets.data"
                      }
                  },
                  "data_extra_pool": "us-east-1.rgw.buckets.non-ec",
                  "index_type": 0
              }
          }
      ],
      "metadata_heap": "",
      "realm_id": ""
  }</pre></div></section><section class="sect3" id="ceph-rgw-fed-deldefzonegrp" data-id-title="Deleting the Default Zone and Group"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.4 </span><span class="title-name">Deleting the Default Zone and Group</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-deldefzonegrp">#</a></h4></div></div></div><p>
     The default installation of Object Gateway creates the default zonegroup called
     <code class="literal">default</code>. Delete the default zone if it exists. Make
     sure to remove it from the default zonegroup first.
    </p><div id="id-1.3.6.2.16.7.6.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following steps assume a multi-site configuration using newly
      installed systems that are not storing data yet. <span class="bold"><strong>Do
      not delete</strong></span> the default zone and its pools if you are already
      using it to store data, or the data will be deleted and unrecoverable.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup delete --rgw-zonegroup=default</pre></div><p>
     Delete the default pools in your Ceph storage cluster if they exist:
    </p><div id="id-1.3.6.2.16.7.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following step assumes a multi-site configuration using newly
      installed systems that are not currently storing data.
      <span class="bold"><strong>Do not delete</strong></span> the default zonegroup if
      you are already using it to store data.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it</pre></div><div id="id-1.3.6.2.16.7.6.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      If you delete the default zonegroup, you are also deleting the system
      user. If your admin user keys are not propagated, the Object Gateway management
      functionality of the Ceph Dashboard will fail. Follow on to the next section
      to re-create your system user if you go ahead with this step.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-createuser" data-id-title="Creating System Users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.5 </span><span class="title-name">Creating System Users</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-createuser">#</a></h4></div></div></div><p>
     The <code class="systemitem">ceph-radosgw</code> daemons must
     authenticate before pulling realm and period information. In the master
     zone, create a system user to facilitate authentication between daemons:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> \
--secret=<em class="replaceable">SYSTEM_SECRET_KEY</em> --system</pre></div><p>
     Make a note of the <code class="option">access_key</code> and
     <code class="option">secret_key</code> as the secondary zones require them to
     authenticate with the master zone.
    </p><p>
     Add the system user to the master zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zone=us-east-1 \
--access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET</em></pre></div><p>
     Update the period to make the changes take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-updateperiod" data-id-title="Update the Period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.6 </span><span class="title-name">Update the Period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-updateperiod">#</a></h4></div></div></div><p>
     After updating the master zone configuration, update the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div><p>
     After updating the period, <code class="command">radosgw-admin</code> returns the
     period configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</pre></div><div id="id-1.3.6.2.16.7.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Updating the period changes the epoch and ensures that other zones
      receive the updated configuration.
     </p></div></section><section class="sect3" id="update-ceph-config-file" data-id-title="Update the Ceph Configuration File"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.7 </span><span class="title-name">Update the Ceph Configuration File</span> <a title="Permalink" class="permalink" href="#update-ceph-config-file">#</a></h4></div></div></div><p>
     Update the Ceph configuration file on master zone hosts by adding the
     <code class="literal">rgw_zone</code> configuration option and the name of the
     master zone to the instance entry.
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable">INSTANCE-NAME</em>]
...
rgw_zone=<em class="replaceable">ZONE-NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.rgw1]
rgw frontends = "beast port=80"
rgw_zone=us-east</pre></div><p>
     For more information on how to do this see
     <a class="xref" href="#ds-custom-cephconf" title="2.14. Adjusting ceph.conf with Custom Settings">Section 2.14, “Adjusting <code class="filename">ceph.conf</code> with Custom Settings”</a>.
    </p></section><section class="sect3" id="ceph-rgw-fed-masterzone-startrgw" data-id-title="Start the Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.3.8 </span><span class="title-name">Start the Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-startrgw">#</a></h4></div></div></div><p>
     On the Object Gateway host, start and enable the Ceph Object Gateway service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl start ceph-radosgw@rgw.`hostname -s`
<code class="prompt user">cephadm@ogw &gt; </code>systemctl enable ceph-radosgw@rgw.`hostname -s`</pre></div></section></section><section class="sect2" id="ceph-rgw-config-secondaryzone" data-id-title="Configure Secondary Zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.4 </span><span class="title-name">Configure Secondary Zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-config-secondaryzone">#</a></h3></div></div></div><p>
    Zones within a zonegroup replicate all data to ensure that each zone has the
    same data. When creating the secondary zone, execute all of the following
    operations on a host identified to serve the secondary zone.
   </p><div id="id-1.3.6.2.16.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     To add a third zone, follow the same procedures as for adding the
     secondary zone. Use different zone name.
    </p></div><div id="id-1.3.6.2.16.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     You must execute metadata operations, such as user creation, on a host
     within the master zone. The master zone and the secondary zone can receive
     bucket operations, but the secondary zone redirects bucket operations to
     the master zone. If the master zone is down, bucket operations will fail.
    </p></div><section class="sect3" id="ceph-rgw-pull-realm" data-id-title="Pull The Realm"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.1 </span><span class="title-name">Pull The Realm</span> <a title="Permalink" class="permalink" href="#ceph-rgw-pull-realm">#</a></h4></div></div></div><p>
     Using the URL path, access key, and secret of the master zone in the
     master zonegroup, pull the realm configuration to the host. To pull a
     non-default realm, specify the realm using the
     <code class="option">--rgw-realm</code> or <code class="option">--realm-id</code> configuration
     options.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin realm pull --url=<em class="replaceable">url-to-master-zone-gateway</em> --access-key=<em class="replaceable">access-key</em> --secret=<em class="replaceable">secret</em></pre></div><div id="id-1.3.6.2.16.8.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Pulling the realm also retrieves the remote’s current period
      configuration, and makes it the current period on this host as well.
     </p></div><p>
     If this realm is the default realm or the only realm, make the realm the
     default realm.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin realm default --rgw-realm=<em class="replaceable">REALM-NAME</em></pre></div></section><section class="sect3" id="cceph-rgw-create-secondaryzone" data-id-title="Create a Secondary Zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.2 </span><span class="title-name">Create a Secondary Zone</span> <a title="Permalink" class="permalink" href="#cceph-rgw-create-secondaryzone">#</a></h4></div></div></div><p>
     Create a secondary zone for the multi-site configuration by opening a
     command line interface on a host identified to serve the secondary zone.
     Specify the zonegroup ID, the new zone name and an endpoint for the zone.
     <span class="emphasis"><em>Do not</em></span> use the <code class="option">--master</code> flag. All
     zones run in an active-active configuration by default. If the secondary
     zone should not accept write operations, specify the
     <code class="option">--read-only</code> flag to create an active-passive
     configuration between the master zone and the secondary zone.
     Additionally, provide the <code class="option">access_key</code> and
     <code class="option">secret_key</code> of the generated system user stored in the
     master zone of the master zonegroup. Execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em>\
                            --rgw-zone=<em class="replaceable">ZONE-NAME</em> --endpoints=<em class="replaceable">URL</em> \
                            --access-key=<em class="replaceable">SYSTEM-KEY</em> --secret=<em class="replaceable">SECRET</em>\
                            --endpoints=http://<em class="replaceable">FQDN</em>:80 \
                            [--read-only]</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">SYSTEM_SECRET_KEY</em>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</pre></div><div id="id-1.3.6.2.16.8.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following steps assume a multi-site configuration using newly
      installed systems that are not storing data. <span class="bold"><strong>Do not
      delete</strong></span> the default zone and its pools if you are already using
      it to store data, or the data will be lost and unrecoverable.
     </p></div><p>
     Delete the default zone if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone rm --rgw-zone=default</pre></div><p>
     Delete the default pools in your Ceph storage cluster if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool rm default.rgw.users.uid default.rgw.users.uid --yes-i-really-really-mean-it</pre></div></section><section class="sect3" id="ceph-rgw-secondzone-update-config" data-id-title="Update the Ceph Configuration File"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.3 </span><span class="title-name">Update the Ceph Configuration File</span> <a title="Permalink" class="permalink" href="#ceph-rgw-secondzone-update-config">#</a></h4></div></div></div><p>
     Update the Ceph configuration file on the secondary zone hosts by adding
     the <code class="literal">rgw_zone</code> configuration option and the name of the
     secondary zone to the instance entry.
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.<em class="replaceable">INSTANCE-NAME</em>]
...
rgw_zone=<em class="replaceable">ZONE-NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">[client.rgw.rgw2]
host = rgw2
rgw frontends = "civetweb port=80"
rgw_zone=us-west</pre></div></section><section class="sect3" id="ceph-rgw-fed-secondzone-updateperiod" data-id-title="Update the Period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.4 </span><span class="title-name">Update the Period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-updateperiod">#</a></h4></div></div></div><p>
     After updating the master zone configuration, update the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</pre></div><div id="id-1.3.6.2.16.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Updating the period changes the epoch and ensures that other zones
      receive the updated configuration.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-secondzone-startrgw" data-id-title="Start the Object Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.5 </span><span class="title-name">Start the Object Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-startrgw">#</a></h4></div></div></div><p>
     On the Object Gateway host, start and enable the Ceph Object Gateway service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl start ceph-radosgw@rgw.us-east-2
<code class="prompt user">cephadm@ogw &gt; </code>systemctl enable ceph-radosgw@rgw.us-east-2</pre></div></section><section class="sect3" id="ceph-rgw-check-sync-status" data-id-title="Check Synchronization Status"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.4.6 </span><span class="title-name">Check Synchronization Status</span> <a title="Permalink" class="permalink" href="#ceph-rgw-check-sync-status">#</a></h4></div></div></div><p>
     Once the secondary zone is up and running, check the synchronization
     status. Synchronization copies users and buckets created in the master
     zone to the secondary zone.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin sync status</pre></div><p>
     The output provides the status of synchronization operations. For example:
    </p><div class="verbatim-wrap"><pre class="screen">realm f3239bc5-e1a8-4206-a81d-e1576480804d (gold)
    zonegroup c50dbb7e-d9ce-47cc-a8bb-97d9b399d388 (us)
         zone 4c453b70-4a16-4ce8-8185-1893b05d346e (us-west)
metadata sync syncing
              full sync: 0/64 shards
              metadata is caught up with master
              incremental sync: 64/64 shards
    data sync source: 1ee9da3e-114d-4ae3-a8a4-056e8a17f532 (us-east)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source</pre></div><div id="id-1.3.6.2.16.8.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Secondary zones accept bucket operations; however, secondary zones
      redirect bucket operations to the master zone and then synchronize with
      the master zone to receive the result of the bucket operations. If the
      master zone is down, bucket operations executed on the secondary zone
      will fail, but object operations should succeed.
     </p></div></section></section><section class="sect2" id="ceph-rgw-maintenance" data-id-title="Maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.5 </span><span class="title-name">Maintenance</span> <a title="Permalink" class="permalink" href="#ceph-rgw-maintenance">#</a></h3></div></div></div><section class="sect3" id="ceph-rgw-check-sync" data-id-title="Checking the Sync Status"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.5.1 </span><span class="title-name">Checking the Sync Status</span> <a title="Permalink" class="permalink" href="#ceph-rgw-check-sync">#</a></h4></div></div></div><p>
     Information about the replication status of a zone can be queried with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin sync status
        realm b3bc1c37-9c44-4b89-a03b-04c269bea5da (gold)
    zonegroup f54f9b22-b4b6-4a0e-9211-fa6ac1693f49 (us)
         zone adce11c9-b8ed-4a90-8bc5-3fc029ff0816 (us-west)
        metadata sync syncing
              full sync: 0/64 shards
              incremental sync: 64/64 shards
              metadata is behind on 1 shards
              oldest incremental change not applied: 2017-03-22 10:20:00.0.881361s
    data sync source: 341c2d81-4574-4d08-ab0f-5a2a7b168028 (us-east)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source
              source: 3b5d1a3f-3f27-4e4a-8f34-6072d4bb1275 (us-3)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source</pre></div></section><section class="sect3" id="ceph-rgw-metadata-master" data-id-title="Changing the Metadata Master Zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.5.2 </span><span class="title-name">Changing the Metadata Master Zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-metadata-master">#</a></h4></div></div></div><div id="id-1.3.6.2.16.9.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Be careful when changing which zone is the metadata master. If a zone has
      not finished syncing metadata from the current master zone, it is unable
      to serve any remaining entries when promoted to master and those changes
      will be lost. For this reason, we recommend waiting for a zone’s
      <code class="command">radosgw-admin</code> sync status to catch up on metadata sync
      before promoting it to master. Similarly, if changes to metadata are
      being processed by the current master zone while another zone is being
      promoted to master, those changes are likely to be lost. To avoid this,
      we recommend shutting down any Object Gateway instances on the previous master
      zone. After promoting another zone, its new period can be fetched with
      <code class="command">radosgw-admin</code> period pull and the gateway(s) can be
      restarted.
     </p></div><p>
     To promote a zone (for example, zone <code class="literal">us-west</code> in
     zonegroup <code class="literal">us</code>) to metadata master, run the following
     commands on that zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>radosgw-admin zone modify --rgw-zone=us-west --master
<code class="prompt user">cephadm@ogw &gt; </code>radosgw-admin zonegroup modify --rgw-zonegroup=us --master
<code class="prompt user">cephadm@ogw &gt; </code>radosgw-admin period update --commit</pre></div><p>
     This generates a new period, and the Object Gateway instance(s) in zone
     <code class="literal">us-west</code> sends this period to other zones.
    </p></section><section class="sect3" id="ceph-rgw-multisite-bucket-reshard" data-id-title="Resharding a bucket index"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">26.13.5.3 </span><span class="title-name">Resharding a bucket index</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multisite-bucket-reshard">#</a></h4></div></div></div><div id="id-1.3.6.2.16.9.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Resharding a bucket index in a multi-site setup requires a full
      resynchronization of the bucket content. Depending on the size and number
      of objects in the bucket, this is a time- and resource-intensive
      operation.
     </p></div><div class="procedure" id="id-1.3.6.2.16.9.4.3" data-id-title="Resharding the bucket index"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 26.2: </span><span class="title-name">Resharding the bucket index </span><a title="Permalink" class="permalink" href="#id-1.3.6.2.16.9.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Make sure that all operations to the bucket are stopped.
      </p></li><li class="step"><p>
       Back up the original bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bi list \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 &gt; <em class="replaceable">BUCKET_NAME</em>.list.backup</pre></div></li><li class="step"><p>
       Disable bucket synchronization for the affected bucket:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket sync disable --bucket=<em class="replaceable">BUCKET_NAME</em></pre></div></li><li class="step"><p>
       Wait for the synchronization to finish on all zones. Check on master and
       slave zones with the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin sync status</pre></div></li><li class="step"><p>
       Stop the Object Gateway instances. First on all slave zones, then on the master
       zone, too.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl stop ceph-radosgw@rgw.<em class="replaceable">NODE</em>.service</pre></div></li><li class="step"><p>
       Reshard the bucket index on the master zone:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket reshard \
  --bucket=<em class="replaceable">BUCKET_NAME</em> \
  --num-shards=<em class="replaceable">NEW_SHARDS_NUMBER</em></pre></div><div id="id-1.3.6.2.16.9.4.3.7.3" data-id-title="Old bucket ID" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Old bucket ID</h6><p>
        As part of its output, this command also prints the new and the old
        bucket ID.
       </p></div></li><li class="step"><p>
       Purge the bucket on all slave zones:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket rm \
  --purge-objects \
  --bucket=<em class="replaceable">BUCKET_NAME</em> \
  --yes-i-really-mean-it</pre></div></li><li class="step"><p>
       Restart the Object Gateway on the master zone first, then on the slave zones as
       well.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl restart ceph-radosgw.target</pre></div></li><li class="step"><p>
       On the master zone, re-enable bucket synchronization.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin bucket sync enable --bucket=<em class="replaceable">BUCKET_NAME</em></pre></div></li></ol></div></div></section></section><section class="sect2" id="ceph-rgw-failover-dr" data-id-title="Failover and Disaster Recovery"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.6 </span><span class="title-name">Failover and Disaster Recovery</span> <a title="Permalink" class="permalink" href="#ceph-rgw-failover-dr">#</a></h3></div></div></div><p>
    If the master zone should fail, failover to the secondary zone for disaster
    recovery.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make the secondary zone the master and default zone. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default</pre></div><p>
      By default, Ceph Object Gateway runs in an active-active configuration. If the cluster
      was configured to run in an active-passive configuration, the secondary
      zone is a read-only zone. Remove the <code class="option">--read-only</code> status
      to allow the zone to receive write operations. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default \
                                                   --read-only=false</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>systemctl restart ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div><p>
    If the former master zone recovers, revert the operation.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      From the recovered zone, pull the latest realm configuration from the
      current master zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin realm pull --url=<em class="replaceable">URL-TO-MASTER-ZONE-GATEWAY</em> \
                           --access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET</em></pre></div></li><li class="step"><p>
      Make the recovered zone the master and default zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway in the recovered zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl restart ceph-radosgw@rgw.`hostname -s`</pre></div></li><li class="step"><p>
      If the secondary zone needs to be a read-only configuration, update the
      secondary zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --read-only</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway in the secondary zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl restart ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div></section><section class="sect2" id="ceph-rgw-single-site-multi" data-id-title="Migrating a Single Site System to Multi-Site"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">26.13.7 </span><span class="title-name">Migrating a Single Site System to Multi-Site</span> <a title="Permalink" class="permalink" href="#ceph-rgw-single-site-multi">#</a></h3></div></div></div><p>
    To migrate from a single site system with a default zonegroup and zone to a
    multi-site system, use the following steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a realm. Replace <em class="replaceable">NAME</em> with the realm
      name.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin realm create --rgw-realm=<em class="replaceable">NAME</em> --default</pre></div></li><li class="step"><p>
      Create a system user. Replace <em class="replaceable">USER-ID</em> with the
      username. Replace <em class="replaceable">DISPLAY-NAME</em> with a display
      name. Only the display name may contain spaces.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USER-ID</em> --display-name="<em class="replaceable">DISPLAY-NAME</em>"\
                            --access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET-KEY</em> --system</pre></div></li><li class="step"><p>
      Rename the default zone and zonegroup. Replace
      <em class="replaceable">NAME</em> with the zonegroup or zone name.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup rename --rgw-zonegroup default --zonegroup-new-name=<em class="replaceable">NAME</em>
<code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone rename --rgw-zone default --zone-new-name us-east-1 --rgw-zonegroup=<em class="replaceable">NAME</em></pre></div></li><li class="step"><p>
      Configure the master zonegroup. Replace <em class="replaceable">NAME</em>
      with the realm or zonegroup name. Replace <em class="replaceable">FQDN</em>
      with the fully qualified domain name(s) in the zonegroup.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zonegroup modify --rgw-realm=<em class="replaceable">NAME</em> --rgw-zonegroup=<em class="replaceable">NAME</em> --endpoints http://<em class="replaceable">FQDN</em>:80 --master --default</pre></div></li><li class="step"><p>
      Configure the master zone. Replace <em class="replaceable">NAME</em> with
      the realm, zonegroup or zone name. Replace <em class="replaceable">FQDN</em>
      with the fully qualified domain name(s) in the zonegroup.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin zone modify --rgw-realm=<em class="replaceable">NAME</em> --rgw-zonegroup=<em class="replaceable">NAME</em> \
                            --rgw-zone=<em class="replaceable">NAME</em> --endpoints http://<em class="replaceable">FQDN</em>:80 \
                            --access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET-KEY</em> \
                            --master --default</pre></div></li><li class="step"><p>
      Commit the updated configuration.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@ogw &gt; </code>systemctl restart ceph-radosgw@rgw.`hostname -s`</pre></div></li></ol></div></div><p>
    After completing this procedure, configure a secondary zone to create a
    secondary zone in the master zonegroup.
   </p></section></section><section class="sect1" id="ogw-haproxy" data-id-title="Load Balancing the Object Gateway Servers with HAProxy"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.14 </span><span class="title-name">Load Balancing the Object Gateway Servers with HAProxy</span> <a title="Permalink" class="permalink" href="#ogw-haproxy">#</a></h2></div></div></div><p>
   You can use the HAProxy load balancer to distribute all requests across
   multiple Object Gateway back-end servers. Refer to
   <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-lb.html#sec-ha-lb-haproxy" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-lb.html#sec-ha-lb-haproxy</a>
   for more details on configuring HAProxy.
  </p><p>
   Following is a simple configuration of HAProxy for balancing Object Gateway nodes
   using round robin as the balancing algorithm:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>cat /etc/haproxy/haproxy.cfg
[...]
frontend <em class="replaceable">HTTPS_FRONTEND</em>
bind *:443 crt <em class="replaceable">path-to-cert.pem</em> [ciphers: ... ]
default_backend rgw

backend rgw
mode http
balance roundrobin
server rgw_server1 <em class="replaceable">RGW-ENDPOINT1</em> weight 1 maxconn 100 check
server rgw_server2 <em class="replaceable">RGW-ENDPOINT2</em> weight 1 maxconn 100 check
[...]</pre></div></section></section><section class="chapter" id="cha-ceph-iscsi" data-id-title="Ceph iSCSI Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27 </span><span class="title-name">Ceph iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-iscsi">#</a></h2></div></div></div><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”</span>.
  
 </p><section class="sect1" id="ceph-iscsi-connect" data-id-title="Connecting to ceph-iscsi Managed Targets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.1 </span><span class="title-name">Connecting to <code class="systemitem">ceph-iscsi</code> Managed Targets</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect">#</a></h2></div></div></div><p>
   This chapter describes how to connect to <code class="systemitem">ceph-iscsi</code> managed targets from
   clients running Linux, Microsoft Windows, or VMware.
  </p><section class="sect2" id="ceph-iscsi-connect-linux" data-id-title="Linux (open-iscsi)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">27.1.1 </span><span class="title-name">Linux (<code class="systemitem">open-iscsi</code>)</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux">#</a></h3></div></div></div><p>
    Connecting to <code class="systemitem">ceph-iscsi</code> backed iSCSI targets with
    <code class="systemitem">open-iscsi</code> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </p><p>
    Both steps require that the <code class="systemitem">open-iscsi</code> daemon is
    running. The way you start the <code class="systemitem">open-iscsi</code> daemon
    is dependent on your Linux distribution:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      On SUSE Linux Enterprise Server (SLES); and Red Hat Enterprise Linux (RHEL) hosts, run <code class="command">systemctl start
      iscsid</code> (or <code class="command">service iscsid start</code> if
      <code class="command">systemctl</code> is not available).
     </p></li><li class="listitem"><p>
      On Debian and Ubuntu hosts, run <code class="command">systemctl start
      open-iscsi</code> (or <code class="command">service open-iscsi start</code>).
     </p></li></ul></div><p>
    If your initiator host runs SUSE Linux Enterprise Server, refer to
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-iscsi.html#sec-iscsi-initiator" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-iscsi.html#sec-iscsi-initiator</a>
    for details on how to connect to an iSCSI target.
   </p><p>
    For any other Linux distribution supporting
    <code class="systemitem">open-iscsi</code>, proceed to discover targets on your
    <code class="systemitem">ceph-iscsi</code> gateway (this example uses iscsi1.example.com as the portal
    address; for multipath access repeat these steps with iscsi2.example.com):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</pre></div><p>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    Repeat this process for other portal IP addresses or hosts.
   </p><p>
    If your system has the <code class="systemitem">lsscsi</code> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </p><div class="verbatim-wrap"><pre class="screen">lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</pre></div><p>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <code class="systemitem">multipath</code> utility:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</pre></div><p>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div><p>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </p><p>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </p><section class="sect3" id="ceph-iscsi-connect-linux-multipath" data-id-title="Multipath Configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">27.1.1.1 </span><span class="title-name">Multipath Configuration</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux-multipath">#</a></h4></div></div></div><p>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <code class="systemitem">ceph-iscsi</code> configuration. Select a strategy prior to
     using block storage. After editing the
     <code class="filename">/etc/multipath.conf</code>, restart
     <code class="systemitem">multipathd</code> with
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart multipathd</pre></div><p>
     For an active-passive configuration with friendly names, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. After connecting to your
     targets successfully, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</pre></div><p>
     Note the status of each link. For an active-active configuration, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. Restart
     <code class="systemitem">multipathd</code> and run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</pre></div></section></section><section class="sect2" id="ceph-iscsi-connect-win" data-id-title="Microsoft Windows (Microsoft iSCSI initiator)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">27.1.2 </span><span class="title-name">Microsoft Windows (Microsoft iSCSI initiator)</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-win">#</a></h3></div></div></div><p>
    To connect to a SUSE Enterprise Storage iSCSI target from a Windows 2012 server,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open Windows Server Manager. From the Dashboard, select
      <span class="guimenu">Tools</span> / <span class="guimenu">iSCSI
      Initiator</span>. The <span class="guimenu">iSCSI Initiator
      Properties</span> dialog appears. Select the
      <span class="guimenu">Discovery</span> tab:
     </p><div class="figure" id="id-1.3.6.3.4.4.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-initiator-props.png" target="_blank"><img src="images/iscsi-initiator-props.png" width="" alt="iSCSI Initiator Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.1: </span><span class="title-name">iSCSI Initiator Properties </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">Discover Target Portal</span> dialog, enter the
      target's host name or IP address in the <span class="guimenu">Target</span> field
      and click <span class="guimenu">OK</span>:
     </p><div class="figure" id="id-1.3.6.3.4.4.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip.png" target="_blank"><img src="images/iscsi-target-ip.png" width="" alt="Discover Target Portal"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.2: </span><span class="title-name">Discover Target Portal </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <span class="guimenu">Target Portals</span> list:
     </p><div class="figure" id="id-1.3.6.3.4.4.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip-list.png" target="_blank"><img src="images/iscsi-target-ip-list.png" width="" alt="Target Portals"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.3: </span><span class="title-name">Target Portals </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.3.2">#</a></h6></div></div></li><li class="step"><p>
      Next, switch to the <span class="guimenu">Targets</span> tab and review your
      discovered target(s).
     </p><div class="figure" id="id-1.3.6.3.4.4.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-targets.png" target="_blank"><img src="images/iscsi-targets.png" width="" alt="Targets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.4: </span><span class="title-name">Targets </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.4.2">#</a></h6></div></div></li><li class="step"><p>
      Click <span class="guimenu">Connect</span> in the <span class="guimenu">Targets</span> tab.
      The <span class="guimenu">Connect To Target</span> dialog appears. Select the
      <span class="guimenu">Enable Multi-path</span> check box to enable multipath I/O
      (MPIO), then click <span class="guimenu">OK</span>:
     </p></li><li class="step"><p>
      When the <span class="guimenu">Connect to Target</span> dialog closes, select
      <span class="guimenu">Properties</span> to review the target's properties:
     </p><div class="figure" id="id-1.3.6.3.4.4.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-properties.png" target="_blank"><img src="images/iscsi-target-properties.png" width="" alt="iSCSI Target Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.5: </span><span class="title-name">iSCSI Target Properties </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.6.2">#</a></h6></div></div></li><li class="step"><p>
      Select <span class="guimenu">Devices</span>, and click <span class="guimenu">MPIO</span> to
      review the multipath I/O configuration:
     </p><div class="figure" id="id-1.3.6.3.4.4.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-device-details.png" target="_blank"><img src="images/iscsi-device-details.png" width="" alt="Device Details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.6: </span><span class="title-name">Device Details </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.3.7.2">#</a></h6></div></div><p>
      The default <span class="guimenu">Load Balance policy</span> is <span class="guimenu">Round
      Robin With Subset</span>. If you prefer a pure fail-over
      configuration, change it to <span class="guimenu">Fail Over Only</span>.
     </p></li></ol></div></div><p>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <span class="guimenu">OK</span> to close the <span class="guimenu">iSCSI
    Initiator Properties</span> dialog, and proceed with the<span class="guimenu"> File
    and Storage Services</span> role from the <span class="guimenu">Server
    Manager</span> dashboard.
   </p><p>
    Observe the newly connected volume. It identifies as <span class="emphasis"><em>SUSE RBD
    SCSI Multi-Path Drive</em></span> on the iSCSI bus, and is initially marked
    with an <span class="emphasis"><em>Offline</em></span> status and a partition table type of
    <span class="emphasis"><em>Unknown</em></span>. If the new volume does not appear
    immediately, select <span class="guimenu">Rescan Storage</span> from the
    <span class="guimenu">Tasks</span> drop-down box to rescan the iSCSI bus.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Right-click on the iSCSI volume and select <span class="guimenu">New Volume</span>
      from the context menu. The <span class="guimenu">New Volume Wizard</span> appears.
      Click <span class="guimenu">Next</span>, highlight the newly connected iSCSI volume
      and click <span class="guimenu">Next</span> to begin.
     </p><div class="figure" id="id-1.3.6.3.4.4.6.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-wizard.png" target="_blank"><img src="images/iscsi-volume-wizard.png" width="" alt="New Volume Wizard"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.7: </span><span class="title-name">New Volume Wizard </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.6.1.2">#</a></h6></div></div></li><li class="step"><p>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </p><div class="figure" id="id-1.3.6.3.4.4.6.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-win-prompt1.png" target="_blank"><img src="images/iscsi-win-prompt1.png" width="" alt="Offline Disk Prompt"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.8: </span><span class="title-name">Offline Disk Prompt </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.6.2.2">#</a></h6></div></div></li><li class="step"><p>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or directory name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <span class="guimenu">Create</span> to finish creating the volume:
     </p><div class="figure" id="id-1.3.6.3.4.4.6.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-confirm.png" target="_blank"><img src="images/iscsi-volume-confirm.png" width="" alt="Confirm Volume Selections"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.9: </span><span class="title-name">Confirm Volume Selections </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.4.6.3.2">#</a></h6></div></div><p>
      When the process finishes, review the results, then
      <span class="guimenu">Close</span> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-iscsi-connect-vmware" data-id-title="VMware"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">27.1.3 </span><span class="title-name">VMware</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-vmware">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To connect to <code class="systemitem">ceph-iscsi</code> managed iSCSI volumes you need a configured iSCSI
      software adapter. If no such adapter is available in your vSphere
      configuration, create one by selecting
      <span class="guimenu">Configuration</span> / <span class="guimenu">Storage
      Adapters</span> / <span class="guimenu">Add</span> / <span class="guimenu">iSCSI Software
      initiator</span>.
     </p></li><li class="step"><p>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <span class="guimenu">Properties</span> from the context
      menu:
     </p><div class="figure" id="id-1.3.6.3.4.5.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi_vmware_adapter_props.png" target="_blank"><img src="images/iscsi_vmware_adapter_props.png" width="" alt="iSCSI Initiator Properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.10: </span><span class="title-name">iSCSI Initiator Properties </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">iSCSI Software Initiator</span> dialog, click the
      <span class="guimenu">Configure</span> button. Then go to the <span class="guimenu">Dynamic
      Discovery</span> tab and select <span class="guimenu">Add</span>.
     </p></li><li class="step"><p>
      Enter the IP address or host name of your <code class="systemitem">ceph-iscsi</code> iSCSI gateway. If you
      run multiple iSCSI gateways in a failover configuration, repeat this step
      for as many gateways as you operate.
     </p><div class="figure" id="id-1.3.6.3.4.5.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-target.png" target="_blank"><img src="images/iscsi-vmware-add-target.png" width="" alt="Add Target Server"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.11: </span><span class="title-name">Add Target Server </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.4.2">#</a></h6></div></div><p>
      When you have entered all iSCSI gateways, click <span class="guimenu">OK</span> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </p></li><li class="step"><p>
      When the rescan completes, the new iSCSI device appears below the
      <span class="guimenu">Storage Adapters</span> list in the
      <span class="guimenu">Details</span> pane. For multipath devices, you can now
      right-click on the adapter and select <span class="guimenu">Manage Paths</span>
      from the context menu:
     </p><div class="figure" id="id-1.3.6.3.4.5.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-multipath.png" target="_blank"><img src="images/iscsi-vmware-multipath.png" width="" alt="Manage Multipath Devices"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.12: </span><span class="title-name">Manage Multipath Devices </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.5.2">#</a></h6></div></div><p>
      You should now see all paths with a green light under
      <span class="guimenu">Status</span>. One of your paths should be marked
      <span class="guimenu">Active (I/O)</span> and all others simply
      <span class="guimenu">Active</span>:
     </p><div class="figure" id="id-1.3.6.3.4.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-paths.png" target="_blank"><img src="images/iscsi-vmware-paths.png" width="" alt="Paths Listing for Multipath"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.13: </span><span class="title-name">Paths Listing for Multipath </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.5.4">#</a></h6></div></div></li><li class="step"><p>
      You can now switch from <span class="guimenu">Storage Adapters</span> to the item
      labeled <span class="guimenu">Storage</span>. Select <span class="guimenu">Add
      Storage...</span> in the top-right corner of the pane to bring up the
      <span class="guimenu">Add Storage</span> dialog. Then, select
      <span class="guimenu">Disk/LUN</span> and click <span class="guimenu">Next</span>. The newly
      added iSCSI device appears in the <span class="guimenu">Select Disk/LUN</span>
      list. Select it, then click <span class="guimenu">Next</span> to proceed:
     </p><div class="figure" id="id-1.3.6.3.4.5.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-storage-dialog.png" target="_blank"><img src="images/iscsi-vmware-add-storage-dialog.png" width="" alt="Add Storage Dialog"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.14: </span><span class="title-name">Add Storage Dialog </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.6.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Next</span> to accept the default disk layout.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Properties</span> pane, assign a name to the new
      datastore, and click <span class="guimenu">Next</span>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <span class="guimenu">Custom Space Setting</span> for a smaller datastore:
     </p><div class="figure" id="id-1.3.6.3.4.5.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-custom-datastore.png" target="_blank"><img src="images/iscsi-vmware-custom-datastore.png" width="" alt="Custom Space Setting"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.15: </span><span class="title-name">Custom Space Setting </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.7.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Finish</span> to complete the datastore creation.
     </p><p>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the <code class="systemitem">ceph-iscsi</code> backed iSCSI
      volume like any other vSphere datastore.
     </p><div class="figure" id="id-1.3.6.3.4.5.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-overview.png" target="_blank"><img src="images/iscsi-vmware-overview.png" width="" alt="iSCSI Datastore Overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 27.16: </span><span class="title-name">iSCSI Datastore Overview </span><a title="Permalink" class="permalink" href="#id-1.3.6.3.4.5.3.7.5">#</a></h6></div></div></li></ol></div></div></section></section><section class="sect1" id="ceph-iscsi-conclude" data-id-title="Conclusion"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.2 </span><span class="title-name">Conclusion</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-conclude">#</a></h2></div></div></div><p>
   <code class="systemitem">ceph-iscsi</code> is a key component of SUSE Enterprise Storage 6 that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using <code class="systemitem">ceph-iscsi</code> on one or
   more iSCSI gateway hosts, Ceph RBD images become available as Logical
   Units (LUs) associated with iSCSI targets, which can be accessed in an
   optionally load-balanced, highly available fashion.
  </p><p>
   Since all of <code class="systemitem">ceph-iscsi</code> configuration is stored in the Ceph RADOS object
   store, <code class="systemitem">ceph-iscsi</code> gateway hosts are inherently without persistent state and
   thus can be replaced, augmented, or reduced at will. As a result,
   SUSE Enterprise Storage 6 enables SUSE customers to run a truly
   distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open source
   platform.
  </p></section></section><section class="chapter" id="cha-ceph-cephfs" data-id-title="Clustered File System"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28 </span><span class="title-name">Clustered File System</span> <a title="Permalink" class="permalink" href="#cha-ceph-cephfs">#</a></h2></div></div></div><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
 </p><section class="sect1" id="ceph-cephfs-cephfs-mount" data-id-title="Mounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.1 </span><span class="title-name">Mounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-mount">#</a></h2></div></div></div><p>
   When the file system is created and the MDS is active, you are ready to
   mount the file system from a client host.
  </p><section class="sect2" id="cephfs-client-preparation" data-id-title="Client Preparation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.1 </span><span class="title-name">Client Preparation</span> <a title="Permalink" class="permalink" href="#cephfs-client-preparation">#</a></h3></div></div></div><p>
    If the client host is running SUSE Linux Enterprise 12 SP2 or SP3, you can skip this
    section as the system is ready to mount CephFS 'out of the box'.
   </p><p>
    If the client host is running SUSE Linux Enterprise 12 SP1, you need to apply all the
    latest patches before mounting CephFS.
   </p><p>
    In any case, everything needed to mount CephFS is included in SUSE Linux Enterprise. The
    SUSE Enterprise Storage 6 product is not needed.
   </p><p>
    To support the full <code class="command">mount</code> syntax, the
    <span class="package">ceph-common</span> package (which is shipped with SUSE Linux Enterprise) should
    be installed before trying to mount CephFS.
   </p><div id="id-1.3.6.4.4.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Without the <span class="package">ceph-common</span> package (and thus without the
     <code class="command">mount.ceph</code> helper), the monitors' IPs will need to be
     used instead of their names. This is because the kernel client will be
     unable to perform name resolution.
    </p><p>
     The basic mount syntax is:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>sudo mount -t ceph monip[:port][,monip2[:port]...]:/[subdir] mnt</pre></div></div></section><section class="sect2" id="Creating-Secret-File" data-id-title="Create a Secret File"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.2 </span><span class="title-name">Create a Secret File</span> <a title="Permalink" class="permalink" href="#Creating-Secret-File">#</a></h3></div></div></div><p>
    The Ceph cluster runs with authentication turned on by default. You
    should create a file that stores your secret key (not the keyring itself).
    To obtain the secret key for a particular user and then create the file, do
    the following:
   </p><div class="procedure" id="id-1.3.6.4.4.4.3" data-id-title="Creating a Secret Key"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 28.1: </span><span class="title-name">Creating a Secret Key </span><a title="Permalink" class="permalink" href="#id-1.3.6.4.4.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      View the key for the particular user in a keyring file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>cat /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
      Copy the key of the user who will be using the mounted Ceph FS file
      system. Usually, the key looks similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen">AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</pre></div></li><li class="step"><p>
      Create a file with the user name as a file name part, for example
      <code class="filename">/etc/ceph/admin.secret</code> for the user
      <span class="emphasis"><em>admin</em></span>.
     </p></li><li class="step"><p>
      Paste the key value to the file created in the previous step.
     </p></li><li class="step"><p>
      Set proper access rights to the file. The user should be the only one who
      can read the file—others may not have any access rights.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-cephfs-krnldrv" data-id-title="Mount CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.3 </span><span class="title-name">Mount CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-krnldrv">#</a></h3></div></div></div><p>
    You can mount CephFS with the <code class="command">mount</code> command. You need
    to specify the monitor host name or IP address. Because the
    <code class="systemitem">cephx</code> authentication is enabled by default in
    SUSE Enterprise Storage, you need to specify a user name and their related secret as
    well:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><p>
    As the previous command remains in the shell history, a more secure
    approach is to read the secret from a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    Note that the secret file should only contain the actual keyring secret. In
    our example, the file will then contain only the following line:
   </p><div class="verbatim-wrap"><pre class="screen">AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><div id="id-1.3.6.4.4.5.8" data-id-title="Specify Multiple Monitors" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Specify Multiple Monitors</h6><p>
     It is a good idea to specify multiple monitors separated by commas on the
     <code class="command">mount</code> command line in case one monitor happens to be
     down at the time of mount. Each monitor address takes the form
     <code class="literal">host[:port]</code>. If the port is not specified, it defaults
     to 6789.
    </p></div><p>
    Create the mount point on the local host:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir /mnt/cephfs</pre></div><p>
    Mount the CephFS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    A subdirectory <code class="filename">subdir</code> may be specified if a subset of
    the file system is to be mounted:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    You can specify more than one monitor host in the <code class="command">mount</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><div id="id-1.3.6.4.4.5.17" data-id-title="Read Access to the Root Directory" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Read Access to the Root Directory</h6><p>
     If clients with path restriction are used, the MDS capabilities need to
     include read access to the root directory. For example, a keyring may look
     as follows:
    </p><div class="verbatim-wrap"><pre class="screen">client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</pre></div><p>
     The <code class="literal">allow r path=/</code> part means that path-restricted
     clients are able to see the root volume, but cannot write to it. This may
     be an issue for use cases where complete isolation is a requirement.
    </p></div></section></section><section class="sect1" id="ceph-cephfs-cephfs-unmount" data-id-title="Unmounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.2 </span><span class="title-name">Unmounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-unmount">#</a></h2></div></div></div><p>
   To unmount the CephFS, use the <code class="command">umount</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>umount /mnt/cephfs</pre></div></section><section class="sect1" id="ceph-cephfs-cephfs-fstab" data-id-title="CephFS in /etc/fstab"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.3 </span><span class="title-name">CephFS in <code class="filename">/etc/fstab</code></span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-fstab">#</a></h2></div></div></div><p>
   To mount CephFS automatically upon client start-up, insert the
   corresponding line in its file systems table
   <code class="filename">/etc/fstab</code>:
  </p><div class="verbatim-wrap"><pre class="screen">mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</pre></div></section><section class="sect1" id="ceph-cephfs-activeactive" data-id-title="Multiple Active MDS Daemons (Active-Active MDS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.4 </span><span class="title-name">Multiple Active MDS Daemons (Active-Active MDS)</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-activeactive">#</a></h2></div></div></div><p>
   CephFS is configured for a single active MDS daemon by default. To scale
   metadata performance for large-scale systems, you can enable multiple active
   MDS daemons, which will share the metadata workload with one another.
  </p><section class="sect2" id="id-1.3.6.4.7.3" data-id-title="When to Use Active-Active MDS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.1 </span><span class="title-name">When to Use Active-Active MDS</span> <a title="Permalink" class="permalink" href="#id-1.3.6.4.7.3">#</a></h3></div></div></div><p>
    Consider using multiple active MDS daemons when your metadata performance
    is bottlenecked on the default single MDS.
   </p><p>
    Adding more daemons does not increase performance on all workload types.
    For example, a single application running on a single client will not
    benefit from an increased number of MDS daemons unless the application is
    doing a lot of metadata operations in parallel.
   </p><p>
    Workloads that typically benefit from a larger number of active MDS daemons
    are those with many clients, perhaps working on many separate directories.
   </p></section><section class="sect2" id="cephfs-activeactive-increase" data-id-title="Increasing the MDS Active Cluster Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.2 </span><span class="title-name">Increasing the MDS Active Cluster Size</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-increase">#</a></h3></div></div></div><p>
    Each CephFS file system has a <code class="option">max_mds</code> setting, which
    controls how many ranks will be created. The actual number of ranks in the
    file system will only be increased if a spare daemon is available to take
    on the new rank. For example, if there is only one MDS daemon running and
    <code class="option">max_mds</code> is set to two, no second rank will be created.
   </p><p>
    In the following example, we set the <code class="option">max_mds</code> option to 2
    to create a new rank apart from the default one. To see the changes, run
    <code class="command">ceph status</code> before and after you set
    <code class="option">max_mds</code>, and watch the line containing
    <code class="literal">fsmap</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 2
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</pre></div><p>
    The newly created rank (1) passes through the 'creating' state and then
    enter its 'active' state.
   </p><div id="id-1.3.6.4.7.4.6" data-id-title="Standby Daemons" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Standby Daemons</h6><p>
     Even with multiple active MDS daemons, a highly available system still
     requires standby daemons to take over if any of the servers running an
     active daemon fail.
    </p><p>
     Consequently, the practical maximum of <code class="option">max_mds</code> for highly
     available systems is one less than the total number of MDS servers in your
     system. To remain available in the event of multiple server failures,
     increase the number of standby daemons in the system to match the number
     of server failures you need to survive.
    </p></div></section><section class="sect2" id="cephfs-activeactive-decrease" data-id-title="Decreasing the Number of Ranks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.3 </span><span class="title-name">Decreasing the Number of Ranks</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-decrease">#</a></h3></div></div></div><p>
    All ranks—including the ranks to be removed—must first be
    active. This means that you need to have at least <code class="option">max_mds</code>
    MDS daemons available.
   </p><p>
    First, set <code class="option">max_mds</code> to a lower number. For example, go back
    to having a single active MDS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 1
<code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</pre></div><p>
    Note that we still have two active MDSs. The ranks still exist even though
    we have decreased <code class="option">max_mds</code>, because
    <code class="option">max_mds</code> only restricts the creation of new ranks.
   </p><p>
    To reduce the number of ranks, reduce the <code class="option">max_mds</code> option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">fs_name</em> max_mds 1</pre></div><p>
    The deactivated rank will first enter the stopping state, for a period of
    time while it hands off its share of the metadata to the remaining active
    daemons. This phase can take from seconds to minutes. If the MDS appears to
    be stuck in the stopping state then that should be investigated as a
    possible bug.
   </p><p>
    If an MDS daemon crashes or is terminated while in the 'stopping' state, a
    standby will take over and the rank will go back to 'active'. You can try
    to deactivate it again when it has come back up.
   </p><p>
    When a daemon finishes stopping, it will start again and go back to being a
    standby.
   </p></section><section class="sect2" id="cephfs-activeactive-pinning" data-id-title="Manually Pinning Directory Trees to a Rank"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.4 </span><span class="title-name">Manually Pinning Directory Trees to a Rank</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-pinning">#</a></h3></div></div></div><p>
    In multiple active metadata server configurations, a balancer runs, which
    works to spread metadata load evenly across the cluster. This usually works
    well enough for most users, but sometimes it is desirable to override the
    dynamic balancer with explicit mappings of metadata to particular ranks.
    This can allow the administrator or users to evenly spread application load
    or limit impact of users' metadata requests on the entire cluster.
   </p><p>
    The mechanism provided for this purpose is called an 'export pin'. It is an
    extended attribute of directories. The name of this extended attribute is
    <code class="literal">ceph.dir.pin</code>. Users can set this attribute using
    standard commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>setfattr -n ceph.dir.pin -v 2 <em class="replaceable">/path/to/dir</em></pre></div><p>
    The value (<code class="option">-v</code>) of the extended attribute is the rank to
    assign the directory sub-tree to. A default value of -1 indicates that the
    directory is not pinned.
   </p><p>
    A directory export pin is inherited from its closest parent with a set
    export pin. Therefore, setting the export pin on a directory affects all of
    its children. However, the parent's pin can be overridden by setting the
    child directory export pin. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</pre></div></section></section><section class="sect1" id="ceph-cephfs-failover" data-id-title="Managing Failover"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.5 </span><span class="title-name">Managing Failover</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover">#</a></h2></div></div></div><p>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <code class="option">mds_beacon_grace</code> seconds (default 15 seconds) before
   marking the daemon as <span class="emphasis"><em>laggy</em></span>. You can configure one or
   more 'standby' daemons that will take over during the MDS daemon failover.
  </p><section class="sect2" id="ceph-cephfs-failover-standby" data-id-title="Configuring Standby Replay"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.1 </span><span class="title-name">Configuring Standby Replay</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover-standby">#</a></h3></div></div></div><p>
    Each CephFS file system may be configured to add standby-replay daemons.
    These standby daemons follow the active MDS's metadata journal to reduce
    failover time in the event that the active MDS becomes unavailable. Each
    active MDS may have only one standby-replay daemon following it.
   </p><p>
    Configure standby-replay on a file system with the following comamnd:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">FS-NAME</em> allow_standby_replay <em class="replaceable">BOOL</em></pre></div><p>
    Once set the monitors will assign available standby daemons to follow the
    active MDSs in that file system.
   </p><p>
    Once an MDS has entered the standby-replay state, it will only be used as a
    standby for the rank that it is following. If another rank fails, this
    standby-replay daemon will not be used as a replacement, even if no other
    standbys are available. For this reason, it is advised that if
    standby-replay is used then every active MDS should have a standby-replay
    daemon.
   </p></section><section class="sect2" id="ceph-cephfs-failover-examples" data-id-title="Examples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.2 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover-examples">#</a></h3></div></div></div><p>
    Several example <code class="filename">ceph.conf</code> configurations follow. You
    can either copy a <code class="filename">ceph.conf</code> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains that server's daemon configuration.
   </p><section class="sect3" id="id-1.3.6.4.8.4.3" data-id-title="Simple Pair"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.2.1 </span><span class="title-name">Simple Pair</span> <a title="Permalink" class="permalink" href="#id-1.3.6.4.8.4.3">#</a></h4></div></div></div><p>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </p><div class="verbatim-wrap"><pre class="screen">[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</pre></div></section></section></section><section class="sect1" id="cephfs-quotas" data-id-title="Setting CephFS Quotas"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.6 </span><span class="title-name">Setting CephFS Quotas</span> <a title="Permalink" class="permalink" href="#cephfs-quotas">#</a></h2></div></div></div><p>
   You can set quotas on any subdirectory of the Ceph file system. The quota
   restricts either the number of <span class="bold"><strong>bytes</strong></span> or
   <span class="bold"><strong>files</strong></span> stored beneath the specified point in
   the directory hierarchy.
  </p><section class="sect2" id="cephfs-quotas-limitation" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.6.1 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#cephfs-quotas-limitation">#</a></h3></div></div></div><p>
    Using quotas with CephFS has the following limitations:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.9.3.3.1"><span class="term">Quotas are cooperative and non-competing.</span></dt><dd><p>
       Ceph quotas rely on the client that is mounting the file system to
       stop writing to it when a limit is reached. The server part cannot
       prevent a malicious client from writing as much data as it needs. Do not
       use quotas to prevent filling the file system in environments where the
       clients are fully untrusted.
      </p></dd><dt id="id-1.3.6.4.9.3.3.2"><span class="term">Quotas are imprecise.</span></dt><dd><p>
       Processes that are writing to the file system will be stopped shortly
       after the quota limit is reached. They will inevitably be allowed to
       write some amount of data over the configured limit. Client writers will
       be stopped within tenths of seconds after crossing the configured limit.
      </p></dd><dt id="id-1.3.6.4.9.3.3.3"><span class="term">Quotas are implemented in the kernel client from version 4.17.</span></dt><dd><p>
       Quotas are supported by the user space client (libcephfs, ceph-fuse).
       Linux kernel clients 4.17 and higher support CephFS quotas on
       SUSE Enterprise Storage 6 clusters. Kernel clients (even recent
       versions) will fail to handle quotas on older clusters, even if they are
       able to set the quotas extended attributes.
      </p></dd><dt id="id-1.3.6.4.9.3.3.4"><span class="term">Configure quotas carefully when used with path-based mount restrictions.</span></dt><dd><p>
       The client needs to have access to the directory inode on which quotas
       are configured in order to enforce them. If the client has restricted
       access to a specific path (for example <code class="filename">/home/user</code>)
       based on the MDS capability, and a quota is configured on an ancestor
       directory they do not have access to (<code class="filename">/home</code>), the
       client will not enforce it. When using path-based access restrictions,
       be sure to configure the quota on the directory that the client can
       access (for example <code class="filename">/home/user</code> or
       <code class="filename">/home/user/quota_dir</code>).
      </p></dd></dl></div></section><section class="sect2" id="cephfs-quotas-config" data-id-title="Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.6.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#cephfs-quotas-config">#</a></h3></div></div></div><p>
    You can configure CephFS quotas by using virtual extended attributes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.4.9.4.3.1"><span class="term"><code class="option">ceph.quota.max_files</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>file</em></span> limit.
      </p></dd><dt id="id-1.3.6.4.9.4.3.2"><span class="term"><code class="option">ceph.quota.max_bytes</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>byte</em></span> limit.
      </p></dd></dl></div><p>
    If the attributes appear on a directory inode, a quota is configured there.
    If they are not present then no quota is set on that directory (although
    one may still be configured on a parent directory).
   </p><p>
    To set a 100 MB quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 100000000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To set a 10,000 files quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_files -v 10000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To view quota setting, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>getfattr -n ceph.quota.max_bytes <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>getfattr -n ceph.quota.max_files <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div id="id-1.3.6.4.9.4.12" data-id-title="Quota Not Set" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quota Not Set</h6><p>
     If the value of the extended attribute is '0', the quota is not set.
    </p></div><p>
    To remove a quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 0 <em class="replaceable">/SOME/DIRECTORY</em>
<code class="prompt user">cephadm@mds &gt; </code>setfattr -n ceph.quota.max_files -v 0 <em class="replaceable">/SOME/DIRECTORY</em></pre></div></section></section><section class="sect1" id="cephfs-snapshots" data-id-title="Managing CephFS Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.7 </span><span class="title-name">Managing CephFS Snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots">#</a></h2></div></div></div><p>
   CephFS snapshots create a read-only view of the file system at the point
   in time they are taken. You can create a snapshot in any directory. The
   snapshot will cover all data in the file system under the specified
   directory. After creating a snapshot, the buffered data is flushed out
   asynchronously from various clients. As a result, creating a snapshot is
   very fast.
  </p><div id="id-1.3.6.4.10.3" data-id-title="Multiple File Systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Multiple File Systems</h6><p>
    If you have multiple CephFS file systems sharing a single pool (via name
    spaces), their snapshots will collide, and deleting one snapshot will
    result in missing file data for other snapshots sharing the same pool.
   </p></div><section class="sect2" id="cephfs-snapshots-create" data-id-title="Creating Snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.7.1 </span><span class="title-name">Creating Snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots-create">#</a></h3></div></div></div><p>
    The CephFS snapshot feature is enabled by default on new file systems. To
    enable it on existing file systems, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph fs set <em class="replaceable">CEPHFS_NAME</em> allow_new_snaps true</pre></div><p>
    After you enable snapshots, all directories in the CephFS will have a
    special <code class="filename">.snap</code> subdirectory.
   </p><div id="id-1.3.6.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This is a <span class="emphasis"><em>virtual</em></span> subdirectory. It does not appear in
     the directory listing of the parent directory, but the name
     <code class="filename">.snap</code> cannot be used as a file or directory name. To
     access the <code class="filename">.snap</code> directory one needs to explicitly
     access it, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls -la /<em class="replaceable">CEPHFS_MOUNT</em>/.snap/</pre></div></div><div id="id-1.3.6.4.10.4.6" data-id-title="Kernel clients limitation" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Kernel clients limitation</h6><p>
     CephFS kernel clients have a limitation: they cannot handle more than
     400 snapshots in a file system. The number of snapshots should always be
     kept below this limit, regardless of which client you are using. If using
     older CephFS clients, such as SLE12-SP3, keep in mind that going above
     400 snapshots is harmful to operations as the client will crash.
    </p></div><div id="id-1.3.6.4.10.4.7" data-id-title="Custom Snapshot Subdirectory Name" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom Snapshot Subdirectory Name</h6><p>
     You may configure a different name for the snapshots subdirectory by
     setting the <code class="option">client snapdir</code> setting.
    </p></div><p>
    To create a snapshot, create a subdirectory under the
    <code class="filename">.snap</code> directory with a custom name. For example, to
    create a snapshot of the directory
    <code class="filename">/<em class="replaceable">CEPHFS_MOUNT</em>/2/3/</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>mkdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section><section class="sect2" id="cephfs-snapshots-delete" data-id-title="Deleting Snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.7.2 </span><span class="title-name">Deleting Snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots-delete">#</a></h3></div></div></div><p>
    To delete a snapshot, remove its subdirectory inside the
    <code class="filename">.snap</code> directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>rmdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section></section></section><section class="chapter" id="cha-ses-cifs" data-id-title="Exporting Ceph Data via Samba"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">29 </span><span class="title-name">Exporting Ceph Data via Samba</span> <a title="Permalink" class="permalink" href="#cha-ses-cifs">#</a></h2></div></div></div><p>
  This chapter describes how to export data stored in a Ceph cluster via a
  Samba/CIFS share so that you can easily access them from Windows* client
  machines. It also includes information that will help you configure a Ceph
  Samba gateway to join Active Directory in the Windows* domain to authenticate and
  authorize users.
 </p><div id="id-1.3.6.5.4" data-id-title="Samba Gateway Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Samba Gateway Performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing CephFS
   via a Samba Gateway may significantly reduce application performance when
   compared to native Ceph clients.
  </p></div><section class="sect1" id="cephfs-samba" data-id-title="Export CephFS via Samba Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">29.1 </span><span class="title-name">Export CephFS via Samba Share</span> <a title="Permalink" class="permalink" href="#cephfs-samba">#</a></h2></div></div></div><div id="id-1.3.6.5.5.2" data-id-title="Cross Protocol Access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Cross Protocol Access</h6><p>
    Native CephFS and NFS clients are not restricted by file locks obtained
    via Samba, and vice versa. Applications that rely on cross protocol file
    locking may experience data corruption if CephFS backed Samba share paths
    are accessed via other means.
   </p></div><section class="sect2" id="cephfs-samba-packages" data-id-title="Samba Related Packages Installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.1.1 </span><span class="title-name">Samba Related Packages Installation</span> <a title="Permalink" class="permalink" href="#cephfs-samba-packages">#</a></h3></div></div></div><p>
    To configure and export a Samba share, the following packages need to be
    installed: <span class="package">samba-ceph</span> and
    <span class="package">samba-winbind</span>. If these packages are not installed,
    install them:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>zypper install samba-ceph samba-winbind</pre></div></section><section class="sect2" id="sec-ses-cifs-example" data-id-title="Single Gateway Example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.1.2 </span><span class="title-name">Single Gateway Example</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-example">#</a></h3></div></div></div><p>
    In preparation for exporting a Samba share, choose an appropriate node to
    act as a Samba Gateway. The node needs to have access to the Ceph client network,
    as well as sufficient CPU, memory, and networking resources.
   </p><p>
    Failover functionality can be provided with CTDB and the SUSE Linux Enterprise High Availability Extension.
    Refer to <a class="xref" href="#sec-ses-cifs-ha" title="29.1.3. High Availability Configuration">Section 29.1.3, “High Availability Configuration”</a> for more information on HA
    setup.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make sure that a working CephFS already exists in your cluster. For
      details, see <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Installation of CephFS”</span>.
     </p></li><li class="step"><p>
      Create a Samba Gateway specific keyring on the Ceph admin node and copy it to
      both Samba Gateway nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephadm@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <em class="replaceable">SAMBA_NODE</em>:/etc/ceph/</pre></div><p>
      Replace <em class="replaceable">SAMBA_NODE</em> with the name of the Samba
      gateway node.
     </p></li><li class="step"><p>
      The following steps are executed on the Samba Gateway node. Install Samba
      together with the Ceph integration package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>sudo zypper in samba samba-ceph</pre></div></li><li class="step"><p>
      Replace the default contents of the
      <code class="filename">/etc/samba/smb.conf</code> file with the following:
     </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<em class="replaceable">SHARE_NAME</em>]
  path = <em class="replaceable">CEPHFS_MOUNT</em>
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><p>
      The <em class="replaceable">CEPHFS_MOUNT</em> path above must be mounted
      prior to starting Samba with a kernel CephFS share configuration. See
      <a class="xref" href="#ceph-cephfs-cephfs-fstab" title="28.3. CephFS in /etc/fstab">Section 28.3, “CephFS in <code class="filename">/etc/fstab</code>”</a>.
     </p><p>
      The above share configuration uses the Linux kernel CephFS client,
      which is recommended for performance reasons. As an alternative, the
      Samba <code class="systemitem">vfs_ceph</code> module can also be used to
      communicate with the Ceph cluster. The instructions are shown below for
      legacy purposes and are not recommended for new Samba deployments:
     </p><div class="verbatim-wrap"><pre class="screen">[<em class="replaceable">SHARE_NAME</em>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><div id="id-1.3.6.5.5.4.4.4.6" data-id-title="Oplocks and Share Modes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Oplocks and Share Modes</h6><p>
       <code class="option">oplocks</code> (also known as SMB2+ leases) allow for improved
       performance through aggressive client caching, but are currently unsafe
       when Samba is deployed together with other CephFS clients, such as
       kernel <code class="literal">mount.ceph</code>, FUSE, or NFS Ganesha.
      </p><p>
       If all CephFS file system path access is exclusively handled by
       Samba, then the <code class="option">oplocks</code> parameter can be safely
       enabled.
      </p><p>
       Currently <code class="option">kernel share modes</code> needs to be disabled in a
       share running with the CephFS vfs module for file serving to work
       properly.
      </p></div><div id="id-1.3.6.5.5.4.4.4.7" data-id-title="Permitting Access" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Permitting Access</h6><p>
       Samba maps SMB users and groups to local accounts. Local users can be
       assigned a password for Samba share access via:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>smbpasswd -a <em class="replaceable">USERNAME</em></pre></div><p>
       For successful I/O, the share path's access control list (ACL) needs to
       permit access to the user connected via Samba. You can modify the ACL
       by temporarily mounting via the CephFS kernel client and using the
       <code class="command">chmod</code>, <code class="command">chown</code>, or
       <code class="command">setfacl</code> utilities against the share path. For
       example, to permit access for all users, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>chmod 777 <em class="replaceable">MOUNTED_SHARE_PATH</em></pre></div></div></li></ol></div></div><section class="sect3" id="samba-service-restart" data-id-title="Starting Samba Services"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.1.2.1 </span><span class="title-name">Starting Samba Services</span> <a title="Permalink" class="permalink" href="#samba-service-restart">#</a></h4></div></div></div><p>
     Start or restart stand-alone Samba services using the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl restart smb.service
<code class="prompt user">root # </code>systemctl restart nmb.service
<code class="prompt user">root # </code>systemctl restart winbind.service</pre></div><p>
     To ensure that Samba services start on boot, enable them via:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl enable smb.service
<code class="prompt user">root # </code>systemctl enable nmb.service
<code class="prompt user">root # </code>systemctl enable winbind.service</pre></div><div id="id-1.3.6.5.5.4.5.6" data-id-title="Optional nmb and winbind services" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Optional <code class="systemitem">nmb</code> and <code class="systemitem">winbind</code> services</h6><p>
      If you do not require network share browsing, you do not need to enable
      and start the <code class="systemitem">nmb</code> service.
     </p><p>
      The <code class="systemitem">winbind</code> service is only
      needed when configured as an Active Directory domain member. See
      <a class="xref" href="#cephfs-ad" title="29.2. Samba Gateway Joining Active Directory">Section 29.2, “Samba Gateway Joining Active Directory”</a>.
     </p></div></section></section><section class="sect2" id="sec-ses-cifs-ha" data-id-title="High Availability Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.1.3 </span><span class="title-name">High Availability Configuration</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-ha">#</a></h3></div></div></div><div id="id-1.3.6.5.5.5.2" data-id-title="Transparent Failover Not Supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Transparent Failover Not Supported</h6><p>
     Although a multi-node Samba + CTDB deployment is more highly available
     compared to the single node (see <a class="xref" href="#cha-ses-cifs" title="Chapter 29. Exporting Ceph Data via Samba">Chapter 29, <em>Exporting Ceph Data via Samba</em></a>),
     client-side transparent failover is not supported. Applications will
     likely experience a short outage on Samba Gateway node failure.
    </p></div><p>
    This section provides an example of how to set up a two-node high
    availability configuration of Samba servers. The setup requires the SUSE Linux Enterprise
    High Availability Extension. The two nodes are called <code class="systemitem">earth</code>
    (<code class="systemitem">192.168.1.1</code>) and <code class="systemitem">mars</code>
    (<code class="systemitem">192.168.1.2</code>).
   </p><p>
    For details about SUSE Linux Enterprise High Availability Extension, see
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
   </p><p>
    Additionally, two floating virtual IP addresses allow clients to connect to
    the service no matter which physical node it is running on.
    <code class="systemitem">192.168.1.10</code> is used for cluster
    administration with Hawk2 and
    <code class="systemitem">192.168.2.1</code> is used exclusively
    for the CIFS exports. This makes it easier to apply security restrictions
    later.
   </p><p>
    The following procedure describes the example installation. More details
    can be found at
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/art-sleha-install-quick.html</a>.
   </p><div class="procedure" id="proc-sec-ses-cifs-ha"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a Samba Gateway specific keyring on the Admin Node and copy it to both nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephadm@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">earth</code>:/etc/ceph/
<code class="prompt user">cephadm@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">mars</code>:/etc/ceph/</pre></div></li><li class="step"><p>
      SLE-HA setup requires a fencing device to avoid a <span class="emphasis"><em>split
      brain</em></span> situation when active cluster nodes become
      unsynchronized. For this purpose, you can use a Ceph RBD image with
      Stonith Block Device (SBD). Refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-storage-protect.html#sec-ha-storage-protect-fencing-setup" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-storage-protect.html#sec-ha-storage-protect-fencing-setup</a>
      for more details.
     </p><p>
      If it does not yet exist, create an RBD pool called
      <code class="literal">rbd</code> (see
      <a class="xref" href="#ceph-pools-operate-add-pool" title="22.2.2. Create a Pool">Section 22.2.2, “Create a Pool”</a>) and associate it
      with <code class="literal">rbd</code> (see
      <a class="xref" href="#ceph-pools-associate" title="22.1. Associate Pools with an Application">Section 22.1, “Associate Pools with an Application”</a>). Then create a related
      RBD image called <code class="literal">sbd01</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create rbd <em class="replaceable">PG_NUM</em> <em class="replaceable">PGP_NUM</em> replicated
<code class="prompt user">cephadm@adm &gt; </code>ceph osd pool application enable rbd rbd
<code class="prompt user">cephadm@adm &gt; </code>rbd -p rbd create sbd01 --size 64M --image-shared</pre></div></li><li class="step"><p>
      Prepare <code class="systemitem">earth</code> and <code class="systemitem">mars</code> to host the Samba service:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Make sure the following packages are installed before you proceed:
        <span class="package">ctdb</span>, <span class="package">tdb-tools</span>, and
        <span class="package">samba</span>.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">zypper</code> in ctdb tdb-tools samba samba-ceph</pre></div></li><li class="step"><p>
        Make sure the Samba and CTDB services are stopped and disabled:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl disable ctdb
<code class="prompt user">root # </code>systemctl disable smb
<code class="prompt user">root # </code>systemctl disable nmb
<code class="prompt user">root # </code>systemctl disable winbind
<code class="prompt user">root # </code>systemctl stop ctdb
<code class="prompt user">root # </code>systemctl stop smb
<code class="prompt user">root # </code>systemctl stop nmb
<code class="prompt user">root # </code>systemctl stop winbind</pre></div></li><li class="step"><p>
        Open port <code class="literal">4379</code> of your firewall on all nodes. This
        is needed for CTDB to communicate with other cluster nodes.
       </p></li></ol></li><li class="step"><p>
      On <code class="systemitem">earth</code>, create the configuration files for Samba. They will later
      automatically synchronize to <code class="systemitem">mars</code>.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Insert a list of private IP addresses of Samba Gateway nodes in the
        <code class="filename">/etc/ctdb/nodes</code> file. Find more details in the
        ctdb manual page (<code class="command">man 7 ctdb</code>).
       </p><div class="verbatim-wrap"><pre class="screen">192.168.1.1
192.168.1.2</pre></div></li><li class="step"><p>
        Configure Samba. Add the following lines in the
        <code class="literal">[global]</code> section of
        <code class="filename">/etc/samba/smb.conf</code>. Use the host name of your
        choice in place of <em class="replaceable">CTDB-SERVER</em> (all nodes in
        the cluster will appear as one big node with this name). Add a share
        definition as well, consider <em class="replaceable">SHARE_NAME</em> as
        an example:
       </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><p>
        Note that the <code class="filename">/etc/ctdb/nodes</code> and
        <code class="filename">/etc/samba/smb.conf</code> files need to match on all
        Samba Gateway nodes.
       </p></li></ol></li><li class="step"><p>
      Install and bootstrap the SUSE Linux Enterprise High Availability cluster.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Register the SUSE Linux Enterprise High Availability Extension on <code class="systemitem">earth</code> and <code class="systemitem">mars</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div></li><li class="step"><p>
        Install <span class="package">ha-cluster-bootstrap</span> on both nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div></li><li class="step"><p>
        Map the RBD image <code class="literal">sbd01</code> on both Samba Gateways via
        <code class="systemitem">rbdmap.service</code>.
       </p><p>
        Edit <code class="filename">/etc/ceph/rbdmap</code> and add an entry for the SBD
        image:
       </p><div class="verbatim-wrap"><pre class="screen">rbd/sbd01 id=samba.gw,keyring=/etc/ceph/ceph.client.samba.gw.keyring</pre></div><p>
        Enable and start
        <code class="systemitem">rbdmap.service</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
<code class="prompt user">root@mars # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service</pre></div><p>
        The <code class="filename">/dev/rbd/rbd/sbd01</code> device should be available
        on both Samba Gateways.
       </p></li><li class="step"><p>
        Initialize the cluster on <code class="systemitem">earth</code> and let <code class="systemitem">mars</code> join it.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">ha-cluster-init</code></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">ha-cluster-join</code> -c earth</pre></div><div id="id-1.3.6.5.5.5.7.5.2.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
         During the process of initialization and joining the cluster, you will
         be interactively asked whether to use SBD. Confirm with
         <code class="option">y</code> and then specify
         <code class="filename">/dev/rbd/rbd/sbd01</code> as a path to the storage
         device.
        </p></div></li></ol></li><li class="step"><p>
      Check the status of the cluster. You should see two nodes added in the
      cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</pre></div></li><li class="step"><p>
      Execute the following commands on <code class="systemitem">earth</code> to configure the CTDB
      resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> configure
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> winbind systemd:winbind \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">group</code> g-ctdb ctdb winbind nmb smb
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ctdb g-ctdb meta interleave="true"
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><div id="id-1.3.6.5.5.5.7.7.3" data-id-title="Optional nmb and winbind primitives" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Optional <code class="systemitem">nmb</code> and <code class="systemitem">winbind</code> primitives</h6><p>
       If you do not require network share browsing, you do not need to add the
       <code class="systemitem">nmb</code> primitive.
      </p><p>
       The <code class="systemitem">winbind</code> primitive is only
       needed when configured as an Active Directory domain member. See
       <a class="xref" href="#cephfs-ad" title="29.2. Samba Gateway Joining Active Directory">Section 29.2, “Samba Gateway Joining Active Directory”</a>.
      </p></div><p>
      The binary
      <code class="command">/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</code> in the
      configuration option <code class="literal">ctdb_recovery_lock</code> has the
      parameters <em class="replaceable">CLUSTER_NAME</em>,
      <em class="replaceable">CEPHX_USER</em>,
      <em class="replaceable">RADOS_POOL</em>, and
      <em class="replaceable">RADOS_OBJECT</em>, in this order.
     </p><p>
      An extra lock-timeout parameter can be appended to override the default
      value used (10 seconds). A higher value will increase the CTDB recovery
      master failover time, whereas a lower value may result in the recovery
      master being incorrectly detected as down, triggering flapping failovers.
     </p></li><li class="step"><p>
      Add a clustered IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<code class="prompt user">crm(live)configure# </code><code class="command">colocation</code> col-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">order</code> o-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
      If <code class="literal">unique_clone_address</code> is set to
      <code class="literal">true</code>, the IPaddr2 resource agent adds a clone ID to
      the specified address, leading to three different IP addresses. These are
      usually not needed, but help with load balancing. For further information
      about this topic, see
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-lb.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/html/SLE-HA-all/cha-ha-lb.html</a>.
     </p></li><li class="step"><p>
      Check the result:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</pre></div></li><li class="step"><p>
      Test from a client machine. On a Linux client, run the following command
      to see if you can copy files from and to the system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">smbclient</code> <code class="option">//192.168.2.1/myshare</code></pre></div></li></ol></div></div><section class="sect3" id="samba-ha-service-restart" data-id-title="Restarting HA Samba Resources"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.1.3.1 </span><span class="title-name">Restarting HA Samba Resources</span> <a title="Permalink" class="permalink" href="#samba-ha-service-restart">#</a></h4></div></div></div><p>
     Following any Samba or CTDB configuration changes, HA resources may need
     to be restarted for the changes to take effect. This can be done by via:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">crm</code> resource restart cl-ctdb</pre></div></section></section></section><section class="sect1" id="cephfs-ad" data-id-title="Samba Gateway Joining Active Directory"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">29.2 </span><span class="title-name">Samba Gateway Joining Active Directory</span> <a title="Permalink" class="permalink" href="#cephfs-ad">#</a></h2></div></div></div><p>
   You can configure the Ceph Samba gateway to become a member of Samba
   domain with Active Directory (AD) support. As a Samba domain member, you can use
   domain users and groups in local access lists (ACLs) on files and
   directories from the exported CephFS.
  </p><section class="sect2" id="cephfs-ad-preparation" data-id-title="Preparation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.1 </span><span class="title-name">Preparation</span> <a title="Permalink" class="permalink" href="#cephfs-ad-preparation">#</a></h3></div></div></div><p>
    This section introduces preparatory steps that you need to take care of
    before configuring the Samba itself. Starting with a clean environment
    helps you prevent confusion and verifies that no files from the previous
    Samba installation are mixed with the new domain member installation.
   </p><div id="id-1.3.6.5.6.3.3" data-id-title="Synchronize Clocks" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Synchronize Clocks</h6><p>
     All Samba Gateway nodes' clocks need to be synchronized with the Active Directory Domain
     controller. Clock skew may result in authentication failures.
    </p></div><p>
    Verify that no Samba or name caching processes are running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>ps ax | egrep "samba|smbd|nmbd|winbindd|nscd"</pre></div><p>
    If the output lists any <code class="literal">samba</code>, <code class="literal">smbd</code>,
    <code class="literal">nmbd</code>, <code class="literal">winbindd</code>, or
    <code class="literal">nscd</code> processes, stop them.
   </p><p>
    If you have previously run a Samba installation on this host, remove the
    <code class="filename">/etc/samba/smb.conf</code> file. Also remove all Samba
    database files, such as <code class="filename">*.tdb</code> and
    <code class="filename">*.ldb</code> files. To list directories containing Samba
    databases, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>smbd -b | egrep "LOCKDIR|STATEDIR|CACHEDIR|PRIVATE_DIR"</pre></div></section><section class="sect2" id="cephfs-ad-dns" data-id-title="Verify DNS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.2 </span><span class="title-name">Verify DNS</span> <a title="Permalink" class="permalink" href="#cephfs-ad-dns">#</a></h3></div></div></div><p>
    Active Directory (AD) uses DNS to locate other domain controllers (DCs) and services,
    such as Kerberos. Therefore AD domain members and servers need to be able
    to resolve the AD DNS zones.
   </p><p>
    Verify that DNS is correctly configured and that both forward and reverse
    lookup resolve correctly, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>nslookup DC1.domain.example.com
Server:         10.99.0.1
Address:        10.99.0.1#53

Name:   DC1.domain.example.com
Address: 10.99.0.1</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>nslookup 10.99.0.1
Server:        10.99.0.1
Address:	10.99.0.1#53

1.0.99.10.in-addr.arpa	name = DC1.domain.example.com.</pre></div></section><section class="sect2" id="cephfs-ad-srv" data-id-title="Resolving SRV Records"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.3 </span><span class="title-name">Resolving SRV Records</span> <a title="Permalink" class="permalink" href="#cephfs-ad-srv">#</a></h3></div></div></div><p>
    AD uses SRV records to locate services, such as Kerberos and LDAP. To
    verify that SRV records are resolved correctly, use the
    <code class="command">nslookup</code> interactive shell, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>nslookup
Default Server:  10.99.0.1
Address:  10.99.0.1

&gt; set type=SRV
&gt; _ldap._tcp.domain.example.com.
Server:  UnKnown
Address:  10.99.0.1

_ldap._tcp.domain.example.com   SRV service location:
          priority       = 0
          weight         = 100
          port           = 389
          svr hostname   = dc1.domain.example.com
domain.example.com      nameserver = dc1.domain.example.com
dc1.domain.example.com  internet address = 10.99.0.1</pre></div></section><section class="sect2" id="cephfs-ad-kerberos" data-id-title="Configuring Kerberos"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.4 </span><span class="title-name">Configuring Kerberos</span> <a title="Permalink" class="permalink" href="#cephfs-ad-kerberos">#</a></h3></div></div></div><p>
    Samba supports Heimdal and MIT Kerberos back-ends. To configure Kerberos
    on the domain member, set the following in your
    <code class="filename">/etc/krb5.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">[libdefaults]
	default_realm = DOMAIN.EXAMPLE.COM
	dns_lookup_realm = false
	dns_lookup_kdc = true</pre></div><p>
    The previous example configures Kerberos for the DOMAIN.EXAMPLE.COM realm.
    We do not recommend to set any further parameters in the
    <code class="filename">/etc/krb5.conf</code> file. If your
    <code class="filename">/etc/krb5.conf</code> contains an <code class="literal">include</code>
    line it will not work—you <span class="bold"><strong>must</strong></span>
    remove this line.
   </p></section><section class="sect2" id="cephfs-ad-local-resolution" data-id-title="Local Host Name Resolution"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.5 </span><span class="title-name">Local Host Name Resolution</span> <a title="Permalink" class="permalink" href="#cephfs-ad-local-resolution">#</a></h3></div></div></div><p>
    When you join a host to the domain, Samba tries to register the host name
    in the AD DNS zone. For this, the <code class="command">net</code> utility needs to
    be able to resolve the host name using DNS or using a correct entry in the
    <code class="filename">/etc/hosts</code> file.
   </p><p>
    To verify that your host name resolves correctly, use the <code class="command">getent
    hosts</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>getent hosts example-host
10.99.0.5      example-host.domain.example.com    example-host</pre></div><p>
    The host name and FQDN must not resolve to the 127.0.0.1 IP address or any
    IP address other than the one used on the LAN interface of the domain
    member. If no output is displayed or the host is resolved to the wrong IP
    address and you are not using DHCP, set the correct entry in the
    <code class="filename">/etc/hosts</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">127.0.0.1      localhost
10.99.0.5      example-host.samdom.example.com    example-host</pre></div><div id="id-1.3.6.5.6.7.7" data-id-title="DHCP and /etc/hosts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: DHCP and <code class="filename">/etc/hosts</code></h6><p>
     If you are using DHCP, check that <code class="filename">/etc/hosts</code> only
     contains the '127.0.0.1' line. If you continue to have problems, contact
     the administrator of your DHCP server.
    </p><p>
     If you need to add aliases to the machine host name, add them to the end
     of the line that starts with the machine's IP address, not to the
     '127.0.0.1' line.
    </p></div></section><section class="sect2" id="cephfs-ad-smb-conf" data-id-title="Configuring Samba"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.6 </span><span class="title-name">Configuring Samba</span> <a title="Permalink" class="permalink" href="#cephfs-ad-smb-conf">#</a></h3></div></div></div><p>
    This section introduces information about specific configuration options
    that you need to include in the Samba configuration.
   </p><p>
    Active Directory domain membership is primarily configured by setting <code class="literal">security
    = ADS</code> alongside appropriate Kerberos realm and ID mapping
    parameters in the <code class="literal">[global]</code> section of
    <code class="filename">/etc/samba/smb.conf</code>.
   </p><div class="verbatim-wrap"><pre class="screen">[global]
  security = ADS
  workgroup = DOMAIN
  realm = DOMAIN.EXAMPLE.COM
  ...</pre></div><section class="sect3" id="id-1.3.6.5.6.8.5" data-id-title="Choose Back-end for ID Mapping in winbindd"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.6.1 </span><span class="title-name">Choose Back-end for ID Mapping in <code class="systemitem">winbindd</code></span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8.5">#</a></h4></div></div></div><p>
     If you need your users to have different login shells and/or Unix home
     directory paths, or you want them to have the same ID everywhere, you will
     need to use the winbind 'ad' back-end and add RFC2307 attributes to AD.
    </p><div id="id-1.3.6.5.6.8.5.3" data-id-title="RFC2307 Attributes and ID Numbers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: RFC2307 Attributes and ID Numbers</h6><p>
      The RFC2307 attributes are not added automatically when users or groups
      are created.
     </p><p>
      The ID numbers found on a DC (numbers in the 3000000 range) are
      <span class="emphasis"><em>not</em></span> RFC2307 attributes and will not be used on Unix
      Domain Members. If you need to have the same ID numbers everywhere, add
      <code class="literal">uidNumber</code> and <code class="literal">gidNumber</code> attributes
      to AD and use the winbind 'ad' back-end on Unix Domain Members. If you do
      decide to add <code class="literal">uidNumber</code> and
      <code class="literal">gidNumber</code> attributes to AD, do not use numbers in the
      3000000 range.
     </p></div><p>
     If your users will only use the Samba AD DC for authentication and will
     not store data on it or log in to it, you can use the winbind 'rid'
     back-end. This calculates the user and group IDs from the Windows* RID. If
     you use the same <code class="literal">[global]</code> section of the
     <code class="filename">smb.conf</code> on every Unix domain member, you will get
     the same IDs. If you use the 'rid' back-end, you do not need to add
     anything to AD and RFC2307 attributes will be ignored. When using the
     'rid' back-end, set the <code class="option">template shell</code> and
     <code class="option">template homedir</code> parameters in
     <code class="filename">smb.conf</code>. These settings are global and everyone gets
     the same login shell and Unix home directory path (unlike the RFC2307
     attributes where you can set individual Unix home directory paths and
     shells).
    </p><p>
     There is another way of setting up Samba—when you require your
     users and groups to have the same ID everywhere, but only need your users
     to have the same login shell and use the same Unix home directory path.
     You can do this by using the winbind 'ad' back-end and using the template
     lines in <code class="filename">smb.conf</code>. This way you only need to add
     <code class="literal">uidNumber</code> and <code class="literal">gidNumber</code> attributes
     to AD.
    </p><div id="id-1.3.6.5.6.8.5.6" data-id-title="More Information about Back-ends for ID Mapping" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information about Back-ends for ID Mapping</h6><p>
      Find more detailed information about available ID mapping back-ends in
      the related manual pages: <code class="command">man 8 idmap_ad</code>, <code class="command">man
      8 idmap_rid</code>, and <code class="command">man 8 idmap_autorid</code>.
     </p></div></section><section class="sect3" id="id-1.3.6.5.6.8.6" data-id-title="Setting User and Group ID Ranges"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.6.2 </span><span class="title-name">Setting User and Group ID Ranges</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8.6">#</a></h4></div></div></div><p>
     After you decide which winbind back-end to use, you need to specify the
     ranges to use with the <code class="option">idmap config</code> option in
     <code class="filename">smb.conf</code>. By default, there are multiple blocks of
     user and group IDs reserved on a Unix domain member:
    </p><div class="table" id="id-1.3.6.5.6.8.6.3" data-id-title="Default Users and Group ID Blocks"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 29.1: </span><span class="title-name">Default Users and Group ID Blocks </span><a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8.6.3">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IDs</th><th style="border-bottom: 1px solid ; ">Range</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">0-999</td><td style="border-bottom: 1px solid ; ">Local system users and groups.</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Starting at 1000</td><td style="border-bottom: 1px solid ; ">Local Unix users and groups.</td></tr><tr><td style="border-right: 1px solid ; ">Starting at 10000</td><td>DOMAIN users and groups.</td></tr></tbody></table></div></div><p>
     As you can see from the above ranges, you should not set either the '*' or
     'DOMAIN' ranges to start at 999 or less, as they would interfere with the
     local system users and groups. You also should leave a space for any local
     Unix users and groups, so starting the <code class="option">idmap config</code>
     ranges at 3000 seems to be a good compromise.
    </p><p>
     You need to decide how large your 'DOMAIN' is likely to grow and if you
     plan to have any trusted domains. Then you can set the <code class="option">idmap
     config</code> ranges as follows:
    </p><div class="table" id="id-1.3.6.5.6.8.6.6" data-id-title="ID Ranges"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 29.2: </span><span class="title-name">ID Ranges </span><a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8.6.6">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Domain</th><th style="border-bottom: 1px solid ; ">Range</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">*</td><td style="border-bottom: 1px solid ; ">3000-7999</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">DOMAIN</td><td style="border-bottom: 1px solid ; ">10000-999999</td></tr><tr><td style="border-right: 1px solid ; ">TRUSTED</td><td>1000000-9999999</td></tr></tbody></table></div></div></section><section class="sect3" id="id-1.3.6.5.6.8.7" data-id-title="Mapping the Domain Administrator Account to the Local root User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.6.3 </span><span class="title-name">Mapping the Domain Administrator Account to the Local <code class="systemitem">root</code> User</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.8.7">#</a></h4></div></div></div><p>
     Samba enables you to map domain accounts to a local account. Use this
     feature to execute file operations on the domain member's file system as a
     different user than the account that requested the operation on the
     client.
    </p><div id="id-1.3.6.5.6.8.7.3" data-id-title="Mapping the Domain Administrator (Optional)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mapping the Domain Administrator (Optional)</h6><p>
      Mapping the domain administrator to the local <code class="systemitem">root</code> account is
      optional. Only configure the mapping if the domain administrator needs to
      be able to execute file operations on the domain member using <code class="systemitem">root</code>
      permissions. Be aware that mapping Administrator to the <code class="systemitem">root</code>
      account does not allow you to log in to Unix domain members as
      'Administrator'.
     </p></div><p>
     To map the domain administrator to the local <code class="systemitem">root</code> account, follow
     these steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Add the following parameter to the <code class="literal">[global]</code> section
       of your <code class="filename">smb.conf</code> file:
      </p><div class="verbatim-wrap"><pre class="screen">username map = /etc/samba/user.map</pre></div></li><li class="step"><p>
       Create the <code class="filename">/etc/samba/user.map</code> file with the
       following content:
      </p><div class="verbatim-wrap"><pre class="screen">!root = <em class="replaceable">DOMAIN</em>\Administrator</pre></div></li></ol></div></div><div id="id-1.3.6.5.6.8.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      When using the 'ad' ID mapping back-end, do not set the
      <code class="option">uidNumber</code> attribute for the domain administrator
      account. If the account has the attribute set, the value overrides the
      local UID '0' of the <code class="systemitem">root</code> user, and therefore the mapping fails.
     </p></div><p>
     For more details, see the <code class="option">username map</code> parameter in the
     <code class="filename">smb.conf</code> manual page (<code class="command">man 5
     smb.conf</code>).
    </p></section></section><section class="sect2" id="cephfs-ad-joining" data-id-title="Joining the Active Directory Domain"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.7 </span><span class="title-name">Joining the Active Directory Domain</span> <a title="Permalink" class="permalink" href="#cephfs-ad-joining">#</a></h3></div></div></div><p>
    To join the host to an Active Directory, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>net ads join -U administrator
Enter administrator's password: <em class="replaceable">PASSWORD</em>
Using short domain name -- <em class="replaceable">DOMAIN</em>
Joined <em class="replaceable">EXAMPLE-HOST</em> to dns domain <em class="replaceable">'DOMAIN</em>.example.com'</pre></div></section><section class="sect2" id="cephfs-ad-nss" data-id-title="Configuring the Name Service Switch"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.8 </span><span class="title-name">Configuring the Name Service Switch</span> <a title="Permalink" class="permalink" href="#cephfs-ad-nss">#</a></h3></div></div></div><p>
    To make domain users and groups available to the local system, you need to
    enable the name service switch (NSS) library. Append the
    <code class="option">winbind</code> entry to the following databases in the
    <code class="filename">/etc/nsswitch.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">passwd: files winbind
group:  files winbind</pre></div><div id="id-1.3.6.5.6.10.4" data-id-title="Points to Consider" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Points to Consider</h6><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Keep the <code class="option">files</code> entry as the first source for both
       databases. This enables NSS to look up domain users and groups from the
       <code class="filename">/etc/passwd</code> and <code class="filename">/etc/group</code>
       files before querying the
       <code class="systemitem">winbind</code> service.
      </p></li><li class="listitem"><p>
       Do not add the <code class="option">winbind</code> entry to the NSS
       <code class="literal">shadow</code> database. This can cause the
       <code class="command">wbinfo</code> utility to fail.
      </p></li><li class="listitem"><p>
       Do not use the same user names in the local
       <code class="filename">/etc/passwd</code> file as in the domain.
      </p></li></ul></div></div></section><section class="sect2" id="cephfs-ad-services" data-id-title="Starting the Services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.9 </span><span class="title-name">Starting the Services</span> <a title="Permalink" class="permalink" href="#cephfs-ad-services">#</a></h3></div></div></div><p>
    Following configuration changes, restart Samba services as per
    <a class="xref" href="#samba-service-restart" title="29.1.2.1. Starting Samba Services">Section 29.1.2.1, “Starting Samba Services”</a> or
    <a class="xref" href="#samba-ha-service-restart" title="29.1.3.1. Restarting HA Samba Resources">Section 29.1.3.1, “Restarting HA Samba Resources”</a>.
   </p></section><section class="sect2" id="cephfs-ad-testing" data-id-title="Testing the winbindd Connectivity"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.2.10 </span><span class="title-name">Testing the <code class="systemitem">winbindd</code> Connectivity</span> <a title="Permalink" class="permalink" href="#cephfs-ad-testing">#</a></h3></div></div></div><section class="sect3" id="id-1.3.6.5.6.12.2" data-id-title="Sending a winbindd Ping"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.10.1 </span><span class="title-name">Sending a <code class="systemitem">winbindd</code> Ping</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.12.2">#</a></h4></div></div></div><p>
     To verify if the <code class="systemitem">winbindd</code> service
     is able to connect to AD Domain Controllers (DC) or a primary domain
     controller (PDC), enter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>wbinfo --ping-dc
checking the NETLOGON for domain[<em class="replaceable">DOMAIN</em>] dc connection to "DC.DOMAIN.EXAMPLE.COM" succeeded</pre></div><p>
     If the previous command fails, verify that the
     <code class="systemitem">winbindd</code> service is running and
     that the <code class="filename">smb.conf</code> file is set up correctly.
    </p></section><section class="sect3" id="id-1.3.6.5.6.12.3" data-id-title="Looking Up Domain Users and Groups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.10.2 </span><span class="title-name">Looking Up Domain Users and Groups</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.12.3">#</a></h4></div></div></div><p>
     The <code class="systemitem">libnss_winbind</code> library enables you to look up
     domain users and groups. For example, to look up the domain user
     'DOMAIN\demo01':
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>getent passwd DOMAIN\\demo01
DOMAIN\demo01:*:10000:10000:demo01:/home/demo01:/bin/bash</pre></div><p>
     To look up the domain group 'Domain Users':
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>getent group "DOMAIN\\Domain Users"
DOMAIN\domain users:x:10000:</pre></div></section><section class="sect3" id="id-1.3.6.5.6.12.4" data-id-title="Assigning File Permissions to Domain Users and Groups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">29.2.10.3 </span><span class="title-name">Assigning File Permissions to Domain Users and Groups</span> <a title="Permalink" class="permalink" href="#id-1.3.6.5.6.12.4">#</a></h4></div></div></div><p>
     The name service switch (NSS) library enables you to use domain user
     accounts and groups in commands. For example to set the owner of a file to
     the 'demo01' domain user and the group to the 'Domain Users' domain group,
     enter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@smb &gt; </code>chown "DOMAIN\\demo01:DOMAIN\\domain users" file.txt</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-nfsganesha" data-id-title="NFS Ganesha: Export Ceph Data via NFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30 </span><span class="title-name">NFS Ganesha: Export Ceph Data via NFS</span> <a title="Permalink" class="permalink" href="#cha-ceph-nfsganesha">#</a></h2></div></div></div><p>
  NFS Ganesha is an NFS server (refer to
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html" target="_blank">Sharing
  File Systems with NFS</a> ) that runs in a user address space instead of
  as part of the operating system kernel. With NFS Ganesha, you can plug in your
  own storage mechanism—such as Ceph—and access it from any NFS
  client.
 </p><p>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/<em class="replaceable">USERNAME</em>/<em class="replaceable">BUCKETNAME</em></code>.
 </p><p>
  A CephFS is exported by default via the path
  <code class="filename"><em class="replaceable">GANESHA_NODE:</em>/cephfs</code>.
 </p><div id="id-1.3.6.6.6" data-id-title="NFS Ganesha Performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha Performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing Ceph via
   an NFS Gateway may significantly reduce application performance when
   compared to native CephFS or Object Gateway clients.
  </p></div><section class="sect1" id="ceph-nfsganesha-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-install">#</a></h2></div></div></div><p>
   For installation instructions, see <span class="intraxref">Book “Deployment Guide”, Chapter 12 “Installation of NFS Ganesha”</span>.
  </p></section><section class="sect1" id="ceph-nfsganesha-config" data-id-title="Configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.2 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config">#</a></h2></div></div></div><p>
   For a list of all parameters available within the configuration file, see:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">man ganesha-config</code>
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-ceph-config</code> for CephFS File System
     Abstraction Layer (FSAL) options.
    </p></li><li class="listitem"><p>
     <code class="command">man ganesha-rgw-config</code> for Object Gateway FSAL options.
    </p></li></ul></div><p>
   This section includes information to help you configure the NFS Ganesha server
   to export the cluster data accessible via Object Gateway and CephFS.
  </p><div id="id-1.3.6.6.8.5" data-id-title="Restart the NFS Ganesha Service" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Restart the NFS Ganesha Service</h6><p>
    When you configure NFS Ganesha via the Ceph Dashboard, the
    <code class="systemitem">nfs-ganesha.service</code> service is
    restarted automatically for the changes to take effect.
   </p><p>
    When you configure NFS Ganesha manually, you need to restart the
    <code class="systemitem">nfs-ganesha.service</code> service on the
    NFS Ganesha node to re-read the new configuration:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart nfs-ganesha.service</pre></div></div><p>
   NFS Ganesha configuration consists of two parts: service configuration and
   exports configuration. The service configuration is controlled by
   <code class="filename">/etc/ganesha/ganesha.conf</code>. Note that changes to this
   file are overwritten when DeepSea stage 4 is executed. To persistently
   change the settings, edit the file
   <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code> located on
   the Salt master. The exports configuration is stored in the Ceph cluster as
   RADOS objects.
  </p><section class="sect2" id="ceph-nfsganesha-config-service-general" data-id-title="Service Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.1 </span><span class="title-name">Service Configuration</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-service-general">#</a></h3></div></div></div><p>
    The service configuration is stored in
    <code class="filename">/etc/ganesha/ganesha.conf</code> and controls all NFS Ganesha
    daemon settings, including where the exports configuration are stored in
    the Ceph cluster. Note that changes to this file are overwritten when
    DeepSea stage 4 is executed. To persistently change the settings, edit
    the file <code class="filename">/srv/salt/ceph/ganesha/files/ganesha.conf.j2</code>
    located on the Salt master.
   </p><section class="sect3" id="ceph-nfsganesha-config-service-rados" data-id-title="RADOS_URLS Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.1 </span><span class="title-name">RADOS_URLS Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-service-rados">#</a></h4></div></div></div><p>
     The <code class="literal">RADOS_URLS</code> section configures the Ceph cluster
     access for reading NFS Ganesha configuration from RADOS objects.
    </p><div class="verbatim-wrap"><pre class="screen">RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<em class="replaceable">MINION_ID</em>";
  watch_url = "rados://<em class="replaceable">RADOS_POOL</em>/ganesha/conf-<em class="replaceable">MINION_ID</em>";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.7.3.4.1"><span class="term">Ceph_Conf</span></dt><dd><p>
        Ceph configuration file path location.
       </p></dd><dt id="id-1.3.6.6.8.7.3.4.2"><span class="term">UserId</span></dt><dd><p>
        The cephx user ID.
       </p></dd><dt id="id-1.3.6.6.8.7.3.4.3"><span class="term">watch_url</span></dt><dd><p>
        The RADOS object URL to watch for reload notifications.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-service-rgw" data-id-title="RGW Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.2 </span><span class="title-name">RGW Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-service-rgw">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.7.4.3.1"><span class="term">ceph_conf</span></dt><dd><p>
        Points to the <code class="filename">ceph.conf</code> file. When deploying with
        DeepSea, it is not necessary to change this value.
       </p></dd><dt id="id-1.3.6.6.8.7.4.3.2"><span class="term">name</span></dt><dd><p>
        The name of the Ceph client user used by NFS Ganesha.
       </p></dd><dt id="id-1.3.6.6.8.7.4.3.3"><span class="term">cluster</span></dt><dd><p>
        The name of the Ceph cluster. SUSE Enterprise Storage 6 currently
        only supports one cluster name, which is <code class="literal">ceph</code> by
        default.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-service-url" data-id-title="RADOS Object URL"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.3 </span><span class="title-name">RADOS Object URL</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-service-url">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">%url rados://<em class="replaceable">RADOS_POOL</em>/ganesha/conf-<em class="replaceable">MINION_ID</em></pre></div><p>
     NFS Ganesha supports reading the configuration from a RADOS object. The
     <code class="literal">%url</code> directive allows to specify a RADOS URL that
     identifies the location of the RADOS object.
    </p><p>
     A RADOS URL can be of two forms:
     <code class="literal">rados://&lt;POOL&gt;/&lt;OBJECT&gt;</code> or
     <code class="literal">rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</code>,
     where <code class="literal">POOL</code> is the RADOS pool where the object is
     stored, <code class="literal">NAMESPACE</code> the pool namespace where the object
     is stored, and <code class="literal">OBJECT</code> the object name.
    </p><p>
     To support the Ceph Dashboard's NFS Ganesha management capabilities, you need
     to follow a convention on the name of the RADOS object for each service
     daemon. The name of the object must be of the form
     <code class="literal">conf-<em class="replaceable">MINION_ID</em></code> where
     MINION_ID corresponds to the Salt minion ID of the node where this service
     is running.
    </p><p>
     DeepSea already takes care of correctly generating this URL, and you do
     not need to make any change.
    </p></section><section class="sect3" id="ganesha-nfsport" data-id-title="Changing Default NFS Ganesha Ports"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.4 </span><span class="title-name">Changing Default NFS Ganesha Ports</span> <a title="Permalink" class="permalink" href="#ganesha-nfsport">#</a></h4></div></div></div><p>
     NFS Ganesha uses the port 2049 for NFS and 875 for the rquota support by
     default. To change the default port numbers, use the
     <code class="option">NFS_Port</code> and <code class="option">RQUOTA_Port</code> options inside
     the <code class="literal">NFS_CORE_PARAM</code> section, for example:
    </p><div class="verbatim-wrap"><pre class="screen">NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}</pre></div></section></section><section class="sect2" id="ceph-nfsganesha-config-exports-general" data-id-title="Exports Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.2 </span><span class="title-name">Exports Configuration</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-exports-general">#</a></h3></div></div></div><p>
    Exports configuration is stored as RADOS objects in the Ceph cluster.
    Each export block is stored in its own RADOS object named
    <code class="literal">export-<em class="replaceable">ID</em></code>, where
    <em class="replaceable">ID</em> must match the <code class="literal">Export_ID</code>
    attribute of the export configuration. The association between exports and
    NFS Ganesha services is done through the
    <code class="literal">conf-<em class="replaceable">MINION_ID</em></code> objects. Each
    service object contains a list of RADOS URLs for each export exported by
    that service. An export block looks like the following:
   </p><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><p>
    To create the RADOS object for the above export block, we first need to
    store the export block code in a file. Then we can use the RADOS CLI tool
    to store the contents of the previously saved file in a RADOS object.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p <em class="replaceable">POOL</em> -N <em class="replaceable">NAMESPACE</em> put export-<em class="replaceable">EXPORT_ID</em> <em class="replaceable">EXPORT_FILE</em></pre></div><p>
    After creating the export object, we can associate the export with a
    service instance by adding the corresponding RADOS URL of the export
    object to the service object. The following sections describe how to
    configure an export block.
   </p><section class="sect3" id="ceph-nfsganesha-config-general-export" data-id-title="Export Main Section"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.1 </span><span class="title-name">Export Main Section</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-general-export">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.8.7.2.1"><span class="term">Export_Id</span></dt><dd><p>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.2"><span class="term">Path</span></dt><dd><p>
        Export path in the related CephFS pool (mandatory). This allows
        subdirectories to be exported from the CephFS.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.3"><span class="term">Pseudo</span></dt><dd><p>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </p><p>
        Example: with the value <code class="literal">/cephfs/</code> and after executing
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mount <em class="replaceable">GANESHA_IP</em>:/cephfs/ /mnt/</pre></div><p>
        The CephFS data is available in the directory
        <code class="filename">/mnt/cephfs/</code> on the client.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.4"><span class="term">Access_Type</span></dt><dd><p>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </p><div id="id-1.3.6.6.8.8.7.2.4.2.2" data-id-title="Limit Access to Clients" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Limit Access to Clients</h6><p>
         If you leave <code class="literal">Access_Type = RW</code> in the main
         <code class="literal">EXPORT</code> section and limit access to a specific
         client in the <code class="literal">CLIENT</code> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <code class="literal">Access_Type =
         None</code> in the <code class="literal">EXPORT</code> section and then
         specify less restrictive access mode for one or more clients in the
         <code class="literal">CLIENT</code> section:
        </p><div class="verbatim-wrap"><pre class="screen">EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}</pre></div></div></dd><dt id="id-1.3.6.6.8.8.7.2.5"><span class="term">Squash</span></dt><dd><p>
        NFS squash option.
       </p></dd><dt id="id-1.3.6.6.8.8.7.2.6"><span class="term">FSAL</span></dt><dd><p>
        Exporting 'File System Abstraction Layer'. See
        <a class="xref" href="#ceph-nfsganesha-config-general-fsal" title="30.2.2.2. FSAL Subsection">Section 30.2.2.2, “FSAL Subsection”</a>.
       </p></dd></dl></div></section><section class="sect3" id="ceph-nfsganesha-config-general-fsal" data-id-title="FSAL Subsection"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.2 </span><span class="title-name">FSAL Subsection</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-config-general-fsal">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.6.6.8.8.8.3.1"><span class="term">Name</span></dt><dd><p>
        Defines which back-end NFS Ganesha uses. Allowed values are
        <code class="literal">CEPH</code> for CephFS or <code class="literal">RGW</code> for
        Object Gateway. Depending on the choice, a <code class="literal">role-mds</code> or
        <code class="literal">role-rgw</code> must be defined in the
        <code class="filename">policy.cfg</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="ganesha-getting-exports" data-id-title="Obtaining Exports Configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.3 </span><span class="title-name">Obtaining Exports Configuration</span> <a title="Permalink" class="permalink" href="#ganesha-getting-exports">#</a></h3></div></div></div><p>
    To obtain existing exports configuration, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Find the RADOS pool name and namespace for NFS Ganesha exports. The
      following command outputs a string of the
      <em class="replaceable">POOL_NAME</em>/<em class="replaceable">NAMESPACE</em>
      form.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph dashboard get-ganesha-clusters-rados-pool-namespace
cephfs_data/ganesha</pre></div></li><li class="step"><p>
      By using the obtained pool name and namespace, list the RADOS objects
      available on that pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p cephfs_data -N ganesha ls
conf-osd-node1
export-1
conf-osd-node2</pre></div><div id="id-1.3.6.6.8.9.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
       To see how each node is configured, view its content using the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>cat conf-osd-node1
%url "rados://cephfs_data/ganesha/export-1"
cat conf-osd-node2
%url "rados://cephfs_data/ganesha/export-1"</pre></div><p>
       In this case, both nodes will use
       <code class="literal">rados://cephfs_data/ganesha/export-1</code>. If there are
       multiple configurations, each node can use a different configuration.
      </p></div></li><li class="step"><p>
      Each export configuration is stored in a single object with the name
      <code class="literal">export-<em class="replaceable">ID</em></code>. Use the
      following command to obtain the contents of the object and save it to
      <code class="filename">/tmp/export-1</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p cephfs_data -N ganesha get export-1 /tmp/export-1
<code class="prompt user">cephadm@adm &gt; </code>cat /tmp/export-1
EXPORT {
    export_id = 1;
    path = "/";
    pseudo = "/cephfs";
    access_type = "RW";
    squash = "no_root_squash";
    protocols = 3, 4;
    transports = "UDP", "TCP";
    FSAL {
        name = "CEPH";
        user_id = "admin";
        filesystem = "cephfs";
        secret_access_key = "<em class="replaceable">SECRET_KEY</em>";
    }

    CLIENT {
        clients = 192.168.3.105;
        access_type = "RW";
        squash = "no_root_squash";
    }
}</pre></div></li></ol></div></div></section></section><section class="sect1" id="ceph-nfsganesha-customrole" data-id-title="Custom NFS Ganesha Roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.3 </span><span class="title-name">Custom NFS Ganesha Roles</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole">#</a></h2></div></div></div><p>
   Custom NFS Ganesha roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <code class="filename">policy.cfg</code>. The roles
   allow for:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Separated NFS Ganesha nodes for accessing Object Gateway and CephFS.
    </p></li><li class="listitem"><p>
     Assigning different Object Gateway users to NFS Ganesha nodes.
    </p></li></ul></div><p>
   Having different Object Gateway users enables NFS Ganesha nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with Ceph buckets used in the CRUSH Map.
  </p><section class="sect2" id="ceph-nfsganesha-customrole-rgw-multiusers" data-id-title="Different Object Gateway Users for NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.1 </span><span class="title-name">Different Object Gateway Users for NFS Ganesha</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole-rgw-multiusers">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create two
    NFS Ganesha roles with different Object Gateway users. In this example, the roles
    <code class="literal">gold</code> and <code class="literal">silver</code> are used, for which
    DeepSea already provides example configuration files.
   </p><div class="procedure" id="proc-ceph-nfsganesha-rgw-multiusers"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/stack/global.yml</code> with
      the editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/gold.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</pre></div><p>
      Create a file
      <code class="filename">/srv/salt/ceph/rgw/users/users.d/silver.yml</code> and add
      the following content:
     </p><div class="verbatim-wrap"><pre class="screen">- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</pre></div></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 silver.conf.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 gold.conf.j2</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 silver.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 gold.j2</pre></div></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/rgw/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> rgw.j2 silver.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> rgw.j2 gold.j2</pre></div></li><li class="step"><p>
      Object Gateway also needs the configuration for the different roles:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/configuration/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ceph.conf.rgw silver.conf
<code class="prompt user">root@master # </code><code class="command">cp</code> ceph.conf.rgw gold.conf</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-silver/cluster/<em class="replaceable">NODE1</em>.sls
role-gold/cluster/<em class="replaceable">NODE2</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-nfsganesha-customrole-rgw-cephfs" data-id-title="Separating CephFS and Object Gateway FSAL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.2 </span><span class="title-name">Separating CephFS and Object Gateway FSAL</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole-rgw-cephfs">#</a></h3></div></div></div><p>
    The following example procedure for the Salt master shows how to create 2 new
    different roles that use CephFS and Object Gateway:
   </p><div class="procedure" id="proc-ceph-nfsganesha-customrole"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open the file <code class="filename">/srv/pillar/ceph/rgw.sls</code> with the
      editor of your choice. Create the file if it does not exist.
     </p></li><li class="step"><p>
      The file needs to contain the following lines:
     </p><div class="verbatim-wrap"><pre class="screen">rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</pre></div><p>
      These roles can later be assigned in the <code class="filename">policy.cfg</code>.
     </p></li><li class="step"><p>
      Now, templates for the <code class="filename">ganesha.conf</code> need to be
      created for each role. The original template of DeepSea is a good
      start. Create two copies:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cd</code> /srv/salt/ceph/ganesha/files/
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_rgw.conf.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.conf.j2 ganesha_cfs.conf.j2</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_rgw.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      Edit the <code class="filename">ganesha_cfs.conf.j2</code> and remove the section:
     </p><div class="verbatim-wrap"><pre class="screen">{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</pre></div></li><li class="step"><p>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <code class="filename">ganesha.j2</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 ganesha_rgw.j2
<code class="prompt user">root@master # </code><code class="command">cp</code> ganesha.j2 ganesha_cfs.j2</pre></div><p>
      The line <code class="literal">caps mds = "allow *"</code> can be removed from the
      <code class="filename">ganesha_rgw.j2</code>.
     </p></li><li class="step"><p>
      Copy the keyring for the Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</pre></div></li><li class="step"><p>
      Object Gateway needs the configuration for the new role:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">cp</code> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</pre></div></li><li class="step"><p>
      Assign the newly created roles to cluster nodes in the
      <code class="filename">/srv/pillar/ceph/proposals/policy.cfg</code>:
     </p><div class="verbatim-wrap"><pre class="screen">role-ganesha_rgw/cluster/<em class="replaceable">NODE1</em>.sls
role-ganesha_cfs/cluster/<em class="replaceable">NODE1</em>.sls</pre></div><p>
      Replace <em class="replaceable">NODE1</em> and
      <em class="replaceable">NODE2</em> with the names of the nodes to which you
      want to assign the roles.
     </p></li><li class="step"><p>
      Execute DeepSea Stages 0 to 4.
     </p></li></ol></div></div></section><section class="sect2" id="ganesha-rgw-supported-operations" data-id-title="Supported Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.3.3 </span><span class="title-name">Supported Operations</span> <a title="Permalink" class="permalink" href="#ganesha-rgw-supported-operations">#</a></h3></div></div></div><p>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Links including symbolic links are not supported.</em></span>
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>NFS access control lists (ACLs) are not supported.</em></span>
      Unix user and group ownership and permissions <span class="emphasis"><em>are</em></span>
      supported.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Directories may not be moved or renamed.</em></span> You
      <span class="emphasis"><em>may</em></span> move files between directories.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Only full, sequential write I/O is supported.</em></span>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example, some versions of GNU
      <code class="command">tar</code>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <code class="option">-o sync</code> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </p></li><li class="listitem"><p>
      NFS RGW supports read-write operations only for block sizes smaller than
      4 MB.
     </p></li></ul></div></section></section><section class="sect1" id="ceph-nfsganesha-services" data-id-title="Starting or Restarting NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.4 </span><span class="title-name">Starting or Restarting NFS Ganesha</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-services">#</a></h2></div></div></div><p>
   To enable and start the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> enable nfs-ganesha
<code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> start nfs-ganesha</pre></div><p>
   Restart NFS Ganesha with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">systemctl</code> restart nfs-ganesha</pre></div><p>
   When NFS Ganesha is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </p></section><section class="sect1" id="ceph-nfsganesha-loglevel" data-id-title="Setting the Log Level"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.5 </span><span class="title-name">Setting the Log Level</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-loglevel">#</a></h2></div></div></div><p>
   You change the default debug level <code class="literal">NIV_EVENT</code> by editing
   the file <code class="filename">/etc/sysconfig/ganesha</code>. Replace
   <code class="literal">NIV_EVENT</code> with <code class="literal">NIV_DEBUG</code> or
   <code class="literal">NIV_FULL_DEBUG</code>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </p><div class="verbatim-wrap"><pre class="screen">OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</pre></div><p>
   A restart of the service is required when changing the log level.
  </p><div id="id-1.3.6.6.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    NFS Ganesha uses Ceph client libraries to connect to the Ceph cluster. By
    default, client libraries do not log errors or any other output. To see
    more details about NFS Ganesha interacting with the Ceph cluster (for
    example, connection issues details) logging needs to be explicitly defined
    in the <code class="filename">ceph.conf</code> configuration file under the
    <code class="literal">[client]</code> section. For example:
   </p><div class="verbatim-wrap"><pre class="screen">[client]
	log_file = "/var/log/ceph/ceph-client.log"</pre></div></div></section><section class="sect1" id="ceph-nfsganesha-verify" data-id-title="Verifying the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.6 </span><span class="title-name">Verifying the Exported NFS Share</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-verify">#</a></h2></div></div></div><p>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   NFS Ganesha server node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code><code class="command">showmount</code> -e
/ (everything)</pre></div></section><section class="sect1" id="ceph-nfsganesha-mount" data-id-title="Mounting the Exported NFS Share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.7 </span><span class="title-name">Mounting the Exported NFS Share</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-mount">#</a></h2></div></div></div><p>
   To mount the exported NFS share (as configured in
   <a class="xref" href="#ceph-nfsganesha-config" title="30.2. Configuration">Section 30.2, “Configuration”</a>) on a client host, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code><code class="command">mount</code> -t nfs -o rw,noatime,sync \
 <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em></pre></div></section></section></div><div class="part" id="part-virt" data-id-title="Integration with Virtualization Tools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part V </span><span class="title-name">Integration with Virtualization Tools </span><a title="Permalink" class="permalink" href="#part-virt">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span></a></span></li><dd class="toc-abstract"><p>The libvirt library creates a virtual machine abstraction layer between hypervisor interfaces and the software applications that use them. With libvirt, developers and system administrators can focus on a common management framework, common API, and common shell interface (virsh) to many different h…</p></dd><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span></a></span></li><dd class="toc-abstract"><p>The most frequent Ceph use case involves providing block device images to virtual machines. For example, a user may create a 'golden' image with an OS and any relevant software in an ideal configuration. Then, the user takes a snapshot of the image. Finally, the user clones the snapshot (usually man…</p></dd></ul></div><section class="chapter" id="cha-ceph-libvirt" data-id-title="Using libvirt with Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31 </span><span class="title-name">Using <code class="systemitem">libvirt</code> with Ceph</span> <a title="Permalink" class="permalink" href="#cha-ceph-libvirt">#</a></h2></div></div></div><p>
  The <code class="systemitem">libvirt</code> library creates a virtual machine abstraction layer between
  hypervisor interfaces and the software applications that use them. With
  <code class="systemitem">libvirt</code>, developers and system administrators can focus on a common
  management framework, common API, and common shell interface
  (<code class="command">virsh</code>) to many different hypervisors, including
  QEMU/KVM, Xen, LXC, or VirtualBox.
 </p><p>
  Ceph block devices support QEMU/KVM. You can use Ceph block devices
  with software that interfaces with <code class="systemitem">libvirt</code>. The cloud solution uses
  <code class="systemitem">libvirt</code> to interact with QEMU/KVM, and QEMU/KVM interacts with Ceph
  block devices via <code class="systemitem">librbd</code>.
 </p><p>
  To create VMs that use Ceph block devices, use the procedures in the
  following sections. In the examples, we have used
  <code class="literal">libvirt-pool</code> for the pool name,
  <code class="literal">client.libvirt</code> for the user name, and
  <code class="literal">new-libvirt-image</code> for the image name. You may use any
  value you like, but ensure you replace those values when executing commands
  in the subsequent procedures.
 </p><section class="sect1" id="ceph-libvirt-cfg-ceph" data-id-title="Configuring Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31.1 </span><span class="title-name">Configuring Ceph</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-ceph">#</a></h2></div></div></div><p>
   To configure Ceph for use with <code class="systemitem">libvirt</code>, perform the following steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a pool. The following example uses the pool name
     <code class="literal">libvirt-pool</code> with 128 placement groups.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create libvirt-pool 128 128</pre></div><p>
     Verify that the pool exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd lspools</pre></div></li><li class="step"><p>
     Create a Ceph User. The following example uses the Ceph user name
     <code class="literal">client.libvirt</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get-or-create client.libvirt mon 'profile rbd' osd \
 'profile rbd pool=libvirt-pool'</pre></div><p>
     Verify the name exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth list</pre></div><div id="id-1.3.7.2.6.3.2.5" data-id-title="User Name or ID" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: User Name or ID</h6><p>
      <code class="systemitem">libvirt</code> will access Ceph using the ID <code class="literal">libvirt</code>, not
      the Ceph name <code class="literal">client.libvirt</code>. See
      <a class="xref" href="#cephx-user" title="19.2.1.1. User">Section 19.2.1.1, “User”</a> for a detailed explanation of the difference
      between ID and name.
     </p></div></li><li class="step"><p>
     Use QEMU to create an image in your RBD pool. The following example uses
     the image name <code class="literal">new-libvirt-image</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div id="id-1.3.7.2.6.3.3.2" data-id-title="Keyring File Location" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Keyring File Location</h6><p>
      The <code class="systemitem">libvirt</code> user key is stored in a keyring file placed in the
      <code class="filename">/etc/ceph</code> directory. The keyring file needs to have
      an appropriate name that includes the name of the Ceph cluster it
      belongs to. For the default cluster name 'ceph', the keyring file name is
      <code class="filename">/etc/ceph/ceph.client.libvirt.keyring</code>.
     </p><p>
      If the keyring does not exist, create it with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get client.libvirt &gt; /etc/ceph/ceph.client.libvirt.keyring</pre></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu-img create -f raw rbd:libvirt-pool/new-libvirt-image:id=libvirt 2G</pre></div><p>
     Verify the image exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd -p libvirt-pool ls</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-virt-manager" data-id-title="Preparing the VM Manager"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31.2 </span><span class="title-name">Preparing the VM Manager</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-virt-manager">#</a></h2></div></div></div><p>
   You may use <code class="systemitem">libvirt</code> without a VM manager, but you may find it simpler to
   create your first domain with <code class="command">virt-manager</code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install a virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper in virt-manager</pre></div></li><li class="step"><p>
     Prepare/download an OS image of the system you want to run virtualized.
    </p></li><li class="step"><p>
     Launch the virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen">virt-manager</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-create-vm" data-id-title="Creating a VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31.3 </span><span class="title-name">Creating a VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-create-vm">#</a></h2></div></div></div><p>
   To create a VM with <code class="command">virt-manager</code>, perform the following
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Choose the connection from the list, right-click it, and select
     <span class="guimenu">New</span>.
    </p></li><li class="step"><p>
     <span class="guimenu">Import existing disk image</span> by providing the path to the
     existing storage. Specify OS type, memory settings, and
     <span class="guimenu">Name</span> the virtual machine, for example
     <code class="literal">libvirt-virtual-machine</code>.
    </p></li><li class="step"><p>
     Finish the configuration and start the VM.
    </p></li><li class="step"><p>
     Verify that the newly created domain exists with <code class="command">sudo virsh
     list</code>. If needed, specify the connection string, such as
    </p><div class="verbatim-wrap"><pre class="screen"><code class="command">virsh -c qemu+ssh://root@vm_host_hostname/system list</code>
Id    Name                           State
-----------------------------------------------
[...]
 9     libvirt-virtual-machine       running</pre></div></li><li class="step"><p>
     Log in to the VM and stop it before configuring it for use with Ceph.
    </p></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-cfg-vm" data-id-title="Configuring the VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31.4 </span><span class="title-name">Configuring the VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-vm">#</a></h2></div></div></div><p>
   In this chapter, we focus on configuring VMs for integration with Ceph
   using <code class="command">virsh</code>. <code class="command">virsh</code> commands often
   require root privileges (<code class="command">sudo</code>) and will not return
   appropriate results or notify you that root privileges are required. For a
   reference of <code class="command">virsh</code> commands, refer to <code class="command">man 1
   virsh</code> (requires the package <span class="package">libvirt-client</span> to
   be installed).

  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open the configuration file with <code class="command">virsh edit</code>
     <em class="replaceable">vm-domain-name</em>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh edit libvirt-virtual-machine</pre></div></li><li class="step"><p>
     Under &lt;devices&gt; there should be a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-<em class="replaceable">SYSTEM-ARCH</em>&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;source file='/path/to/image/recent-linux.img'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;address type='drive' controller='0' bus='0' unit='0'/&gt;
    &lt;/disk&gt;</pre></div><p>
     Replace <code class="filename">/path/to/image/recent-linux.img</code> with the path
     to the OS image.
    </p><div id="id-1.3.7.2.9.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Use <code class="command">sudo virsh edit</code> instead of a text editor. If you
      edit the configuration file under <code class="filename">/etc/libvirt/qemu</code>
      with a text editor, <code class="systemitem">libvirt</code> may not recognize the change. If there is a
      discrepancy between the contents of the XML file under
      <code class="filename">/etc/libvirt/qemu</code> and the result of <code class="command">sudo
      virsh dumpxml</code> <em class="replaceable">vm-domain-name</em>, then
      your VM may not work properly.
     </p></div></li><li class="step"><p>
     Add the Ceph RBD image you previously created as a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='network' device='disk'&gt;
        &lt;source protocol='rbd' name='libvirt-pool/new-libvirt-image'&gt;
                &lt;host name='<em class="replaceable">monitor-host</em>' port='6789'/&gt;
        &lt;/source&gt;
        &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><p>
     Replace <em class="replaceable">monitor-host</em> with the name of your
     host, and replace the pool and/or image name as necessary. You may add
     multiple &lt;host&gt; entries for your Ceph monitors. The
     <code class="literal">dev</code> attribute is the logical device name that will
     appear under the <code class="filename">/dev</code> directory of your VM. The
     optional bus attribute indicates the type of disk device to emulate. The
     valid settings are driver specific (for example ide, scsi, virtio, xen,
     usb or sata).
    </p></li><li class="step"><p>
     Save the file.
    </p></li><li class="step"><p>
     If your Ceph cluster has authentication enabled (it does by default),
     you must generate a secret. Open an editor of your choice and create a
     file called <code class="filename">secret.xml</code> with the following content:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;secret ephemeral='no' private='no'&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.libvirt secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;</pre></div></li><li class="step"><p>
     Define the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;</pre></div></li><li class="step"><p>
     Get the <code class="literal">client.libvirt</code> key and save the key string to a
     file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</pre></div></li><li class="step"><p>
     Set the UUID of the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh secret-set-value --secret <em class="replaceable">uuid of secret</em> \
--base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</pre></div><p>
     You must also set the secret manually by adding the following
     <code class="literal">&lt;auth&gt;</code> entry to the
     <code class="literal">&lt;disk&gt;</code> element you entered earlier (replacing the
     uuid value with the result from the command line example above).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh edit libvirt-virtual-machine</pre></div><p>
     Then, add <code class="literal">&lt;auth&gt;&lt;/auth&gt;</code> element to the
     domain configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">...
&lt;/source&gt;
&lt;auth username='libvirt'&gt;
        &lt;secret type='ceph' uuid='9ec59067-fdbc-a6c0-03ff-df165c0587b8'/&gt;
&lt;/auth&gt;
&lt;target ...</pre></div><div id="id-1.3.7.2.9.3.8.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The exemplary ID is <code class="literal">libvirt</code>, not the Ceph name
      <code class="literal">client.libvirt</code> as generated at step 2 of
      <a class="xref" href="#ceph-libvirt-cfg-ceph" title="31.1. Configuring Ceph">Section 31.1, “Configuring Ceph”</a>. Ensure you use the ID component
      of the Ceph name you generated. If for some reason you need to regenerate
      the secret, you will need to execute <code class="command">sudo virsh
      secret-undefine</code> <em class="replaceable">uuid</em> before
      executing <code class="command">sudo virsh secret-set-value</code> again.
     </p></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-summary" data-id-title="Summary"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">31.5 </span><span class="title-name">Summary</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-summary">#</a></h2></div></div></div><p>
   Once you have configured the VM for use with Ceph, you can start the VM.
   To verify that the VM and Ceph are communicating, you may perform the
   following procedures.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check to see if Ceph is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health</pre></div></li><li class="step"><p>
     Check to see if the VM is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh list</pre></div></li><li class="step"><p>
     Check to see if the VM is communicating with Ceph. Replace
     <em class="replaceable">vm-domain-name</em> with the name of your VM domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>virsh qemu-monitor-command --hmp <em class="replaceable">vm-domain-name</em> 'info block'</pre></div></li><li class="step"><p>
     Check to see if the device from <code class="literal">&amp;target dev='hdb'
     bus='ide'/&gt;</code> appears under <code class="filename">/dev</code> or under
     <code class="filename">/proc/partitions</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>ls /dev
<code class="prompt user">tux &gt; </code>cat /proc/partitions</pre></div></li></ol></div></div></section></section><section class="chapter" id="cha-ceph-kvm" data-id-title="Ceph as a Back-end for QEMU KVM Instance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32 </span><span class="title-name">Ceph as a Back-end for QEMU KVM Instance</span> <a title="Permalink" class="permalink" href="#cha-ceph-kvm">#</a></h2></div></div></div><p>
  The most frequent Ceph use case involves providing block device images to
  virtual machines. For example, a user may create a 'golden' image with an OS
  and any relevant software in an ideal configuration. Then, the user takes a
  snapshot of the image. Finally, the user clones the snapshot (usually many
  times, see <a class="xref" href="#cha-ceph-snapshots-rbd" title="23.3. Snapshots">Section 23.3, “Snapshots”</a> for details). The ability
  to make copy-on-write clones of a snapshot means that Ceph can provision
  block device images to virtual machines quickly, because the client does not
  need to download an entire image each time it spins up a new virtual machine.
 </p><p>
  Ceph block devices can integrate with the QEMU virtual machines. For more
  information on QEMU KVM, see
  <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-virt-qemu.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/part-virt-qemu.html</a>.
 </p><section class="sect1" id="ceph-kvm-install" data-id-title="Installation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.1 </span><span class="title-name">Installation</span> <a title="Permalink" class="permalink" href="#ceph-kvm-install">#</a></h2></div></div></div><p>
   In order to use Ceph block devices, QEMU needs to have the appropriate
   driver installed. Check whether the <code class="systemitem">qemu-block-rbd</code>
   package is installed, and install it if needed:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper install qemu-block-rbd</pre></div></section><section class="sect1" id="ceph-kvm-usage" data-id-title="Usage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.2 </span><span class="title-name">Usage</span> <a title="Permalink" class="permalink" href="#ceph-kvm-usage">#</a></h2></div></div></div><p>
   The QEMU command line expects you to specify the pool name and image name.
   You may also specify a snapshot name.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em><em class="replaceable">:option1=value1</em><em class="replaceable">:option2=value2...</em></pre></div><p>
   For example, specifying the <em class="replaceable">id</em> and
   <em class="replaceable">conf</em> options might look like the following:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>:<code class="option">id=glance:conf=/etc/ceph/ceph.conf</code></pre></div></section><section class="sect1" id="id-1.3.7.3.7" data-id-title="Creating Images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.3 </span><span class="title-name">Creating Images with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.7">#</a></h2></div></div></div><p>
   You can create a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   create. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:pool1/image1 10G
Formatting 'rbd:pool1/image1', fmt=raw size=10737418240 nocow=off cluster_size=0</pre></div><div id="id-1.3.7.3.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The <code class="literal">raw</code> data format is really the only sensible format
    option to use with RBD. Technically, you could use other QEMU-supported
    formats such as <code class="literal">qcow2</code>, but doing so would add additional
    overhead, and would also render the volume unsafe for virtual machine live
    migration when caching is enabled.
   </p></div></section><section class="sect1" id="id-1.3.7.3.8" data-id-title="Resizing Images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.4 </span><span class="title-name">Resizing Images with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.8">#</a></h2></div></div></div><p>
   You can resize a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   resize. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:pool1/image1 9G
Image resized.</pre></div></section><section class="sect1" id="id-1.3.7.3.9" data-id-title="Retrieving Image Info with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.5 </span><span class="title-name">Retrieving Image Info with QEMU</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.9">#</a></h2></div></div></div><p>
   You can retrieve block device image information from QEMU. You must
   specify <code class="literal">rbd</code>, the pool name, and the name of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:pool1/image1
image: rbd:pool1/image1
file format: raw
virtual size: 9.0G (9663676416 bytes)
disk size: unavailable
cluster_size: 4194304</pre></div></section><section class="sect1" id="id-1.3.7.3.10" data-id-title="Running QEMU with RBD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.6 </span><span class="title-name">Running QEMU with RBD</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.10">#</a></h2></div></div></div><p>
   QEMU can access an image as a virtual block device directly via
   <code class="systemitem">librbd</code>. This avoids an additional context switch,
   and can take advantage of RBD caching.
  </p><p>
   You can use <code class="command">qemu-img</code> to convert existing virtual machine
   images to Ceph block device images. For example, if you have a qcow2
   image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img convert -f qcow2 -O raw sles12.qcow2 rbd:pool1/sles12</pre></div><p>
   To run a virtual machine booting from that image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12</pre></div><p>
   RBD caching can significantly improve performance. QEMU’s cache options
   control <code class="systemitem">librbd</code> caching:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=rbd,file=rbd:pool1/sles12,cache=writeback</pre></div><p>
   For more information on RBD caching, refer to
   <a class="xref" href="#rbd-cache-settings" title="23.5. Cache Settings">Section 23.5, “Cache Settings”</a>.
  </p></section><section class="sect1" id="id-1.3.7.3.11" data-id-title="Enabling Discard/TRIM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.7 </span><span class="title-name">Enabling Discard/TRIM</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.11">#</a></h2></div></div></div><p>
   Ceph block devices support the discard operation. This means that a guest
   can send TRIM requests to let a Ceph block device reclaim unused space.
   This can be enabled in the guest by mounting <code class="systemitem">XFS</code>
   with the discard option.
  </p><p>
   For this to be available to the guest, it must be explicitly enabled for the
   block device. To do this, you must specify a
   <code class="option">discard_granularity</code> associated with the drive:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12,id=drive1,if=none \
-device driver=ide-hd,drive=drive1,discard_granularity=512</pre></div><div id="id-1.3.7.3.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The above example uses the IDE driver. The virtio driver does not support
    discard.
   </p></div><p>
   If using <code class="systemitem">libvirt</code>, edit your libvirt domain’s
   configuration file using <code class="command">virsh edit</code> to include the
   <code class="literal">xmlns:qemu</code> value. Then, add a <code class="literal">qemu:commandline
   block</code> as a child of that domain. The following example shows how
   to set two devices with <code class="literal">qemu id=</code> to different
   <code class="literal">discard_granularity</code> values.
  </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
 &lt;qemu:commandline&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-0.discard_granularity=4096'/&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-1.discard_granularity=65536'/&gt;
 &lt;/qemu:commandline&gt;
&lt;/domain&gt;</pre></div></section><section class="sect1" id="id-1.3.7.3.12" data-id-title="QEMU Cache Options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">32.8 </span><span class="title-name">QEMU Cache Options</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.12">#</a></h2></div></div></div><p>
   QEMU’s cache options correspond to the following Ceph RBD Cache
   settings.
  </p><p>
   Writeback:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true</pre></div><p>
   Writethrough:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true
rbd_cache_max_dirty = 0</pre></div><p>
   None:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = false</pre></div><p>
   QEMU’s cache settings override Ceph’s default settings (settings that
   are not explicitly set in the Ceph configuration file). If you explicitly
   set RBD Cache settings in your Ceph configuration file (refer to
   <a class="xref" href="#rbd-cache-settings" title="23.5. Cache Settings">Section 23.5, “Cache Settings”</a>), your Ceph settings override the
   QEMU cache settings. If you set cache settings on the QEMU command line,
   the QEMU command line settings override the Ceph configuration file
   settings.
  </p></section></section></div><div class="part" id="part-troubleshooting" data-id-title="FAQs, Tips and Troubleshooting"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part VI </span><span class="title-name">FAQs, Tips and Troubleshooting </span><a title="Permalink" class="permalink" href="#part-troubleshooting">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#storage-tips"><span class="title-number">33 </span><span class="title-name">Hints and Tips</span></a></span></li><dd class="toc-abstract"><p>
  The chapter provides information to help you enhance performance of your
  Ceph cluster and provides tips how to set the cluster up.
 </p></dd><li><span class="chapter"><a href="#storage-faqs"><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span></a></span></li><dd class="toc-abstract"><p/></dd><li><span class="chapter"><a href="#storage-troubleshooting"><span class="title-number">35 </span><span class="title-name">Troubleshooting</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes several issues that you may face when you operate a
  Ceph cluster.
 </p></dd></ul></div><section class="chapter" id="storage-tips" data-id-title="Hints and Tips"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33 </span><span class="title-name">Hints and Tips</span> <a title="Permalink" class="permalink" href="#storage-tips">#</a></h2></div></div></div><p>
  The chapter provides information to help you enhance performance of your
  Ceph cluster and provides tips how to set the cluster up.
 </p><section class="sect1" id="tips-orphaned-partitions" data-id-title="Identifying Orphaned Partitions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.1 </span><span class="title-name">Identifying Orphaned Partitions</span> <a title="Permalink" class="permalink" href="#tips-orphaned-partitions">#</a></h2></div></div></div><p>
   To identify possibly orphaned journal/WAL/DB devices, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Pick the device that may have orphaned partitions and save the list of its
     partitions to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ls /dev/sdd?* &gt; /tmp/partitions</pre></div></li><li class="step"><p>
     Run <code class="command">readlink</code> against all block.wal, block.db, and
     journal devices, and compare the output to the previously saved list of
     partitions:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -</pre></div><p>
     The output is the list of partitions that are <span class="emphasis"><em>not</em></span>
     used by Ceph.
    </p></li><li class="step"><p>
     Remove the orphaned partitions that do not belong to Ceph with your
     preferred command (for example, <code class="command">fdisk</code>,
     <code class="command">parted</code>, or <code class="command">sgdisk</code>).
    </p></li></ol></div></div></section><section class="sect1" id="tips-scrubbing" data-id-title="Adjusting Scrubbing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.2 </span><span class="title-name">Adjusting Scrubbing</span> <a title="Permalink" class="permalink" href="#tips-scrubbing">#</a></h2></div></div></div><p>
   By default, Ceph performs light scrubbing daily (find more details in
   <a class="xref" href="#scrubbing" title="20.6. Scrubbing">Section 20.6, “Scrubbing”</a>) and deep scrubbing weekly.
   <span class="emphasis"><em>Light</em></span> scrubbing checks object sizes and checksums to
   ensure that placement groups are storing the same object data.
   <span class="emphasis"><em>Deep</em></span> scrubbing checks an object’s content with that of
   its replicas to ensure that the actual contents are the same. The price for
   checking data integrity is increased I/O load on the cluster during the
   scrubbing procedure.
  </p><p>
   The default settings allow Ceph OSDs to initiate scrubbing at inappropriate
   times, such as during periods of heavy loads. Customers may experience
   latency and poor performance when scrubbing operations conflict with their
   operations. Ceph provides several scrubbing settings that can limit
   scrubbing to periods with lower loads or during off-peak hours.
  </p><p>
   If the cluster experiences high loads during the day and low loads late at
   night, consider restricting scrubbing to night time hours, such as 11pm till
   6am:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6</pre></div><p>
   If time restriction is not an effective method of determining a scrubbing
   schedule, consider using the <code class="option">osd_scrub_load_threshold</code>
   option. The default value is 0.5, but it could be modified for low load
   conditions:
  </p><div class="verbatim-wrap"><pre class="screen">[osd]
osd_scrub_load_threshold = 0.25</pre></div></section><section class="sect1" id="tips-stopping-osd-without-rebalancing" data-id-title="Stopping OSDs without Rebalancing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.3 </span><span class="title-name">Stopping OSDs without Rebalancing</span> <a title="Permalink" class="permalink" href="#tips-stopping-osd-without-rebalancing">#</a></h2></div></div></div><p>
   You may need to stop OSDs for maintenance periodically. If you do not want
   CRUSH to automatically rebalance the cluster in order to avoid huge data
   transfers, set the cluster to <code class="literal">noout</code> first:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>ceph osd set noout</pre></div><p>
   When the cluster is set to <code class="literal">noout</code>, you can begin stopping
   the OSDs within the failure domain that requires maintenance work:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl stop ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   Find more information in
   <a class="xref" href="#ceph-operating-services-individual" title="16.1.2. Starting, Stopping, and Restarting Individual Services">Section 16.1.2, “Starting, Stopping, and Restarting Individual Services”</a>.
  </p><p>
   After you complete the maintenance, start OSDs again:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl start ceph-osd@<em class="replaceable">OSD_NUMBER</em>.service</pre></div><p>
   After OSD services are started, unset the cluster from
   <code class="literal">noout</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd unset noout</pre></div></section><section class="sect1" id="Cluster-Time-Setting" data-id-title="Time Synchronization of Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.4 </span><span class="title-name">Time Synchronization of Nodes</span> <a title="Permalink" class="permalink" href="#Cluster-Time-Setting">#</a></h2></div></div></div><p>
   Ceph requires precise time synchronization between all nodes.
  </p><p>
   We recommend synchronizing all Ceph cluster nodes with at least three
   reliable time sources that are located on the internal network. The internal
   time sources can point to a public time server or have their own time
   source.
  </p><div id="id-1.3.8.2.7.4" data-id-title="Public Time Servers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Public Time Servers</h6><p>
    Do not synchronize all Ceph cluster nodes directly with remote public
    time servers. With such a configuration, each node in the cluster has its
    own NTP daemon that communicates continually over the Internet with a set
    of three or four time servers that may provide slightly different times.
    This solution introduces a large degree of latency variability that makes
    it difficult or impossible to keep the clock drift under 0.05 seconds,
    which is what the Ceph monitors require.
   </p></div><p>
   For details how to set up the NTP server refer to
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html" target="_blank">SUSE Linux Enterprise Server
   Administration Guide</a>.
  </p><p>
   Then to change the time on your cluster, do the following:
  </p><div id="id-1.3.8.2.7.7" data-id-title="Setting Time" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Setting Time</h6><p>
    You may face a situation when you need to set the time back, for example if
    the time changes from the summer to the standard time. We do not recommend
    to move the time backward for a longer period than the cluster is down.
    Moving the time forward does not cause any trouble.
   </p></div><div class="procedure" id="id-1.3.8.2.7.8" data-id-title="Time Synchronization on the Cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 33.1: </span><span class="title-name">Time Synchronization on the Cluster </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.7.8">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop all clients accessing the Ceph cluster, especially those using
     iSCSI.
    </p></li><li class="step"><p>
     Shut down your Ceph cluster. On each node run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop ceph.target</pre></div><div id="id-1.3.8.2.7.8.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      If you use Ceph and SUSE OpenStack Cloud, stop also the SUSE OpenStack Cloud.
     </p></div></li><li class="step"><p>
     Verify that your NTP server is set up correctly—all
     <code class="systemitem">chronyd</code> daemons get their time
     from a source or sources in the local network.
    </p></li><li class="step"><p>
     Set the correct time on your NTP server.
    </p></li><li class="step"><p>
     Verify that NTP is running and working properly, on all nodes run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl status chronyd.service</pre></div></li><li class="step"><p>
     Start all monitoring nodes and verify that there is no clock skew:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start <em class="replaceable">target</em></pre></div></li><li class="step"><p>
     Start all OSD nodes.
    </p></li><li class="step"><p>
     Start other Ceph services.
    </p></li><li class="step"><p>
     Start the SUSE OpenStack Cloud if you have it.
    </p></li></ol></div></div></section><section class="sect1" id="storage-bp-cluster-mntc-unbalanced" data-id-title="Checking for Unbalanced Data Writing"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.5 </span><span class="title-name">Checking for Unbalanced Data Writing</span> <a title="Permalink" class="permalink" href="#storage-bp-cluster-mntc-unbalanced">#</a></h2></div></div></div><p>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <span class="emphasis"><em>weight</em></span>. The
   weight is a relative number and tells Ceph how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </p><p>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </p><p>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given ruleset, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written—you can increase their weight to
   have Ceph write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </p><div class="verbatim-wrap"><pre class="screen">$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</pre></div><div id="id-1.3.8.2.8.7" data-id-title="OSD Reweight by Utilization" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: OSD Reweight by Utilization</h6><p>
    The <code class="command">ceph osd reweight-by-utilization</code>
    <em class="replaceable">threshold</em> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </p></div></section><section class="sect1" id="storage-tips-ceph-btrfs-subvol" data-id-title="Btrfs Subvolume for /var/lib/ceph on Ceph Monitor Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.6 </span><span class="title-name">Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol">#</a></h2></div></div></div><p>
   SUSE Linux Enterprise is by default installed on a Btrfs partition. Ceph Monitors store their state
   and database in the <code class="filename">/var/lib/ceph</code> directory. To prevent
   corruption of a Ceph Monitor from a system rollback of a previous snapshot, create
   a Btrfs subvolume for <code class="filename">/var/lib/ceph</code>. A dedicated
   subvolume excludes the monitor data from snapshots of the root subvolume.
  </p><div id="id-1.3.8.2.9.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    Create the <code class="filename">/var/lib/ceph</code> subvolume before running
    DeepSea stage 0 because stage 0 installs Ceph related packages and
    creates the <code class="filename">/var/lib/ceph</code> directory.
   </p></div><p>
   DeepSea stage 3 then verifies whether <code class="filename">@/var/lib/ceph</code>
   is a Btrfs subvolume and fails if it is a normal directory.
  </p><section class="sect2" id="btrfs-subvol-requirements" data-id-title="Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.1 </span><span class="title-name">Requirements</span> <a title="Permalink" class="permalink" href="#btrfs-subvol-requirements">#</a></h3></div></div></div><section class="sect3" id="tips-ceph-btrfs-subvol-new" data-id-title="New Deployments"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.1.1 </span><span class="title-name">New Deployments</span> <a title="Permalink" class="permalink" href="#tips-ceph-btrfs-subvol-new">#</a></h4></div></div></div><p>
     Salt and DeepSea need to be properly installed and working.
    </p></section><section class="sect3" id="storage-tips-ceph-btrfs-subvol-req-existing" data-id-title="Existing Deployments"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.1.2 </span><span class="title-name">Existing Deployments</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-req-existing">#</a></h4></div></div></div><p>
     If your cluster is already installed, the following requirements must be
     met:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Nodes are upgraded to SUSE Enterprise Storage 6 and cluster is under
       DeepSea control.
      </p></li><li class="listitem"><p>
       Ceph cluster is up and healthy.
      </p></li><li class="listitem"><p>
       Upgrade process has synchronized Salt and DeepSea modules to all
       minion nodes.
      </p></li></ul></div></section></section><section class="sect2" id="storage-tips-ceph-btrfs-subvol-automatic" data-id-title="Steps Required during a New Cluster Deployment"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.2 </span><span class="title-name">Steps Required during a New Cluster Deployment</span> <a title="Permalink" class="permalink" href="#storage-tips-ceph-btrfs-subvol-automatic">#</a></h3></div></div></div><section class="sect3" id="var-lib-ceph-stage0" data-id-title="Before Running DeepSea stage 0"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.2.1 </span><span class="title-name">Before Running DeepSea stage 0</span> <a title="Permalink" class="permalink" href="#var-lib-ceph-stage0">#</a></h4></div></div></div><p>
     Prior to running DeepSea stage 0, apply the following commands to each
     of the Salt minions that will become Ceph Monitors:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">MONITOR_NODES</em>' saltutil.sync_all
<code class="prompt user">root@master # </code>salt '<em class="replaceable">MONITOR_NODES</em>' state.apply ceph.subvolume</pre></div><p>
     The <code class="command">ceph.subvolume</code> command does the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creates <code class="filename">/var/lib/ceph</code> as a
       <code class="literal">@/var/lib/ceph</code> Btrfs subvolume.
      </p></li><li class="listitem"><p>
       Mounts the new subvolume and updates <code class="filename">/etc/fstab</code>
       appropriately.
      </p></li></ul></div></section><section class="sect3" id="id-1.3.8.2.9.6.3" data-id-title="DeepSea stage 3 Validation Fails"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">33.6.2.2 </span><span class="title-name">DeepSea stage 3 Validation Fails</span> <a title="Permalink" class="permalink" href="#id-1.3.8.2.9.6.3">#</a></h4></div></div></div><p>
     If you forgot to run the commands mentioned in
     <a class="xref" href="#var-lib-ceph-stage0" title="33.6.2.1. Before Running DeepSea stage 0">Section 33.6.2.1, “Before Running DeepSea stage 0”</a> before running stage 0, the
     <code class="filename">/var/lib/ceph</code> subdirectory already exists, causing
     DeepSea stage 3 validation failure. To convert it into a subvolume, do
     the following:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Change directory to <code class="filename">/var/lib</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib</pre></div></li><li class="step"><p>
       Back up the current content of the <code class="filename">ceph</code>
       subdirectory:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>sudo mv ceph ceph-</pre></div></li><li class="step"><p>
       Create the subvolume and, mount it, and update
       <code class="filename">/etc/fstab</code>:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt 'MONITOR_NODES' state.apply ceph.subvolume</pre></div></li><li class="step"><p>
       Change to the backup subdirectory, synchronize its content with the new
       subvolume, then remove it:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib/ceph-
<code class="prompt user">cephadm@mon &gt; </code>rsync -av . ../ceph
<code class="prompt user">cephadm@mon &gt; </code>cd ..
<code class="prompt user">cephadm@mon &gt; </code>rm -rf ./ceph-</pre></div></li></ol></div></div></section></section><section class="sect2" id="btrfs-subvol-upgrades" data-id-title="Steps Required during Cluster Upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.3 </span><span class="title-name">Steps Required during Cluster Upgrade</span> <a title="Permalink" class="permalink" href="#btrfs-subvol-upgrades">#</a></h3></div></div></div><p>
    On SUSE Enterprise Storage 5.5, the <code class="filename">/var</code>
    directory is not on a Btrfs subvolume, but its subfolders (such as
    <code class="filename">/var/log</code> or <code class="filename">/var/cache</code>) are Btrfs
    subvolumes under '@'. Creating <code class="filename">@/var/lib/ceph</code>
    subvolumes requires mounting the '@' subvolume first (it is not mounted by
    default) and creating the <code class="filename">@/var/lib/ceph</code> subvolume
    under it.
   </p><p>
    Following are example commands that illustrate the process:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>mkdir -p /mnt/btrfs
<code class="prompt user">root # </code>mount -o subvol=@ <em class="replaceable">ROOT_DEVICE</em> /mnt/btrfs
<code class="prompt user">root # </code>btrfs subvolume create /mnt/btrfs/var/lib/ceph
<code class="prompt user">root # </code>umount /mnt/btrfs</pre></div><p>
    At this point the <code class="filename">@/var/lib/ceph</code> subvolume is created
    and you can continue as described in
    <a class="xref" href="#storage-tips-ceph-btrfs-subvol-automatic" title="33.6.2. Steps Required during a New Cluster Deployment">Section 33.6.2, “Steps Required during a New Cluster Deployment”</a>.
   </p></section><section class="sect2" id="var-lib-ceph-subvol-manual" data-id-title="Manual Setup"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.4 </span><span class="title-name">Manual Setup</span> <a title="Permalink" class="permalink" href="#var-lib-ceph-subvol-manual">#</a></h3></div></div></div><p>
    Automatic setup of the <code class="filename">@/var/lib/ceph</code> Btrfs subvolume
    on the Ceph Monitor nodes may not be suitable for all scenarios. You can migrate
    your <code class="filename">/var/lib/ceph</code> directory to a
    <code class="filename">@/var/lib/ceph</code> subvolume by following these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Terminate running Ceph processes.
     </p></li><li class="step"><p>
      Unmount OSDs on the node.
     </p></li><li class="step"><p>
      Change to the backup subdirectory, synchronize its content with the new
      subvolume, then remove it:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code>cd /var/lib/ceph-
<code class="prompt user">cephadm@mon &gt; </code>rsync -av . ../ceph
<code class="prompt user">cephadm@mon &gt; </code>cd ..
<code class="prompt user">cephadm@mon &gt; </code>rm -rf ./ceph-</pre></div></li><li class="step"><p>
      Remount OSDs.
     </p></li><li class="step"><p>
      Restart Ceph daemons.
     </p></li></ol></div></div></section><section class="sect2" id="var-lib-ceph-subvol-moreinfo" data-id-title="For More Information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.6.5 </span><span class="title-name">For More Information</span> <a title="Permalink" class="permalink" href="#var-lib-ceph-subvol-moreinfo">#</a></h3></div></div></div><p>
    Find more detailed information about manual setup in the file
    <code class="filename">/srv/salt/ceph/subvolume/README.md</code> on the Salt master
    node.
   </p></section></section><section class="sect1" id="storage-bp-srv-maint-fds-inc" data-id-title="Increasing File Descriptors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.7 </span><span class="title-name">Increasing File Descriptors</span> <a title="Permalink" class="permalink" href="#storage-bp-srv-maint-fds-inc">#</a></h2></div></div></div><p>
   For OSD daemons, the read/write operations are critical to keep the Ceph
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </p><p>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <code class="filename">/etc/ceph/ceph.conf</code>, for example:
  </p><div class="verbatim-wrap"><pre class="screen">max_open_files = 131072</pre></div><p>
   After you change <code class="option">max_open_files</code>, you need to restart the
   OSD service on the relevant Ceph node.
  </p></section><section class="sect1" id="storage-admin-integration" data-id-title="Integration with Virtualization Software"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.8 </span><span class="title-name">Integration with Virtualization Software</span> <a title="Permalink" class="permalink" href="#storage-admin-integration">#</a></h2></div></div></div><section class="sect2" id="storage-bp-integration-kvm" data-id-title="Storing KVM Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.1 </span><span class="title-name">Storing KVM Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-kvm">#</a></h3></div></div></div><p>
    You can create a disk image for KVM-driven virtual machine, store it in a
    Ceph pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <code class="command">qemu-kvm</code> making use of
    the disk image stored in the cluster. For more detailed information, see
    <a class="xref" href="#cha-ceph-kvm" title="Chapter 32. Ceph as a Back-end for QEMU KVM Instance">Chapter 32, <em>Ceph as a Back-end for QEMU KVM Instance</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-libvirt" data-id-title="Storing libvirt Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.2 </span><span class="title-name">Storing <code class="systemitem">libvirt</code> Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-libvirt">#</a></h3></div></div></div><p>
    Similar to KVM (see <a class="xref" href="#storage-bp-integration-kvm" title="33.8.1. Storing KVM Disks in Ceph Cluster">Section 33.8.1, “Storing KVM Disks in Ceph Cluster”</a>), you
    can use Ceph to store virtual machines driven by <code class="systemitem">libvirt</code>. The advantage
    is that you can run any <code class="systemitem">libvirt</code>-supported virtualization solution, such
    as KVM, Xen, or LXC. For more information, see
    <a class="xref" href="#cha-ceph-libvirt" title="Chapter 31. Using libvirt with Ceph">Chapter 31, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p></section><section class="sect2" id="storage-bp-integration-xen" data-id-title="Storing Xen Disks in Ceph Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">33.8.3 </span><span class="title-name">Storing Xen Disks in Ceph Cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-xen">#</a></h3></div></div></div><p>
    One way to use Ceph for storing Xen disks is to make use of <code class="systemitem">libvirt</code>
    as described in <a class="xref" href="#cha-ceph-libvirt" title="Chapter 31. Using libvirt with Ceph">Chapter 31, <em>Using <code class="systemitem">libvirt</code> with Ceph</em></a>.
   </p><p>
    Another option is to make Xen talk to the <code class="systemitem">rbd</code>
    block device driver directly:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you have no disk image prepared for Xen, create a new one:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd create myimage --size 8000 --pool mypool</pre></div></li><li class="step"><p>
      List images in the pool <code class="literal">mypool</code> and check if your new
      image is there:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
      Create a new block device by mapping the <code class="literal">myimage</code> image
      to the <code class="systemitem">rbd</code> kernel module:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool mypool myimage</pre></div><div id="id-1.3.8.2.11.4.4.3.3" data-id-title="User Name and Authentication" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: User Name and Authentication</h6><p>
       To specify a user name, use <code class="option">--id
       <em class="replaceable">user-name</em></code>. Moreover, if you use
       <code class="systemitem">cephx</code> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
       or
      </p><div class="verbatim-wrap"><pre class="screen"><code class="systemitem">cephadm</code>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></div></li><li class="step"><p>
      List all mapped devices:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="command">rbd showmapped</code>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</pre></div></li><li class="step"><p>
      Now you can configure Xen to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <code class="command">xl</code>-style domain configuration file:
     </p><div class="verbatim-wrap"><pre class="screen">disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</pre></div></li></ol></div></div></section></section><section class="sect1" id="storage-bp-net-firewall" data-id-title="Firewall Settings for Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.9 </span><span class="title-name">Firewall Settings for Ceph</span> <a title="Permalink" class="permalink" href="#storage-bp-net-firewall">#</a></h2></div></div></div><div id="id-1.3.8.2.12.2" data-id-title="DeepSea Stages Fail with Firewall" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: DeepSea Stages Fail with Firewall</h6><p>
    DeepSea deployment stages fail when firewall is active (and even
    configured). To pass the stages correctly, you need to either turn the
    firewall off by running
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop SuSEfirewall2.service</pre></div><p>
    or set the <code class="option">FAIL_ON_WARNING</code> option to 'False' in
    <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">FAIL_ON_WARNING: False</pre></div></div><p>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <span class="guimenu">YaST</span> / <span class="guimenu">Security and
   Users</span> / <span class="guimenu">Firewall</span> / <span class="guimenu">Allowed
   Services</span>.
  </p><p>
   Following is a list of Ceph related services and numbers of the ports that
   they normally use:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.8.2.12.5.1"><span class="term">Ceph Monitor</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph MON</span> service or port 6789 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.2"><span class="term">Ceph OSD or Metadata Server</span></dt><dd><p>
      Enable the <span class="guimenu">Ceph OSD/MDS</span> service or ports 6800-7300
      (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.3"><span class="term">iSCSI Gateway</span></dt><dd><p>
      Open port 3260 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.4"><span class="term">Object Gateway</span></dt><dd><p>
      Open the port where Object Gateway communication occurs. It is set in
      <code class="filename">/etc/ceph.conf</code> on the line starting with
      <code class="literal">rgw frontends =</code>. Default is 80 for HTTP and 443 for
      HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.5"><span class="term">NFS Ganesha</span></dt><dd><p>
      By default, NFS Ganesha uses ports 2049 (NFS service, TCP) and 875 (rquota
      support, TCP). Refer to <a class="xref" href="#ganesha-nfsport" title="30.2.1.4. Changing Default NFS Ganesha Ports">Section 30.2.1.4, “Changing Default NFS Ganesha Ports”</a> for more
      information on changing the default NFS Ganesha ports.
     </p></dd><dt id="id-1.3.8.2.12.5.6"><span class="term">Apache based services, such as SMT, or SUSE Manager</span></dt><dd><p>
      Open ports 80 for HTTP and 443 for HTTPS (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.7"><span class="term">SSH</span></dt><dd><p>
      Open port 22 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.8"><span class="term">NTP</span></dt><dd><p>
      Open port 123 (UDP).
     </p></dd><dt id="id-1.3.8.2.12.5.9"><span class="term">Salt</span></dt><dd><p>
      Open ports 4505 and 4506 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.10"><span class="term">Grafana</span></dt><dd><p>
      Open port 3000 (TCP).
     </p></dd><dt id="id-1.3.8.2.12.5.11"><span class="term">Prometheus</span></dt><dd><p>
      Open port 9100 (TCP).
     </p></dd></dl></div></section><section class="sect1" id="storage-bp-network-test" data-id-title="Testing Network Performance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.10 </span><span class="title-name">Testing Network Performance</span> <a title="Permalink" class="permalink" href="#storage-bp-network-test">#</a></h2></div></div></div><p>
   To test the network performance, DeepSea's <code class="literal">net</code> runner
   provides the following commands:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A simple ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</pre></div></li><li class="listitem"><p>
     A jumbo ping to all nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</pre></div></li><li class="listitem"><p>
     A bandwidth test:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">salt-run</code> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</pre></div><div id="id-1.3.8.2.13.3.3.3" data-id-title="Stop iperf3 Processes Manually" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Stop 'iperf3' Processes Manually</h6><p>
      When running a test using the <code class="command">net.iperf</code> runner, the
      'iperf3' server processes that are started do not stop automatically when
      a test is completed. To stop the processes, use the following runner:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' multi.kill_iperf_cmd</pre></div></div></li></ul></div></section><section class="sect1" id="bp-flash-led-lights" data-id-title="How to Locate Physical Disks Using LED Lights"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">33.11 </span><span class="title-name">How to Locate Physical Disks Using LED Lights</span> <a title="Permalink" class="permalink" href="#bp-flash-led-lights">#</a></h2></div></div></div><p>
   This section describes using <code class="systemitem">libstoragemgmt</code> and/or
   third party tools to adjust the LED lights on physical disks. This
   capability may not be available for all hardware platforms.
  </p><p>
   Matching an OSD disk to a physical disk can be challenging, especially on
   nodes with a high density of disks. Some hardware environments include LED
   lights that can be adjusted via software to flash or illuminate a different
   color for identification purposes. SUSE Enterprise Storage offers support for this
   capability through Salt, <code class="systemitem">libstoragemgmt</code>, and
   third party tools specific to the hardware in use. The configuration for
   this capability is defined in the
   <code class="filename">/srv/pillar/ceph/disk_led.sls</code> Salt pillar:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> cat /srv/pillar/ceph/disk_led.sls
# This is the default configuration for the storage enclosure LED blinking.
# The placeholder {device_file} will be replaced with the device file of
# the disk when the command is executed.
#
# Have a look into the /srv/pillar/ceph/README file to find out how to
# customize this configuration per minion/host.

disk_led:
  cmd:
    ident:
      'on': lsmcli local-disk-ident-led-on --path '{device_file}'
      'off': lsmcli local-disk-ident-led-off --path '{device_file}'
    fault:
      'on': lsmcli local-disk-fault-led-on --path '{device_file}'
      'off': lsmcli local-disk-fault-led-off --path '{device_file}'</pre></div><p>
   The default configuration for <code class="filename">disk_led.sls</code> offers disk
   LED support through the <code class="systemitem">libstoragemgmt</code> layer. The
   <code class="systemitem">libstoragemgmt</code> layer provides this support through
   a hardware-specific plug-in and third party tools. The default behavior can
   be customized using DeepSea by adding the following for the
   <code class="literal">ledmon</code> package and <code class="literal">ledctl</code> tool to
   <code class="filename">/srv/pillar/ceph/stack/global.yml</code> (or any other YAML
   file in <code class="filename">/stack/ceph/</code>):
  </p><div class="verbatim-wrap"><pre class="screen">  disk_led:
    cmd:
      ident:
        'on': ledctl locate='{device_file}'
        'off': ledctl locate_off='{device_file}'
      fault:
        'on': ledctl locate='{device_file}'
        'off': ledctl locate_off='{device_file}'</pre></div><p>
   If the customization should only apply to a special node (minion), then the
   file <code class="filename">stack/ceph/minions/{{minion}}.yml</code> needs to be
   used.
  </p><p>
   With or without <code class="systemitem">libstoragemgmt</code>, third party tools
   may be required to adjust LED lights. These third party tools are available
   through various hardware vendors. Some of the common vendors and tools are:
  </p><div class="table" id="id-1.3.8.2.14.9" data-id-title="Third Party Storage Tools"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 33.1: </span><span class="title-name">Third Party Storage Tools </span><a title="Permalink" class="permalink" href="#id-1.3.8.2.14.9">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Vendor/Disk Controller</th><th style="border-bottom: 1px solid ; ">Tool</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">HPE SmartArray</td><td style="border-bottom: 1px solid ; ">hpssacli</td></tr><tr><td style="border-right: 1px solid ; ">LSI MegaRAID</td><td>storcli</td></tr></tbody></table></div></div><p>
   SUSE Linux Enterprise Server also provides the <span class="package">ledmon</span> package and
   <code class="command">ledctl</code> tool. This tool may also work for hardware
   environments utilizing Intel storage enclosures. Proper syntax when using
   this tool is as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code> cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'
    fault:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'</pre></div><p>
   If you are on supported hardware, with all required third party tools, LEDs
   can be enabled or disabled using the following command syntax from the
   Salt master node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run disk_led.device <em class="replaceable">NODE</em> <em class="replaceable">DISK</em> <em class="replaceable">fault|ident</em> <em class="replaceable">on|off</em></pre></div><p>
   For example, to enable or disable LED identification or fault lights on
   <code class="filename">/dev/sdd</code> on OSD node <code class="filename">srv16.ceph</code>,
   run the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd ident on
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd ident off
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd fault on
<code class="prompt user">root # </code>salt-run disk_led.device srv16.ceph sdd fault off</pre></div><div id="id-1.3.8.2.14.16" data-id-title="Device Naming" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Device Naming</h6><p>
    The device name used in the <code class="command">salt-run</code> command needs to
    match the name recognized by Salt. The following command can be used to
    display these names:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '<em class="replaceable">minion_name</em>' grains.get disks</pre></div></div><p>
   In many environments, the <code class="filename">/srv/pillar/ceph/disk_led.sls</code>
   configuration will require changes in order to adjust the LED lights for
   specific hardware needs. Simple changes may be performed by replacing
   <code class="command">lsmcli</code> with another tool, or adjusting command line
   parameters. Complex changes may be accomplished by calling an external
   script in place of the <code class="filename">lsmcli</code> command. When making any
   changes to <code class="filename">/srv/pillar/ceph/disk_led.sls</code>, follow these
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Make required changes to
     <code class="filename">/srv/pillar/ceph/disk_led.sls</code> on the Salt master node.
    </p></li><li class="step"><p>
     Verify that the changes are reflected correctly in the pillar data:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt '<em class="replaceable">SALT MASTER</em>*' pillar.get disk_led</pre></div></li><li class="step"><p>
     Refresh the pillar data on all nodes using:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>salt '*' saltutil.pillar_refresh</pre></div></li></ol></div></div><p>
   It is possible to use an external script to directly use third-party tools
   to adjust LED lights. The following examples show how to adjust
   <code class="filename">/srv/pillar/ceph/disk_led.sls</code> to support an external
   script, and two sample scripts for HP and LSI environments.
  </p><p>
   Modified <code class="filename">/srv/pillar/ceph/disk_led.sls</code> which calls an
   external script:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off
    fault:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off</pre></div><p>
   Sample script for flashing LED lights on HP hardware using the
   <code class="systemitem">hpssacli</code> utilities:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /usr/local/bin/flash_led_hp.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

FOUND=0
MAX_CTRLS=10
MAX_DISKS=50

for i in $(seq 0 $MAX_CTRLS); do
  # Search for valid controllers
  if hpssacli ctrl slot=$i show summary &gt;/dev/null; then
    # Search all disks on the current controller
    for j in $(seq 0 $MAX_DISKS); do
      if hpssacli ctrl slot=$i ld $j show | grep -q $1; then
        FOUND=1
        echo "Found $1 on ctrl=$i, ld=$j. Turning LED $2."
        hpssacli ctrl slot=$i ld $j modify led=$2
        break;
      fi
    done
    [[ "$FOUND" = "1" ]] &amp;&amp; break
  fi
done</pre></div><p>
   Sample script for flashing LED lights on LSI hardware using the
   <code class="systemitem">storcli</code> utilities:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>cat /usr/local/bin/flash_led_lsi.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

[[ "$2" = "on" ]] &amp;&amp; ACTION="start" || ACTION="stop"

# Determine serial number for the disk
SERIAL=$(lshw -class disk | grep -A2 $1 | grep serial | awk '{print $NF}')
if [ ! -z "$SERIAL" ]; then
  # Search for disk serial number across all controllers and enclosures
  DEVICE=$(/opt/MegaRAID/storcli/storcli64 /call/eall/sall show all | grep -B6 $SERIAL | grep Drive | awk '{print $2}')
  if [ ! -z "$DEVICE" ]; then
    echo "Found $1 on device $DEVICE. Turning LED $2."
    /opt/MegaRAID/storcli/storcli64 $DEVICE $ACTION locate
  else
    echo "Device not found!"
    exit -1
  fi
else
  echo "Disk serial number not found!"
  exit -1
fi</pre></div></section></section><section class="chapter" id="storage-faqs" data-id-title="Frequently Asked Questions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34 </span><span class="title-name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="#storage-faqs">#</a></h2></div></div></div><section class="sect1" id="storage-bp-tuneups-pg-num" data-id-title="How Does the Number of Placement Groups Affect the Cluster Performance?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34.1 </span><span class="title-name">How Does the Number of Placement Groups Affect the Cluster Performance?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-pg-num">#</a></h2></div></div></div><p>
   When your cluster is becoming 70% to 80% full, it is time to add more OSDs
   to it. When you increase the number of OSDs, you may consider increasing the
   number of placement groups as well.
  </p><div id="id-1.3.8.3.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Changing the number of PGs causes a lot of data transfer within the
    cluster.
   </p></div><p>
   To calculate the optimal value for your newly-resized cluster is a complex
   task.
  </p><p>
   A high number of PGs creates small chunks of data. This speeds up recovery
   after an OSD failure, but puts a lot of load on the monitor nodes as they
   are responsible for calculating the data location.
  </p><p>
   On the other hand, a low number of PGs takes more time and data transfer to
   recover from an OSD failure, but does not impose that much load on monitor
   nodes as they need to calculate locations for less (but larger) data chunks.
  </p><p>
   Find more information on the optimal number of PGs for your cluster in
   <a class="xref" href="#op-pgs-pg-num" title="20.4.2. Determining the Value of PG_NUM">Section 20.4.2, “Determining the Value of <em class="replaceable">PG_NUM</em>”</a>.
  </p></section><section class="sect1" id="storage-bp-tuneups-mix-ssd" data-id-title="Can I Use SSDs and Hard Disks on the Same Cluster?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34.2 </span><span class="title-name">Can I Use SSDs and Hard Disks on the Same Cluster?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-mix-ssd">#</a></h2></div></div></div><p>
   Solid-state drives (SSD) are generally faster than hard disks. If you mix
   the two types of disks for the same write operation, the data writing to the
   SSD disk will be slowed down by the hard disk performance. Thus, you should
   <span class="emphasis"><em>never mix SSDs and hard disks</em></span> for data writing
   following <span class="emphasis"><em>the same rule</em></span> (see
   <a class="xref" href="#datamgm-rules" title="20.3. Rule Sets">Section 20.3, “Rule Sets”</a> for more information on rules for storing
   data).
  </p><p>
   There are generally 2 cases where using SSD and hard disk on the same
   cluster makes sense:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Use each disk type for writing data following different rules. Then you
     need to have a separate rule for the SSD disk, and another rule for the
     hard disk.
    </p></li><li class="listitem"><p>
     Use each disk type for a specific purpose. For example the SSD disk for
     journal, and the hard disk for storing data.
    </p></li></ol></div></section><section class="sect1" id="storage-bp-tuneups-ssd-tradeoffs" data-id-title="What are the Trade-offs of Using a Journal on SSD?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34.3 </span><span class="title-name">What are the Trade-offs of Using a Journal on SSD?</span> <a title="Permalink" class="permalink" href="#storage-bp-tuneups-ssd-tradeoffs">#</a></h2></div></div></div><p>
   Using SSDs for OSD journal(s) is better for performance as the journal is
   usually the bottleneck of hard disk-only OSDs. SSDs are often used to share
   journals of several OSDs.
  </p><p>
   Following is a list of potential disadvantages of using SSDs for OSD
   journal:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     SSD disks are more expensive than hard disks. But as one OSD journal
     requires up to 6GB of disk space only, the price may not be so crucial.
    </p></li><li class="listitem"><p>
     SSD disk consumes storage slots which can be otherwise used by a large
     hard disk to extend the cluster capacity.
    </p></li><li class="listitem"><p>
     SSD disks have reduced write cycles compared to hard disks, but modern
     technologies are beginning to eliminate the problem.
    </p></li><li class="listitem"><p>
     If you share more journals on the same SSD disk, you risk losing all the
     related OSDs after the SSD disk fails. This will require a lot of data to
     be moved to rebalance the cluster.
    </p></li><li class="listitem"><p>
     Hotplugging disks becomes more complex as the data mapping is not 1:1 the
     failed OSD and the journal disk.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-diskfails" data-id-title="What Happens When a Disk Fails?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34.4 </span><span class="title-name">What Happens When a Disk Fails?</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-diskfails">#</a></h2></div></div></div><p>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The related OSD crashed and is automatically removed from the cluster.
    </p></li><li class="listitem"><p>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </p></li><li class="listitem"><p>
     Then you should remove the disk from the cluster CRUSH Map, and
     physically from the host hardware.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-journalfails" data-id-title="What Happens When a Journal Disk Fails?"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">34.5 </span><span class="title-name">What Happens When a Journal Disk Fails?</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-journalfails">#</a></h2></div></div></div><p>
   Ceph can be configured to store journals or write ahead logs on devices
   separate from the OSDs. When a disk dedicated to a journal fails, the
   related OSD(s) fail as well (see
   <a class="xref" href="#storage-bp-monitoring-diskfails" title="34.4. What Happens When a Disk Fails?">Section 34.4, “What Happens When a Disk Fails?”</a>).
  </p><div id="id-1.3.8.3.8.3" data-id-title="Hosting Multiple Journals on One Disk" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Hosting Multiple Journals on One Disk</h6><p>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </p></div></section></section><section class="chapter" id="storage-troubleshooting" data-id-title="Troubleshooting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35 </span><span class="title-name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#storage-troubleshooting">#</a></h2></div></div></div><p>
  This chapter describes several issues that you may face when you operate a
  Ceph cluster.
 </p><section class="sect1" id="storage-bp-report-bug" data-id-title="Reporting Software Problems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.1 </span><span class="title-name">Reporting Software Problems</span> <a title="Permalink" class="permalink" href="#storage-bp-report-bug">#</a></h2></div></div></div><p>
   If you come across a problem when running SUSE Enterprise Storage 6
   related to some of its components, such as Ceph or Object Gateway, report the
   problem to SUSE Technical Support. The recommended way is with the
   <code class="command">supportconfig</code> utility.
  </p><div id="id-1.3.8.4.4.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    Because <code class="command">supportconfig</code> is modular software, make sure
    that the <code class="systemitem">supportutils-plugin-ses</code> package is
    installed.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">tux &gt; </code>rpm -q supportutils-plugin-ses</pre></div><p>
    If it is missing on the Ceph server, install it with
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</pre></div></div><p>
   Although you can use <code class="command">supportconfig</code> on the command line,
   we recommend using the related YaST module. Find more information about
   <code class="command">supportconfig</code> in
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-adm-support.html#sec-admsupport-supportconfig" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-adm-support.html#sec-admsupport-supportconfig</a>.
  </p></section><section class="sect1" id="storage-bp-cluster-mntc-rados-striping" data-id-title="Sending Large Objects with rados Fails with Full OSD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.2 </span><span class="title-name">Sending Large Objects with <code class="command">rados</code> Fails with Full OSD</span> <a title="Permalink" class="permalink" href="#storage-bp-cluster-mntc-rados-striping">#</a></h2></div></div></div><p>
   <code class="command">rados</code> is a command line utility to manage RADOS object
   storage. For more information, see <code class="command">man 8 rados</code>.
  </p><p>
   If you send a large object to a Ceph cluster with the
   <code class="command">rados</code> utility, such as
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rados -p mypool put myobject /file/to/send</pre></div><p>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance.
  </p></section><section class="sect1" id="ceph-xfs-corruption" data-id-title="Corrupted XFS File system"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.3 </span><span class="title-name">Corrupted XFS File system</span> <a title="Permalink" class="permalink" href="#ceph-xfs-corruption">#</a></h2></div></div></div><p>
   In rare circumstances like kernel bug or broken/misconfigured hardware, the
   underlying file system (XFS) in which an OSD stores its data might be
   damaged and unmountable.
  </p><p>
   If you are sure there is no problem with your hardware and the system is
   configured properly, raise a bug against the XFS subsystem of the SUSE Linux Enterprise Server
   kernel and mark the particular OSD as down:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd down <em class="replaceable">OSD_ID</em></pre></div><div id="id-1.3.8.4.6.5" data-id-title="Do Not Format or Otherwise Modify the Damaged Device" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Do Not Format or Otherwise Modify the Damaged Device</h6><p>
    Even though using <code class="command">xfs_repair</code> to fix the problem in the
    file system may seem reasonable, do not use it as the command modifies the
    file system. The OSD may start but its functioning may be influenced.
   </p></div><p>
   Now zap the underlying disk and re-create the OSD by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm zap --data /dev/<em class="replaceable">OSD_DISK_DEVICE</em>
<code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvm prepare --bluestore --data /dev/<em class="replaceable">OSD_DISK_DEVICE</em></pre></div></section><section class="sect1" id="storage-bp-recover-toomanypgs" data-id-title="Too Many PGs per OSD Status Message"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.4 </span><span class="title-name">'Too Many PGs per OSD' Status Message</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-toomanypgs">#</a></h2></div></div></div><p>
   If you receive a <code class="literal">Too Many PGs per OSD</code> message after
   running <code class="command">ceph status</code>, it means that the
   <code class="option">mon_pg_warn_max_per_osd</code> value (300 by default) was
   exceeded. This value is compared to the number of PGs per OSD ratio. This
   means that the cluster setup is not optimal.
  </p><p>
   As of the Nautilus release, the number of PGs can be decreased after a pool
   is already created. To increase or decrease the amount of PGs for an
   existing pool, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> <em class="replaceable">NUM_OF_PG</em></pre></div><p>
   Note that this operation is resource intensive and you are encouraged to
   make such changes incrementally, especially when decreasing (merging) the
   amount of PGs. We recommend to instead have the PG autoscaler manager module
   enabled to manage the amount of PGs per pool. For details, see
   <a class="xref" href="#op-pgs-autoscaler" title="20.4.12. PG Auto-scaler">Section 20.4.12, “PG Auto-scaler”</a>
  </p></section><section class="sect1" id="storage-bp-recover-stuckinactive" data-id-title="nn pg stuck inactive Status Message"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.5 </span><span class="title-name">'<span class="emphasis"><em>nn</em></span> pg stuck inactive' Status Message</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-stuckinactive">#</a></h2></div></div></div><p>
   If you receive a <code class="literal">stuck inactive</code> status message after
   running <code class="command">ceph status</code>, it means that Ceph does not know
   where to replicate the stored data to fulfill the replication rules. It can
   happen shortly after the initial Ceph setup and fix itself automatically.
   In other cases, this may require a manual interaction, such as bringing up a
   broken OSD, or adding a new OSD to the cluster. In very rare cases, reducing
   the replication level may help.
  </p><p>
   If the placement groups are stuck perpetually, you need to check the output
   of <code class="command">ceph osd tree</code>. The output should look tree-structured,
   similar to the example in <a class="xref" href="#storage-bp-recover-osddown" title="35.7. OSD is Down">Section 35.7, “OSD is Down”</a>.
  </p><p>
   If the output of <code class="command">ceph osd tree</code> is rather flat as in the
   following example
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</pre></div><p>
   You should check that the related CRUSH map has a tree structure. If it is
   also flat, or with no hosts as in the above example, it may mean that host
   name resolution is not working correctly across the cluster.
  </p><p>
   If the hierarchy is incorrect—for example the root contains hosts, but
   the OSDs are at the top level and are not themselves assigned to
   hosts—you will need to move the OSDs to the correct place in the
   hierarchy. This can be done using the <code class="command">ceph osd crush move</code>
   and/or <code class="command">ceph osd crush set</code> commands. For further details
   see <a class="xref" href="#op-crush" title="20.5. CRUSH Map Manipulation">Section 20.5, “CRUSH Map Manipulation”</a>.
  </p></section><section class="sect1" id="storage-bp-recover-osdweight" data-id-title="OSD Weight is 0"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.6 </span><span class="title-name">OSD Weight is 0</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-osdweight">#</a></h2></div></div></div><p>
   When OSD starts, it is assigned a weight. The higher the weight, the bigger
   the chance that the cluster writes data to the OSD. The weight is either
   specified in a cluster CRUSH Map, or calculated by the OSDs' start-up
   script.
  </p><p>
   In some cases, the calculated value for OSDs' weight may be rounded down to
   zero. It means that the OSD is not scheduled to store data, and no data is
   written to it. The reason is usually that the disk is too small (smaller
   than 15GB) and should be replaced with a bigger one.
  </p></section><section class="sect1" id="storage-bp-recover-osddown" data-id-title="OSD is Down"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.7 </span><span class="title-name">OSD is Down</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-osddown">#</a></h2></div></div></div><p>
   OSD daemon is either running, or stopped/down. There are 3 general reasons
   why an OSD is down:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Hard disk failure.
    </p></li><li class="listitem"><p>
     The OSD crashed.
    </p></li><li class="listitem"><p>
     The server crashed.
    </p></li></ul></div><p>
   You can see the detailed status of OSDs by running
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</pre></div><p>
   The example listing shows that the <code class="literal">osd.2</code> is down. Then
   you may check if the disk where the OSD is located is mounted:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</pre></div><p>
   You can track the reason why the OSD is down by inspecting its log file
   <code class="filename">/var/log/ceph/ceph-osd.2.log</code>. After you find and fix
   the reason why the OSD is not running, start it with
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl start ceph-osd@2.service</pre></div><p>
   Do not forget to replace <code class="literal">2</code> with the actual number of your
   stopped OSD.
  </p></section><section class="sect1" id="storage-bp-performance-slowosd" data-id-title="Finding Slow OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.8 </span><span class="title-name">Finding Slow OSDs</span> <a title="Permalink" class="permalink" href="#storage-bp-performance-slowosd">#</a></h2></div></div></div><p>
   When tuning the cluster performance, it is very important to identify slow
   storage/OSDs within the cluster. The reason is that if the data is written
   to the slow(est) disk, the complete write operation slows down as it always
   waits until it is finished on all the related disks.
  </p><p>
   It is not trivial to locate the storage bottleneck. You need to examine each
   and every OSD to find out the ones slowing down the write process. To do a
   benchmark on a single OSD, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="command">ceph tell</code> osd.<em class="replaceable">OSD_ID_NUMBER</em> bench</pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</pre></div><p>
   Then you need to run this command on each OSD and compare the
   <code class="literal">bytes_per_sec</code> value to get the slow(est) OSDs.
  </p></section><section class="sect1" id="storage-bp-recover-clockskew" data-id-title="Fixing Clock Skew Warnings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.9 </span><span class="title-name">Fixing Clock Skew Warnings</span> <a title="Permalink" class="permalink" href="#storage-bp-recover-clockskew">#</a></h2></div></div></div><p>
   The time information in all cluster nodes must be synchronized. If a node's
   time is not fully synchronized, you may get clock skew warnings when
   checking the state of the cluster.
  </p><p>
   By default, DeepSea uses the Admin Node as the time server for other cluster
   nodes. Therefore, if the Admin Node is not virtualized, select one or more time
   servers or pools, and synchronize the local time against them. Verify that
   the time synchronization service is enabled on each system start-up. Find
   more information on setting up time synchronization in
   <a class="link" href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html" target="_blank">https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html</a>.
  </p><p>
   If the Admin Node is a virtual machine, provide better time sources for the
   cluster nodes by overriding the default NTP client configuration:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Edit <code class="filename">/srv/pillar/ceph/stack/global.yml</code> on the
     Salt master node and add the following line:
    </p><div class="verbatim-wrap"><pre class="screen">time_server: <em class="replaceable">CUSTOM_NTP_SERVER</em></pre></div><p>
     To add multiple time servers, the format is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">time_server:
 - <em class="replaceable">CUSTOM_NTP_SERVER1</em>
 - <em class="replaceable">CUSTOM_NTP_SERVER2</em>
 - <em class="replaceable">CUSTOM_NTP_SERVER3</em>
 [...]</pre></div></li><li class="listitem"><p>
     Refresh the Salt pillar:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="listitem"><p>
     Verify the changed value:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' pillar.items</pre></div></li><li class="listitem"><p>
     Apply the new setting:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' state.apply ceph.time</pre></div></li></ol></div><p>
   If the time skew still occurs on a node, follow these steps to fix it:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>systemctl stop chronyd.service
<code class="prompt user">root # </code>systemctl stop ceph-mon.target
<code class="prompt user">root # </code>systemctl start chronyd.service
<code class="prompt user">root # </code>systemctl start ceph-mon.target</pre></div><p>
   You can then check the time offset with <code class="command">chronyc
   sourcestats</code>.
  </p><p>
   The Ceph monitors need to have their clocks synchronized to within 0.05
   seconds of each other. Refer to <a class="xref" href="#Cluster-Time-Setting" title="33.4. Time Synchronization of Nodes">Section 33.4, “Time Synchronization of Nodes”</a> for
   more information.
  </p></section><section class="sect1" id="storage-bp-performance-net-issues" data-id-title="Poor Cluster Performance Caused by Network Problems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.10 </span><span class="title-name">Poor Cluster Performance Caused by Network Problems</span> <a title="Permalink" class="permalink" href="#storage-bp-performance-net-issues">#</a></h2></div></div></div><p>
   There are more reasons why the cluster performance may become weak. One of
   them can be network problems. In such case, you may notice the cluster
   reaching quorum, OSD and monitor nodes going offline, data transfers taking
   a long time, or a lot of reconnect attempts.
  </p><p>
   To check whether cluster performance is degraded by network problems,
   inspect the Ceph log files under the <code class="filename">/var/log/ceph</code>
   directory.
  </p><p>
   To fix network issues on the cluster, focus on the following points:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Basic network diagnostics. Try DeepSea diagnostics tools runner
     <code class="literal">net.ping</code> to ping between cluster nodes to see if
     individual interface can reach to specific interface and the average
     response time. Any specific response time much slower then average will
     also be reported. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.ping
  Succeeded: 8 addresses from 7 minions average rtt 0.15 ms</pre></div><p>
     Try validating all interface with JumboFrame enable:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.jumbo_ping
  Succeeded: 8 addresses from 7 minions average rtt 0.26 ms</pre></div></li><li class="listitem"><p>
     Network performance benchmark. Try DeepSea's network performance runner
     <code class="literal">net.iperf</code> to test intern-node network bandwidth. On a
     given cluster node, a number of <code class="command">iperf</code> processes
     (according to the number of CPU cores) are started as servers. The
     remaining cluster nodes will be used as clients to generate network
     traffic. The accumulated bandwidth of all per-node
     <code class="command">iperf</code> processes is reported. This should reflect the
     maximum achievable network throughput on all cluster nodes. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run net.iperf cluster=ceph output=full
192.168.128.1:
    8644.0 Mbits/sec
192.168.128.2:
    10360.0 Mbits/sec
192.168.128.3:
    9336.0 Mbits/sec
192.168.128.4:
    9588.56 Mbits/sec
192.168.128.5:
    10187.0 Mbits/sec
192.168.128.6:
    10465.0 Mbits/sec</pre></div></li><li class="listitem"><p>
     Check firewall settings on cluster nodes. Make sure they do not block
     ports/protocols required by Ceph operation. See
     <a class="xref" href="#storage-bp-net-firewall" title="33.9. Firewall Settings for Ceph">Section 33.9, “Firewall Settings for Ceph”</a> for more information on firewall
     settings.
    </p></li><li class="listitem"><p>
     Check the networking hardware, such as network cards, cables, or switches,
     for proper operation.
    </p></li></ul></div><div id="id-1.3.8.4.13.6" data-id-title="Separate Network" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Separate Network</h6><p>
    To ensure fast and safe network communication between cluster nodes, set up
    a separate network used exclusively by the cluster OSD and monitor nodes.
   </p></div></section><section class="sect1" id="trouble-jobcache" data-id-title="/var Running Out of Space"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.11 </span><span class="title-name"><code class="filename">/var</code> Running Out of Space</span> <a title="Permalink" class="permalink" href="#trouble-jobcache">#</a></h2></div></div></div><p>
   By default, the Salt master saves every minion's return for every job in its
   <span class="emphasis"><em>job cache</em></span>. The cache can then be used later to look up
   results from previous jobs. The cache directory defaults to
   <code class="filename">/var/cache/salt/master/jobs/</code>.
  </p><p>
   Each job return from every minion is saved in a single file. Over time this
   directory can grow very large, depending on the number of published jobs and
   the value of the <code class="option">keep_jobs</code> option in the
   <code class="filename">/etc/salt/master</code> file. <code class="option">keep_jobs</code> sets
   the number of hours (24 by default) to keep information about past minion
   jobs.
  </p><div class="verbatim-wrap"><pre class="screen">keep_jobs: 24</pre></div><div id="id-1.3.8.4.14.5" data-id-title="Do Not Set keep_jobs: 0" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Set <code class="option">keep_jobs: 0</code></h6><p>
    Setting <code class="option">keep_jobs</code> to '0' will cause the job cache cleaner
    to <span class="emphasis"><em>never</em></span> run, possibly resulting in a full partition.
   </p></div><p>
   If you want to disable the job cache, set <code class="option">job_cache</code> to
   'False':
  </p><div class="verbatim-wrap"><pre class="screen">job_cache: False</pre></div><div id="id-1.3.8.4.14.8" data-id-title="Restoring Partition Full because of Job Cache" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Restoring Partition Full because of Job Cache</h6><p>
    When the partition with job cache files gets full because of wrong
    <code class="option">keep_jobs</code> setting, follow these steps to free disk space
    and improve the job cache settings:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop the Salt master service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-master</pre></div></li><li class="step"><p>
      Change the Salt master configuration related to job cache by editing
      <code class="filename">/etc/salt/master</code>:
     </p><div class="verbatim-wrap"><pre class="screen">job_cache: False
keep_jobs: 1</pre></div></li><li class="step"><p>
      Clear the Salt master job cache:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root # </code>rm -rfv /var/cache/salt/master/jobs/*</pre></div></li><li class="step"><p>
      Start the Salt master service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl start salt-master</pre></div></li></ol></div></div></div></section><section class="sect1" id="trouble-osd-panic" data-id-title="OSD Panic Occurs when Media Error Happens during FileStore Directory Split"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">35.12 </span><span class="title-name">OSD Panic Occurs when Media Error Happens during FileStore Directory Split</span> <a title="Permalink" class="permalink" href="#trouble-osd-panic">#</a></h2></div></div></div><p>
   When a directory split error is generated on FileStore OSDs, the
   corresponding OSD is designed to terminate. It is then easier to detect the
   problem and avoids introducing inconsistencies. If the OSD terminates
   frequently, <code class="systemitem">systemd</code> may disable it permanently depending on the <code class="systemitem">systemd</code>
   configuration. After the OSD is disabled, it will be set out and process of
   data migration will be started.
  </p><p>
   You should notice the OSD termination by regularly running the <code class="command">ceph
   status</code> command and perform necessary steps to investigate the
   cause. If media error is the cause of OSD termination, replace the OSD and
   wait for all PGs to complete their backfill to the new replaced OSD. Then
   run the deep-scrub for these PGs.
  </p><div id="id-1.3.8.4.15.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    Do not initiate a new deep-scrub by adding new OSDs or changing their
    weight until the deep-scrub is complete.
   </p></div><p>
   For more details about scrubbing, refer to <a class="xref" href="#tips-scrubbing" title="33.2. Adjusting Scrubbing">Section 33.2, “Adjusting Scrubbing”</a>.
  </p></section></section></div><section class="appendix" id="app-stage1-custom" data-id-title="DeepSea Stage 1 Custom Example"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">DeepSea Stage 1 Custom Example</span> <a title="Permalink" class="permalink" href="#app-stage1-custom">#</a></h1></div></div></div><div class="verbatim-wrap"><pre class="screen">{% set master = salt['master.minion']() %}

include:
  - ..validate

ready:
  salt.runner:
    - name: minions.ready
    - timeout: {{ salt['pillar.get']('ready_timeout', 300) }}

refresh_pillar0:
  salt.state:
    - tgt: {{ master }}
    - sls: ceph.refresh

discover roles:
  salt.runner:
    - name: populate.proposals
    - require:
        - salt: refresh_pillar0


discover storage profiles:
  salt.runner:
    - name: proposal.populate
    - kwargs:
        'name': 'prod'
        'db-size': '59G'
        'wal-size': '1G'
        'nvme-spinner': True
        'ratio': 12
    - require:
        - salt: refresh_pillar0</pre></div></section><section class="appendix" id="id-1.3.10" data-id-title="Ceph Maintenance Updates Based on Upstream Nautilus Point Releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">B </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span> <a title="Permalink" class="permalink" href="#id-1.3.10">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 6 are based on the
  Nautilus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Nautilus series, SUSE Enterprise Storage 6 is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.5"><span class="name">Nautilus 14.2.20 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.12"><span class="name">Nautilus 14.2.18 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.12">#</a></h2></div><p>
  This release fixes a regression introduced in 14.2.17 in which the manager
  module tries to use a couple of Python modules that do not exist in some
  environments.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    This release fixes issues loading the dashboard and volumes manager modules
    in some environments.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.15"><span class="name">Nautilus 14.2.17 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.15">#</a></h2></div><p>
  This release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="varname">$pid</code> expansion in configuration paths such as
    <code class="literal">admin_socket</code> will now properly expand to the daemon PID
    for commands like <code class="command">ceph-mds</code> or
    <code class="command">ceph-osd</code>. Previously, only <code class="command">ceph-fuse</code>
    and <code class="command">rbd-nbd</code> expanded <code class="varname">$pid</code> with the
    actual daemon PID.
   </p></li><li class="listitem"><p>
    RADOS: PG removal has been optimized.
   </p></li><li class="listitem"><p>
    RADOS: Memory allocations are tracked in finer detail in BlueStore and
    displayed as a part of the <code class="command">dump_mempools</code> command.
   </p></li><li class="listitem"><p>
    CephFS: clients which acquire capabilities too quickly are throttled to
    prevent instability. See new config option
    <code class="option">mds_session_cap_acquisition_throttle</code> to control this
    behavior.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.18"><span class="name">Nautilus 14.2.16 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-27781 : OpenStack Manila use of
    <code class="command">ceph_volume_client.py</code> library allowed tenant access to
    any Ceph credentials' secret.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.21"><span class="name">Nautilus 14.2.15 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.21">#</a></h2></div><p>
  This release fixes a ceph-volume regression introduced in v14.2.13 and
  includes few other fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    ceph-volume: Fixes <code class="command">lvm batch –auto</code>, which breaks
    backward compatibility when using non rotational devices only (SSD and/or
    NVMe).
   </p></li><li class="listitem"><p>
    BlueStore: Fixes a bug in <code class="literal">collection_list_legacy</code> which
    makes PGs inconsistent during scrub when running OSDs older than 14.2.12
    with newer ones.
   </p></li><li class="listitem"><p>
    MGR: progress module can now be turned on or off, using the commands
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.24"><span class="name">Nautilus 14.2.14 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.24">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus, among other fixes across components.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE 2020-25660: Fix a regression in Messenger V2 replay attacks.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.27"><span class="name">Nautilus 14.2.13 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.27">#</a></h2></div><p>
  This release fixes a regression introduced in v14.2.12, and a few ceph-volume
  amd RGW fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a regression that caused breakage in clusters that referred to
    ceph-mon hosts using dns names instead of IP addresses in the
    <code class="option">mon_host</code> parameter in <code class="filename">ceph.conf</code>.
   </p></li><li class="listitem"><p>
    ceph-volume: the <code class="command">lvm batch</code> subcommand received a major
    rewrite.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.30"><span class="name">Nautilus 14.2.12 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.30">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="command">ceph df command</code> now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    MONs now have a config option <code class="option">mon_osd_warn_num_repaired</code>,
    10 by default. If any OSD has repaired more than this many I/O errors in
    stored data, a <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is
    generated. In order to allow clearing of the warning, a new command
    <code class="command">ceph tell osd.<em class="replaceable">SERVICE_ID</em>
    clear_shards_repaired <em class="replaceable">COUNT</em></code> has been
    added. By default, it will set the repair count to 0. If you want to be
    warned again if additional repairs are performed, you can provide a value
    to the command and specify the value of
    <code class="option">mon_osd_warn_num_repaired</code>. This command will be replaced
    in future releases by the health mute/unmute feature.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial MON to contact for Ceph tools
    and daemons using the <code class="option">mon_host_override config</code> option or
    <code class="option">--mon-host-override <em class="replaceable">IP</em></code>
    command-line switch. This generally should only be used for debugging and
    only affects initial communication with Ceph’s MON cluster.
   </p></li><li class="listitem"><p>
    Fix an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.33"><span class="name">Nautilus 14.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.33">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    RGW: The <code class="command">radosgw-admin</code> sub-commands dealing with orphans
    – <code class="command">radosgw-admin orphans find</code>, <code class="command">radosgw-admin
    orphans finish</code>, <code class="command">radosgw-admin orphans
    list-jobs</code> – have been deprecated. They have not been actively
    maintained and they store intermediate results on the cluster, which could
    fill a nearly-full cluster. They have been replaced by a tool, currently
    considered experimental, <code class="command">rgw-orphan-list</code>.
   </p></li><li class="listitem"><p>
    Now, when <code class="option">noscrub</code> and/or <code class="option">nodeep-scrub</code>
    flags are set globally or per pool, scheduled scrubs of the type disabled
    will be aborted. All user initiated scrubs are <span class="emphasis"><em>not</em></span>
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed a ceph-osd crash in committed OSD maps when there is a failure to
    encode the first incremental map.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.36"><span class="name">Nautilus 14.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.36">#</a></h2></div><p>
  This upstream release patched one security flaw:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li></ul></div><p>
  In addition to security flaws, this major upstream release brought a number
  of notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The pool parameter <code class="option">target_size_ratio</code>, used by the PG
    autoscaler, has changed meaning. It is now normalized across pools, rather
    than specifying an absolute ratio. If you have set target size ratios on
    any pools, you may want to set these pools to autoscale
    <code class="literal">warn</code> mode to avoid data movement during the upgrade:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_autoscale_mode warn</pre></div></li><li class="listitem"><p>
    The behaviour of the <code class="option">-o</code> argument to the RADOS tool has
    been reverted to its original behaviour of indicating an output file. This
    reverts it to a more consistent behaviour when compared to other tools.
    Specifying object size is now accomplished by using an upper case O
    <code class="option">-O</code>.
   </p></li><li class="listitem"><p>
    The format of MDSs in <code class="command">ceph fs dump</code> has changed.
   </p></li><li class="listitem"><p>
    Ceph will issue a health warning if a RADOS pool’s
    <code class="literal">size</code> is set to 1 or, in other words, the pool is
    configured with no redundancy. This can be fixed by setting the pool size
    to the minimum recommended value with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool-name</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The warning can be silenced with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    RGW: bucket listing performance on sharded bucket indexes has been notably
    improved by heuristically – and significantly, in many cases – reducing the
    number of entries requested from each bucket index shard.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.41"><span class="name">Nautilus 14.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.41">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS due to RGW GetObject header-splitting
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.389+gb0f23ac248.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.45"><span class="name">Nautilus 14.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.45">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The default value of <code class="option">bluestore_min_alloc_size_ssd</code> has been
    changed to 4K to improve performance across all workloads.
   </p></li><li class="listitem"><p>
    The following OSD memory config options related to BlueStore cache
    autotuning can now be configured during runtime:
   </p><div class="verbatim-wrap"><pre class="screen">osd_memory_base (default: 768 MB)
osd_memory_cache_min (default: 128 MB)
osd_memory_expected_fragmentation (default: 0.15)
osd_memory_target (default: 4 GB)</pre></div><p>
    You can set the above options by running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div></li><li class="listitem"><p>
    The Ceph Manager now accepts <code class="literal">profile rbd</code> and <code class="literal">profile
    rbd-read-only</code> user capabilities. You can use these capabilities
    to provide users access to MGR-based RBD functionality such as <code class="literal">rbd
    perf image iostat</code> and <code class="literal">rbd perf image iotop</code>.
   </p></li><li class="listitem"><p>
    The configuration value <code class="option">osd_calc_pg_upmaps_max_stddev</code> used
    for upmap balancing has been removed. Instead, use the Ceph Manager balancer
    configuration option <code class="option">upmap_max_deviation</code> which now is an
    integer number of PGs of deviation from the target PGs per OSD. You can set
    it with a following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/balancer/upmap_max_deviation 2</pre></div><p>
    The default <code class="option">upmap_max_deviation</code> is 5. There are situations
    where crush rules would not allow a pool to ever have completely balanced
    PGs. For example, if crush requires 1 replica on each of 3 racks, but there
    are fewer OSDs in 1 of the racks. In those cases, the configuration value
    can be increased.
   </p></li><li class="listitem"><p>
    CephFS: multiple active Metadata Server forward scrub is now rejected. Scrub is
    currently only permitted on a file system with a single rank. Reduce the
    ranks to one via <code class="command">ceph fs set <em class="replaceable">FS_NAME</em>
    max_mds 1</code>.
   </p></li><li class="listitem"><p>
    Ceph will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. This can be fixed
    by adjusting the pool to an adjacent power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">NEW_PG_NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.48"><span class="name">Nautilus 14.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.48">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1699: a path traversal flaw in Ceph Dashboard that could allow for
    potential information disclosure.
   </p></li><li class="listitem"><p>
    CVE-2020-1700: a flaw in the RGW beast front-end that could lead to denial
    of service from an unauthenticated client.
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.382+g8881d33957b.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.52"><span class="name">Nautilus 14.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.52">#</a></h2></div><p>
  This release fixed a Ceph Manager bug that caused MGRs becoming unresponsive on
  larger clusters. SES users were never exposed to the bug.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.54"><span class="name">Nautilus 14.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.54">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Health warnings are now issued if daemons have
    recently crashed.</strong></span> Ceph will now issue health warnings if
    daemons have recently crashed. Ceph has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash ls-new</pre></div><p>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive <em class="replaceable">CRASH-ID</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph crash archive-all</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong><code class="option">pg_num</code> must be a power of two,
    otherwise <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph
    will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Pool size needs to be greater than 1 otherwise
    <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph will issue a
    health warning if a RADOS pool’s size is set to 1 or if the pool is
    configured with no redundancy. Ceph will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> size <em class="replaceable">NUM-REPLICAS</em></pre></div><p>
    You can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</strong></span> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_ratio</code>,
    specifies a percentage of <code class="option">osd_heartbeat_grace</code> to determine
    the threshold. A value of zero disables the warning.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_time</code>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </p><p>
    A new command <code class="command">ceph daemon
    mgr.<em class="replaceable">MGR-NUMBER</em> dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </p><p>
    A new command <code class="command">ceph daemon osd.# dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Changes in the telemetry MGR module.</strong></span>
   </p><p>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <code class="literal">telemetry.ceph.com</code> in
    order to build and improve device failure prediction algorithms.
   </p><p>
    Telemetry reports information about CephFS file systems, including:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      How many MDS daemons (in total and per file system).
     </p></li><li class="listitem"><p>
      Which features are (or have been) enabled.
     </p></li><li class="listitem"><p>
      How many data pools.
     </p></li><li class="listitem"><p>
      Approximate file system age (year and the month of creation).
     </p></li><li class="listitem"><p>
      How many files, bytes, and snapshots.
     </p></li><li class="listitem"><p>
      How much metadata is being cached.
     </p></li></ul></div><p>
    Other miscellaneous information:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Which Ceph release the monitors are running.
     </p></li><li class="listitem"><p>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether RADOS cache tiering is enabled (and the mode).
     </p></li><li class="listitem"><p>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </p></li><li class="listitem"><p>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </p></li><li class="listitem"><p>
      Whether a separate OSD cluster network is being used.
     </p></li><li class="listitem"><p>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </p></li><li class="listitem"><p>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </p></li><li class="listitem"><p>
      Aggregate stats about the CRUSH Map, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </p></li></ul></div><p>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/channel_device false
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    You can view exactly what information will be reported first with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show        # see everything
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show device # just the device info
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show basic  # basic cluster info</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_recovery_reservations</code></strong></span>. It reveals the
    recovery locks held (<code class="option">in_progress</code>) and waiting in priority
    queues. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_recovery_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_scrub_reservations</code>. </strong></span> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_scrub_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports S3 Object Lock set of
    APIs.</strong></span> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports List Objects V2.</strong></span> RGW now
    supports List Objects V2 as specified at
    <a class="link" href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html</a>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.56"><span class="name">Nautilus 14.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.56">#</a></h2></div><p>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect SUSE Enterprise Storage customers
  because we did not ship a version based on 14.2.3.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.58"><span class="name">Nautilus 14.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.58">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a denial of service vulnerability where an unauthenticated client of
    Ceph Object Gateway could trigger a crash from an uncaught exception.
   </p></li><li class="listitem"><p>
    Nautilus-based librbd clients can now open images on Jewel clusters.
   </p></li><li class="listitem"><p>
    The Object Gateway <code class="option">num_rados_handles</code> has been removed. If you were
    using a value of <code class="option">num_rados_handles</code> greater than 1,
    multiply your current <code class="option">objecter_inflight_ops</code> and
    <code class="option">objecter_inflight_op_bytes</code> parameters by the old
    <code class="option">num_rados_handles</code> to get the same throttle behavior.
   </p></li><li class="listitem"><p>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </p></li><li class="listitem"><p>
    <code class="option">osd_deep_scrub_large_omap_object_key_threshold</code> has been
    lowered to detect an object with a large number of omap keys more easily.
   </p></li><li class="listitem"><p>
    The Ceph Dashboard now supports silencing Prometheus notifications.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.60"><span class="name">Nautilus 14.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.60">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="literal">no{up,down,in,out}</code> related commands have been
    revamped. There are now two ways to set the
    <code class="literal">no{up,down,in,out}</code> flags: the old command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set <em class="replaceable">FLAG</em></pre></div><p>
    which sets cluster-wide flags; and the new command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set-group <em class="replaceable">FLAGS</em> <em class="replaceable">WHO</em></pre></div><p>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </p></li><li class="listitem"><p>
    <code class="command">radosgw-admin</code> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of Object Gateway. Expire-stale objects are expired
    objects that should have been automatically erased but still exist and need
    to be listed and removed manually. One subcommand lists such objects and
    the other deletes them.
   </p></li><li class="listitem"><p>
    Earlier Nautilus releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new Nautilus BlueStore OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-Nautilus) breaks the pool
    utilization statistics reported by <code class="command">ceph df</code>. Until all
    OSDs have been reprovisioned or updated (via <code class="command">ceph-bluestore-tool
    repair</code>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <span class="emphasis"><em>all</em></span> OSDs are 14.2.2 or later, are Block Storage, and
    have been updated via the repair function if they were created prior to
    Nautilus.
   </p></li><li class="listitem"><p>
    The default value for <code class="option">mon_crush_min_required_version</code> has
    been changed from <code class="literal">firefly</code> to <code class="literal">hammer</code>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </p><p>
    If possible, we recommend that you set the oldest allowed client to
    <code class="literal">hammer</code> or later. To display what the current oldest
    allowed client is, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep min_compat_client</pre></div><p>
    If the current value is older than <code class="literal">hammer</code>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph features</pre></div><p>
    The newer <code class="literal">straw2</code> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <code class="literal">straw2</code> buckets to be used,
    including the <code class="literal">crush-compat</code> mode for the Balancer
    (<a class="xref" href="#mgr-modules-balancer" title="21.1. Balancer">Section 21.1, “Balancer”</a>).
   </p></li></ul></div><p>
  Find detailed information about the patch at
  <a class="link" href="https://download.suse.com/Download?buildid=D38A7mekBz4~" target="_blank">https://download.suse.com/Download?buildid=D38A7mekBz4~</a>
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.10.63"><span class="name">Nautilus 14.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.10.63">#</a></h2></div><p>
  This was the first point release following the original Nautilus release
  (14.2.0). The original ('General Availability' or 'GA') version of
  SUSE Enterprise Storage 6 was based on this point release.
 </p></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.3.11">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.3.11.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.3.11.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.2">#</a></span></dt><dd class="glossdef"><p>
     The node from which you run the <code class="command">ceph-deploy</code> utility to
     deploy Ceph on OSD nodes.
    </p></dd><dt id="id-1.3.11.3.3"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.3">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p><div id="id-1.3.11.3.3.2.2" data-id-title="Do Not Mix with S3 Buckets" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Mix with S3 Buckets</h6><p>
      S3 <span class="emphasis"><em>buckets</em></span> or <span class="emphasis"><em>containers</em></span>
      represent different terms meaning <span class="emphasis"><em>folders</em></span> for
      storing objects.
     </p></div></dd><dt id="id-1.3.11.3.4"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.4">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.3.11.3.5"><span><span class="glossterm">Monitor node, MON</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.5">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </p></dd><dt id="id-1.3.11.3.8"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.8">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.3.11.3.6"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.6">#</a></span></dt><dd class="glossdef"><p>
     Depending on context, <span class="emphasis"><em>Object Storage Device</em></span> or
     <span class="emphasis"><em>Object Storage Daemon</em></span>. The
     <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.3.11.3.7"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.7">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.3.11.3.9"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.9">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.3.11.3.10"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.10">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.3.11.3.12"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.12">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.3.11.3.11"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.3.11.3.11">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd></dl></div><div class="glossdiv" id="id-1.3.11.4" data-id-title="Ceph Specific Terms"><h3 class="title">Ceph Specific Terms</h3><dl><dt id="id-1.3.11.4.5"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.3.11.4.5">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies end user.
    </p></dd><dt id="id-1.3.11.4.2"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.11.4.2">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user’s data. Such a set
     consists of Ceph monitors and OSDs.
    </p><p>
     AKA <span class="quote">“<span class="quote">Ceph Object Store</span>”</span>.
    </p></dd><dt id="id-1.3.11.4.3"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.3.11.4.3">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.3.11.4.4"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.3.11.4.4">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd></dl></div><div class="glossdiv" id="id-1.3.11.5" data-id-title="Object Gateway Specific Terms"><h3 class="title">Object Gateway Specific Terms</h3><dl><dt id="id-1.3.11.5.3"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.3.11.5.3">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.3.11.5.2"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.3.11.5.2">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store.
    </p></dd></dl></div></section><section class="appendix" id="ap-adm-docupdate" data-id-title="Documentation Updates"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">C </span><span class="title-name">Documentation Updates</span> <a title="Permalink" class="permalink" href="#ap-adm-docupdate">#</a></h1></div></div></div><p>
  This chapter lists content changes for this document since the release of the
  latest maintenance update of SUSE Enterprise Storage 5. You can find changes that apply
  to previous versions in
  <a class="link" href="https://documentation.suse.com/ses/5.5/html/ses-all/ap-deploy-docupdate.html" target="_blank">https://documentation.suse.com/ses/5.5/html/ses-all/ap-deploy-docupdate.html</a>.
 </p><p>
  The document was updated on the following dates:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-6mu" title="C.1. Maintenance update of SUSE Enterprise Storage 6 documentation">Section C.1, “Maintenance update of SUSE Enterprise Storage 6 documentation”</a>
   </p></li><li class="listitem"><p>
    <a class="xref" href="#sec-adm-docupdates-6" title="C.2. June 2019 (Release of SUSE Enterprise Storage 6)">Section C.2, “June 2019 (Release of SUSE Enterprise Storage 6)”</a>
   </p></li></ul></div><section class="sect1" id="sec-adm-docupdates-6mu" data-id-title="Maintenance update of SUSE Enterprise Storage 6 documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">C.1 </span><span class="title-name">Maintenance update of SUSE Enterprise Storage 6 documentation</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-6mu">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Added a list of new features for Ceph 14.2.5 in the 'Ceph Maintenance
     Updates Based on Upstream 'Nautilus' Point Releases' appendix.
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#archive-sync-module" title="26.8.5. Archive Synchronization Module">Section 26.8.5, “Archive Synchronization Module”</a>
     (<a class="link" href="https://jira.suse.com/browse/SES-380" target="_blank">https://jira.suse.com/browse/SES-380</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#dash-rbd-mirror" title="9.8. RBD Mirroring">Section 9.8, “RBD Mirroring”</a>
     (<a class="link" href="https://jira.suse.com/browse/SES-235" target="_blank">https://jira.suse.com/browse/SES-235</a>).
    </p></li><li class="listitem"><p>
     Added <span class="intraxref">Book “Tuning Guide”, Chapter 8 “Improving Performance with LVM cache”</span>
     (<a class="link" href="https://jira.suse.com/browse/SES-269" target="_blank">https://jira.suse.com/browse/SES-269</a>).
    </p></li><li class="listitem"><p>
     Added <code class="filename">/etc/ceph</code> to the list of backup content in
     <a class="xref" href="#backup-ceph" title="3.1. Back Up Ceph Configuration">Section 3.1, “Back Up Ceph Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1153342" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1153342</a>).
    </p></li><li class="listitem"><p>
     Fixed targeting and re-weighting commands when adding new cluster nodes in
     <a class="xref" href="#salt-adding-nodes" title="2.1. Adding New Cluster Nodes">Section 2.1, “Adding New Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1151861" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1151861</a>).
    </p></li><li class="listitem"><p>
     Added changelog entry for Ceph 14.2.4
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1151881" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1151881</a>).
    </p></li><li class="listitem"><p>
     Before running stage 5, verify which OSDs are going to be possibly removed
     in <a class="xref" href="#storage-salt-cluster" title="Chapter 2. Salt Cluster Administration">Chapter 2, <em>Salt Cluster Administration</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1150406" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1150406</a>).
    </p></li><li class="listitem"><p>
     Added two additional steps to the procedure that describes recovering an
     OSD in <a class="xref" href="#ds-osd-recover" title="2.9. Recovering a Reinstalled OSD Node">Section 2.9, “Recovering a Reinstalled OSD Node”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1137132" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1137132</a>).
    </p></li><li class="listitem"><p>
     Add a new chapter about Ceph Manager modules, focusing on the balancer module in
     <a class="xref" href="#mgr-modules-balancer" title="21.1. Balancer">Section 21.1, “Balancer”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1133550" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1133550</a>).
    </p></li><li class="listitem"><p>
     Rewrote the section about applying rolling updates of the cluster software
     and replaced stage 0 with manual commands to prevent an infinite loop in
     <a class="xref" href="#deepsea-rolling-updates" title="2.12. Updating the Cluster Nodes">Section 2.12, “Updating the Cluster Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1134444" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1134444</a>).
    </p></li><li class="listitem"><p>
     Added a new section that describes how to replace the Admin Node:
     <a class="xref" href="#moving-saltmaster" title="2.10. Moving the Admin Node to a New Server">Section 2.10, “Moving the Admin Node to a New Server”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1145080" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1145080</a>).
    </p></li><li class="listitem"><p>
     Added a warning that enabling image journaling on all new images can have
     negative impact on cluster performance in
     <a class="xref" href="#rbd-mirror-imageconfig" title="23.4.3. Image Configuration">Section 23.4.3, “Image Configuration”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1134734" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1134734</a>).
    </p></li><li class="listitem"><p>
     Added check for <code class="systemitem">nscd</code> process and a
     tip to keep clocks synchronized from the Active Directory controller
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1144696" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1144696</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#deepsea-ceph-purge" title="2.17. Removing an Entire Ceph Cluster">Section 2.17, “Removing an Entire Ceph Cluster”</a> and tips about purging custom
     roles in <a class="xref" href="#dashboard-adding-roles" title="6.6. Adding Custom Roles">Section 6.6, “Adding Custom Roles”</a> and
     <a class="xref" href="#dashboard-permissions" title="14.2. User Roles and Permissions">Section 14.2, “User Roles and Permissions”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138846" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138846</a>).
    </p></li><li class="listitem"><p>
     Added a missing command in <a class="xref" href="#crush-placement-rules" title="20.1.1.3. CRUSH Placement Rules">Section 20.1.1.3, “CRUSH Placement Rules”</a> and
     added two new commands in <a class="xref" href="#crush-additional-commands" title="20.1.1.4. Additional Commands">Section 20.1.1.4, “Additional Commands”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1132696" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1132696</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#remove-all-osds-per-host" title="2.7.2. Removing All OSDs on a Host">Section 2.7.2, “Removing All OSDs on a Host”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1142746" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1142746</a>).
    </p></li><li class="listitem"><p>
     Updated the output of <code class="command">ceph df</code> command in
     <a class="xref" href="#monitor-stats" title="17.4. Checking a Cluster's Usage Stats">Section 17.4, “Checking a Cluster's Usage Stats”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1143551" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1143551</a>).
    </p></li><li class="listitem"><p>
     Added two new advanced settings in
     <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”, Section 10.4.5 “Advanced Settings”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1142341" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1142341</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#deactivate-tuned-profiles" title="2.16. Deactivating Tuned Profiles">Section 2.16, “Deactivating Tuned Profiles”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1130430" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1130430</a>).
    </p></li><li class="listitem"><p>
     Added a statement that the cache tier migration method works only from a
     replicated pool in <a class="xref" href="#pool-migrate-cache-tier" title="22.3.2. Migrate Using Cache Tier">Section 22.3.2, “Migrate Using Cache Tier”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1102242" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1102242</a>).
    </p></li><li class="listitem"><p>
     Updated <code class="filename">@/var/lib/ceph</code> sub-volume creation in
     <a class="xref" href="#storage-tips-ceph-btrfs-subvol" title="33.6. Btrfs Subvolume for /var/lib/ceph on Ceph Monitor Nodes">Section 33.6, “Btrfs Subvolume for <code class="filename">/var/lib/ceph</code> on Ceph Monitor Nodes”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138603" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138603</a>).
    </p></li></ul></div></section><section class="sect1" id="sec-adm-docupdates-6" data-id-title="June 2019 (Release of SUSE Enterprise Storage 6)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">C.2 </span><span class="title-name">June 2019 (Release of SUSE Enterprise Storage 6)</span> <a title="Permalink" class="permalink" href="#sec-adm-docupdates-6">#</a></h2></div></div></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">General Updates </span><a title="Permalink" class="permalink" href="#id-1.3.12.7.2">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#ogw-http-frontends" title="26.6. HTTP Front-ends">Section 26.6, “HTTP Front-ends”</a> and moved Object Gateway configuration
     options to <a class="xref" href="#config-ogw" title="25.3. Ceph Object Gateway">Section 25.3, “Ceph Object Gateway”</a>
     (<a class="link" href="https://jira.suse.com/browse/SES-453" target="_blank">https://jira.suse.com/browse/SES-453</a>).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#cha-ceph-configuration" title="Chapter 25. Ceph Cluster Configuration">Chapter 25, <em>Ceph Cluster Configuration</em></a> (jsc#SES-526).
    </p></li><li class="listitem"><p>
     Added more information on the volatile character of Ceph Dashboard's URL in
     <a class="xref" href="#part-dashboard" title="Part II. Ceph Dashboard">Part II, “Ceph Dashboard”</a> (jsc#SES-461).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#bp-flash-led-lights" title="33.11. How to Locate Physical Disks Using LED Lights">Section 33.11, “How to Locate Physical Disks Using LED Lights”</a> (jsc#SES-100).
    </p></li><li class="listitem"><p>
     Updated <a class="xref" href="#dash-webui-nfs" title="Chapter 10. Managing NFS Ganesha">Chapter 10, <em>Managing NFS Ganesha</em></a> (jsc#SES-533).
    </p></li><li class="listitem"><p>
     Added hints on using the default Ceph Dashboard user account called 'admin'
     in <a class="xref" href="#dashboard-username-password" title="13.3. User Name and Password">Section 13.3, “User Name and Password”</a> (jsc#SES-485).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-storage-classes" title="26.12. Pool Placement and Storage Classes">Section 26.12, “Pool Placement and Storage Classes”</a> (jsc#SES-326).
    </p></li><li class="listitem"><p>
     Added <code class="option">username</code> and <code class="option">password</code> parameters
     in <a class="xref" href="#ceph-rgw-sync-elastic-config" title="26.8.3.1. ElasticSearch Tier Type Configuration Parameters">Section 26.8.3.1, “ElasticSearch Tier Type Configuration Parameters”</a> (jsc#SES-130).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#cephfs-ad" title="29.2. Samba Gateway Joining Active Directory">Section 29.2, “Samba Gateway Joining Active Directory”</a> (jsc#SES-38).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ogw-sync-general-config" title="26.8.1. General Configuration">Section 26.8.1, “General Configuration”</a> and
     <a class="xref" href="#ogw-cloud-sync" title="26.8.4. Cloud Synchronization Module">Section 26.8.4, “Cloud Synchronization Module”</a> (jsc#SES-96).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#rbd-qos" title="23.6. QoS Settings">Section 23.6, “QoS Settings”</a> (Fate#324269).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#cephfs-snapshots" title="28.7. Managing CephFS Snapshots">Section 28.7, “Managing CephFS Snapshots”</a> (Fate#325488).
    </p></li><li class="listitem"><p>
     Removed/updated obsoleted <code class="command">ceph-disk</code> occurrences
     (Fate#324466).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#cephfs-quotas" title="28.6. Setting CephFS Quotas">Section 28.6, “Setting CephFS Quotas”</a> (Fate#323422).
    </p></li><li class="listitem"><p>
     Made Block Storage the default storage back-end (Fate#325658).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ceph-cluster-shutdown" title="16.3. Shutdown and Start of the Whole Ceph Cluster">Section 16.3, “Shutdown and Start of the Whole Ceph Cluster”</a> (Fate#323666).
    </p></li><li class="listitem"><p>
     Added <a class="xref" href="#ds-osd-replace" title="2.8. Replacing an OSD Disk">Section 2.8, “Replacing an OSD Disk”</a>.
    </p></li><li class="listitem"><p>
     Removed all references to external online documentation, replaced with the
     relevant content (Fate#320121).
    </p></li></ul></div><div class="itemizedlist"><div class="itemizedlist-title-wrap"><h6 class="itemizedlist-title"><span class="title-name">Bugfixes </span><a title="Permalink" class="permalink" href="#id-1.3.12.7.3">#</a></h6></div><ul class="itemizedlist"><li class="listitem"><p>
     Added <a class="xref" href="#rbd-old-clients-map" title="23.9. Mapping RBD Using Old Kernel Clients">Section 23.9, “Mapping RBD Using Old Kernel Clients”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1134992" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1134992</a>).
    </p></li><li class="listitem"><p>
     Updated Object Gateway sections to use Beast instead of CivetWeb
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1138191" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1138191</a>).
    </p></li><li class="listitem"><p>
     Removed RBD image format 1 mentions + aligned pool/volume name in
     <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Installation of iSCSI Gateway”, Section 10.4.3 “Export RBD Images via iSCSI”</span>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135175" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135175</a>).
    </p></li><li class="listitem"><p>
     QoS features not supported by iSCSI in <a class="xref" href="#rbd-qos" title="23.6. QoS Settings">Section 23.6, “QoS Settings”</a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1135063" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1135063</a>).
    </p></li><li class="listitem"><p>
     Added performance notes to <a class="xref" href="#cha-ses-cifs" title="Chapter 29. Exporting Ceph Data via Samba">Chapter 29, <em>Exporting Ceph Data via Samba</em></a> and
     <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 30. NFS Ganesha: Export Ceph Data via NFS">Chapter 30, <em>NFS Ganesha: Export Ceph Data via NFS</em></a>
     (<a class="link" href="https://bugzilla.suse.com/show_bug.cgi?id=1124674" target="_blank">https://bugzilla.suse.com/show_bug.cgi?id=1124674</a>).
    </p></li></ul></div></section></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/book_storage_admin.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>