<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Tuning Guide | SUSE Enterprise Storage 6</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Tuning Guide | SES 6"/>
<meta name="description" content=""/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="6"/>
<meta name="book-title" content="Tuning Guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new/choose"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="asettle"/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Tuning Guide | SES 6"/>
<meta property="og:description" content=""/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Tuning Guide | SES 6"/>
<meta name="twitter:description" content=""/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-tuning">Tuning Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-tuning" data-id-title="Tuning Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">6</span></div><div><h1 class="title">Tuning Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">David</span> <span class="surname">Byte</span>, <span class="firstname">Lars</span> <span class="surname">Marowsky-Bree</span>, <span class="firstname">Igor</span> <span class="surname">Fedotov</span>, <span class="firstname">Jan</span> <span class="surname">Fajerski</span>, <span class="firstname">Abhishek</span> <span class="surname">Lekshmanan</span>, and <span class="firstname">Alexandra</span> <span class="surname">Settle</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>06/27/2022</div></div></div><div class="toc"><ul><li><span class="part"><a href="#part-intro"><span class="title-number">I </span><span class="title-name">Introduction</span></a></span><ul><li><span class="chapter"><a href="#id-1.5.2.2"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></span><ul><li><span class="sect1"><a href="#id-1.5.2.2.4"><span class="title-number">1.1 </span><span class="title-name">Salt/DeepSea Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.5.2.2.5"><span class="title-number">1.2 </span><span class="title-name">Ceph Related Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.5.2.2.6"><span class="title-number">1.3 </span><span class="title-name">General Linux Commands</span></a></span></li><li><span class="sect1"><a href="#id-1.5.2.2.7"><span class="title-number">1.4 </span><span class="title-name">Additional Information</span></a></span></li></ul></li><li><span class="chapter"><a href="#tuning-how"><span class="title-number">2 </span><span class="title-name">General Notes on System Tuning</span></a></span><ul><li><span class="sect1"><a href="#sec-tuning-basics-what"><span class="title-number">2.1 </span><span class="title-name">Understand The Problem You are Solving</span></a></span></li><li><span class="sect1"><a href="#tuning-basics-common"><span class="title-number">2.2 </span><span class="title-name">Rule Out Common Problems</span></a></span></li><li><span class="sect1"><a href="#sec-tuning-basics-bottleneck"><span class="title-number">2.3 </span><span class="title-name">Finding the Bottleneck</span></a></span></li><li><span class="sect1"><a href="#sec-tuning-basics-tuning"><span class="title-number">2.4 </span><span class="title-name">Step-by-step Tuning</span></a></span></li></ul></li><li><span class="chapter"><a href="#tuning-intro"><span class="title-number">3 </span><span class="title-name">Introduction to Tuning SUSE Enterprise Storage Clusters</span></a></span><ul><li><span class="sect1"><a href="#tuning-philosophy"><span class="title-number">3.1 </span><span class="title-name">Philosophy of Tuning</span></a></span></li><li><span class="sect1"><a href="#tuning-process"><span class="title-number">3.2 </span><span class="title-name">The Process</span></a></span></li><li><span class="sect1"><a href="#tuning-hardware-software"><span class="title-number">3.3 </span><span class="title-name">Hardware and Software</span></a></span></li><li><span class="sect1"><a href="#tuning-measurement"><span class="title-number">3.4 </span><span class="title-name">Determining What to Measure</span></a></span></li><li><span class="sect1"><a href="#tuning-testing-tools-protocol"><span class="title-number">3.5 </span><span class="title-name">Testing Tools and Protocol</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-tuning-guide"><span class="title-number">II </span><span class="title-name">SUSE Enterprise Storage Tuning</span></a></span><ul><li><span class="chapter"><a href="#tuning-architecture"><span class="title-number">4 </span><span class="title-name">Architecture and Hardware Tuning</span></a></span><ul><li><span class="sect1"><a href="#tuning-network"><span class="title-number">4.1 </span><span class="title-name">Network</span></a></span></li><li><span class="sect1"><a href="#tuning-node-hw-recommendations"><span class="title-number">4.2 </span><span class="title-name">Node Hardware Recommendations</span></a></span></li><li><span class="sect1"><a href="#tuning-cpeh-rocksdb-wal"><span class="title-number">4.3 </span><span class="title-name">Ceph</span></a></span></li></ul></li><li><span class="chapter"><a href="#tuning-os"><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span></a></span><ul><li><span class="sect1"><a href="#tuning-sles"><span class="title-number">5.1 </span><span class="title-name">SUSE Linux Enterprise Install and Validation of Base Performance</span></a></span></li><li><span class="sect1"><a href="#tuning-kernel-tuning"><span class="title-number">5.2 </span><span class="title-name">Kernel Tuning</span></a></span></li></ul></li><li><span class="chapter"><a href="#tuning-ceph"><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span></a></span><ul><li><span class="sect1"><a href="#tuning-obtaining-metrics"><span class="title-number">6.1 </span><span class="title-name">Obtaining Ceph Metrics</span></a></span></li><li><span class="sect1"><a href="#tuning-tuning-persistent"><span class="title-number">6.2 </span><span class="title-name">Making Tuning Persistent</span></a></span></li><li><span class="sect1"><a href="#tuning-core"><span class="title-number">6.3 </span><span class="title-name">Core</span></a></span></li><li><span class="sect1"><a href="#tuning-rbd"><span class="title-number">6.4 </span><span class="title-name">RBD</span></a></span></li><li><span class="sect1"><a href="#tuning-cephfs"><span class="title-number">6.5 </span><span class="title-name">CephFS</span></a></span></li><li><span class="sect1"><a href="#tuning-rgw"><span class="title-number">6.6 </span><span class="title-name">RGW</span></a></span></li><li><span class="sect1"><a href="#tuning-admin-usage"><span class="title-number">6.7 </span><span class="title-name">Administrative and Usage Choices</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-tiered"><span class="title-number">7 </span><span class="title-name">Cache Tiering</span></a></span><ul><li><span class="sect1"><a href="#id-1.5.3.5.5"><span class="title-number">7.1 </span><span class="title-name">Tiered Storage Terminology</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-tiered-caution"><span class="title-number">7.2 </span><span class="title-name">Points to Consider</span></a></span></li><li><span class="sect1"><a href="#id-1.5.3.5.7"><span class="title-number">7.3 </span><span class="title-name">When to Use Cache Tiering</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-tiered-cachemodes"><span class="title-number">7.4 </span><span class="title-name">Cache Modes</span></a></span></li><li><span class="sect1"><a href="#ceph-tier-erasure"><span class="title-number">7.5 </span><span class="title-name">Erasure Coded Pool and Cache Tiering</span></a></span></li><li><span class="sect1"><a href="#ses-tiered-storage"><span class="title-number">7.6 </span><span class="title-name">Setting Up an Example Tiered Storage</span></a></span></li><li><span class="sect1"><a href="#cache-tier-configure"><span class="title-number">7.7 </span><span class="title-name">Configuring a Cache Tier</span></a></span></li></ul></li><li><span class="chapter"><a href="#lvmcache"><span class="title-number">8 </span><span class="title-name">Improving Performance with LVM cache</span></a></span><ul><li><span class="sect1"><a href="#lvmcache-prerequisites"><span class="title-number">8.1 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="sect1"><a href="#lvmcache-planning"><span class="title-number">8.2 </span><span class="title-name">Points to Consider</span></a></span></li><li><span class="sect1"><a href="#lvmcache-preparation"><span class="title-number">8.3 </span><span class="title-name">Preparation</span></a></span></li><li><span class="sect1"><a href="#lvmcache-configuring"><span class="title-number">8.4 </span><span class="title-name">Configuring LVM cache</span></a></span></li><li><span class="sect1"><a href="#lvmcache-failures"><span class="title-number">8.5 </span><span class="title-name">Handling Failures</span></a></span></li><li><span class="sect1"><a href="#lvmcache-faq"><span class="title-number">8.6 </span><span class="title-name">Frequently Asked Questions</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#tuning-appendix-a"><span class="title-number">A </span><span class="title-name">Salt State for Kernel Tuning</span></a></span></li><li><span class="appendix"><a href="#tuning-appendix-b"><span class="title-number">B </span><span class="title-name">Ring Buffer Max Value Script</span></a></span></li><li><span class="appendix"><a href="#tuning-appendix-c"><span class="title-number">C </span><span class="title-name">Network Tuning</span></a></span></li><li><span class="appendix"><a href="#id-1.5.7"><span class="title-number">D </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span></a></span></li><li><span class="glossary"><a href="#id-1.5.8"><span class="title-name">Glossary</span></a></span></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#yast-config-module"><span class="number">5.1 </span><span class="name">YaST Configuration Module</span></a></span></li><li><span class="figure"><a href="#ses-tiered-hitset-overview-bloom"><span class="number">7.1 </span><span class="name">Bloom Filter with 3 Stored Objects</span></a></span></li></ul></div><div><div class="legalnotice" id="id-1.5.1.6"><p>
  Copyright ©
2022

  SUSE LLC
 </p><p>
  Copyright © 2016, RedHat, Inc, and contributors.

 </p><p>
  The text of and illustrations in this document are licensed under a Creative
  Commons Attribution-Share Alike 4.0 International ("CC-BY-SA"). An
  explanation of CC-BY-SA is available at
  <a class="link" href="http://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">http://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
  In accordance with CC-BY-SA, if you distribute this document or an adaptation
  of it, you must provide the URL for the original version.
 </p><p>
  Red Hat, Red Hat Enterprise Linux, the Shadowman logo, JBoss, MetaMatrix,
  Fedora, the Infinity Logo, and RHCE are trademarks of Red Hat, Inc.,
  registered in the United States and other countries. Linux® is the
  registered trademark of Linus Torvalds in the United States and other
  countries. Java® is a registered trademark of Oracle and/or its
  affiliates. XFS® is a trademark of Silicon Graphics International Corp.
  or its subsidiaries in the United States and/or other countries. All other
  trademarks are the property of their respective owners.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All other
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither SUSE LLC,
  its affiliates, the authors nor the translators shall be held liable for
  possible errors or the consequences thereof.
 </p></div></div><div class="part" id="part-intro" data-id-title="Introduction"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Introduction </span><a title="Permalink" class="permalink" href="#part-intro">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#id-1.5.2.2"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span></a></span></li><dd class="toc-abstract"><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p></dd><li><span class="chapter"><a href="#tuning-how"><span class="title-number">2 </span><span class="title-name">General Notes on System Tuning</span></a></span></li><dd class="toc-abstract"><p>This manual discusses how to find the reasons for performance problems and provides means to solve these problems. Before you start tuning your system, you should make sure you have ruled out common problems and have found the cause for the problem. You should also have a detailed plan on how to tun…</p></dd><li><span class="chapter"><a href="#tuning-intro"><span class="title-number">3 </span><span class="title-name">Introduction to Tuning SUSE Enterprise Storage Clusters</span></a></span></li><dd class="toc-abstract"><p>Tuning a distributed cluster is a foray into the use of the scientific method, backed with iterative testing. By taking a holistic look at the cluster and then delving into all the components, it is possible to achieve dramatic improvements. Over the course of the work that contributed to the author…</p></dd></ul></div><section class="chapter" id="id-1.5.2.2" data-id-title="User Privileges and Command Prompts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">User Privileges and Command Prompts</span> <a title="Permalink" class="permalink" href="#id-1.5.2.2">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect1" id="id-1.5.2.2.4" data-id-title="Salt/DeepSea Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Salt/DeepSea Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.5.2.2.4">#</a></h2></div></div></div><p>
   These commands help you to deploy or upgrade the Ceph cluster, run
   commands on several (or all) cluster nodes at the same time, or assist you
   when adding or removing cluster nodes. The most frequently used are
   <code class="command">salt</code>, <code class="command">salt-run</code>, and
   <code class="command">deepsea</code>. You need to run Salt commands on the Salt master
   node (refer to <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Deploying with DeepSea/Salt”, Section 5.2 “Introduction to DeepSea”</span> for details) as
   <code class="systemitem">root</code>. These commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*.example.net' test.ping</pre></div></section><section class="sect1" id="id-1.5.2.2.5" data-id-title="Ceph Related Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Ceph Related Commands</span> <a title="Permalink" class="permalink" href="#id-1.5.2.2.5">#</a></h2></div></div></div><p>
   These are lower level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">rbd</code>,
   <code class="command">radosgw-admin</code>, or <code class="command">crushtool</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   Safer and recommended option is to create a more restrictive individual key
   for each administrator user and put it in a directory where the users can
   read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.5.2.2.5.6" data-id-title="Path to Ceph Keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph Keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephadm</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph auth list</pre></div><div id="id-1.5.2.2.5.11" data-id-title="Commands for Specific Nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for Specific Nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@mon &gt; </code></pre></div></div></section><section class="sect1" id="id-1.5.2.2.6" data-id-title="General Linux Commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">General Linux Commands</span> <a title="Permalink" class="permalink" href="#id-1.5.2.2.6">#</a></h2></div></div></div><p>
   Linux commands not related to Ceph or DeepSea, such as
   <code class="command">mount</code>, <code class="command">cat</code>, or
   <code class="command">openssl</code>, are introduced either with the <code class="prompt user">cephadm@adm &gt; </code>
   or <code class="prompt user">root # </code> prompts, depending on which privileges the related command
   requires.
  </p></section><section class="sect1" id="id-1.5.2.2.7" data-id-title="Additional Information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Additional Information</span> <a title="Permalink" class="permalink" href="#id-1.5.2.2.7">#</a></h2></div></div></div><p>
   For more information on Ceph key management, refer to
   <span class="intraxref">Book “Administration Guide”, Chapter 19 “Authentication with <code class="systemitem">cephx</code>”, Section 19.2 “Key Management”</span>.
  </p></section></section><section class="chapter" id="tuning-how" data-id-title="General Notes on System Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">General Notes on System Tuning</span> <a title="Permalink" class="permalink" href="#tuning-how">#</a></h2></div></div></div><p>
  This manual discusses how to find the reasons for performance problems and
  provides means to solve these problems. Before you start tuning your system,
  you should make sure you have ruled out common problems and have found the
  cause for the problem. You should also have a detailed plan on how to tune
  the system, because applying random tuning tips often will not help and could
  make things worse.
 </p><div class="procedure" id="id-1.5.2.3.4" data-id-title="General Approach When Tuning a System"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 2.1: </span><span class="title-name">General Approach When Tuning a System </span><a title="Permalink" class="permalink" href="#id-1.5.2.3.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Specify the problem that needs to be solved.
   </p></li><li class="step"><p>
    In case the degradation is new, identify any recent changes to the system.
   </p></li><li class="step" id="tuning-basics-whyproblem"><p>
    Identify why the issue is considered a performance problem.
   </p></li><li class="step"><p>
    Specify a metric that can be used to analyze performance. This metric could
    for example be latency, throughput, the maximum number of users that are
    simultaneously logged in, or the maximum number of active users.
   </p></li><li class="step"><p>
    Measure current performance using the metric from the previous step.
   </p></li><li class="step"><p>
    Identify the subsystem(s) where the application is spending the most time.
   </p></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
      Monitor the system and/or the application.
     </p></li><li class="step"><p>
      Analyze the data and categorize where time is being spent.
     </p></li></ol></li><li class="step"><p>
    Tune the subsystem identified in the previous step.
   </p></li><li class="step"><p>
    Remeasure the current performance without monitoring using the same metric
    as before.
   </p></li><li class="step"><p>
    If performance is still not acceptable, start over with
    <a class="xref" href="#tuning-basics-whyproblem" title="Step 3">Step 3</a>.
   </p></li></ol></div></div><section class="sect1" id="sec-tuning-basics-what" data-id-title="Understand The Problem You are Solving"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Understand The Problem You are Solving</span> <a title="Permalink" class="permalink" href="#sec-tuning-basics-what">#</a></h2></div></div></div><p>
   Before starting to tuning a system, try to describe the problem as exactly
   as possible. A statement like <span class="quote">“<span class="quote">The system is slow!</span>”</span> is not a
   helpful problem description. For example, it could make a difference whether
   the system speed needs to be improved in general or only at peak times.
  </p><p>
   Furthermore, make sure you can apply a measurement to your problem,
   otherwise you cannot verify if the tuning was a success or not. You should
   always be able to compare <span class="quote">“<span class="quote">before</span>”</span> and <span class="quote">“<span class="quote">after</span>”</span>.
   Which metrics to use depends on the scenario or application you are looking
   into. Relevant Web server metrics, for example, could be expressed in terms
   of:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.2.3.5.4.1"><span class="term">Latency</span></dt><dd><p>
      The time to deliver a page.
     </p></dd><dt id="id-1.5.2.3.5.4.2"><span class="term">Throughput</span></dt><dd><p>
      Number of pages served per second or megabytes transferred per second.
     </p></dd><dt id="id-1.5.2.3.5.4.3"><span class="term">Active Users</span></dt><dd><p>
      The maximum number of users that can be downloading pages while still
      receiving pages within an acceptable latency.
     </p></dd></dl></div></section><section class="sect1" id="tuning-basics-common" data-id-title="Rule Out Common Problems"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Rule Out Common Problems</span> <a title="Permalink" class="permalink" href="#tuning-basics-common">#</a></h2></div></div></div><p>
   A performance problem often is caused by network or hardware problems, bugs,
   or configuration issues. Make sure to rule out problems such as the ones
   listed below before attempting to tune your system:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Check the output of the <code class="systemitem">systemd</code> journal for unusual entries.
    </p></li><li class="listitem"><p>
     Check (using <code class="command">top</code> or <code class="command">ps</code>) whether a
     certain process misbehaves by eating up unusual amounts of CPU time or
     memory.
    </p></li><li class="listitem"><p>
     Check for network problems by inspecting
     <code class="filename">/proc/net/dev</code>.
    </p></li><li class="listitem"><p>
     In case of I/O problems with physical disks, make sure they are not caused
     by hardware problems (check the disk with the
     <code class="systemitem">smartmontools</code>) or by a full disk.
    </p></li><li class="listitem"><p>
     Ensure that background jobs are scheduled to be carried out in times the
     server load is low. Those jobs should also run with low priority (set via
     <code class="command">nice</code>).
    </p></li><li class="listitem"><p>
     If the machine runs several services using the same resources, consider
     moving services to another server.
    </p></li><li class="listitem"><p>
     Last, make sure your software is up-to-date.
    </p></li></ul></div></section><section class="sect1" id="sec-tuning-basics-bottleneck" data-id-title="Finding the Bottleneck"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Finding the Bottleneck</span> <a title="Permalink" class="permalink" href="#sec-tuning-basics-bottleneck">#</a></h2></div></div></div><p>
   Finding the bottleneck very often is the hardest part when tuning a system.
   SUSE Enterprise Storage offers many tools to help you with this task. If the problem
   requires a long-time in-depth analysis, the Linux kernel offers means to
   perform such analysis.
  </p><p>
   Once you have collected the data, it needs to be analyzed. First, inspect if
   the server's hardware (memory, CPU, bus) and its I/O capacities (disk,
   network) are sufficient. If these basic conditions are met, the system might
   benefit from tuning.
  </p></section><section class="sect1" id="sec-tuning-basics-tuning" data-id-title="Step-by-step Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Step-by-step Tuning</span> <a title="Permalink" class="permalink" href="#sec-tuning-basics-tuning">#</a></h2></div></div></div><p>
   Make sure to carefully plan the tuning itself. It is of vital importance to
   only do one step at a time. Only by doing so can you measure whether the
   change made an improvement or even had a negative impact. Each tuning
   activity should be measured over a sufficient time period to ensure you can
   do an analysis based on significant data. If you cannot measure a positive
   effect, do not make the change permanent: it might have a negative effect in
   the future.
  </p></section></section><section class="chapter" id="tuning-intro" data-id-title="Introduction to Tuning SUSE Enterprise Storage Clusters"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Introduction to Tuning SUSE Enterprise Storage Clusters</span> <a title="Permalink" class="permalink" href="#tuning-intro">#</a></h2></div></div></div><p>
  Tuning a distributed cluster is a foray into the use of the scientific
  method, backed with iterative testing. By taking a holistic look at the
  cluster and then delving into all the components, it is possible to achieve
  dramatic improvements. Over the course of the work that contributed to the
  authoring of this guide, the authors have seen performance more than double
  in some specific cases.
 </p><p>
  This guide is intended to assist the reader in understanding the what and how
  of tuning a SUSE Enterprise Storage cluster. There are topics that are beyond the scope
  of this guide, and it is expected that there are further tweaks that may be
  performed to an individual cluster in order to achieve optimum performance in
  a particular end user environment.
 </p><p>
  This reference guide is targeted at architects and administrators who need to
  tune their SUSE Enterprise Storage cluster for optimal performance. Familiarity with
  Linux and Ceph are assumed.
 </p><section class="sect1" id="tuning-philosophy" data-id-title="Philosophy of Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Philosophy of Tuning</span> <a title="Permalink" class="permalink" href="#tuning-philosophy">#</a></h2></div></div></div><p>
   Tuning requires looking at the entire system being tuned and approaching the
   process with scientific rigor. An exhaustive approach requires taking an
   initial baseline and altering a single variable at a time, measuring the
   result, and then reverting it back to default while moving on to the next
   tuning parameter. At the end of that process, it is then possible to examine
   the results, whether it be increased throughput, reduced latency, reduced
   CPU consumption, and then decide which are likely candidates for combining
   for additive benefit. This second phase should also be iterated through in
   the same fashion as the first. This general process would be continued until
   all possible combinations were tried and the optimal settings discovered.
  </p><p>
   Unfortunately, few have the time to perform such an exhaustive effort. This
   being reality, it is possible to utilize some knowledge and begin an
   iterative process of combining and measuring well-known candidates for
   performance improvements and measuring the resulting changes. That is the
   process followed during the research that produced this work.
  </p></section><section class="sect1" id="tuning-process" data-id-title="The Process"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">The Process</span> <a title="Permalink" class="permalink" href="#tuning-process">#</a></h2></div></div></div><p>
   A proper process is required for effective tuning to occur. The tenets of
   this process are:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.2.4.7.3.1"><span class="term">Measure</span></dt><dd><p>
      Start with a baseline and measure the same way after each iteration. Make
      sure you are measuring all relevant dimensions. Discovering that you are
      CPU-bound only after working through multiple iterations invalidates all
      the time spent on the iterations.
     </p></dd><dt id="id-1.5.2.4.7.3.2"><span class="term">Document</span></dt><dd><p>
      Document all results for future analysis. Patterns may not be evident
      until later review.
     </p></dd><dt id="id-1.5.2.4.7.3.3"><span class="term">Discuss</span></dt><dd><p>
      When possible, discuss the results you are seeing with others for their
      insights and feedback.
     </p></dd><dt id="id-1.5.2.4.7.3.4"><span class="term">Repeat</span></dt><dd><p>
      A single measurement is not a guarantee of repeatability. Performing the
      same test multiple times helps to better establish an outcome.
     </p></dd><dt id="id-1.5.2.4.7.3.5"><span class="term">Isolate variables</span></dt><dd><p>
      Having only a single change affect the environment at a time may cause
      the process to be longer, but it also helps validate the particular
      adjustment being tested.
     </p></dd></dl></div></section><section class="sect1" id="tuning-hardware-software" data-id-title="Hardware and Software"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.3 </span><span class="title-name">Hardware and Software</span> <a title="Permalink" class="permalink" href="#tuning-hardware-software">#</a></h2></div></div></div><p>
   This work leveraged for SUSE Enterprise Storage was performed on two models of
   servers. Any results referenced in this guide are from this specific
   hardware environment. Variations of the environment can and will have an
   effect on the results.
  </p><p>
   Storage nodes:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     2U Server
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       1x Intel Skylake 6124
      </p></li><li class="listitem"><p>
       96 GB RAM
      </p></li><li class="listitem"><p>
       Mellanox Dual Port ConnectX-4 100 GbE
      </p></li><li class="listitem"><p>
       12x Intel SSD D3-S4510 960 GB
      </p></li><li class="listitem"><p>
       RAID-1 480 GB M.2 Boot Device
      </p></li></ul></div></li></ul></div><p>
   Admin, monitor, and protocol gateways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     1U Server
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       1x Intel Skylake 4112
      </p></li><li class="listitem"><p>
       32 GB RAM
      </p></li><li class="listitem"><p>
       Mellanox Dual Port ConnectX-4 100 GbE
      </p></li><li class="listitem"><p>
       RAID-1 480 GB M.2 Boot Device
      </p></li></ul></div></li></ul></div><p>
   Switches:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     2x 32-port 100 GbE
    </p></li></ul></div><p>
   Software:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     SUSE Enterprise Storage 5.5
    </p></li><li class="listitem"><p>
     SUSE Linux Enterprise Server 15 SP1
    </p></li></ul></div><div id="id-1.5.2.4.8.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Limited-use subscriptions are provided with SUSE Enterprise Storage as part of the
    subscription entitlement.
   </p></div><section class="sect2" id="id-1.5.2.4.8.12" data-id-title="Performance Metrics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.3.1 </span><span class="title-name">Performance Metrics</span> <a title="Permalink" class="permalink" href="#id-1.5.2.4.8.12">#</a></h3></div></div></div><p>
    The performance of storage is measured on two different, but related axes:
    <code class="literal">latency</code> and <code class="literal">throughput</code>. In some
    cases, one is more important than the other. An example of throughput being
    more important is that of backup use cases, where throughput is the most
    critical measurement and maximum performance is achieved with larger
    transfer sizes. Conversely, for a high-performance database, latency is the
    most important measurement.
   </p><p>
    <span class="emphasis"><em>Latency</em></span> is the time from when the request was made to
    when it was completed. This is usually measured in terms of milliseconds.
    This performance can be directly tied to CPU clock speed, the system bus,
    and device performance. <span class="emphasis"><em>Throughput</em></span> is the measure of
    an amount of data that can be written or retrieved within a particular time
    period. The measure is usually in MB/s and GB/s, or perhaps MiB/s and
    GiB/s.
   </p><p>
    A third measurement that is often referred to is <code class="literal">IOPS</code>.
    This stands for Input/output Operations Per Second. This measure is
    somewhat ambiguous as the result is reliant on the size of the I/O
    operation, what type (read/write), and details about the I/O pattern: fresh
    write, overwrite, random write, mix of reads and writes. While ambiguous,
    it is still a valid tool to use when measuring the changes in the
    performance of your storage environment. For example, it is possible to
    make adjustments that may not affect the latency of a single 4K sequential
    write operation, but would allow many more of the operations to happen in
    parallel, resulting in a change in throughput and IOPS.
   </p></section></section><section class="sect1" id="tuning-measurement" data-id-title="Determining What to Measure"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.4 </span><span class="title-name">Determining What to Measure</span> <a title="Permalink" class="permalink" href="#tuning-measurement">#</a></h2></div></div></div><p>
   When tuning an environment, it is important to understand the I/O that is
   being tuned for. By properly understanding the I/O pattern, it is then
   possible to match the tests to the environment, resulting in a close
   simulation of the environment.
  </p><p>
   Tools that can be useful for understanding the I/O patterns are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">iostat</code>
    </p></li><li class="listitem"><p>
     <code class="command">blktrace</code>, <code class="command">blkparse</code>
    </p></li><li class="listitem"><p>
     <code class="command">systemtap (stap)</code>
    </p></li><li class="listitem"><p>
     <code class="command">dtrace</code>
    </p></li></ul></div><p>
   While discussing these tools is beyond the scope of this document, the
   information they can provide may be very helpful in understanding the I/O
   profile that needs to be tuned.
  </p><section class="sect2" id="id-1.5.2.4.9.6" data-id-title="Single Thread vs Aggregate IO"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">3.4.1 </span><span class="title-name">Single Thread vs Aggregate IO</span> <a title="Permalink" class="permalink" href="#id-1.5.2.4.9.6">#</a></h3></div></div></div><p>
    Understanding the requirements of the workload in relation to whether it
    needs scale-up or scale-out performance is often a key to proper tuning as
    well, particularly when it comes to tuning the hardware architecture and to
    creating test scenarios that provide valid information regarding tuning for
    the application(s) in question.
   </p></section></section><section class="sect1" id="tuning-testing-tools-protocol" data-id-title="Testing Tools and Protocol"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.5 </span><span class="title-name">Testing Tools and Protocol</span> <a title="Permalink" class="permalink" href="#tuning-testing-tools-protocol">#</a></h2></div></div></div><p>
   Proper testing involves selection of the right tools. For most performance
   test cases, use of <code class="command">fio</code> is recommended, as it provides a
   vast array of options for constructing test cases. For some use cases, such
   as S3, it may be necessary to use alternative tools to test all phases of
   I/O.
  </p><p>
   Tools that are commonly used to simulate I/O are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">fio</code>
    </p></li><li class="listitem"><p>
     <code class="command">iometer</code>
    </p></li></ul></div><p>
   When performing testing, it is imperative that sound practices be utilized.
   This involves:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Pre-conditioning the media.
    </p></li><li class="listitem"><p>
     Ensuring what you are measuring is what you intend to measure.
    </p></li><li class="listitem"><p>
     Validate that the results you see make sense.
    </p></li><li class="listitem"><p>
     Test each component individually and then in aggregate, one layer at a
     time.
    </p></li></ul></div></section></section></div><div class="part" id="part-tuning-guide" data-id-title="SUSE Enterprise Storage Tuning"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">SUSE Enterprise Storage Tuning </span><a title="Permalink" class="permalink" href="#part-tuning-guide">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#tuning-architecture"><span class="title-number">4 </span><span class="title-name">Architecture and Hardware Tuning</span></a></span></li><dd class="toc-abstract"><p>
  Architectural tuning includes aspects that range from the low-level design of
  the systems being deployed, up to macro-level decisions about network
  topology and cooling. Not all of these can be covered in this work, but
  guidance will be given where possible.
 </p></dd><li><span class="chapter"><a href="#tuning-os"><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span></a></span></li><dd class="toc-abstract"><p>A significant amount of the performance tuning for Ceph clusters can be done at the operating system (OS) layer. This tuning involves ensuring that unnecessary services are not running, and extends down to ensuring buffers are not being overrun and interrupts are being spread properly. There are man…</p></dd><li><span class="chapter"><a href="#tuning-ceph"><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span></a></span></li><dd class="toc-abstract"><p>Ceph includes a telemetry module that provides anonymized information back to the Ceph developer community. The information contained in the telemetry report provides information that helps the developers prioritize efforts and identify areas where more work may be needed. It may be necessary to ena…</p></dd><li><span class="chapter"><a href="#cha-ceph-tiered"><span class="title-number">7 </span><span class="title-name">Cache Tiering</span></a></span></li><dd class="toc-abstract"><p>
  A <span class="emphasis"><em>cache tier</em></span> is an additional storage layer implemented
  between the client and the standard storage. It is designed to speed up
  access to pools stored on slow hard disks and erasure coded pools.
 </p></dd><li><span class="chapter"><a href="#lvmcache"><span class="title-number">8 </span><span class="title-name">Improving Performance with LVM cache</span></a></span></li><dd class="toc-abstract"><p>
   LVM cache is currently a technology preview.
  </p></dd></ul></div><section class="chapter" id="tuning-architecture" data-id-title="Architecture and Hardware Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Architecture and Hardware Tuning</span> <a title="Permalink" class="permalink" href="#tuning-architecture">#</a></h2></div></div></div><p>
  Architectural tuning includes aspects that range from the low-level design of
  the systems being deployed, up to macro-level decisions about network
  topology and cooling. Not all of these can be covered in this work, but
  guidance will be given where possible.
 </p><p>
  From the top-down view, it is important to think about the physical location
  of nodes, the connectivity available between them, and implications of such
  items as power routing and fire compartments. However, from a performance
  perspective, it is most important to think in terms of the connectivity and
  what a write actually looks like from the cluster perspective.
 </p><p>
  The intention of architectural tuning is to take control of where the
  performance bottlenecks lie. In most cases, it is preferable for the
  bottleneck to lie with the storage device, as that creates a predictable
  pattern of performance degradation and associated limitations. Placing the
  bottleneck in other areas, such as CPU or network, may create inconsistent
  behavior when resources are stressed. This is due to the possibility of other
  processes running, whether recovery, rebalancing, garbage collection, and
  causing inconsistent behavior due to being network or CPU bound. When storage
  is the bottleneck, the degradation results in longer response times to queued
  requests, making this the most desireable point at which to place the
  bottleneck.
 </p><section class="sect1" id="tuning-network" data-id-title="Network"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Network</span> <a title="Permalink" class="permalink" href="#tuning-network">#</a></h2></div></div></div><p>
   The network is the backbone of the cluster, and a failure to implement it in
   a robust manner can cripple the performance of the cluster, no matter what
   other steps are taken. From a best practices perspective, this entails
   implementing a fault-tolerant switching infrastructure that can scale the
   aggregate core bandwidth as the cluster grows. For very large clusters, this
   may entail a leaf and spine architecture, while for smaller clusters, it may
   look like a more familiar hub-and-spoke or mesh network.
  </p><p>
   No matter which network architecture is chosen, it is important to carefully
   examine each hop along the path and consider the maximum network traffic
   that could occur during adverse conditions.
  </p><p>
   An example of a bad decision here would be to have a multi-rack cluster
   where each rack contains 16 storage nodes of 24× 7200rpm drives where
   each node is connected to a pair of stacked Top-of-Rack (TOR) switches via
   2x25Gb connections. This connectivity is sufficient for 3 GB/s per
   connection or 6 GB/s total for the node, which is near the maximum that
   a 7200rpm drive will sustain. The bad decision comes in
   <span class="emphasis"><em>only</em></span> using 4× of the 25 GB interfaces for
   the uplink. While 100 Gb may seem like a substantial amount of
   bandwidth, it translates into only about 10 GB/s, while the rack is
   capable of an aggregate of around 48 GB/s.
  </p><p>
   During an adverse situation, it is possible that network congestion will
   result in dramatically increased latency, packet delays, drops, etc. A
   simple rememdy would have been to select switches with 4× 100 Gb
   uplink ports, resulting in each switch being able to transmit the complete,
   aggregate bandwidth load to the network core.
  </p><p>
   In reality, some level of over-subscription is going to happen in most
   networks, but the smaller the ratio, the better the resulting cluster will
   be able to handle adverse conditions.
  </p><section class="sect2" id="id-1.5.3.2.6.7" data-id-title="Network Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.1.1 </span><span class="title-name">Network Tuning</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.6.7">#</a></h3></div></div></div><p>
    Here are some general principals to follow when planning a Ceph cluster
    network:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Utilize 25/50/100 GbE connections - The signaling rate is 2.5 times
      faster than 10/40 GbE resulting in lower latency over the wire. The
      impact from the faster signaling rate would be minimal with spinning
      media and more impactful with faster technologies such as NVMe.
     </p></li><li class="listitem"><p>
      Network bandwidth should be at least the total bandwidth of all storage
      devices present in the storage node
     </p></li><li class="listitem"><p>
      Usage of VLANs on LACP-bonded Ethernet provides the best balance of
      bandwidth aggregation + fault tolerance
     </p></li><li class="listitem"><p>
      The network should utilize jumbo-frame Ethernet if all nodes connecting
      to the storage are able to do so, otherwise use the standard MTU.
     </p></li></ul></div></section></section><section class="sect1" id="tuning-node-hw-recommendations" data-id-title="Node Hardware Recommendations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Node Hardware Recommendations</span> <a title="Permalink" class="permalink" href="#tuning-node-hw-recommendations">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.2.7.2" data-id-title="CPU"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.1 </span><span class="title-name">CPU</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.7.2">#</a></h3></div></div></div><p>
    There are a lot of options when it comes to processor selection in the
    market today. There are multiple varieties available from the major vendors
    in the x86 space and a wide array of options in the 64-bit Arm space as
    well. Without regard to the core architecture, there is one rule that is
    always true, and that is that higher clockspeed allows more work to be done
    in the same amount of time. This consideration is most important when
    working with higher speed storage and network devices.
   </p><p>
    CPU selection is also an important consideration for specific services.
    Some services, such as metadata servers, NFS, Samba, and ISCSI gateways
    benefit from a smaller number of much faster cores, while the OSD nodes
    need a more core dense solution.
   </p><p>
    A second consideration is whether to use a single socket or multiple
    sockets. The answer to this will depend on the device density, type of
    network hardware being utilized, etc. In many nodes, a single socket will
    provide better performance as the processor interlink is a bottleneck,
    though this would most likely be noticed in an all NVMe based node type.
    The general recommendation is to use a single socket whenever possible.
   </p><p>
    When considering which processor to choose, there are several
    considerations aside from clock-speed that should be taken into
    consideration. They include:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Memory bandwidth</em></span>: Ceph is a heavy user of RAM,
      thus the more memory bandwidth available, the more performant the node
      can be.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Memory layout</em></span>: Even if the memory selected is fast,
      if all memory channels are not leveraged, performance is being left
      untapped. It is advantageous to ensure that RAM is distributed evenly
      across all channels.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Offload capabilities</em></span>: For example, Intel CPUs offer
      <code class="literal">zlib</code> and Reed-Solomon offloads, the latter being used
      with erasure coding when the ISA plugin is specified.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>PCIe bus speed and lanes</em></span>: This is particularly
      important when looking at devices with a large number of PCIe devices,
      like NVMe. The bus speed also affects the performance of network devices
      as well.
     </p></li></ul></div></section><section class="sect2" id="id-1.5.3.2.7.3" data-id-title="Storage Device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.2 </span><span class="title-name">Storage Device</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.7.3">#</a></h3></div></div></div><p>
    Storage device selection can dramatically affect the performance and
    reliability of a Ceph cluster. When building for performance, it is
    important to understand the nature of the device in regard to reads/writes
    and the workloads that will be applied. This is particularly true with
    flash media.
   </p><section class="sect3" id="id-1.5.3.2.7.3.3" data-id-title="Device Type"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.2.2.1 </span><span class="title-name">Device Type</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.7.3.3">#</a></h4></div></div></div><p>
     The first recommendation is to ensure that systems utilize
     Enterprise-class storage media. With NVMe and SSD devices, this implies a
     few key items.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       More spare cells to deal with media wear-out
      </p></li><li class="listitem"><p>
       Battery/capacitor to allow completion of buffer dumps during unexpected
       power events
      </p></li></ul></div><p>
     It is also important to ensure the media being utilized will support the
     workloads of the cluster. For example, if the applications using the
     cluster have a read/write mix of 90:10, it is likely acceptable to utilize
     a read-intensive NVMe device. However, if the ratio is flipped or even
     50:50, it is a better choice to at least consider mixed-use, or write
     intensive media. This selection goes beyond just the media durability, but
     also includes considerations around the design. Write-intensive media
     typically allocate more PCIe lanes to handling write requests to the
     media, ensuring faster commits than a read-intensive device would provide
     under load. Also, write intensive devices will most often use faster
     classes of non-volitaile memory technology and/or have large, supercap
     backed caches.
    </p></section></section><section class="sect2" id="id-1.5.3.2.7.4" data-id-title="Device Buses"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.3 </span><span class="title-name">Device Buses</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.7.4">#</a></h3></div></div></div><p>
    It is also important to understand the impact of the storage bus and
    hardware pieces along the way. Clearly, 6 Gb/s is slower than
    12 Gb/s, and 12 Gb/s is slower than PCIe Gen3 (8 Gb/s per
    lane), but what about mixing SATA 3 Gb/s and SATA 6 Gb/s, or
    mixing 6 Gb/s and 12 Gb/s SAS?
   </p><p>
    The general rule is not to mix. When a 6 Gb/s device is introduced to
    a 12 Gb/s bus, the entire bus slows down to 6 Gb/s, greatly
    reducing the overall throughput capability. Where this really would hurt is
    in a dense SAS SSD system. If there are 24 SAS SSDs on a two-channel,
    12 Gb/s bus and one of the devices is only 6 Gb/s, then the
    12 Gb/s SAS drives that can push 850 MB/s now oversubscribe the
    bus due to the reduced data rate.
   </p><p>
    Another consideration is the presence of bus expanders. Bus expanders allow
    a system to multiplex multiple devices on a single channel. The result is
    higher density at lower performance maximum. In some cases, the expanders
    may work acceptably, such as with HDDs, but with SSDs, they are likely to
    quickly become a bottleneck.
   </p></section><section class="sect2" id="id-1.5.3.2.7.5" data-id-title="General Recommendations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2.4 </span><span class="title-name">General Recommendations</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.7.5">#</a></h3></div></div></div><p>
    Below are some generic tuning options applicable to performance tuning for
    server platforms:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Set firmware Power/Performance controls to the performance profile. This
      should eliminate frequency scaling and ensure that there is no added
      latency caused by it.
     </p></li><li class="listitem"><p>
      Enable multi-threading on SMT-capable CPUs. This extra processing power
      is utilized effectively by Ceph.
     </p></li><li class="listitem"><p>
      Ensure all add-in cards are in the optimal slots for performance.
     </p></li></ul></div></section></section><section class="sect1" id="tuning-cpeh-rocksdb-wal" data-id-title="Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">Ceph</span> <a title="Permalink" class="permalink" href="#tuning-cpeh-rocksdb-wal">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.2.8.2" data-id-title="RocksDB and WAL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.3.1 </span><span class="title-name">RocksDB and WAL</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.8.2">#</a></h3></div></div></div><p>
    Ceph makes use of both a Write-Ahead-Log (WAL) and
    <code class="literal">RocksDB</code>. The WAL is the internal journal where small
    writes are queued before commiting to the backend storage. RocksDB is where
    Ceph stores metadata associated with the objects written to BlueStore.
    When using spinning media, or perhaps even SSDs, it generally makes sense
    to locate the RocksDB and WAL on a faster device, such as NVMe. When doing
    so, proper sizing of these is critical to ensuring a stable performance
    profile of the cluster over time.
   </p><p>
    From a performance perspective, the rule of thumb is to divide the write
    performance of the WAL/RocksDB device by the write performance of the data
    device. This yields what should be considered to be the maximum ratio of
    data devices per WAL/RocksDB device.
   </p><section class="sect3" id="id-1.5.3.2.8.2.4" data-id-title="WAL"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.3.1.1 </span><span class="title-name">WAL</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.8.2.4">#</a></h4></div></div></div><p>
     The WAL maxes out a bit under two gigabytes. In order to leave room for
     maintenance activities, having about four gigabytes of space
     allocated/allowed is optimal.
    </p></section><section class="sect3" id="id-1.5.3.2.8.2.5" data-id-title="RocksDB"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">4.3.1.2 </span><span class="title-name">RocksDB</span> <a title="Permalink" class="permalink" href="#id-1.5.3.2.8.2.5">#</a></h4></div></div></div><p>
     RocksDB operates with a series of tiered levels, each being an order of
     magnitude larger than the last. Levels one through four are 256 MB,
     2.56 GB, 25.6 GB, 256 GB respectively. Allocating
     appropriate space for these is an act of aggregation. Given that few
     installations utilize enough metadata to require the fourth tier,
     allocating for the first three and associated maintenance is sufficient.
     25.6+2.56+.256 GB = 28.416 GB. Rounding up to 30 GB and
     providing 100% overhead to allow for maintenance takes the space
     allocation suggested for the first three tiers to 60 GB.
    </p><div id="id-1.5.3.2.8.2.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Keep in mind that we recommend reserving 4 GB for the WAL device.
      The recommended size for DB is a total of 64 GB for most workloads.
      See <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware Requirements and Recommendations”, Section 2.4.3 “Recommended Size for the BlueStore's WAL and DB Device”</span> for more information.
     </p></div><p>
     Making the decision to provision fast space for the fourth tier of RocksDB
     is entirely related to the expected metadata load. Protocols like RBD use
     little metadata, while CephFS is somewhere from a mild to moderate
     amount. S3 and native RADOS can utilize the highest amounts of metadata
     and are generally the cases where it starts making sense to evaluate
     whether it makes sense to move the fourth tier makes to faster media.
    </p></section></section></section></section><section class="chapter" id="tuning-os" data-id-title="Operating System Level Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Operating System Level Tuning</span> <a title="Permalink" class="permalink" href="#tuning-os">#</a></h2></div></div></div><p>
  A significant amount of the performance tuning for Ceph clusters can be
  done at the operating system (OS) layer. This tuning involves ensuring that
  unnecessary services are not running, and extends down to ensuring buffers
  are not being overrun and interrupts are being spread properly. There are
  many additional tuning options for the OS that are not included here, either
  due to statistically-insignificant performance changes, or not being deemed
  candidates for significant performance improvement at the time of this work.
 </p><section class="sect1" id="tuning-sles" data-id-title="SUSE Linux Enterprise Install and Validation of Base Performance"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">SUSE Linux Enterprise Install and Validation of Base Performance</span> <a title="Permalink" class="permalink" href="#tuning-sles">#</a></h2></div></div></div><p>
   During the OS installation, do <span class="emphasis"><em>not</em></span> select an install
   pattern that includes an X Server. Doing so utilizes RAM and CPU resources
   that would be better allocated to tuning storage-related daemons. We
   recommend a pattern that includes the minimal pattern with the addition of
   the <code class="literal">YaST management</code> pattern.
  </p><p>
   After the OS is installed, it is proper to evaluate the various individual
   components that are critical to the overall performance of the storage
   cluster.
  </p><div id="id-1.5.3.3.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Check performance of individual components before SUSE Enterprise Storage is setup to
    ensure they are performing as desired.
   </p></div><section class="sect2" id="id-1.5.3.3.4.5" data-id-title="Network Performance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.1 </span><span class="title-name">Network Performance</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.4.5">#</a></h3></div></div></div><p>
    To perform <code class="command">iperf3</code> tests for network performance,
    consider increasing the window size (with the <code class="option">-w</code> option)
    and running multiple streams to fully test the bandwidth capability. If
    using the standard MTU, your NICs should be capable of running at
    approximately 70-80% of the advertised bandwidth. If you move up to jumbo
    frames, the NIC should be able to saturate the link.
   </p><p>
    This is not necessarily true for faster topologies such as 100 Gb. In
    those topologies, saturating the NIC can require substantial OS and driver
    tuning, in combination with ensuring the hardware has the appropriate CPU
    clock speeds and settings.
   </p><p>
    This is a sample of the <code class="command">iperf3</code> commands used on a
    100 Gb network. In the command line, the <code class="option">-N</code> disables
    Nagle's buffering algorithm and the <code class="option">-l</code> sets the buffer
    length to higher than the default 128 kB, resulting in slightly higher
    throughput.
   </p><div class="verbatim-wrap"><pre class="screen">  server# iperf3 -s

  client# iperf3   -c server -N -l 256k
  Connecting to host sr650-1, port 5201
  [  4] local 172.16.227.22 port 36628 connected to 172.16.227.21 port 5201
  [ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
  [  4]   0.00-1.00   sec  4.76 GBytes  40.9 Gbits/sec    0   1.48 MBytes
  [  4]   1.00-2.00   sec  4.79 GBytes  41.1 Gbits/sec    0   2.52 MBytes
  [  4]   2.00-3.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   3.00-4.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   4.00-5.00   sec  4.74 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   5.00-6.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   6.00-7.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   7.00-8.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   8.00-9.00   sec  4.73 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   9.00-10.00  sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  - - - - - - - - - - - - - - - - - - - - - - - - -
  [ ID] Interval           Transfer     Bandwidth       Retr
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec    0             sender
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec                  receiver</pre></div></section><section class="sect2" id="id-1.5.3.3.4.6" data-id-title="Storage Performance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.2 </span><span class="title-name">Storage Performance</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.4.6">#</a></h3></div></div></div><p>
    Use <code class="command">fio</code> to test individual storage devices to understand
    the per-device performance maxima. Do this for all devices to ensure that
    none are out of specification, or are connected to expanders which lower
    bandwidth. Doing an exhaustive study of different I/O sizes and patterns
    would provide the most information about performance expectations, but is
    beyond the scope of this document.
   </p><p>
    We recommend testing at least random 4 kB, random 64 kB, and
    sequential 64 kB and 1 MB buffers. This should give a reasonable
    overall view of the device’s performance characteristics. When testing, it
    is important to use the raw device
    (<code class="literal">/dev/sd<em class="replaceable">X</em></code>) and to use the
    <code class="option">direct=1</code> option with multiple jobs to maximize device
    performance under stress.
   </p><p>
    Make sure that the test size (dataset) is large enough to over-run any
    caches that may apply. We recommend using the <code class="literal">ramp-time</code>
    parameter to allow for enough time for the cache overrun to occur before
    performance measurements are made. This helps to ensure that performance
    numbers are not tainted by cache-only performance.
   </p><p>
    Run <code class="command">fio</code> against all devices in an OSD node
    simultaneously to identify bottlenecks. It should scale very near linearly;
    if not, check controller firmware, slot placement, and if necessary, split
    devices across multiple controllers. This simulates the node under heavy
    load.
   </p><p>
    We recommend using the same I/O patterns and block sizes that the
    individual devices were tested with. The job count should be a multiple of
    the total number of devices in the system, to allow for even distribution
    across all devices and buses.
   </p><section class="sect3" id="id-1.5.3.3.4.6.7" data-id-title="Latency Bound and Maximum"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.1.2.1 </span><span class="title-name">Latency Bound and Maximum</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.4.6.7">#</a></h4></div></div></div><p>
     There is value in performing both latency-bound and worst-case-scenario
     tests. Changing a particular value may not improve latency, but rather
     enable the cluster to handle even more total load, even though latencies
     continue to rise. The inverse may also be true, where a change affects
     latency of operations in a measurable way. To identify both possibilities,
     we recommend that tests be performed that represent both positions.
     Latency-bounded tests in <code class="command">fio</code> have the following set:
    </p><div class="verbatim-wrap"><pre class="screen">  latency_target=10ms
  latency_window=5s
  latency_percentile=99</pre></div><p>
     The settings above cause <code class="command">fio</code> to increase the I/O queue
     depth until 1% or more of IOPS no longer maintain a 10 ms average
     during a sliding five-second window. It then backs down the queue depth
     until the latency average is maintained.
    </p></section></section></section><section class="sect1" id="tuning-kernel-tuning" data-id-title="Kernel Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Kernel Tuning</span> <a title="Permalink" class="permalink" href="#tuning-kernel-tuning">#</a></h2></div></div></div><p>
   There are several aspects of the kernel that can be tuned on both the
   cluster and some of the clients. It is important to understand that most
   tuning is about gaining very small incremental improvements, which in
   aggregate represent a measureable (and hopefully meaningful) improvement in
   performance. For this document, information on tuning comes from a variety
   of sources. The primary source is
   <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html</a>.
   We also used numerous other references, including documentation from
   hardware vendors.
  </p><section class="sect2" id="id-1.5.3.3.5.3" data-id-title="CPU Mitigations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.1 </span><span class="title-name">CPU Mitigations</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.3">#</a></h3></div></div></div><p>
    One key area of kernel performance tuning is to disable its side-channel
    attack mitigations. The bulk of the benefits from this occur with smaller
    I/Os, ranging in size from 4 kB to 64 kB. In particular,
    64 kB random read and sequential writes doubled in performance in a
    limited test environment using only two client nodes.
   </p><p>
    Changing these options requires a clear understanding of the security
    implications, as they involve disabling mitigations for side-channel
    attacks on some CPUs. The need to disable these mitigations may be
    minimized with newer processors. You must carefully evaluate whether this
    is something needed for the particular hardware being utilized. In the test
    configuration, a Salt state was utilized to apply these changes. The
    Salt state should be in a subdirectory of <code class="filename">/srv/salt</code>
    on the Salt master and is applied by using a <code class="command">salt
    state.apply</code> command similar to below:
   </p><div class="verbatim-wrap"><pre class="screen">salt '*' state.apply my_kerntune</pre></div><p>
    The Salt state and steps used in this testing can be found in
    <a class="xref" href="#tuning-appendix-a" title="Appendix A. Salt State for Kernel Tuning">Appendix A, <em>Salt State for Kernel Tuning</em></a>. This needs to be adjusted to work in
    each customer environment. An example of adjusting the
    <code class="literal">grub2</code> configuration file can also be found in the
    appendix.
   </p></section><section class="sect2" id="id-1.5.3.3.5.4" data-id-title="I/O tuning - Multiqueue Block I/O"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.2 </span><span class="title-name">I/O tuning - Multiqueue Block I/O</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.4">#</a></h3></div></div></div><p>
    The first aspect to tune is to ensure that I/O is flowing in the most
    optimal pattern. For the test cluster used in this test, that means
    enabling multi-queue block I/O. This is done through adding a boot-time
    kernel parameter, as found in Section 12.4
    <a class="link" href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-tuning-io.html#cha-tuning-io-barrier" target="_blank">https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-tuning-io.html#cha-tuning-io-barrier</a>.
    This is not an action that should be taken unilaterally on clusters which
    contain spinning media devices, due to potential performance degradation
    for those devices. The general result is that there multiple I/O queues are
    assigned to each device, allowing more jobs to be handled simultaneously by
    those device that can service large numbers of requests, such as SSD and
    NVMe drives.
   </p></section><section class="sect2" id="id-1.5.3.3.5.5" data-id-title="SSD Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.3 </span><span class="title-name">SSD Tuning</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.5">#</a></h3></div></div></div><p>
    To get the best read performance, it may be necessary to adjust the
    <code class="option">read_ahead</code> and write cache settings for the SSD devices.
    In our particular testing environment, disabling write cache and forcing
    <code class="option">read_ahead</code> to 2 MB resulted in the best overall
    performance.
   </p><p>
    Before tuning, it is important to check and record the default values, and
    measure any differences in performance compared to that baseline.
   </p><p>
    By placing the following file in <code class="filename">/etc/udev/rules.d</code>,
    devices will be detected according to the model name shown in
    <code class="filename">/sys/block/{devname}/device/model</code>, and instructed to
    disable write caching and set the <code class="option">read_ahead_kb</code> option to
    2 MB.
   </p><div class="verbatim-wrap"><pre class="screen">  /etc/udev/rules.d/99-ssd.rules

  # Setting specific kernel parameters for a subset of block devices (Intel SSDs)
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", ATTR{queue/read_ahead_kb}="2048"
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", RUN+="/sbin/hdparm -W 0 /dev/%k"</pre></div></section><section class="sect2" id="id-1.5.3.3.5.6" data-id-title="Network Stack and Device Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.2.4 </span><span class="title-name">Network Stack and Device Tuning</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6">#</a></h3></div></div></div><p>
    Proper tuning of the network stack can substantially assist improving the
    latency and throughput of the cluster. A full script for the testing we
    performed can be found in <a class="xref" href="#tuning-appendix-c" title="Appendix C. Network Tuning">Appendix C, <em>Network Tuning</em></a>.
   </p><p>
    The first change, and one with the highest impact, is to utilize jumbo
    frame packets. For this to be done, all interfaces utilizing the cluster
    must be set to use an MTU of 9000 bytes. Network switches are often set to
    use 9100 or higher. This is suitable, as they are only passing packets, not
    creating them.
   </p><section class="sect3" id="id-1.5.3.3.5.6.4" data-id-title="Network Device Tuning"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.4.1 </span><span class="title-name">Network Device Tuning</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.4">#</a></h4></div></div></div><section class="sect4" id="id-1.5.3.3.5.6.4.2" data-id-title="Jumbo Frames"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.1 </span><span class="title-name">Jumbo Frames</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.4.2">#</a></h5></div></div></div><p>
      The following Salt command ensures the bonded interfaces on all nodes
      under control (including the test load generation nodes) were utilizing
      an MTU of 9000:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'ip link set bond0 mtu 9000'</pre></div><p>
      To set this persistently, utilize YaST to set the MTU for the bonded
      interface.
     </p></section><section class="sect4" id="id-1.5.3.3.5.6.4.3" data-id-title="PCIe Bus Adjustment"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.2 </span><span class="title-name">PCIe Bus Adjustment</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.4.3">#</a></h5></div></div></div><p>
      Adjusting the PCIe maximum read request size can provide a slight boost
      to performance. Be aware that this tuning is card- and slot-specific and
      must only be done in conjunction with the conditions and instructions
      supplied by the manufacturer. The maximum PCIe read request size was set
      with the following Salt commands:
     </p><div id="id-1.5.3.3.5.6.4.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       This should only be done with guidance from the NIC manufacturer and is
       specific to bus location, driver version and hardware.
      </p></div><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'setpci -s 5b:00.0 68.w=5936'
  salt '*' cmd.run 'setpci -s 5b:00.1 68.w=5936'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.4.4" data-id-title="TCP RSS"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.3 </span><span class="title-name">TCP RSS</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.4.4">#</a></h5></div></div></div><p>
      The next item on the tuning list is helpful in ensuring that a single CPU
      core is not responsible for all packet processing. A small script is used
      to spread the I/O across multiple local (from a NUMA perspective) cores.
     </p><div id="id-1.5.3.3.5.6.4.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       This is not necessary if the number of queues returned by <code class="command">ls
       /sys/class/net/{ifname}/queues/rx-*|wc -l</code> is equal to the
       number of physical cores in a single CPU socket.
      </p></div><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'for j in `cat /sys/class/net/bond0/bonding/slaves`;do \
LOCAL_CPUS=`cat /sys/class/net/$j/device/local_cpus`;echo $LOCAL_CPUS &gt; \
/sys/class/net/$j/queues/rx-0/rps_cpus;done'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.4.5" data-id-title="Ring Buffers"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.1.4 </span><span class="title-name">Ring Buffers</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.4.5">#</a></h5></div></div></div><p>
      Many NIC drivers start with a default value for the receive (RX) and
      transmit (TX) buffers that is not optimal for high-throughput scenarios,
      and does not allow enough time for the kernel to drain the buffer before
      it fills up.
     </p><p>
      The current and maximum settings can be revealed by issuing the following
      command to the proper NICs:
     </p><div class="verbatim-wrap"><pre class="screen">ethtool -g eth4</pre></div><p>
      The output from this command should look similar to this:
     </p><div class="verbatim-wrap"><pre class="screen">  Ring parameters for eth4:
  Pre-set maximums:
  RX:		8192
  RX Mini:	0
  RX Jumbo:	0
  TX:		8192
  Current hardware settings:
  RX:		1024
  RX Mini:	0
  RX Jumbo:	0
  TX:		1024</pre></div><p>
      Here we can see that the NIC can allocate buffers of up to 8 kB, but
      it iscurrently using ones of only 1 kB. To adjust this for the
      cluster, issue the following command:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'ethtool -G eth4 rx 8192 tx 8192'
  salt '*' cmd.run 'ethtool -G eth5 rx 8192 tx 8192'</pre></div><p>
      Setting this value persistently can be achieved via the YaST
      configuration module:
     </p><div class="figure" id="yast-config-module"><div class="figure-contents"><div class="mediaobject"><a href="images/yast_ring_buffers.png" target="_blank"><img src="images/yast_ring_buffers.png" width="" alt="YaST Configuration Module"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">YaST Configuration Module </span><a title="Permalink" class="permalink" href="#yast-config-module">#</a></h6></div></div><p>
      Additionally, the settings can be made persistent by editing the
      configuration files for the physical interfaces found in
      <code class="filename">/etc/sysconfig/network</code>. A script can be found in
      Appendix B that will change all interfaces to the maximum ring buffer
      value.
     </p></section></section><section class="sect3" id="id-1.5.3.3.5.6.5" data-id-title="Network Stack"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.2.4.2 </span><span class="title-name">Network Stack</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.5">#</a></h4></div></div></div><p>
     The following settings can all be made persistent by modifying
     <code class="filename">/etc/sysctl.conf</code>. They are represented as arguments
     in a Salt command to allow testing and validation in your environment
     before making them permanent.
    </p><section class="sect4" id="id-1.5.3.3.5.6.5.3" data-id-title="Lower TCP Latency"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.1 </span><span class="title-name">Lower TCP Latency</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.5.3">#</a></h5></div></div></div><p>
      Setting the <code class="literal">TCP low latency</code> option disables IPv4 TCP
      pre-queue processing and improves latency. We recommend experimenting
      with setting this to both <code class="literal">0</code> and <code class="literal">1</code>.
      In laboratory testing, setting the value to <code class="literal">1</code> provided
      slightly better performance:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_low_latency=1'</pre></div><p>
      The TCP <code class="option">fastopen</code> option allows the sending of data in
      the first <code class="literal">syn</code> packet, resulting in a slight
      improvement in latency:
     </p><div class="verbatim-wrap"><pre class="screen">  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_fastopen=1'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.4" data-id-title="TCP Stack Buffers"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.2 </span><span class="title-name">TCP Stack Buffers</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.5.4">#</a></h5></div></div></div><p>
      Ensure that the TCP stack has sufficent buffer space to queue both
      inbound and outbound traffic:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_rmem="10240 87380 2147483647"'
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_wmem="10240 87380 2147483647"'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.5" data-id-title="TCP Sequence and Timestamps"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.3 </span><span class="title-name">TCP Sequence and Timestamps</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.5.5">#</a></h5></div></div></div><p>
      In fast networks, TCP sequence numbers can be re-used in a very short
      timeframe. The result is that the system thinks a packet has been
      received out of order, resulting in a drop. TCP timestamps were added to
      help ensure that packet sequence could be tracked better:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_timestamps=1'</pre></div><p>
      <code class="literal">TCP Selective Acknowledgement</code> is a feature that is
      primarily useful for WAN or lower-speed networks. However, disabling it
      may have negative effects in other ways:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.ipv4.tcp_sack=1'</pre></div></section><section class="sect4" id="id-1.5.3.3.5.6.5.6" data-id-title="Kernel Network Buffers and Connections"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">5.2.4.2.4 </span><span class="title-name">Kernel Network Buffers and Connections</span> <a title="Permalink" class="permalink" href="#id-1.5.3.3.5.6.5.6">#</a></h5></div></div></div><p>
      Providing plenty of buffer space is a recurring theme in tuning networks
      for high performance. The <code class="literal">netdev_max_backlog</code> is where
      traffic is queued after it has been received by the NIC, but before it is
      processed by the protocol stack (IP, TCP, etc):
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.netdev_max_backlog=250000'</pre></div><p>
      Other preventative measures for the system and gateway nodes include
      ensuring that the maximum connection count is high enough to prevent the
      generation of <code class="literal">syn</code> cookies. This is useful to set on
      all nodes involved:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.somaxconn=2048'</pre></div><p>
      Increasing the network stack buffers is useful to ensure that sufficient
      buffers exist for all transactions:
     </p><div class="verbatim-wrap"><pre class="screen">salt '*' cmd.run 'sysctl -w net.core.rmem_max=2147483647'
salt '*' cmd.run 'sysctl -w net.core.wmem_max=2147483647'</pre></div></section></section></section></section></section><section class="chapter" id="tuning-ceph" data-id-title="Ceph Tuning"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Ceph Tuning</span> <a title="Permalink" class="permalink" href="#tuning-ceph">#</a></h2></div></div></div><p>
  Ceph includes a telemetry module that provides anonymized information back
  to the Ceph developer community. The information contained in the telemetry
  report provides information that helps the developers prioritize efforts and
  identify areas where more work may be needed. It may be necessary to enable
  the telemetry module before turning it on. To enable the module:
 </p><div class="verbatim-wrap"><pre class="screen">ceph mgr module enable telemetry</pre></div><p>
  To turn on telemetry reporting use the following command:
 </p><div class="verbatim-wrap"><pre class="screen">ceph telemetry on</pre></div><p>
  Additional information about the Ceph telemetry module may be found in the
  <span class="intraxref">Book “Administration Guide”</span>.
 </p><section class="sect1" id="tuning-obtaining-metrics" data-id-title="Obtaining Ceph Metrics"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Obtaining Ceph Metrics</span> <a title="Permalink" class="permalink" href="#tuning-obtaining-metrics">#</a></h2></div></div></div><p>
   Before adjusting Ceph tunables, it is helpful to have an understanding of
   the critical metrics to monitor and what they indicate. Many of these
   parameters are found by dumping raw data from the daemons. This is done by
   means of the <code class="command">ceph daemon dump</code> command. The following
   example shows the dump command being utilized for
   <code class="literal">osd.104</code>.
  </p><div class="verbatim-wrap"><pre class="screen">ceph --admin-daemon /var/run/ceph/ceph-osd.104.asok perf dump</pre></div><p>
   Starting with the Ceph Nautilus release, the following command may be used
   as well:
  </p><div class="verbatim-wrap"><pre class="screen">ceph daemon osd.104 perf dump</pre></div><p>
   The output of the command is quite lengthy and may benefit from being
   redirected to a file.
  </p></section><section class="sect1" id="tuning-tuning-persistent" data-id-title="Making Tuning Persistent"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Making Tuning Persistent</span> <a title="Permalink" class="permalink" href="#tuning-tuning-persistent">#</a></h2></div></div></div><p>
   To make parameter adjustment persistent requires modifying the
   <code class="filename">/etc/ceph/ceph.conf</code> file. This is best done through
   modifying the source component files that DeepSea uses to manage the
   cluster. Each section is identified with a header such as:
  </p><div class="verbatim-wrap"><pre class="screen">  [global]
  [osd]
  [mds]
  [mon]
  [mgr]
  [client]</pre></div><p>
   The section of the configuration is tuned by modifying the correct
   <code class="filename">[sectionname].conf</code> in the
   <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>
   directory. After modifying the configuration file, replace
   <code class="literal">master</code> with the master minion-name (usually the admin
   node). The result is that the changes are pushed to all cluster nodes.
  </p><div class="verbatim-wrap"><pre class="screen">  salt 'master' state.apply ceph.configuration.create
  salt '*' state.apply ceph.configuration</pre></div><p>
   Changes made in this way require the affected services to be restarted
   before taking effect. It is also possible to deploy these files before
   running stage 2 of the SUSE Enterprise Storage deployment process. It is especially
   desirable to do so if changing the settings that require node or device
   re-deployment.
  </p></section><section class="sect1" id="tuning-core" data-id-title="Core"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Core</span> <a title="Permalink" class="permalink" href="#tuning-core">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.10.2" data-id-title="Logging"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.1 </span><span class="title-name">Logging</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.10.2">#</a></h3></div></div></div><p>
    It is possible to disable all logging to reduce latency in the various
    codepaths.
   </p><div id="id-1.5.3.4.10.2.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     This tuning should be used with caution and understanding that logging
     <span class="emphasis"><em>will</em></span> need to be re-enabled should support be
     required. This implies that an incident would need to be reproduced
     <span class="emphasis"><em>after</em></span> logging is re-enabled.
    </p></div><div class="verbatim-wrap"><pre class="screen">  debug ms=0
  debug mds=0
  debug osd=0
  debug optracker=0
  debug auth=0
  debug asok=0
  debug bluestore=0
  debug bluefs=0
  debug bdev=0
  debug kstore=0
  debug rocksdb=0
  debug eventtrace=0
  debug default=0
  debug rados=0
  debug client=0
  debug perfcounter=0
  debug finisher=0</pre></div></section><section class="sect2" id="id-1.5.3.4.10.3" data-id-title="Authentication Parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.2 </span><span class="title-name">Authentication Parameters</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.10.3">#</a></h3></div></div></div><p>
    Under certain conditions where the cluster is physically secure and
    isolated inside a secured network with no external exposure, it is possible
    to disable <code class="systemitem">cephx</code>. There are two levels at which <code class="systemitem">cephx</code> can be disabled.
    The first is to disable signing of authentication traffic. This can be
    accomplished with the following settings:
   </p><div class="verbatim-wrap"><pre class="screen">cephx_require_signatures = <em class="replaceable">false</em>
cephx_cluster_require_signatures = <em class="replaceable">false</em>
cephx_sign_messages = <em class="replaceable">false</em></pre></div><p>
    The second level of tuning completely disables <code class="systemitem">cephx</code> authentication. This
    should only be done on networks that are isolated from public network
    infrastructure. This change is achieved by adding the following three lines
    in the global section:
   </p><div class="verbatim-wrap"><pre class="screen">auth cluster required = none
auth service required = none
auth client required = none</pre></div></section><section class="sect2" id="id-1.5.3.4.10.4" data-id-title="RADOS Operations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.3 </span><span class="title-name">RADOS Operations</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.10.4">#</a></h3></div></div></div><p>
    The backend processes for performing RADOS operations show up in
    <code class="literal">throttle-*objector_ops</code> when dumping various daemons. If
    there is too much time being spent in <code class="literal">wait</code>, there may be
    some performance to gain by increasing the memory for in-flight ops or by
    increasing the total number of inflight operations overall.
   </p><div class="verbatim-wrap"><pre class="screen">objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576</pre></div></section><section class="sect2" id="id-1.5.3.4.10.5" data-id-title="OSD parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.4 </span><span class="title-name">OSD parameters</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.10.5">#</a></h3></div></div></div><p>
    Increasing the number of <code class="literal">op threads</code> may be helpful with
    SSD and NVMe devices, as it provides more work queues for operations.
   </p><div class="verbatim-wrap"><pre class="screen">osd_op_num_threads_per_shard = 4</pre></div></section><section class="sect2" id="id-1.5.3.4.10.6" data-id-title="RocksDB or WAL device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.5 </span><span class="title-name">RocksDB or WAL device</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.10.6">#</a></h3></div></div></div><p>
    In checking the performance of BlueStore, it is important to understand
    if your metadata is spilling over from the high-speed device, if defined,
    to the bulk data storage device. The parameters useful in this case are
    found under bluefs <code class="literal">slow_used_bytes</code>. If
    <code class="literal">slow_used_bytes</code> is greater than zero, the cluster is
    using the storage device instead of the RocksDB/WAL device. This is an
    indicator that more space needs to be allocated to RocksDB/WAL.
   </p><p>
    Starting with the Ceph Nautilus release, spillover is shown in the output
    of the <code class="command">ceph health</code> command.
   </p><p>
    The process of allocating more space depends on how the OSD was deployed.
    If it was deployed by a version prior to SUSE Enterprise Storage 6, the OSD will need
    to be re-deployed. If it was deployed with version 6 or after, it may be
    possible to expand the LVM that the RocksDB and WAL reside on, subject to
    available space.
   </p></section><section class="sect2" id="bluestore-parameters" data-id-title="BlueStore parameters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.6 </span><span class="title-name">BlueStore parameters</span> <a title="Permalink" class="permalink" href="#bluestore-parameters">#</a></h3></div></div></div><p>
    Ceph is thin provisioned, including the Write-Ahead Log (WAL) files. By
    pre-extending the files for the WAL, time is saved by not having to engage
    the allocator. It also potentially reduces the likelihood of fragmentation
    of the WAL files. This likely only provides benefit during the early life
    of the cluster.
   </p><div class="verbatim-wrap"><pre class="screen">bluefs_preextend_wal_files=1</pre></div><p>
    BlueStore has the ability to perform buffered writes. Buffered writes
    enable populating the read cache during the write process. This setting, in
    effect, changes the BlueStore cache into a write-through cache.
   </p><div class="verbatim-wrap"><pre class="screen">bluestore_default_buffered_write = true</pre></div><p>
    To prevent writes to the WAL when using a fast device, such as SSD and
    NVMe, set:
   </p><div class="verbatim-wrap"><pre class="screen">prefer_deferred_size_ssd=0 (pre-deployment)</pre></div></section><section class="sect2" id="bluestore-alloc-size" data-id-title="BlueStore Allocation Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.7 </span><span class="title-name">BlueStore Allocation Size</span> <a title="Permalink" class="permalink" href="#bluestore-alloc-size">#</a></h3></div></div></div><div id="id-1.5.3.4.10.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     The following settings are not necessary for fresh deployments. Apply only
     to upgraded deployments or early SUSE Enterprise Storage 6 deployments as they may
     still benefit.
    </p></div><p>
    The following settings have been shown to slightly improve small object
    write performance under mixed workload conditions. Reducing the
    <code class="literal">alloc_size</code> to 4 kB helps reduce write amplification
    for small objects and with erasure coded pools of smaller objects. This
    change needs to be done before OSD deployment. If done after the fact, the
    OSDs will need to be re-deployed for it to take effect.
   </p><p>
    It is advised that spinning media continue to use 64 kB while SSD/NVMe
    are likely to benefit from setting to 4 kB.
   </p><div class="verbatim-wrap"><pre class="screen">min_alloc_size_ssd=4096
min_alloc_size_hdd=65536</pre></div><div id="id-1.5.3.4.10.8.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Setting the <code class="literal">alloc_size_ssd</code> to 64 kB may reduce
     maximum throughput capability of the OSD.
    </p></div></section><section class="sect2" id="bluestore-cache" data-id-title="BlueStore Cache"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.8 </span><span class="title-name">BlueStore Cache</span> <a title="Permalink" class="permalink" href="#bluestore-cache">#</a></h3></div></div></div><p>
    Increasing the BlueStore cache size can improve performance with many
    workloads. While FileStore OSDs cache data in the kernel's page cache,
    BlueStore OSDs cache data within the memory allocated by the OSD daemon
    itself. The OSD daemon will allocate memory up to its memory target (as
    controlled by the <code class="literal">osd_memory_target</code> parameter), and this
    determines the potential size of the BlueStore cache. The BlueStore
    cache is a read cache that by default is populated when objects are read.
    By setting the cache's minimum size higher than the default, it is
    guaranteed that the value specified will be the minimum cache available for
    each OSD. The idea is that more low-probability cache hits may occur.
   </p><p>
    The default <code class="literal">osd_memory_target</code> value is 4 GB: For
    example, each OSD daemon running on a node can be expected to consume that
    much memory. If a node's total RAM is significantly higher than
    <code class="literal">number of OSDs × 4 GB</code> and there are no other
    daemons running on the node, performance can be increased by increasing the
    value of <code class="literal">osd_memory_target</code>. This should be done with
    care to ensure that the operating system will still have enough memory for
    its needs, while leaving a safety margin.
   </p><p>
    If you want to ensure that the BlueStore cache will not fall below a
    certain minimum, use the <code class="literal">osd_memory_cache_min</code> parameter.
    Here is an example (the values are expressed in bytes):
   </p><div class="verbatim-wrap"><pre class="screen">osd_memory_target = 6442450944
osd_memory_cache_min = 4294967296</pre></div><div id="id-1.5.3.4.10.9.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     As a best practice, start with the full memory of the node. Deduct
     16 GB or 32 GB for the OS and then deduct appropriate amounts
     for any other workloads running on the node. For example, MDS cache if the
     MDS is colocated. Divide the remainder by the number of OSDs on that host.
     Ensure you leave room for improvement. For example:
    </p><div class="verbatim-wrap"><pre class="screen">(256 GB - 32 GB ) / 20 OSDs = 11,2 GB/OSD (max)</pre></div><p>
     Using this example, configure approximately 8 or 10 GB per OSD.
    </p></div><p>
    By default, BlueStore automatically tunes cache ratios between data and
    key-value data. In some cases it may be helpful to manually tune the ratios
    or even increase the cache size. There are several relevant counters for
    the cache:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_onode_hits</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_misses</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_shard_hits</code>
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_onode_shard_misses</code>
     </p></li></ul></div><p>
    If the misses are high, it is worth experimenting with increasing the cache
    settings or adjusting the ratios.
   </p><p>
    Adjusting the BlueStore cache size above default has the potential to
    improve performance of small-block workloads. This can be done globally by
    adjusting the <code class="option">_cache_size</code> value. By default, the cluster
    utilizes different values for HDD and SSD/NVMe devices. The best practice
    would be to increase the specific media cache you are interested in tuning:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_cache_size_hdd</code> (default 1073741824 -
      1 GB)
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_cache_size_ssd</code> (default 3221225472 -
      3 GB)
     </p></li></ul></div><div id="id-1.5.3.4.10.9.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the cache size parameters are adjusted and auto mode is utilized,
     <code class="option">osd_memory_target</code> should be adjusted to accomodate the
     OSD base RAM and cache allocation.
    </p></div><p>
    In some cases, manually tuning the cache allocation percentage may improve
    performance. This is achieved by modifying the disabling autotuning of the
    cache with this configuration line:
   </p><div class="verbatim-wrap"><pre class="screen">bluestore_cache_autotune=0</pre></div><p>
    Changing this value invalidates tuning of the
    <code class="option">osd_memory_cache_min</code> value.
   </p><p>
    The cache allocations are modified by adjusting the following:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">bluestore_cache_kv_ratio</code> (default .4)
     </p></li><li class="listitem"><p>
      <code class="literal">bluestore_cache_meta_ratio values</code> (default .4)
     </p></li></ul></div><p>
    Any unspecified portion is used for caching the objects themselves.
   </p></section></section><section class="sect1" id="tuning-rbd" data-id-title="RBD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.4 </span><span class="title-name">RBD</span> <a title="Permalink" class="permalink" href="#tuning-rbd">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.11.2" data-id-title="RBD Cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4.1 </span><span class="title-name">RBD Cluster</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.11.2">#</a></h3></div></div></div><p>
    As RBD is a native protocol, the tuning is directly related to OSD or
    general Ceph core options that are covered in previous sections.
   </p></section><section class="sect2" id="id-1.5.3.4.11.3" data-id-title="RBD Client"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4.2 </span><span class="title-name">RBD Client</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.11.3">#</a></h3></div></div></div><p>
    Read ahead cache defaults to 512 kB; test by tuning up and down on the
    client nodes.
   </p><div class="verbatim-wrap"><pre class="screen">echo {bytes} &gt; /sys/block/rbd0/queue/read_ahead_kb</pre></div><p>
    If your workload performs large sequential reads such as backup and
    restore, then this can make a significant difference in restore
    performance.
   </p></section></section><section class="sect1" id="tuning-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.5 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="#tuning-cephfs">#</a></h2></div></div></div><p>
   Most of the performance tuning covered in this section pertains to the
   CephFS Metadata Servers. Because CephFS is a native protocol, much of
   the performance tuning is handled at the operating system, OSD and
   BlueStore layers. Being a file system that is mounted by a client, there
   are some client options that are covered in the client section.
  </p><section class="sect2" id="id-1.5.3.4.12.3" data-id-title="MDS Tuning"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.1 </span><span class="title-name">MDS Tuning</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.12.3">#</a></h3></div></div></div><p>
    In filesystems with millions of files, there is some advantage to utilizing
    very low-latency media, such as NVMe, for the CephFS metadata pool.
   </p><p>
    Utlizing the <code class="command">ceph-daemon perf dump</code> command, there is a
    significant amount of data that can be examined for the Ceph Metadata
    Servers. It should be noted that the MDS perf counters only apply to
    metadata operations. The actual IO path is from clients straight to OSDs.
   </p><p>
    CephFS supports multiple metadata servers. These servers can operate in a
    multiple-active mode to provide load balancing of the metadata operation
    requests. To identify whether the MDS infrastructure is under-performing,
    one would examine the MDS data for request count and reply latencies. This
    should be done during an idle period on the cluster to form a baseline and
    then compared when under load. If the average time for reply latencies
    climbs too high, the MDS server needs to be examined further to identify
    whether the number of active metadata servers should be augmented, or
    whether simply increasing the metadata server cache may be sufficient. A
    sample of the output from the general MDS data for count and reply latency
    are below:
   </p><div class="verbatim-wrap"><pre class="screen">  "mds": {
    # request count, interesting to get a sense of MDS load
           "request": 0,
           "reply": 0,
    # reply and the latencies of replies can point to load issues
           "reply_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           }
          }</pre></div><p>
    Examining the <code class="literal">mds_mem</code> section of the output can help
    with understanding how the cache is utilized. High inode counters can
    indicate that a large number of files are open concurrently. This generally
    indicates that more memory may need to be provided to the MDS. If MDS
    memory cannot be increased, additional active MDS daemons should be
    deployed.
   </p><div class="verbatim-wrap"><pre class="screen">  "mds_mem": {

           "ino": 13,
           "ino+": 13,
           "ino-": 0,
           "dir": 12,
           "dir+": 12,
           "dir-": 0,
           "dn": 10,
           "dn+": 10,
           "dn-": 0,</pre></div><p>
    A high <code class="literal">cap</code> count can indicate misbehaving clients. For
    example, clients that do not hand back caps. This may indicate that some
    clients need to be upgraded to a more recent version, or that the client
    needs to be investigated for possible issues.
   </p><div class="verbatim-wrap"><pre class="screen">  "cap": 0,
  "cap+": 0,
  "cap-": 0,</pre></div><p>
    This final section shows memory utilization. The RSS value is the current
    memory size used. If this is roughly equal to the
    <code class="literal">mds_cache_memory_limit</code>, the MDS could probably use more
    memory.
   </p><div class="verbatim-wrap"><pre class="screen">  "rss": 41524,
  "heap": 314072
},</pre></div><p>
    Another important aspect of tuning a distributed file system is recognizing
    problematic workloads. The output values below provide some insight to what
    the MDS daemon is spending its time on. Each heading has the same three
    attributes as the <code class="literal">req_create_latency</code>. With this
    information, it may be possible to better tune the workloads.
   </p><div class="verbatim-wrap"><pre class="screen">  "mds_server": {
           "dispatch_client_request": 0,
           "dispatch_server_request": 0,
           "handle_client_request": 0,
           "handle_client_session": 0,
           "handle_slave_request": 0,
           "req_create_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           },
           "req_getattr_latency": {},
           "req_getfilelock_latency": {},
           "req_link_latency": {},
           "req_lookup_latency": {},
           "req_lookuphash_latency": {},
           "req_lookupino_latency": {},
           "req_lookupname_latency": {},
           "req_lookupparent_latency": {},
           "req_lookupsnap_latency": {},
           "req_lssnap_latency": {},
           "req_mkdir_latency": {},
           "req_mknod_latency": {},
           "req_mksnap_latency": {},
           "req_open_latency": {},
           "req_readdir_latency": {},
           "req_rename_latency": {},
           "req_renamesnap_latency": {},
           "req_rmdir_latency": {},
           "req_rmsnap_latency": {},
           "req_rmxattr_latency": {},
           "req_setattr_latency": {},
           "req_setdirlayout_latency": {},
           "req_setfilelock_latency": {},
           "req_setlayout_latency": {},
           "req_setxattr_latency": {},
           "req_symlink_latency": {},
           "req_unlink_latency": {},
       }</pre></div><p>
    Tuning the metadata server cache allows for more metadata operations to
    come from RAM, resulting in improved performance. The example below sets
    the cache to 16 GB.
   </p><div class="verbatim-wrap"><pre class="screen">mds_cache_memory_limit=17179869184</pre></div></section><section class="sect2" id="id-1.5.3.4.12.4" data-id-title="CephFS - Client"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.5.2 </span><span class="title-name">CephFS - Client</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.12.4">#</a></h3></div></div></div><p>
    From the client side, there are a number of performance affecting mount
    options that can be employed. It is important to understand the potential
    impact on the applications being utilized before employing these options.
   </p><p>
    The following mount options may be adjusted to improve performance, but we
    recommend that their impact is clearly understood prior to implementation
    in a production environment.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.3.4.12.4.4.1"><span class="term">noacl</span></dt><dd><p>
       Setting this mount option disables POSIX Access Control Lists for the
       CephFS mount, lowering potential metadata overhead.
      </p></dd><dt id="id-1.5.3.4.12.4.4.2"><span class="term">noatime</span></dt><dd><p>
       This option prevents the access time metadata for files from being
       updated.
      </p></dd><dt id="id-1.5.3.4.12.4.4.3"><span class="term">nodiratime</span></dt><dd><p>
       Setting this option prevents the metadata for access time of a directory
       from being updated.
      </p></dd><dt id="id-1.5.3.4.12.4.4.4"><span class="term">nocrc</span></dt><dd><p>
       This disables CephFS CRCs, thus relying on TCP Checksums for the
       correctness of the data to be verified.
      </p></dd><dt id="id-1.5.3.4.12.4.4.5"><span class="term">rasize</span></dt><dd><p>
       Setting a larger read-ahead for the mount may increase performance for
       large, sequential operations. Default is 8 MiB.
      </p></dd></dl></div></section></section><section class="sect1" id="tuning-rgw" data-id-title="RGW"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.6 </span><span class="title-name">RGW</span> <a title="Permalink" class="permalink" href="#tuning-rgw">#</a></h2></div></div></div><p>
   There are a large number of tunables for the Rados GateWay (RGW). These may
   be specific to the types of workloads being handled by the gateway and it
   may make sense to have different gateways handling distictly different
   workloads.
  </p><section class="sect2" id="id-1.5.3.4.13.3" data-id-title="Sharding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.1 </span><span class="title-name">Sharding</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.13.3">#</a></h3></div></div></div><p>
    The ideal situation is to understand how many total objects a bucket will
    host as this allows a bucket to be created with an appropriate number of
    shards at the outset. To gather information on bucket sharding, issue:
   </p><div class="verbatim-wrap"><pre class="screen">radosgw-admin bucket limit check</pre></div><p>
    The output of this command appears like the following format:
   </p><div class="verbatim-wrap"><pre class="screen">  "user_id": "myusername",
          "buckets": [
              {
                  "bucket": "mybucketname",
                  "tenant": "",
                  "num_objects": 611493,
                  "num_shards": 50,
                  "objects_per_shard": 12229,
                  "fill_status": "OK"
              }
          ]</pre></div><p>
    By default, Ceph reshards buckets to try and maintain reasonable
    performance. If it is known ahead of time how many shards a bucket may
    need, based on a ratio of 1 shard per 100 000 objects, it may be
    pre-sharded. This reduces contention and potential latency issues when
    resharding will occur. To pre-shard the bucket, it should be created and
    then submitted for sharding with the <code class="command">rgw-admin</code> command.
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">radosgw-admin bucket reshard --bucket={bucket name} --num-shards={prime number}</pre></div><p>
    Where the <code class="literal">num-shards</code> is a prime number. Each shard
    should represent about 100 000 objects.
   </p></section><section class="sect2" id="id-1.5.3.4.13.4" data-id-title="Limiting Bucket Listing Results"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.2 </span><span class="title-name">Limiting Bucket Listing Results</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.13.4">#</a></h3></div></div></div><p>
    If a process relies on listing the buckets on a frequent basis to iterate
    through results, yet only uses a small number of results for each iteration
    it is useful to set the <code class="literal">rgw_max_listing_results</code>
    parameter.
   </p></section><section class="sect2" id="id-1.5.3.4.13.5" data-id-title="Parallel I/O Requests"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.3 </span><span class="title-name">Parallel I/O Requests</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.13.5">#</a></h3></div></div></div><p>
    By default, the Object Gateway process is limited to eight simultaneous I/O
    operations for the index. This can be adjusted with the
    <code class="literal">rgw_bucket_index_max_aio</code> parameter.
   </p></section><section class="sect2" id="id-1.5.3.4.13.6" data-id-title="Window Size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.4 </span><span class="title-name">Window Size</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.13.6">#</a></h3></div></div></div><p>
    When working with larger objects, increasing the size of the Object Gateway windows
    for <code class="literal">put</code> and <code class="literal">get</code> can help with
    performance. Modify the following values in the Object Gateway section of the
    configuration:
   </p><div class="verbatim-wrap"><pre class="screen">rgw put obj min window size = [size in bytes, 16MiB default]
rgw get obj min window size = [size in bytes, 16MiB default]</pre></div></section><section class="sect2" id="id-1.5.3.4.13.7" data-id-title="Nagles Algorithm"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.5 </span><span class="title-name">Nagle's Algorithm</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.13.7">#</a></h3></div></div></div><p>
    Nagle's algorithm was introduced to maximize the use of buffers and attempt
    to reduce the number of small packets transmitted over the network. While
    this is helpful in lower bandwdith environments, it can represent a
    performance degredation in high-bandwidth environments. Disabling it from
    RGW nodes can improve performance. Including the following in the Ceph
    configuation RGW section:
   </p><div class="verbatim-wrap"><pre class="screen">tcp_nodelay=1</pre></div></section></section><section class="sect1" id="tuning-admin-usage" data-id-title="Administrative and Usage Choices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.7 </span><span class="title-name">Administrative and Usage Choices</span> <a title="Permalink" class="permalink" href="#tuning-admin-usage">#</a></h2></div></div></div><section class="sect2" id="id-1.5.3.4.14.2" data-id-title="Data Protection Schemes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.1 </span><span class="title-name">Data Protection Schemes</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.14.2">#</a></h3></div></div></div><p>
    The default replication setting keeps three total copies of every object
    written. The provides a high level of data protection by allowing up to two
    devices or nodes to fail while still protecting the data.
   </p><p>
    There are use cases where protecting the data is not important, but where
    performance is. In these cases, such as HPC scratch storage, it may be
    worthwhile to lower the replication count. This can be achieved by issuing
    a command such as:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd pool set rbd size 2</pre></div></section><section class="sect2" id="id-1.5.3.4.14.3" data-id-title="Erasure Coding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.2 </span><span class="title-name">Erasure Coding</span> <a title="Permalink" class="permalink" href="#id-1.5.3.4.14.3">#</a></h3></div></div></div><p>
    When using erasure coding, it is best to utilize optimized coding pool
    sizes. Experimental data suggests that the optimial pool sizes have either
    four or eight data chunks. It is also important to map this in relation to
    your failure domain model. If your cluster failure domain is at the node
    level, you will need at least <code class="literal">k+m</code> number of nodes.
    Similarly, if your failure domain it at the rack level, then your cluster
    needs to be spread over <code class="literal">k+m</code> racks. The key consideration
    is that distribution of the data in relation to the failure domain should
    be taken into consideration.
   </p><p>
    When using erasure coding schemes with failure domains larger than a single
    node, the use of Local Reconstruction Codes (LRC) may be beneficial due to
    lowered utilization of the network backbone, especially during failure and
    recovery scenatios.
   </p><p>
    There are particular use cases where erasure coding may even increase
    performance. These are mostly limited to large block (1 MB+)
    sequential read/write workloads. This is due to the parallelization of I/O
    requests that occurs when splitting objects into chunks to write to
    multiple OSDs.
   </p></section></section></section><section class="chapter" id="cha-ceph-tiered" data-id-title="Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Cache Tiering</span> <a title="Permalink" class="permalink" href="#cha-ceph-tiered">#</a></h2></div></div></div><p>
  A <span class="emphasis"><em>cache tier</em></span> is an additional storage layer implemented
  between the client and the standard storage. It is designed to speed up
  access to pools stored on slow hard disks and erasure coded pools.
 </p><p>
  Typically, cache tiering involves creating a pool of relatively fast storage
  devices (for example, SSD drives) configured to act as a cache tier, and a
  backing pool of slower and cheaper devices configured to act as a storage
  tier. The size of the cache pool is usually 10-20% of the storage pool.
 </p><section class="sect1" id="id-1.5.3.5.5" data-id-title="Tiered Storage Terminology"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Tiered Storage Terminology</span> <a title="Permalink" class="permalink" href="#id-1.5.3.5.5">#</a></h2></div></div></div><p>
   Cache tiering recognizes two types of pools: a <span class="emphasis"><em>cache
   pool</em></span> and a <span class="emphasis"><em>storage pool</em></span>.
  </p><div id="id-1.5.3.5.5.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    For general information on pools, see <span class="intraxref">Book “Administration Guide”, Chapter 22 “Managing Storage Pools”</span>.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.3.5.5.4.1"><span class="term">storage pool</span></dt><dd><p>
      Either a standard replicated pool that stores several copies of an object
      in the Ceph storage cluster, or an erasure coded pool (see
      <span class="intraxref">Book “Administration Guide”, Chapter 24 “Erasure Coded Pools”</span>).
     </p><p>
      The storage pool is sometimes referred to as 'backing' or 'cold' storage.
     </p></dd><dt id="id-1.5.3.5.5.4.2"><span class="term">cache pool</span></dt><dd><p>
      A standard replicated pool stored on a relatively small but fast storage
      device with their own ruleset in a CRUSH Map.
     </p><p>
      The cache pool is also referred to as 'hot' storage.
     </p></dd></dl></div></section><section class="sect1" id="sec-ceph-tiered-caution" data-id-title="Points to Consider"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Points to Consider</span> <a title="Permalink" class="permalink" href="#sec-ceph-tiered-caution">#</a></h2></div></div></div><p>
   Cache tiering may <span class="emphasis"><em>degrade</em></span> the cluster performance for
   specific workloads. The following points show some of its aspects that you
   need to consider:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="emphasis"><em>Workload-dependent</em></span>: Whether a cache will improve
     performance is dependent on the workload. Because there is a cost
     associated with moving objects into or out of the cache, it can be more
     effective when most of the requests touch a small number of objects. The
     cache pool should be large enough to capture the working set for your
     workload to avoid thrashing.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em>Difficult to benchmark</em></span>: Most performance benchmarks
     may show low performance with cache tiering. The reason is that they
     request a big set of objects, and it takes a long time for the cache to
     'warm up'.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em>Possibly low performance</em></span>: For workloads that are not
     suitable for cache tiering, performance is often slower than a normal
     replicated pool without cache tiering enabled.
    </p></li><li class="listitem"><p>
     <span class="emphasis"><em><code class="systemitem">librados</code> object enumeration</em></span>:
     If your application is using <code class="systemitem">librados</code> directly
     and relies on object enumeration, cache tiering may not work as expected.
     (This is not a problem for Object Gateway, RBD, or CephFS.)
    </p></li></ul></div></section><section class="sect1" id="id-1.5.3.5.7" data-id-title="When to Use Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.3 </span><span class="title-name">When to Use Cache Tiering</span> <a title="Permalink" class="permalink" href="#id-1.5.3.5.7">#</a></h2></div></div></div><p>
   Consider using cache tiering in the following cases:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Your erasure coded pools are stored on FileStore and you need to access
     them via RADOS Block Device. For more information on RBD, see
     <span class="intraxref">Book “Administration Guide”, Chapter 23 “RADOS Block Device”</span>.
    </p></li><li class="listitem"><p>
     Your erasure coded pools are stored on FileStore and you need to access
     them via iSCSI. For more information on iSCSI, refer to
     <span class="intraxref">Book “Administration Guide”, Chapter 27 “Ceph iSCSI Gateway”</span>.
    </p></li><li class="listitem"><p>
     You have a limited number of high-performance storage and a large
     collection of low-performance storage, and need to access the stored data
     faster.
    </p></li></ul></div></section><section class="sect1" id="sec-ceph-tiered-cachemodes" data-id-title="Cache Modes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.4 </span><span class="title-name">Cache Modes</span> <a title="Permalink" class="permalink" href="#sec-ceph-tiered-cachemodes">#</a></h2></div></div></div><p>
   The cache tiering agent handles the migration of data between the cache tier
   and the backing storage tier. Administrators have the ability to configure
   how this migration takes place. There are two main scenarios:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.3.5.8.3.1"><span class="term">write-back mode</span></dt><dd><p>
      In write-back mode, Ceph clients write data to the cache tier and
      receive an ACK from the cache tier. In time, the data written to the
      cache tier migrates to the storage tier and gets flushed from the cache
      tier. Conceptually, the cache tier is overlaid 'in front' of the backing
      storage tier. When a Ceph client needs data that resides in the storage
      tier, the cache tiering agent migrates the data to the cache tier on
      read, then it is sent to the Ceph client. Thereafter, the Ceph client
      can perform I/O using the cache tier, until the data becomes inactive.
      This is ideal for mutable, data such as photo or video editing, or
      transactional data.
     </p></dd><dt id="id-1.5.3.5.8.3.2"><span class="term">read-only mode</span></dt><dd><p>
      In read-only mode, Ceph clients write data directly to the backing
      tier. On read, Ceph copies the requested objects from the backing tier
      to the cache tier. Stale objects get removed from the cache tier based on
      the defined policy. This approach is ideal for immutable data such as
      presenting pictures or videos on a social network, DNA data, or X-ray
      imaging, because reading data from a cache pool that might contain
      out-of-date data provides weak consistency. Do not use read-only mode for
      mutable data.
     </p></dd></dl></div></section><section class="sect1" id="ceph-tier-erasure" data-id-title="Erasure Coded Pool and Cache Tiering"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.5 </span><span class="title-name">Erasure Coded Pool and Cache Tiering</span> <a title="Permalink" class="permalink" href="#ceph-tier-erasure">#</a></h2></div></div></div><p>
   Erasure coded pools require more resources than replicated pools. To
   overcome these limitations, we recommend to set a cache tier before the
   erasure coded pool. This is a requirement when using FileStore.
  </p><p>
   For example, if the <span class="quote">“<span class="quote">hot-storage</span>”</span> pool is made of fast storage,
   the <span class="quote">“<span class="quote">ecpool</span>”</span> created in
   <span class="intraxref">Book “Administration Guide”, Chapter 24 “Erasure Coded Pools”, Section 24.3 “Erasure Code Profiles”</span> can be speeded up with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier add ecpool hot-storage
<code class="prompt user">cephadm@adm &gt; </code>ceph osd tier cache-mode hot-storage writeback
<code class="prompt user">cephadm@adm &gt; </code>ceph osd tier set-overlay ecpool hot-storage</pre></div><p>
   This will place the <span class="quote">“<span class="quote">hot-storage</span>”</span> pool as a tier of ecpool in
   write-back mode so that every write and read to the ecpool is actually using
   the hot storage and benefits from its flexibility and speed.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>rbd --pool ecpool create --size 10 myvolume</pre></div><p>
   For more information about cache tiering, see
   <a class="xref" href="#cha-ceph-tiered" title="Chapter 7. Cache Tiering">Chapter 7, <em>Cache Tiering</em></a>.
  </p></section><section class="sect1" id="ses-tiered-storage" data-id-title="Setting Up an Example Tiered Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.6 </span><span class="title-name">Setting Up an Example Tiered Storage</span> <a title="Permalink" class="permalink" href="#ses-tiered-storage">#</a></h2></div></div></div><p>
   This section illustrates how to set up a fast SSD cache tier (hot storage)
   in front of a standard hard disk (cold storage).
  </p><div id="id-1.5.3.5.10.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    The following example is for illustration purposes only and includes a
    setup with one root and one rule for the SSD part residing on a single
    Ceph node.
   </p><p>
    In the production environment, cluster setups typically include more root
    and rule entries for the hot storage, and also mixed nodes, with both SSDs
    and SATA disks.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create two additional CRUSH rules, 'replicated_ssd' for the fast SSD
     caching device class and 'replicated_hdd' for the slower HDD device class:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule create-replicated replicated_ssd default host ssd
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rule create-replicated replicated_hdd default host hdd</pre></div></li><li class="step"><p>
     Switch all existing pools to the 'replicated_hdd' rule. This prevents
     Ceph from storing data to the newly added SSD devices:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> crush_rule replicated_hdd</pre></div></li><li class="step"><p>
     Turn the machine into a Ceph node using <code class="systemitem">ceph-salt</code>. Install the software
     and configure the host machine as described in
     <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.1 “Adding New Cluster Nodes”</span>. Let us assume that its name is
     <em class="replaceable">node-4</em>. This node needs to have 4 OSD disks.
    </p><div class="verbatim-wrap"><pre class="screen">[...]
host node-4 {
   id -5  # do not change unnecessarily
   # weight 0.012
   alg straw
   hash 0  # rjenkins1
   item osd.6 weight 0.003
   item osd.7 weight 0.003
   item osd.8 weight 0.003
   item osd.9 weight 0.003
}
[...]</pre></div></li><li class="step"><p>
     Edit the CRUSH map for the hot storage pool mapped to the OSDs backed by
     the fast SSD drives. Define a second hierarchy with a root node for the
     SSDs (as 'root ssd'). Additionally, change the weight and add a CRUSH rule
     for the SSDs. For more information on CRUSH Map, see
     <span class="intraxref">Book “Administration Guide”, Chapter 20 “Stored Data Management”, Section 20.5 “CRUSH Map Manipulation”</span>.
    </p><p>
     Edit the CRUSH Map directly with command line tools such as
     <code class="command">getcrushmap</code> and <code class="command">crushtool</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd crush rm-device-class osd.6 osd.7 osd.8 osd.9
<code class="prompt user">cephadm@adm &gt; </code>ceph osd crush set-device-class ssd osd.6 osd.7 osd.8 osd.9</pre></div></li><li class="step"><p>
     Create the hot storage pool to be used for cache tiering. Use the new
     'ssd' rule for it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create hot-storage 100 100 replicated ssd</pre></div></li><li class="step"><p>
     Create the cold storage pool using the default 'replicated_ruleset' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</pre></div></li><li class="step"><p>
     Then, setting up a cache tier involves associating a backing storage pool
     with a cache pool, in this case, cold storage (= storage pool) with hot
     storage (= cache pool):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier add cold-storage hot-storage</pre></div></li><li class="step"><p>
     To set the cache mode to 'writeback', execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier cache-mode hot-storage writeback</pre></div><p>
     For more information about cache modes, see
     <a class="xref" href="#sec-ceph-tiered-cachemodes" title="7.4. Cache Modes">Section 7.4, “Cache Modes”</a>.
    </p><p>
     Writeback cache tiers overlay the backing storage tier, so they require
     one additional step: you must direct all client traffic from the storage
     pool to the cache pool. To direct client traffic directly to the cache
     pool, execute the following, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd tier set-overlay cold-storage hot-storage</pre></div></li></ol></div></div></section><section class="sect1" id="cache-tier-configure" data-id-title="Configuring a Cache Tier"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.7 </span><span class="title-name">Configuring a Cache Tier</span> <a title="Permalink" class="permalink" href="#cache-tier-configure">#</a></h2></div></div></div><p>
   There are several options you can use to configure cache tiers. Use the
   following syntax:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> <em class="replaceable">key</em> <em class="replaceable">value</em></pre></div><section class="sect2" id="ses-tiered-hitset" data-id-title="Hit Set"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.7.1 </span><span class="title-name">Hit Set</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Hit set</em></span> parameters allow for tuning of <span class="emphasis"><em>cache
    pools</em></span>. Hit sets in Ceph are usually bloom filters and provide
    a memory-efficient way of tracking objects that are already in the cache
    pool.
   </p><p>
    The hit set is a bit array that is used to store the result of a set of
    hashing functions applied on object names. Initially, all bits are set to
    <code class="literal">0</code>. When an object is added to the hit set, its name is
    hashed and the result is mapped on different positions in the hit set,
    where the value of the bit is then set to <code class="literal">1</code>.
   </p><p>
    To find out whether an object exists in the cache, the object name is
    hashed again. If any bit is <code class="literal">0</code>, the object is definitely
    not in the cache and needs to be retrieved from cold storage.
   </p><p>
    It is possible that the results of different objects are stored in the same
    location of the hit set. By chance, all bits can be <code class="literal">1</code>
    without the object being in the cache. Therefore, hit sets working with a
    bloom filter can only tell whether an object is definitely not in the cache
    and needs to be retrieved from cold storage.
   </p><p>
    A cache pool can have more than one hit set tracking file access over time.
    The setting <code class="literal">hit_set_count</code> defines how many hit sets are
    being used, and <code class="literal">hit_set_period</code> defines for how long each
    hit set has been used. After the period has expired, the next hit set is
    used. If the number of hit sets is exhausted, the memory from the oldest
    hit set is freed and a new hit set is created. The values of
    <code class="literal">hit_set_count</code> and <code class="literal">hit_set_period</code>
    multiplied by each other define the overall time frame in which access to
    objects has been tracked.
   </p><div class="figure" id="ses-tiered-hitset-overview-bloom"><div class="figure-contents"><div class="mediaobject"><a href="images/bloom-filter.png" target="_blank"><img src="images/bloom-filter.png" width="" alt="Bloom Filter with 3 Stored Objects"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.1: </span><span class="title-name">Bloom Filter with 3 Stored Objects </span><a title="Permalink" class="permalink" href="#ses-tiered-hitset-overview-bloom">#</a></h6></div></div><p>
    Compared to the number of hashed objects, a hit set based on a bloom filter
    is very memory-efficient. Less than 10 bits are required to reduce the
    false positive probability below 1%. The false positive probability can be
    defined with <code class="literal">hit_set_fpp</code>. Based on the number of objects
    in a placement group and the false positive probability Ceph
    automatically calculates the size of the hit set.
   </p><p>
    The required storage on the cache pool can be limited with
    <code class="literal">min_write_recency_for_promote</code> and
    <code class="literal">min_read_recency_for_promote</code>. If the value is set to
    <code class="literal">0</code>, all objects are promoted to the cache pool as soon as
    they are read or written and this persists until they are evicted. Any
    value greater than <code class="literal">0</code> defines the number of hit sets
    ordered by age that are searched for the object. If the object is found in
    a hit set, it will be promoted to the cache pool. Keep in mind that backing
    up objects may also cause them to be promoted to the cache. A full backup
    with the value of '0' can cause all data to be promoted to the cache tier
    while active data gets removed from the cache tier. Therefore, changing
    this setting based on the backup strategy may be useful.
   </p><div id="id-1.5.3.5.11.4.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The longer the period and the higher the
     <code class="option">min_read_recency_for_promote</code> and
     <code class="option">min_write_recency_for_promote</code> values, the more RAM the
     <code class="systemitem">ceph-osd</code> daemon consumes. In
     particular, when the agent is active to flush or evict cache objects, all
     <code class="option">hit_set_count</code> hit sets are loaded into RAM.
    </p></div><section class="sect3" id="ceph-tier-gmt-hitset" data-id-title="Use GMT for Hit Set"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.7.1.1 </span><span class="title-name">Use GMT for Hit Set</span> <a title="Permalink" class="permalink" href="#ceph-tier-gmt-hitset">#</a></h4></div></div></div><p>
     Cache tier setups have a bloom filter called <span class="emphasis"><em>hit set</em></span>.
     The filter tests whether an object belongs to a set of either hot or cold
     objects. The objects are added to the hit set using time stamps appended
     to their names.
    </p><p>
     If cluster machines are placed in different time zones and the time stamps
     are derived from the local time, objects in a hit set can have misleading
     names consisting of future or past time stamps. In the worst case, objects
     may not exist in the hit set at all.
    </p><p>
     To prevent this, the <code class="option">use_gmt_hitset</code> defaults to '1' on a
     newly created cache tier setups. This way, you force OSDs to use GMT
     (Greenwich Mean Time) time stamps when creating the object names for the
     hit set.
    </p><div id="id-1.5.3.5.11.4.11.5" data-id-title="Leave the Default Value" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Leave the Default Value</h6><p>
      Do not touch the default value '1' of <code class="option">use_gmt_hitset</code>. If
      errors related to this option are not caused by your cluster setup, never
      change it manually. Otherwise, the cluster behavior may become
      unpredictable.
     </p></div></section></section><section class="sect2" id="id-1.5.3.5.11.5" data-id-title="Cache Sizing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.7.2 </span><span class="title-name">Cache Sizing</span> <a title="Permalink" class="permalink" href="#id-1.5.3.5.11.5">#</a></h3></div></div></div><p>
    The cache tiering agent performs two main functions:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.5.3.5.11.5.3.1"><span class="term">Flushing</span></dt><dd><p>
       The agent identifies modified (dirty) objects and forwards them to the
       storage pool for long-term storage.
      </p></dd><dt id="id-1.5.3.5.11.5.3.2"><span class="term">Evicting</span></dt><dd><p>
       The agent identifies objects that have not been modified (clean) and
       evicts the least recently used among them from the cache.
      </p></dd></dl></div><section class="sect3" id="cache-tier-config-absizing" data-id-title="Absolute Sizing"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.7.2.1 </span><span class="title-name">Absolute Sizing</span> <a title="Permalink" class="permalink" href="#cache-tier-config-absizing">#</a></h4></div></div></div><p>
     The cache tiering agent can flush or evict objects based on the total
     number of bytes or the total number of objects. To specify a maximum
     number of bytes, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> target_max_bytes <em class="replaceable">num_of_bytes</em></pre></div><p>
     To specify the maximum number of objects, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> target_max_objects <em class="replaceable">num_of_objects</em></pre></div><div id="id-1.5.3.5.11.5.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph is not able to determine the size of a cache pool automatically,
      therefore configuration of the absolute size is required here. Otherwise,
      flush and evict will not work. If you specify both limits, the cache
      tiering agent will begin flushing or evicting when either threshold is
      triggered.
     </p></div><div id="id-1.5.3.5.11.5.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      All client requests will be blocked only when
      <code class="option">target_max_bytes</code> or <code class="option">target_max_objects</code>
      is reached.
     </p></div></section><section class="sect3" id="cache-tier-config-relsizing" data-id-title="Relative Sizing"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.7.2.2 </span><span class="title-name">Relative Sizing</span> <a title="Permalink" class="permalink" href="#cache-tier-config-relsizing">#</a></h4></div></div></div><p>
     The cache tiering agent can flush or evict objects relative to the size of
     the cache pool (specified by <code class="option">target_max_bytes</code> or
     <code class="option">target_max_objects</code> in
     <a class="xref" href="#cache-tier-config-absizing" title="7.7.2.1. Absolute Sizing">Section 7.7.2.1, “Absolute Sizing”</a>). When the cache pool
     consists of a certain percentage of modified (dirty) objects, the cache
     tiering agent will flush them to the storage pool. To set the
     <code class="option">cache_target_dirty_ratio</code>, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_dirty_ratio <em class="replaceable">0.0...1.0</em></pre></div><p>
     For example, setting the value to 0.4 will begin flushing modified (dirty)
     objects when they reach 40% of the cache pool's capacity:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</pre></div><p>
     When the dirty objects reach a certain percentage of the capacity, flush
     them at a higher speed. Use
     <code class="option">cache_target_dirty_high_ratio</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_dirty_high_ratio <em class="replaceable">0.0..1.0</em></pre></div><p>
     When the cache pool reaches a certain percentage of its capacity, the
     cache tiering agent will evict objects to maintain free capacity. To set
     the <code class="option">cache_target_full_ratio</code>, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_target_full_ratio <em class="replaceable">0.0..1.0</em></pre></div></section></section><section class="sect2" id="id-1.5.3.5.11.6" data-id-title="Cache Age"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.7.3 </span><span class="title-name">Cache Age</span> <a title="Permalink" class="permalink" href="#id-1.5.3.5.11.6">#</a></h3></div></div></div><p>
    You can specify the minimum age of a recently modified (dirty) object
    before the cache tiering agent flushes it to the backing storage pool. Note
    that this will only apply if the cache actually needs to flush/evict
    objects:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_min_flush_age <em class="replaceable">num_of_seconds</em></pre></div><p>
    You can specify the minimum age of an object before it will be evicted from
    the cache tier:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">cachepool</em> cache_min_evict_age <em class="replaceable">num_of_seconds</em></pre></div></section><section class="sect2" id="ses-tiered-hitset-examples" data-id-title="Examples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.7.4 </span><span class="title-name">Examples</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples">#</a></h3></div></div></div><section class="sect3" id="ses-tiered-hitset-examples-memory" data-id-title="Large Cache Pool and Small Memory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.7.4.1 </span><span class="title-name">Large Cache Pool and Small Memory</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples-memory">#</a></h4></div></div></div><p>
     If lots of storage and only a small amount of RAM is available, all
     objects can be promoted to the cache pool as soon as they are accessed.
     The hit set is kept small. The following is a set of example configuration
     values:
    </p><div class="verbatim-wrap"><pre class="screen">hit_set_count = 1
hit_set_period = 3600
hit_set_fpp = 0.05
min_write_recency_for_promote = 0
min_read_recency_for_promote = 0</pre></div></section><section class="sect3" id="ses-tiered-hitset-examples-storage" data-id-title="Small Cache Pool and Large Memory"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.7.4.2 </span><span class="title-name">Small Cache Pool and Large Memory</span> <a title="Permalink" class="permalink" href="#ses-tiered-hitset-examples-storage">#</a></h4></div></div></div><p>
     If a small amount of storage but a comparably large amount of memory is
     available, the cache tier can be configured to promote a limited number of
     objects into the cache pool. Twelve hit sets, of which each is used over a
     period of 14,400 seconds, provide tracking for a total of 48 hours. If an
     object has been accessed in the last 8 hours, it is promoted to the cache
     pool. The set of example configuration values then is:
    </p><div class="verbatim-wrap"><pre class="screen">hit_set_count = 12
hit_set_period = 14400
hit_set_fpp = 0.01
min_write_recency_for_promote = 2
min_read_recency_for_promote = 2</pre></div></section></section></section></section><section class="chapter" id="lvmcache" data-id-title="Improving Performance with LVM cache"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Improving Performance with LVM cache</span> <a title="Permalink" class="permalink" href="#lvmcache">#</a></h2></div></div></div><div id="id-1.5.3.6.3" data-id-title="Technology Preview" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Technology Preview</h6><p>
   LVM cache is currently a technology preview.
  </p></div><p>
  LVM cache is a caching mechanism used to improve the performance of a
  logical volume (LV). Typically, a smaller and faster device is used to
  improve I/O performance of a larger and slower LV. Refer to its manual page
  (<code class="command">man 7 lvmcache</code>) to find more details about LVM cache.
 </p><p>
  In SUSE Enterprise Storage, LVM cache can improve the performance of OSDs. Support for
  LVM cache is provided via a <code class="command">ceph-volume</code> plugin. You can
  find detailed information about its usage by running <code class="command">ceph-volume
  lvmcache</code>.
 </p><section class="sect1" id="lvmcache-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#lvmcache-prerequisites">#</a></h2></div></div></div><p>
   To use LVM cache features to improve the performance of a Ceph cluster,
   you need to have:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A running Ceph cluster in a stable state ('HEALTH_OK').
    </p></li><li class="listitem"><p>
     OSDs deployed with BlueStore and LVM. This is the default if the OSDs
     were deployed using SUSE Enterprise Storage 6 or later.
    </p></li><li class="listitem"><p>
     Empty disks or partitions that will be used for caching.
    </p></li></ul></div></section><section class="sect1" id="lvmcache-planning" data-id-title="Points to Consider"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.2 </span><span class="title-name">Points to Consider</span> <a title="Permalink" class="permalink" href="#lvmcache-planning">#</a></h2></div></div></div><p>
   Consider the following points before configuring your OSDs to use
   LVM cache:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <span class="bold"><strong>Verify that LVM cache is suitable for your use
     case.</strong></span> If you have only a few fast drives available that are
     <span class="bold"><strong>not</strong></span> used for OSDs, the general
     recommendation is to use them as WAL/DB devices for the OSDs. In such a
     case, WAL and DB operations (small and rare operations) are applied on the
     fast drive while data operations are applied on the slower OSD drive.
    </p><div id="id-1.5.3.6.7.3.1.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If latency is more important for your deployment than IOPS or throughput,
      you can use the fast drives as LVM cache rather than WAL/DB partitions.
     </p></div></li><li class="listitem"><p>
     If you plan to use a fast drive as an LVM cache for multiple OSDs, be
     aware that <span class="bold"><strong>all OSD operations (including
     replication) will go through the caching device</strong></span>. All reads will
     be queried from the caching device, and are only served from the slow
     device in case of a cache miss. Writes are always applied to the caching
     device first, and are flushed to the slow device at a later time
     ('writeback' is the default caching mode).
    </p><p>
     When deciding whether to utilize an LVM cache, verify whether the fast
     drive can serve as a front for multiple OSDs while still providing an
     acceptable amount of IOPS. You can test it by measuring the maximum amount
     of IOPS that the fast device can serve, and then dividing the result by
     the number of OSDs behind the fast device. If the result is lower or close
     to the maximum amount of IOPS that the OSD can provide without the cache,
     LVM cache is probably not suited for this setup.
    </p></li><li class="listitem"><p>
     <span class="bold"><strong>The interaction of the LVM cache device with OSDs
     is important.</strong></span> Writes are periodically flushed from the caching
     device to the slow device. If the incoming traffic is sustained and
     significant, the caching device will struggle to keep up with incoming
     requests as well as the flushing process, resulting in performance drop.
     Unless the fast device can provide much more IOPS with better latency than
     the slow device, do not use LVM cache with a sustained high volume
     workload. Traffic in a burst pattern is more suited for LVM cache as it
     gives the cache time to flush its dirty data without interfering with
     client traffic. For a sustained low traffic workload, it is difficult to
     guess in advance whether using LVM cache will improve performance. The
     best test is to benchmark and compare the LVM cache setup against the
     WAL/DB setup. Moreover, as small writes are heavy on the WAL partition, it
     is suggested to use the fast device for the DB and/or WAL instead of an
     LVM cache.
    </p></li><li class="listitem"><p>
     If you are not sure whether to use LVM cache, use the fast device as a
     WAL and/or DB device.
    </p></li></ul></div></section><section class="sect1" id="lvmcache-preparation" data-id-title="Preparation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.3 </span><span class="title-name">Preparation</span> <a title="Permalink" class="permalink" href="#lvmcache-preparation">#</a></h2></div></div></div><p>
   You need to split the fast device into multiple partitions. Each OSD needs
   two partitions for its cache: one for the cache data, and one for the cache
   metadata. The minimum size of either partition is 2 GB. You can use a single
   fast device to cache multiple OSDs. It simply needs to be partitioned
   accordingly.
  </p></section><section class="sect1" id="lvmcache-configuring" data-id-title="Configuring LVM cache"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.4 </span><span class="title-name">Configuring LVM cache</span> <a title="Permalink" class="permalink" href="#lvmcache-configuring">#</a></h2></div></div></div><p>
   You can find detailed information about adding, removing, and configuring
   LVM cache by running the <code class="command">ceph-volume lvmcache</code> command.
  </p><section class="sect2" id="lvmcache-adding" data-id-title="Adding LVM cache"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.1 </span><span class="title-name">Adding LVM cache</span> <a title="Permalink" class="permalink" href="#lvmcache-adding">#</a></h3></div></div></div><p>
    To add LVM cache to an existing OSD, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvmcache add
 --cachemetadata <em class="replaceable">METADATA-PARTITION</em>
 --cachedata <em class="replaceable">DATA-PARTITION</em>
 --osd-id <em class="replaceable">OSD-ID</em></pre></div><p>
    The optional <code class="option">--data</code>, <code class="option">--db</code> or
    <code class="option">--wal</code> specifies which partition to cache. Default is
    <code class="option">--data</code>.
   </p><div id="id-1.5.3.6.9.3.5" data-id-title="Specify Logical Volume (LV)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Specify Logical Volume (LV)</h6><p>
     Alternatively, you can use the <code class="option">--origin</code> instead of the
     <code class="option">--osd-id</code> option to specify which LV to cache:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
--origin <em class="replaceable">VOLUME-GROUP</em>/<em class="replaceable">LOGICAL-VOLUME</em></pre></div></div></section><section class="sect2" id="lvmcache-removing" data-id-title="Removing LVM cache"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.2 </span><span class="title-name">Removing LVM cache</span> <a title="Permalink" class="permalink" href="#lvmcache-removing">#</a></h3></div></div></div><p>
    To remove existing LVM cache from an OSD, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvmcache rm --osd-id <em class="replaceable">OSD-ID</em></pre></div></section><section class="sect2" id="lvmcache-mode" data-id-title="Setting LVM cache Mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.4.3 </span><span class="title-name">Setting LVM cache Mode</span> <a title="Permalink" class="permalink" href="#lvmcache-mode">#</a></h3></div></div></div><p>
    To specify caching mode, use the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>ceph-volume lvmcache mode --set <em class="replaceable">CACHING-MODE</em> --osd-id <em class="replaceable">OSD-ID</em></pre></div><p>
    <em class="replaceable">CACHING-MODE</em> is either 'writeback' (default) or
    'writethrough'
   </p></section></section><section class="sect1" id="lvmcache-failures" data-id-title="Handling Failures"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.5 </span><span class="title-name">Handling Failures</span> <a title="Permalink" class="permalink" href="#lvmcache-failures">#</a></h2></div></div></div><p>
   If the caching device fails, all OSDs behind the caching device need to be
   removed from the cluster (see <span class="intraxref">Book “Administration Guide”, Chapter 2 “Salt Cluster Administration”, Section 2.7 “Removing an OSD”</span>), purged,
   and redeployed. If the OSD drive fails, the OSD's LV as well as its cache's
   LV will be active but not functioning. Use <code class="command">pvremove
   <em class="replaceable">PARTITION</em></code> to purge the partitions
   (physical volumes) used for the OSD's cache data and metadata partitions.
   You can use <code class="command">pvs</code> to list all physical volumes.
  </p></section><section class="sect1" id="lvmcache-faq" data-id-title="Frequently Asked Questions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.6 </span><span class="title-name">Frequently Asked Questions</span> <a title="Permalink" class="permalink" href="#lvmcache-faq">#</a></h2></div></div></div><div class="qandaset" id="id-1.5.3.6.11.2"><div class="free-id" id="id-1.5.3.6.11.2.1"> </div><dl class="qandaentry"><dt class="question" id="id-1.5.3.6.11.2.1.1"><strong>Q: 1.</strong>
      What happens if an OSD is removed?
     </dt><dd class="answer" id="id-1.5.3.6.11.2.1.2"><p>
      When removing the OSD's LV using <code class="command">lvremove</code>, the cache
      LVs will be removed as well. However, you will still need to call
      <code class="command">pvremove</code> on the partitions to make sure all labels
      have been wiped out.
     </p></dd></dl><div class="free-id" id="id-1.5.3.6.11.2.2"> </div><dl class="qandaentry"><dt class="question" id="id-1.5.3.6.11.2.2.1"><strong>Q: 2.</strong>
      What happens if the OSD is zapped using <code class="command">ceph-volume
      zap</code>?
     </dt><dd class="answer" id="id-1.5.3.6.11.2.2.2"><p>
      The same answer applies as to the question <span class="bold"><strong>What
      happens if an OSD is removed?</strong></span>
     </p></dd></dl><div class="free-id" id="id-1.5.3.6.11.2.3"> </div><dl class="qandaentry"><dt class="question" id="id-1.5.3.6.11.2.3.1"><strong>Q: 3.</strong>
      What happens if the origin drive fails?
     </dt><dd class="answer" id="id-1.5.3.6.11.2.3.2"><p>
      The cache LVs still exist and <code class="command">cache info</code> still shows
      them as being available. You will not be able to uncache because LVM will
      fail to flush the cache as the origin LV's device is gone. The situation
      now is that the origin LV exists, but its backing device does not. You
      can fix it by using the <code class="command">pvs</code> command and locating the
      devices that are associated with the origin LV. You can then remove them
      using
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>sudo pvremove /dev/<em class="replaceable">DEVICE</em> or <em class="replaceable">PARTITION</em></pre></div><p>
      You can do the same for the cache partitions. This procedure will make
      the origin LV as well as the cache LVs disappear. You can also use
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@osd &gt; </code>sudo dd if=/dev/zero of=/dev/<em class="replaceable">DEVICE</em> or <em class="replaceable">PARTITION</em></pre></div><p>
      to wipe them out before using <code class="command">pvremove</code>.
     </p></dd></dl></div></section></section></div><section class="appendix" id="tuning-appendix-a" data-id-title="Salt State for Kernel Tuning"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Salt State for Kernel Tuning</span> <a title="Permalink" class="permalink" href="#tuning-appendix-a">#</a></h1></div></div></div><p>
  There are a significant number of items that can be tuned to provide improved
  performance of a Ceph cluster. While the material provided in previous
  sections attempts to showcase important items, it should not be considered
  exhaustive. It is highly recommended that use of the information contained
  herein be done so with a scientific approach to understanding the performance
  needs and outcomes of specific tuning in comparison with baseline
  performance.
 </p><p>
  To utilize this Salt state follow these steps:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Create directory named <code class="filename">my_kerntune</code> in
    <code class="filename">/srv/salt/</code>.
   </p></li><li class="step"><p>
    Create <code class="filename">/srv/salt/my_kerntune/init.sls</code> with the
    following contents:
   </p><div class="verbatim-wrap"><pre class="screen">  my_kern_tune:
    file.replace:
      - name: /etc/default/grub
      - pattern: showopts.*
      - repl: showopts intel_idle.max_cstate=0 processor.max_cstate=0 idle=poll scsi_mod.use_blk_mq=1 nospec spectre_v2=off pti=off spec_store_bypass_disable=off l1tf=off"

  grub2_mkconfig:
    cmd.run:
      - runas: root
      - name: grub2-mkconfig -o /boot/grub2/grub.cfg
      - onchanges:
        - file: my_kern_tune</pre></div></li><li class="step"><p>
    Issue the following command to set the state:
   </p><div class="verbatim-wrap"><pre class="screen">salt '*' state.apply my_kerntune</pre></div></li><li class="step"><p>
    Reboot the nodes.
   </p></li></ol></div></div><div id="id-1.5.4.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
   Verify the <code class="command">grub</code> kernel command line and ensure the
   pattern match specified by the pattern: parameter is appropriate. As is,
   this will overwrite anything after the <code class="option">showopts</code> kernel
   command line argument.
  </p></div></section><section class="appendix" id="tuning-appendix-b" data-id-title="Ring Buffer Max Value Script"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">B </span><span class="title-name">Ring Buffer Max Value Script</span> <a title="Permalink" class="permalink" href="#tuning-appendix-b">#</a></h1></div></div></div><p>
  This script is intended to be run on a single host at a time. It will set all
  interfaces on the host to the maximum ring buffer values. It may be used with
  some sort of orchestration, for example Salt, to apply to all hosts in a
  cluster.
 </p><div class="verbatim-wrap"><pre class="screen"> for i in `ls /etc/sysconfig/network/ifcfg-*`;do
   nicname=`echo $i|cut -f2 -d"-"`
   echo nicname=$nicname
   if [ `ethtool -g $nicname 2&gt;/dev/null |wc -l ` -gt 6 ]; then
     ethtoolcmd="-G $nicname rx `ethtool -g $nicname|head -6|grep RX:|cut -f2 -d":"|xargs` tx `ethtool -g $nicname|head -6|grep TX:|cut -f2 -d":"|xargs`"
     echo ethtoolcmd=$ethtoolcmd
     sed -i "s/ETHTOOL_OPTIONS=''/ETHTOOL_OPTIONS='$ethtoolcmd'/g" $i
   fi
   done</pre></div></section><section class="appendix" id="tuning-appendix-c" data-id-title="Network Tuning"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">C </span><span class="title-name">Network Tuning</span> <a title="Permalink" class="permalink" href="#tuning-appendix-c">#</a></h1></div></div></div><p>
  This script uses Salt to apply a number of settings to a cluster. This was
  used during testing so that parameters could be rapidly adjusted. It is
  intended to be an example only and is not intended for use without
  modification.
 </p><div class="verbatim-wrap"><pre class="screen"> #!/bin/bash
 #These commands tune the write size of the pcie interface for the NIC cards.
 salt '*' cmd.run 'setpci -s 5b:00.0 68.w=5936'
 salt '*' cmd.run 'setpci -s 5b:00.1 68.w=5936'

 #Set Jumbo Frames
 salt '*' cmd.run 'ip link set bond0 mtu 9000'

 #Set the rx queue for rx-0 to use all CPU cores local to the device.
 salt '*' cmd.run 'for j in `cat /sys/class/net/bond0/bonding/slaves`;do LOCAL_CPUS=`cat /sys/class/net/$j/device/local_cpus`;echo $LOCAL_CPUS &gt; /sys/class/net/$j/queues/rx-0/rps_cpus;done'

 #Set send and recieve buffers for both network interfaces involved in the bond
 salt '*' cmd.run 'ethtool -G eth4 rx 8192 tx 8192'
 salt '*' cmd.run 'ethtool -G eth5 rx 8192 tx 8192'

 #Ensure SACK is on
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_sack=1'
 #Ensure timestamps are on to prevent possible drops
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_timestamps=1'

 #Set the max conections to 2048
 salt '*' cmd.run 'sysctl -w net.core.somaxconn=2048'

 #Set TCP to low latency
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_low_latency=1'
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_fastopen=1'

 #Set min, default, and max send and receive buffers
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_rmem="10240 87380 2147483647"'
 salt '*' cmd.run 'sysctl -w net.ipv4.tcp_wmem="10240 87380 2147483647"'

 salt '*' cmd.run 'sysctl -w net.core.netdev_max_backlog=250000'</pre></div></section><section class="appendix" id="id-1.5.7" data-id-title="Ceph Maintenance Updates Based on Upstream Nautilus Point Releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">D </span><span class="title-name">Ceph Maintenance Updates Based on Upstream 'Nautilus' Point Releases</span> <a title="Permalink" class="permalink" href="#id-1.5.7">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 6 are based on the
  Nautilus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Nautilus series, SUSE Enterprise Storage 6 is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.5"><span class="name">Nautilus 14.2.20 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephadm@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.12"><span class="name">Nautilus 14.2.18 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.12">#</a></h2></div><p>
  This release fixes a regression introduced in 14.2.17 in which the manager
  module tries to use a couple of Python modules that do not exist in some
  environments.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    This release fixes issues loading the dashboard and volumes manager modules
    in some environments.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.15"><span class="name">Nautilus 14.2.17 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.15">#</a></h2></div><p>
  This release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="varname">$pid</code> expansion in configuration paths such as
    <code class="literal">admin_socket</code> will now properly expand to the daemon PID
    for commands like <code class="command">ceph-mds</code> or
    <code class="command">ceph-osd</code>. Previously, only <code class="command">ceph-fuse</code>
    and <code class="command">rbd-nbd</code> expanded <code class="varname">$pid</code> with the
    actual daemon PID.
   </p></li><li class="listitem"><p>
    RADOS: PG removal has been optimized.
   </p></li><li class="listitem"><p>
    RADOS: Memory allocations are tracked in finer detail in BlueStore and
    displayed as a part of the <code class="command">dump_mempools</code> command.
   </p></li><li class="listitem"><p>
    CephFS: clients which acquire capabilities too quickly are throttled to
    prevent instability. See new config option
    <code class="option">mds_session_cap_acquisition_throttle</code> to control this
    behavior.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.18"><span class="name">Nautilus 14.2.16 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-27781 : OpenStack Manila use of
    <code class="command">ceph_volume_client.py</code> library allowed tenant access to
    any Ceph credentials' secret.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.21"><span class="name">Nautilus 14.2.15 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.21">#</a></h2></div><p>
  This release fixes a ceph-volume regression introduced in v14.2.13 and
  includes few other fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    ceph-volume: Fixes <code class="command">lvm batch –auto</code>, which breaks
    backward compatibility when using non rotational devices only (SSD and/or
    NVMe).
   </p></li><li class="listitem"><p>
    BlueStore: Fixes a bug in <code class="literal">collection_list_legacy</code> which
    makes PGs inconsistent during scrub when running OSDs older than 14.2.12
    with newer ones.
   </p></li><li class="listitem"><p>
    MGR: progress module can now be turned on or off, using the commands
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.24"><span class="name">Nautilus 14.2.14 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.24">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus, among other fixes across components.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE 2020-25660: Fix a regression in Messenger V2 replay attacks.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.27"><span class="name">Nautilus 14.2.13 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.27">#</a></h2></div><p>
  This release fixes a regression introduced in v14.2.12, and a few ceph-volume
  amd RGW fixes.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a regression that caused breakage in clusters that referred to
    ceph-mon hosts using dns names instead of IP addresses in the
    <code class="option">mon_host</code> parameter in <code class="filename">ceph.conf</code>.
   </p></li><li class="listitem"><p>
    ceph-volume: the <code class="command">lvm batch</code> subcommand received a major
    rewrite.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.30"><span class="name">Nautilus 14.2.12 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.30">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="command">ceph df command</code> now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    MONs now have a config option <code class="option">mon_osd_warn_num_repaired</code>,
    10 by default. If any OSD has repaired more than this many I/O errors in
    stored data, a <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is
    generated. In order to allow clearing of the warning, a new command
    <code class="command">ceph tell osd.<em class="replaceable">SERVICE_ID</em>
    clear_shards_repaired <em class="replaceable">COUNT</em></code> has been
    added. By default, it will set the repair count to 0. If you want to be
    warned again if additional repairs are performed, you can provide a value
    to the command and specify the value of
    <code class="option">mon_osd_warn_num_repaired</code>. This command will be replaced
    in future releases by the health mute/unmute feature.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial MON to contact for Ceph tools
    and daemons using the <code class="option">mon_host_override config</code> option or
    <code class="option">--mon-host-override <em class="replaceable">IP</em></code>
    command-line switch. This generally should only be used for debugging and
    only affects initial communication with Ceph’s MON cluster.
   </p></li><li class="listitem"><p>
    Fix an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.33"><span class="name">Nautilus 14.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.33">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    RGW: The <code class="command">radosgw-admin</code> sub-commands dealing with orphans
    – <code class="command">radosgw-admin orphans find</code>, <code class="command">radosgw-admin
    orphans finish</code>, <code class="command">radosgw-admin orphans
    list-jobs</code> – have been deprecated. They have not been actively
    maintained and they store intermediate results on the cluster, which could
    fill a nearly-full cluster. They have been replaced by a tool, currently
    considered experimental, <code class="command">rgw-orphan-list</code>.
   </p></li><li class="listitem"><p>
    Now, when <code class="option">noscrub</code> and/or <code class="option">nodeep-scrub</code>
    flags are set globally or per pool, scheduled scrubs of the type disabled
    will be aborted. All user initiated scrubs are <span class="emphasis"><em>not</em></span>
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed a ceph-osd crash in committed OSD maps when there is a failure to
    encode the first incremental map.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.36"><span class="name">Nautilus 14.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.36">#</a></h2></div><p>
  This upstream release patched one security flaw:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li></ul></div><p>
  In addition to security flaws, this major upstream release brought a number
  of notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The pool parameter <code class="option">target_size_ratio</code>, used by the PG
    autoscaler, has changed meaning. It is now normalized across pools, rather
    than specifying an absolute ratio. If you have set target size ratios on
    any pools, you may want to set these pools to autoscale
    <code class="literal">warn</code> mode to avoid data movement during the upgrade:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_autoscale_mode warn</pre></div></li><li class="listitem"><p>
    The behaviour of the <code class="option">-o</code> argument to the RADOS tool has
    been reverted to its original behaviour of indicating an output file. This
    reverts it to a more consistent behaviour when compared to other tools.
    Specifying object size is now accomplished by using an upper case O
    <code class="option">-O</code>.
   </p></li><li class="listitem"><p>
    The format of MDSs in <code class="command">ceph fs dump</code> has changed.
   </p></li><li class="listitem"><p>
    Ceph will issue a health warning if a RADOS pool’s
    <code class="literal">size</code> is set to 1 or, in other words, the pool is
    configured with no redundancy. This can be fixed by setting the pool size
    to the minimum recommended value with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">pool-name</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The warning can be silenced with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    RGW: bucket listing performance on sharded bucket indexes has been notably
    improved by heuristically – and significantly, in many cases – reducing the
    number of entries requested from each bucket index shard.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.41"><span class="name">Nautilus 14.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.41">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS due to RGW GetObject header-splitting
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.389+gb0f23ac248.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.45"><span class="name">Nautilus 14.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.45">#</a></h2></div><p>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The default value of <code class="option">bluestore_min_alloc_size_ssd</code> has been
    changed to 4K to improve performance across all workloads.
   </p></li><li class="listitem"><p>
    The following OSD memory config options related to BlueStore cache
    autotuning can now be configured during runtime:
   </p><div class="verbatim-wrap"><pre class="screen">osd_memory_base (default: 768 MB)
osd_memory_cache_min (default: 128 MB)
osd_memory_expected_fragmentation (default: 0.15)
osd_memory_target (default: 4 GB)</pre></div><p>
    You can set the above options by running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set osd <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div></li><li class="listitem"><p>
    The Ceph Manager now accepts <code class="literal">profile rbd</code> and <code class="literal">profile
    rbd-read-only</code> user capabilities. You can use these capabilities
    to provide users access to MGR-based RBD functionality such as <code class="literal">rbd
    perf image iostat</code> and <code class="literal">rbd perf image iotop</code>.
   </p></li><li class="listitem"><p>
    The configuration value <code class="option">osd_calc_pg_upmaps_max_stddev</code> used
    for upmap balancing has been removed. Instead, use the Ceph Manager balancer
    configuration option <code class="option">upmap_max_deviation</code> which now is an
    integer number of PGs of deviation from the target PGs per OSD. You can set
    it with a following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/balancer/upmap_max_deviation 2</pre></div><p>
    The default <code class="option">upmap_max_deviation</code> is 5. There are situations
    where crush rules would not allow a pool to ever have completely balanced
    PGs. For example, if crush requires 1 replica on each of 3 racks, but there
    are fewer OSDs in 1 of the racks. In those cases, the configuration value
    can be increased.
   </p></li><li class="listitem"><p>
    CephFS: multiple active Metadata Server forward scrub is now rejected. Scrub is
    currently only permitted on a file system with a single rank. Reduce the
    ranks to one via <code class="command">ceph fs set <em class="replaceable">FS_NAME</em>
    max_mds 1</code>.
   </p></li><li class="listitem"><p>
    Ceph will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. This can be fixed
    by adjusting the pool to an adjacent power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">NEW_PG_NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.48"><span class="name">Nautilus 14.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.48">#</a></h2></div><p>
  This upstream release patched two security flaws:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1699: a path traversal flaw in Ceph Dashboard that could allow for
    potential information disclosure.
   </p></li><li class="listitem"><p>
    CVE-2020-1700: a flaw in the RGW beast front-end that could lead to denial
    of service from an unauthenticated client.
   </p></li></ul></div><p>
  In SES 6, these flaws were patched in Ceph version 14.2.5.382+g8881d33957b.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.52"><span class="name">Nautilus 14.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.52">#</a></h2></div><p>
  This release fixed a Ceph Manager bug that caused MGRs becoming unresponsive on
  larger clusters. SES users were never exposed to the bug.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.54"><span class="name">Nautilus 14.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.54">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Health warnings are now issued if daemons have
    recently crashed.</strong></span> Ceph will now issue health warnings if
    daemons have recently crashed. Ceph has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash ls-new</pre></div><p>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph crash archive <em class="replaceable">CRASH-ID</em>
<code class="prompt user">cephadm@adm &gt; </code>ceph crash archive-all</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong><code class="option">pg_num</code> must be a power of two,
    otherwise <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph
    will now issue a health warning if a RADOS pool has a
    <code class="option">pg_num</code> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> pg_num <em class="replaceable">NEW-PG-NUM</em></pre></div><p>
    Alternatively, you can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Pool size needs to be greater than 1 otherwise
    <code class="literal">HEALTH_WARN</code> is reported.</strong></span> Ceph will issue a
    health warning if a RADOS pool’s size is set to 1 or if the pool is
    configured with no redundancy. Ceph will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL-NAME</em> size <em class="replaceable">NUM-REPLICAS</em></pre></div><p>
    You can silence the warning with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set global mon_warn_on_pool_no_redundancy false</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</strong></span> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_ratio</code>,
    specifies a percentage of <code class="option">osd_heartbeat_grace</code> to determine
    the threshold. A value of zero disables the warning.
   </p><p>
    A new configuration option, <code class="option">mon_warn_on_slow_ping_time</code>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </p><p>
    A new command <code class="command">ceph daemon
    mgr.<em class="replaceable">MGR-NUMBER</em> dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </p><p>
    A new command <code class="command">ceph daemon osd.# dump_osd_network
    <em class="replaceable">THRESHOLD</em></code> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Changes in the telemetry MGR module.</strong></span>
   </p><p>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <code class="literal">telemetry.ceph.com</code> in
    order to build and improve device failure prediction algorithms.
   </p><p>
    Telemetry reports information about CephFS file systems, including:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      How many MDS daemons (in total and per file system).
     </p></li><li class="listitem"><p>
      Which features are (or have been) enabled.
     </p></li><li class="listitem"><p>
      How many data pools.
     </p></li><li class="listitem"><p>
      Approximate file system age (year and the month of creation).
     </p></li><li class="listitem"><p>
      How many files, bytes, and snapshots.
     </p></li><li class="listitem"><p>
      How much metadata is being cached.
     </p></li></ul></div><p>
    Other miscellaneous information:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Which Ceph release the monitors are running.
     </p></li><li class="listitem"><p>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </p></li><li class="listitem"><p>
      Whether RADOS cache tiering is enabled (and the mode).
     </p></li><li class="listitem"><p>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </p></li><li class="listitem"><p>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </p></li><li class="listitem"><p>
      Whether a separate OSD cluster network is being used.
     </p></li><li class="listitem"><p>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </p></li><li class="listitem"><p>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </p></li><li class="listitem"><p>
      Aggregate stats about the CRUSH Map, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </p></li></ul></div><p>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph config set mgr mgr/telemetry/channel_device false
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry on</pre></div><p>
    You can view exactly what information will be reported first with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show        # see everything
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show device # just the device info
<code class="prompt user">cephadm@adm &gt; </code>ceph telemetry show basic  # basic cluster info</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_recovery_reservations</code></strong></span>. It reveals the
    recovery locks held (<code class="option">in_progress</code>) and waiting in priority
    queues. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_recovery_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>New OSD daemon command
    <code class="command">dump_scrub_reservations</code>. </strong></span> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph daemon osd.<em class="replaceable">ID</em> dump_scrub_reservations</pre></div></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports S3 Object Lock set of
    APIs.</strong></span> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>RGW now supports List Objects V2.</strong></span> RGW now
    supports List Objects V2 as specified at
    <a class="link" href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html" target="_blank">https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html</a>.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.56"><span class="name">Nautilus 14.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.56">#</a></h2></div><p>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect SUSE Enterprise Storage customers
  because we did not ship a version based on 14.2.3.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.58"><span class="name">Nautilus 14.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.58">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Fixed a denial of service vulnerability where an unauthenticated client of
    Ceph Object Gateway could trigger a crash from an uncaught exception.
   </p></li><li class="listitem"><p>
    Nautilus-based librbd clients can now open images on Jewel clusters.
   </p></li><li class="listitem"><p>
    The Object Gateway <code class="option">num_rados_handles</code> has been removed. If you were
    using a value of <code class="option">num_rados_handles</code> greater than 1,
    multiply your current <code class="option">objecter_inflight_ops</code> and
    <code class="option">objecter_inflight_op_bytes</code> parameters by the old
    <code class="option">num_rados_handles</code> to get the same throttle behavior.
   </p></li><li class="listitem"><p>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </p></li><li class="listitem"><p>
    <code class="option">osd_deep_scrub_large_omap_object_key_threshold</code> has been
    lowered to detect an object with a large number of omap keys more easily.
   </p></li><li class="listitem"><p>
    The Ceph Dashboard now supports silencing Prometheus notifications.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.60"><span class="name">Nautilus 14.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.60">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The <code class="literal">no{up,down,in,out}</code> related commands have been
    revamped. There are now two ways to set the
    <code class="literal">no{up,down,in,out}</code> flags: the old command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set <em class="replaceable">FLAG</em></pre></div><p>
    which sets cluster-wide flags; and the new command
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd [un]set-group <em class="replaceable">FLAGS</em> <em class="replaceable">WHO</em></pre></div><p>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </p></li><li class="listitem"><p>
    <code class="command">radosgw-admin</code> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of Object Gateway. Expire-stale objects are expired
    objects that should have been automatically erased but still exist and need
    to be listed and removed manually. One subcommand lists such objects and
    the other deletes them.
   </p></li><li class="listitem"><p>
    Earlier Nautilus releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new Nautilus BlueStore OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-Nautilus) breaks the pool
    utilization statistics reported by <code class="command">ceph df</code>. Until all
    OSDs have been reprovisioned or updated (via <code class="command">ceph-bluestore-tool
    repair</code>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <span class="emphasis"><em>all</em></span> OSDs are 14.2.2 or later, are Block Storage, and
    have been updated via the repair function if they were created prior to
    Nautilus.
   </p></li><li class="listitem"><p>
    The default value for <code class="option">mon_crush_min_required_version</code> has
    been changed from <code class="literal">firefly</code> to <code class="literal">hammer</code>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </p><p>
    If possible, we recommend that you set the oldest allowed client to
    <code class="literal">hammer</code> or later. To display what the current oldest
    allowed client is, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph osd dump | grep min_compat_client</pre></div><p>
    If the current value is older than <code class="literal">hammer</code>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephadm@adm &gt; </code>ceph features</pre></div><p>
    The newer <code class="literal">straw2</code> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <code class="literal">straw2</code> buckets to be used,
    including the <code class="literal">crush-compat</code> mode for the Balancer
    (<span class="intraxref">Book “Administration Guide”, Chapter 21 “Ceph Manager Modules”, Section 21.1 “Balancer”</span>).
   </p></li></ul></div><p>
  Find detailed information about the patch at
  <a class="link" href="https://download.suse.com/Download?buildid=D38A7mekBz4~" target="_blank">https://download.suse.com/Download?buildid=D38A7mekBz4~</a>
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.7.63"><span class="name">Nautilus 14.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.5.7.63">#</a></h2></div><p>
  This was the first point release following the original Nautilus release
  (14.2.0). The original ('General Availability' or 'GA') version of
  SUSE Enterprise Storage 6 was based on this point release.
 </p></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.5.8">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.5.8.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.5.8.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.2">#</a></span></dt><dd class="glossdef"><p>
     The node from which you run the <code class="command">ceph-deploy</code> utility to
     deploy Ceph on OSD nodes.
    </p></dd><dt id="id-1.5.8.3.3"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.3">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p><div id="id-1.5.8.3.3.2.2" data-id-title="Do Not Mix with S3 Buckets" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do Not Mix with S3 Buckets</h6><p>
      S3 <span class="emphasis"><em>buckets</em></span> or <span class="emphasis"><em>containers</em></span>
      represent different terms meaning <span class="emphasis"><em>folders</em></span> for
      storing objects.
     </p></div></dd><dt id="id-1.5.8.3.4"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.4">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.5.8.3.5"><span><span class="glossterm">Monitor node, MON</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.5">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </p></dd><dt id="id-1.5.8.3.8"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.8">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.5.8.3.6"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.6">#</a></span></dt><dd class="glossdef"><p>
     Depending on context, <span class="emphasis"><em>Object Storage Device</em></span> or
     <span class="emphasis"><em>Object Storage Daemon</em></span>. The
     <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.5.8.3.7"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.7">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.5.8.3.9"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.9">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.5.8.3.10"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.10">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.5.8.3.12"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.12">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.5.8.3.11"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.5.8.3.11">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd></dl></div><div class="glossdiv" id="id-1.5.8.4" data-id-title="Ceph Specific Terms"><h3 class="title">Ceph Specific Terms</h3><dl><dt id="id-1.5.8.4.5"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.5.8.4.5">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies end user.
    </p></dd><dt id="id-1.5.8.4.2"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.5.8.4.2">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user’s data. Such a set
     consists of Ceph monitors and OSDs.
    </p><p>
     AKA <span class="quote">“<span class="quote">Ceph Object Store</span>”</span>.
    </p></dd><dt id="id-1.5.8.4.3"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.5.8.4.3">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.5.8.4.4"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.5.8.4.4">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd></dl></div><div class="glossdiv" id="id-1.5.8.5" data-id-title="Object Gateway Specific Terms"><h3 class="title">Object Gateway Specific Terms</h3><dl><dt id="id-1.5.8.5.3"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.5.8.5.3">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.5.8.5.2"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.5.8.5.2">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store.
    </p></dd></dl></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/book_storage_tuning.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>