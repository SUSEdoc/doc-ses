<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Administration and Operations Guide | SUSE Enterprise Storage 7.1</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Administration and Operations Guide | SES 7.1"/>
<meta name="description" content="This guide focuses on routine tasks that you as an administrator need to take care of after the basic Ceph cluster has been deployed (day 2 operation…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7.1"/>
<meta name="book-title" content="Administration and Operations Guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Administration and Operations Guide | SES 7.1"/>
<meta property="og:description" content="This guide focuses on routine tasks that you as an administrator need to take care of after the basic Ceph cluster has been deployed (day 2 operation…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Administration and Operations Guide | SES 7.1"/>
<meta name="twitter:description" content="This guide focuses on routine tasks that you as an administrator need to take care of after the basic Ceph cluster has been deployed (day 2 operation…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-admin">Administration and Operations Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-admin" data-id-title="Administration and Operations Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7.1</span></div><div><h1 class="title">Administration and Operations Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Alexandra</span> <span class="surname">Settle</span>, and <span class="firstname">Liam</span> <span class="surname">Proven</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>31 Aug 2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#preface-admin"><span class="title-name">About this guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.4.2.5"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.6"><span class="title-name">Giving feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.7"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.8"><span class="title-name">Support</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.9"><span class="title-name">Ceph contributors</span></a></span></li><li><span class="sect1"><a href="#id-1.4.2.10"><span class="title-name">Commands and command prompts used in this guide</span></a></span></li></ul></li><li><span class="part"><a href="#part-dashboard"><span class="title-number">I </span><span class="title-name">Ceph Dashboard</span></a></span><ul><li><span class="chapter"><a href="#dashboard-about"><span class="title-number">1 </span><span class="title-name">About the Ceph Dashboard</span></a></span></li><li><span class="chapter"><a href="#dashboard-webui-general"><span class="title-number">2 </span><span class="title-name">Dashboard's Web user interface</span></a></span><ul><li><span class="sect1"><a href="#dashboard-webui-login"><span class="title-number">2.1 </span><span class="title-name">Logging in</span></a></span></li><li><span class="sect1"><a href="#dashboard-util-menu"><span class="title-number">2.2 </span><span class="title-name">Utility menu</span></a></span></li><li><span class="sect1"><a href="#dashboard-main-menu"><span class="title-number">2.3 </span><span class="title-name">Main menu</span></a></span></li><li><span class="sect1"><a href="#dashboard-cpane"><span class="title-number">2.4 </span><span class="title-name">Content pane</span></a></span></li><li><span class="sect1"><a href="#dashboard-ui-common"><span class="title-number">2.5 </span><span class="title-name">Common Web UI features</span></a></span></li><li><span class="sect1"><a href="#dashboard-widgets"><span class="title-number">2.6 </span><span class="title-name">Dashboard widgets</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-user-mgmt"><span class="title-number">3 </span><span class="title-name">Manage Ceph Dashboard users and roles</span></a></span><ul><li><span class="sect1"><a href="#dashboard-listing-users"><span class="title-number">3.1 </span><span class="title-name">Listing users</span></a></span></li><li><span class="sect1"><a href="#dashboard-adding-users"><span class="title-number">3.2 </span><span class="title-name">Adding new users</span></a></span></li><li><span class="sect1"><a href="#dashboard-editing-users"><span class="title-number">3.3 </span><span class="title-name">Editing users</span></a></span></li><li><span class="sect1"><a href="#dashboard-deleting-users"><span class="title-number">3.4 </span><span class="title-name">Deleting users</span></a></span></li><li><span class="sect1"><a href="#dashboard-listing-user-roles"><span class="title-number">3.5 </span><span class="title-name">Listing user roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-adding-roles"><span class="title-number">3.6 </span><span class="title-name">Adding custom roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-editing-roles"><span class="title-number">3.7 </span><span class="title-name">Editing custom roles</span></a></span></li><li><span class="sect1"><a href="#dashboard-deleting-roles"><span class="title-number">3.8 </span><span class="title-name">Deleting custom roles</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-cluster"><span class="title-number">4 </span><span class="title-name">View cluster internals</span></a></span><ul><li><span class="sect1"><a href="#dashboard-cluster-hosts"><span class="title-number">4.1 </span><span class="title-name">Viewing cluster nodes</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-inventory"><span class="title-number">4.2 </span><span class="title-name">Listing physical disks</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-monitors"><span class="title-number">4.3 </span><span class="title-name">Viewing Ceph Monitors</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-services"><span class="title-number">4.4 </span><span class="title-name">Displaying services</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-osds"><span class="title-number">4.5 </span><span class="title-name">Displaying Ceph OSDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-config"><span class="title-number">4.6 </span><span class="title-name">Viewing cluster configuration</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-crushmap"><span class="title-number">4.7 </span><span class="title-name">Viewing the CRUSH Map</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-mgr-plugins"><span class="title-number">4.8 </span><span class="title-name">Viewing manager modules</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-logs"><span class="title-number">4.9 </span><span class="title-name">Viewing logs</span></a></span></li><li><span class="sect1"><a href="#dashboard-cluster-monitoring"><span class="title-number">4.10 </span><span class="title-name">Viewing monitoring</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-pools"><span class="title-number">5 </span><span class="title-name">Manage pools</span></a></span><ul><li><span class="sect1"><a href="#dashboard-pools-create"><span class="title-number">5.1 </span><span class="title-name">Adding a new pool</span></a></span></li><li><span class="sect1"><a href="#dashboard-pools-delete"><span class="title-number">5.2 </span><span class="title-name">Deleting pools</span></a></span></li><li><span class="sect1"><a href="#dashboard-pools-edit"><span class="title-number">5.3 </span><span class="title-name">Editing a pool's options</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-rbds"><span class="title-number">6 </span><span class="title-name">Manage RADOS Block Device</span></a></span><ul><li><span class="sect1"><a href="#dashboard-rbds-details"><span class="title-number">6.1 </span><span class="title-name">Viewing details about RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-configuration"><span class="title-number">6.2 </span><span class="title-name">Viewing RBD's configuration</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-create"><span class="title-number">6.3 </span><span class="title-name">Creating RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbd-delete"><span class="title-number">6.4 </span><span class="title-name">Deleting RBDs</span></a></span></li><li><span class="sect1"><a href="#dashboard-rbds-snapshots"><span class="title-number">6.5 </span><span class="title-name">Creating RADOS Block Device snapshots</span></a></span></li><li><span class="sect1"><a href="#dash-rbd-mirror"><span class="title-number">6.6 </span><span class="title-name">RBD mirroring</span></a></span></li><li><span class="sect1"><a href="#dashboard-iscsi"><span class="title-number">6.7 </span><span class="title-name">Managing iSCSI Gateways</span></a></span></li><li><span class="sect1"><a href="#dash-rbd-qos"><span class="title-number">6.8 </span><span class="title-name">RBD Quality of Service (QoS)</span></a></span></li></ul></li><li><span class="chapter"><a href="#dash-webui-nfs"><span class="title-number">7 </span><span class="title-name">Manage NFS Ganesha</span></a></span><ul><li><span class="sect1"><a href="#dash-webui-nfs-create"><span class="title-number">7.1 </span><span class="title-name">Creating NFS exports</span></a></span></li><li><span class="sect1"><a href="#dash-webui-nfs-delete"><span class="title-number">7.2 </span><span class="title-name">Deleting NFS exports</span></a></span></li><li><span class="sect1"><a href="#dash-webui-nfs-edit"><span class="title-number">7.3 </span><span class="title-name">Editing NFS exports</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-mds"><span class="title-number">8 </span><span class="title-name">Manage CephFS</span></a></span><ul><li><span class="sect1"><a href="#dashboard-mds-overview"><span class="title-number">8.1 </span><span class="title-name">Viewing CephFS overview</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-ogw"><span class="title-number">9 </span><span class="title-name">Manage the Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#dashboard-ogw-view"><span class="title-number">9.1 </span><span class="title-name">Viewing Object Gateways</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-user"><span class="title-number">9.2 </span><span class="title-name">Managing Object Gateway users</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-bucket"><span class="title-number">9.3 </span><span class="title-name">Managing the Object Gateway buckets</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-initial-configuration"><span class="title-number">10 </span><span class="title-name">Manual configuration</span></a></span><ul><li><span class="sect1"><a href="#dashboard-ssl"><span class="title-number">10.1 </span><span class="title-name">Configuring TLS/SSL support</span></a></span></li><li><span class="sect1"><a href="#dashboard-hostname-port"><span class="title-number">10.2 </span><span class="title-name">Changing host name and port number</span></a></span></li><li><span class="sect1"><a href="#dashboard-username-password"><span class="title-number">10.3 </span><span class="title-name">Adjusting user names and passwords</span></a></span></li><li><span class="sect1"><a href="#dashboard-ogw-enabling"><span class="title-number">10.4 </span><span class="title-name">Enabling the Object Gateway management front-end</span></a></span></li><li><span class="sect1"><a href="#dashboard-iscsi-management"><span class="title-number">10.5 </span><span class="title-name">Enabling iSCSI management</span></a></span></li><li><span class="sect1"><a href="#dashboard-sso"><span class="title-number">10.6 </span><span class="title-name">Enabling Single Sign-On</span></a></span></li></ul></li><li><span class="chapter"><a href="#dashboard-user-roles"><span class="title-number">11 </span><span class="title-name">Manage users and roles on the command line</span></a></span><ul><li><span class="sect1"><a href="#dashboard-password-policy"><span class="title-number">11.1 </span><span class="title-name">Managing the password policy</span></a></span></li><li><span class="sect1"><a href="#dashboard-user-accounts"><span class="title-number">11.2 </span><span class="title-name">Managing user accounts</span></a></span></li><li><span class="sect1"><a href="#dashboard-permissions"><span class="title-number">11.3 </span><span class="title-name">User roles and permissions</span></a></span></li><li><span class="sect1"><a href="#dashboard-proxy-config"><span class="title-number">11.4 </span><span class="title-name">Proxy configuration</span></a></span></li><li><span class="sect1"><a href="#dashboard-auditing"><span class="title-number">11.5 </span><span class="title-name">Auditing API requests</span></a></span></li><li><span class="sect1"><a href="#dashboard-config-nfs-ganesha"><span class="title-number">11.6 </span><span class="title-name">Configuring NFS Ganesha in the Ceph Dashboard</span></a></span></li><li><span class="sect1"><a href="#dashboard-debug-plugin"><span class="title-number">11.7 </span><span class="title-name">Debugging plugins</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-cluster-operation"><span class="title-number">II </span><span class="title-name">Cluster Operation</span></a></span><ul><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">12 </span><span class="title-name">Determine the cluster state</span></a></span><ul><li><span class="sect1"><a href="#monitor-status"><span class="title-number">12.1 </span><span class="title-name">Checking a cluster's status</span></a></span></li><li><span class="sect1"><a href="#monitor-health"><span class="title-number">12.2 </span><span class="title-name">Checking cluster health</span></a></span></li><li><span class="sect1"><a href="#monitor-stats"><span class="title-number">12.3 </span><span class="title-name">Checking a cluster's usage stats</span></a></span></li><li><span class="sect1"><a href="#monitor-osdstatus"><span class="title-number">12.4 </span><span class="title-name">Checking OSD status</span></a></span></li><li><span class="sect1"><a href="#storage-bp-monitoring-fullosd"><span class="title-number">12.5 </span><span class="title-name">Checking for full OSDs</span></a></span></li><li><span class="sect1"><a href="#monitor-monstatus"><span class="title-number">12.6 </span><span class="title-name">Checking the monitor status</span></a></span></li><li><span class="sect1"><a href="#monitor-pgroupstatus"><span class="title-number">12.7 </span><span class="title-name">Checking placement group states</span></a></span></li><li><span class="sect1"><a href="#storage-capacity"><span class="title-number">12.8 </span><span class="title-name">Storage capacity</span></a></span></li><li><span class="sect1"><a href="#op-mon-osd-pg"><span class="title-number">12.9 </span><span class="title-name">Monitoring OSDs and placement groups</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">13 </span><span class="title-name">Operational tasks</span></a></span><ul><li><span class="sect1"><a href="#modifying-cluster-configuration"><span class="title-number">13.1 </span><span class="title-name">Modifying the cluster configuration</span></a></span></li><li><span class="sect1"><a href="#adding-node"><span class="title-number">13.2 </span><span class="title-name">Adding nodes</span></a></span></li><li><span class="sect1"><a href="#salt-node-removing"><span class="title-number">13.3 </span><span class="title-name">Removing nodes</span></a></span></li><li><span class="sect1"><a href="#osd-management"><span class="title-number">13.4 </span><span class="title-name">OSD management</span></a></span></li><li><span class="sect1"><a href="#moving-saltmaster"><span class="title-number">13.5 </span><span class="title-name">Moving the Salt Master to a new node</span></a></span></li><li><span class="sect1"><a href="#cephadm-rolling-updates"><span class="title-number">13.6 </span><span class="title-name">Updating the cluster nodes</span></a></span></li><li><span class="sect1"><a href="#deploy-cephadm-day2-cephupdate"><span class="title-number">13.7 </span><span class="title-name">Updating Ceph</span></a></span></li><li><span class="sect1"><a href="#sec-salt-cluster-reboot"><span class="title-number">13.8 </span><span class="title-name">Halting or rebooting cluster</span></a></span></li><li><span class="sect1"><a href="#ceph-cluster-purge"><span class="title-number">13.9 </span><span class="title-name">Removing an entire Ceph cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">14 </span><span class="title-name">Operation of Ceph services</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-operating-individual"><span class="title-number">14.1 </span><span class="title-name">Operating individual services</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-operating-service-types"><span class="title-number">14.2 </span><span class="title-name">Operating service types</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-operating-node"><span class="title-number">14.3 </span><span class="title-name">Operating services on a single node</span></a></span></li><li><span class="sect1"><a href="#ceph-cluster-shutdown"><span class="title-number">14.4 </span><span class="title-name">Shutting down and restarting the whole Ceph cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">15 </span><span class="title-name">Backup and restore</span></a></span><ul><li><span class="sect1"><a href="#backrest-ceph"><span class="title-number">15.1 </span><span class="title-name">Back Up Cluster Configuration and Data</span></a></span></li><li><span class="sect1"><a href="#restore-ceph"><span class="title-number">15.2 </span><span class="title-name">Restoring a Ceph node</span></a></span></li></ul></li><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">16 </span><span class="title-name">Monitoring and alerting</span></a></span><ul><li><span class="sect1"><a href="#monitoring-custom-images"><span class="title-number">16.1 </span><span class="title-name">Configuring custom or local images</span></a></span></li><li><span class="sect1"><a href="#monitoring-applying-updates"><span class="title-number">16.2 </span><span class="title-name">Updating monitoring services</span></a></span></li><li><span class="sect1"><a href="#monitoring-stack-disable"><span class="title-number">16.3 </span><span class="title-name">Disabling monitoring</span></a></span></li><li><span class="sect1"><a href="#monitoring-grafana-config"><span class="title-number">16.4 </span><span class="title-name">Configuring Grafana</span></a></span></li><li><span class="sect1"><a href="#monitoring-cephadm-config"><span class="title-number">16.5 </span><span class="title-name">Configuring the Prometheus Manager Module</span></a></span></li><li><span class="sect1"><a href="#prometheus-security-model"><span class="title-number">16.6 </span><span class="title-name">Prometheus security model</span></a></span></li><li><span class="sect1"><a href="#prometheus-webhook-snmp"><span class="title-number">16.7 </span><span class="title-name">Prometheus Alertmanager SNMP gateway</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-storing-data"><span class="title-number">III </span><span class="title-name">Storing Data in a Cluster</span></a></span><ul><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">17 </span><span class="title-name">Stored data management</span></a></span><ul><li><span class="sect1"><a href="#datamgm-devices"><span class="title-number">17.1 </span><span class="title-name">OSD devices</span></a></span></li><li><span class="sect1"><a href="#datamgm-buckets"><span class="title-number">17.2 </span><span class="title-name">Buckets</span></a></span></li><li><span class="sect1"><a href="#datamgm-rules"><span class="title-number">17.3 </span><span class="title-name">Rule sets</span></a></span></li><li><span class="sect1"><a href="#op-pgs"><span class="title-number">17.4 </span><span class="title-name">Placement groups</span></a></span></li><li><span class="sect1"><a href="#op-crush"><span class="title-number">17.5 </span><span class="title-name">CRUSH Map manipulation</span></a></span></li><li><span class="sect1"><a href="#scrubbing-pgs"><span class="title-number">17.6 </span><span class="title-name">Scrubbing placement groups</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">18 </span><span class="title-name">Manage storage pools</span></a></span><ul><li><span class="sect1"><a href="#ceph-pools-operate-add-pool"><span class="title-number">18.1 </span><span class="title-name">Creating a pool</span></a></span></li><li><span class="sect1"><a href="#ceph-listing-pools"><span class="title-number">18.2 </span><span class="title-name">Listing pools</span></a></span></li><li><span class="sect1"><a href="#ceph-renaming-pool"><span class="title-number">18.3 </span><span class="title-name">Renaming a pool</span></a></span></li><li><span class="sect1"><a href="#ceph-pools-operate-del-pool"><span class="title-number">18.4 </span><span class="title-name">Deleting a pool</span></a></span></li><li><span class="sect1"><a href="#ceph-pool-other-operations"><span class="title-number">18.5 </span><span class="title-name">Other operations</span></a></span></li><li><span class="sect1"><a href="#pools-migration"><span class="title-number">18.6 </span><span class="title-name">Pool migration</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-pool"><span class="title-number">18.7 </span><span class="title-name">Pool snapshots</span></a></span></li><li><span class="sect1"><a href="#sec-ceph-pool-compression"><span class="title-number">18.8 </span><span class="title-name">Data compression</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">19 </span><span class="title-name">Erasure coded pools</span></a></span><ul><li><span class="sect1"><a href="#ec-prerequisite"><span class="title-number">19.1 </span><span class="title-name">Prerequisite for erasure coded Pools</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-default-profile"><span class="title-number">19.2 </span><span class="title-name">Creating a sample erasure coded pool</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-erasure-erasure-profiles"><span class="title-number">19.3 </span><span class="title-name">Erasure code profiles</span></a></span></li><li><span class="sect1"><a href="#ec-rbd"><span class="title-number">19.4 </span><span class="title-name">Marking erasure coded pools with RADOS Block Device</span></a></span></li></ul></li><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">20 </span><span class="title-name">RADOS Block Device</span></a></span><ul><li><span class="sect1"><a href="#ceph-rbd-commands"><span class="title-number">20.1 </span><span class="title-name">Block device commands</span></a></span></li><li><span class="sect1"><a href="#storage-bp-integration-mount-rbd"><span class="title-number">20.2 </span><span class="title-name">Mounting and unmounting</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-snapshots-rbd"><span class="title-number">20.3 </span><span class="title-name">Snapshots</span></a></span></li><li><span class="sect1"><a href="#ceph-rbd-mirror"><span class="title-number">20.4 </span><span class="title-name">RBD image mirrors</span></a></span></li><li><span class="sect1"><a href="#rbd-cache-settings"><span class="title-number">20.5 </span><span class="title-name">Cache settings</span></a></span></li><li><span class="sect1"><a href="#rbd-qos"><span class="title-number">20.6 </span><span class="title-name">QoS settings</span></a></span></li><li><span class="sect1"><a href="#rbd-readahead-settings"><span class="title-number">20.7 </span><span class="title-name">Read-ahead settings</span></a></span></li><li><span class="sect1"><a href="#rbd-features"><span class="title-number">20.8 </span><span class="title-name">Advanced features</span></a></span></li><li><span class="sect1"><a href="#rbd-old-clients-map"><span class="title-number">20.9 </span><span class="title-name">Mapping RBD using old kernel clients</span></a></span></li><li><span class="sect1"><a href="#rbd-kubernetes"><span class="title-number">20.10 </span><span class="title-name">Enabling block devices and Kubernetes</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-accessing-data"><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">21 </span><span class="title-name">Ceph Object Gateway</span></a></span><ul><li><span class="sect1"><a href="#sec-ceph-rgw-limits"><span class="title-number">21.1 </span><span class="title-name">Object Gateway restrictions and naming limitations</span></a></span></li><li><span class="sect1"><a href="#ogw-deploy"><span class="title-number">21.2 </span><span class="title-name">Deploying the Object Gateway</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-operating"><span class="title-number">21.3 </span><span class="title-name">Operating the Object Gateway service</span></a></span></li><li><span class="sect1"><a href="#ogw-config-parameters"><span class="title-number">21.4 </span><span class="title-name">Configuration options</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-access"><span class="title-number">21.5 </span><span class="title-name">Managing Object Gateway access</span></a></span></li><li><span class="sect1"><a href="#ogw-http-frontends"><span class="title-number">21.6 </span><span class="title-name">HTTP front-ends</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-https"><span class="title-number">21.7 </span><span class="title-name">Enable HTTPS/SSL for Object Gateways</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-sync"><span class="title-number">21.8 </span><span class="title-name">Synchronization modules</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-ldap"><span class="title-number">21.9 </span><span class="title-name">LDAP authentication</span></a></span></li><li><span class="sect1"><a href="#ogw-bucket-sharding"><span class="title-number">21.10 </span><span class="title-name">Bucket index sharding</span></a></span></li><li><span class="sect1"><a href="#ogw-keystone"><span class="title-number">21.11 </span><span class="title-name">OpenStack Keystone integration</span></a></span></li><li><span class="sect1"><a href="#ogw-storage-classes"><span class="title-number">21.12 </span><span class="title-name">Pool placement and storage classes</span></a></span></li><li><span class="sect1"><a href="#ceph-rgw-fed"><span class="title-number">21.13 </span><span class="title-name">Multisite Object Gateways</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">22 </span><span class="title-name">Ceph iSCSI gateway</span></a></span><ul><li><span class="sect1"><a href="#ceph-iscsi-connect"><span class="title-number">22.1 </span><span class="title-name"><code class="systemitem">ceph-iscsi</code> managed targets</span></a></span></li><li><span class="sect1"><a href="#ceph-iscsi-conclude"><span class="title-number">22.2 </span><span class="title-name">Conclusion</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">23 </span><span class="title-name">Clustered file system</span></a></span><ul><li><span class="sect1"><a href="#ceph-cephfs-cephfs-mount"><span class="title-number">23.1 </span><span class="title-name">Mounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-unmount"><span class="title-number">23.2 </span><span class="title-name">Unmounting CephFS</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-cephfs-fstab"><span class="title-number">23.3 </span><span class="title-name">Mounting CephFS in <code class="filename">/etc/fstab</code></span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-activeactive"><span class="title-number">23.4 </span><span class="title-name">Multiple active MDS daemons (active-active MDS)</span></a></span></li><li><span class="sect1"><a href="#ceph-cephfs-failover"><span class="title-number">23.5 </span><span class="title-name">Managing failover</span></a></span></li><li><span class="sect1"><a href="#cephfs-quotas"><span class="title-number">23.6 </span><span class="title-name">Setting CephFS quotas</span></a></span></li><li><span class="sect1"><a href="#cephfs-snapshots"><span class="title-number">23.7 </span><span class="title-name">Managing CephFS snapshots</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">24 </span><span class="title-name">Export Ceph data via Samba</span></a></span><ul><li><span class="sect1"><a href="#cephfs-samba"><span class="title-number">24.1 </span><span class="title-name">Export CephFS via Samba share</span></a></span></li><li><span class="sect1"><a href="#cephfs-ad"><span class="title-number">24.2 </span><span class="title-name">Joining Samba Gateway and Active Directory</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">25 </span><span class="title-name">NFS Ganesha</span></a></span><ul><li><span class="sect1"><a href="#ceph-nfsganesha-nfservice"><span class="title-number">25.1 </span><span class="title-name">Creating an NFS service</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-services"><span class="title-number">25.2 </span><span class="title-name">Starting or Restarting NFS Ganesha</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-list-objects"><span class="title-number">25.3 </span><span class="title-name">Listing objects in the NFS recovery pool</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-create-export"><span class="title-number">25.4 </span><span class="title-name">Creating an NFS export</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-verify"><span class="title-number">25.5 </span><span class="title-name">Verifying the NFS export</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-mount"><span class="title-number">25.6 </span><span class="title-name">Mounting the NFS export</span></a></span></li><li><span class="sect1"><a href="#ceph-nfsganesha-customrole"><span class="title-number">25.7 </span><span class="title-name">Multiple NFS Ganesha clusters</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-integration-virt"><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">26 </span><span class="title-name"><code class="systemitem">libvirt</code> and Ceph</span></a></span><ul><li><span class="sect1"><a href="#ceph-libvirt-cfg-ceph"><span class="title-number">26.1 </span><span class="title-name">Configuring Ceph with <code class="systemitem">libvirt</code></span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-virt-manager"><span class="title-number">26.2 </span><span class="title-name">Preparing the VM manager</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-create-vm"><span class="title-number">26.3 </span><span class="title-name">Creating a VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-cfg-vm"><span class="title-number">26.4 </span><span class="title-name">Configuring the VM</span></a></span></li><li><span class="sect1"><a href="#ceph-libvirt-summary"><span class="title-number">26.5 </span><span class="title-name">Summary</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">27 </span><span class="title-name">Ceph as a back-end for QEMU KVM instance</span></a></span><ul><li><span class="sect1"><a href="#ceph-kvm-install"><span class="title-number">27.1 </span><span class="title-name">Installing <code class="systemitem">qemu-block-rbd</code></span></a></span></li><li><span class="sect1"><a href="#ceph-kvm-usage"><span class="title-number">27.2 </span><span class="title-name">Using QEMU</span></a></span></li><li><span class="sect1"><a href="#creating-images-qemu"><span class="title-number">27.3 </span><span class="title-name">Creating images with QEMU</span></a></span></li><li><span class="sect1"><a href="#resizing-images-qemu"><span class="title-number">27.4 </span><span class="title-name">Resizing images with QEMU</span></a></span></li><li><span class="sect1"><a href="#retrieving-image-info-qemu"><span class="title-number">27.5 </span><span class="title-name">Retrieving image info with QEMU</span></a></span></li><li><span class="sect1"><a href="#running-qemu-rbd"><span class="title-number">27.6 </span><span class="title-name">Running QEMU with RBD</span></a></span></li><li><span class="sect1"><a href="#enabling-dicard-trim"><span class="title-number">27.7 </span><span class="title-name">Enabling discard and TRIM</span></a></span></li><li><span class="sect1"><a href="#qemu-cache-options"><span class="title-number">27.8 </span><span class="title-name">Setting QEMU cache options</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#part-cluster-configuration"><span class="title-number">VI </span><span class="title-name">Configuring a Cluster</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">28 </span><span class="title-name">Ceph cluster configuration</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-configuration-ceph-conf"><span class="title-number">28.1 </span><span class="title-name">Configure the <code class="filename">ceph.conf</code> file</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-configuration-db"><span class="title-number">28.2 </span><span class="title-name">Configuration database</span></a></span></li><li><span class="sect1"><a href="#cha-ceph-config-key-store"><span class="title-number">28.3 </span><span class="title-name"><code class="systemitem">config-key</code> store</span></a></span></li><li><span class="sect1"><a href="#config-osd-and-bluestore"><span class="title-number">28.4 </span><span class="title-name">Ceph OSD and BlueStore</span></a></span></li><li><span class="sect1"><a href="#config-ogw"><span class="title-number">28.5 </span><span class="title-name">Ceph Object Gateway</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-mgr-modules"><span class="title-number">29 </span><span class="title-name">Ceph Manager modules</span></a></span><ul><li><span class="sect1"><a href="#mgr-modules-balancer"><span class="title-number">29.1 </span><span class="title-name">Balancer</span></a></span></li><li><span class="sect1"><a href="#mgr-modules-telemetry"><span class="title-number">29.2 </span><span class="title-name">Enabling the telemetry module</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">30 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span><ul><li><span class="sect1"><a href="#storage-cephx-arch"><span class="title-number">30.1 </span><span class="title-name">Authentication architecture</span></a></span></li><li><span class="sect1"><a href="#storage-cephx-keymgmt"><span class="title-number">30.2 </span><span class="title-name">Key management</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#id-1.4.9"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Pacific' point releases</span></a></span></li><li><span class="glossary"><a href="#id-1.4.10"><span class="title-name">Glossary</span></a></span></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.4.3.3.3.5"><span class="number">2.1 </span><span class="name">Ceph Dashboard login screen</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.3.8.2"><span class="number">2.2 </span><span class="name">Notification about a new SUSE Enterprise Storage release</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.3.10"><span class="number">2.3 </span><span class="name">Ceph Dashboard home page</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.8.4.3"><span class="number">2.4 </span><span class="name">Status widgets</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.8.5.3"><span class="number">2.5 </span><span class="name">Capacity widgets</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.3.8.6.3"><span class="number">2.6 </span><span class="name">performance widgets</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.5.4"><span class="number">3.1 </span><span class="name">User management</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.6.3"><span class="number">3.2 </span><span class="name">Adding a user</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.9.4"><span class="number">3.3 </span><span class="name">User roles</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.4.10.4"><span class="number">3.4 </span><span class="name">Adding a role</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.4.3"><span class="number">4.1 </span><span class="name">Hosts</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.5.4"><span class="number">4.2 </span><span class="name">Physical disks</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.6.7"><span class="number">4.3 </span><span class="name">Ceph Monitors</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.7.4"><span class="number">4.4 </span><span class="name">Services</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.7.5.4"><span class="number">4.5 </span><span class="name">Creating a new cluster service</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.3"><span class="number">4.6 </span><span class="name">Ceph OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.5"><span class="number">4.7 </span><span class="name">OSD flags</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.7"><span class="number">4.8 </span><span class="name">OSD recovery priority</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.9"><span class="number">4.9 </span><span class="name">OSD details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.11.3.1.2"><span class="number">4.10 </span><span class="name">Create OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.11.3.2.2"><span class="number">4.11 </span><span class="name">Adding primary devices</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.11.3.3.2"><span class="number">4.12 </span><span class="name">Create OSDs with primary devices added</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.11.3.4.2"><span class="number">4.13 </span><span class="name"/></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.8.11.3.5.2"><span class="number">4.14 </span><span class="name">Newly added OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.9.3"><span class="number">4.15 </span><span class="name">Cluster configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.10.4"><span class="number">4.16 </span><span class="name">CRUSH Map</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.11.3"><span class="number">4.17 </span><span class="name">Manager modules</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.5.12.4"><span class="number">4.18 </span><span class="name">Logs</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.6"><span class="number">5.1 </span><span class="name">List of pools</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.6.8.3"><span class="number">5.2 </span><span class="name">Adding a new pool</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.5"><span class="number">6.1 </span><span class="name">List of RBD images</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.6.3"><span class="number">6.2 </span><span class="name">RBD details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.7.3"><span class="number">6.3 </span><span class="name">RBD configuration</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.8.3"><span class="number">6.4 </span><span class="name">Adding a new RBD</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.10.4"><span class="number">6.5 </span><span class="name">RBD snapshots</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.11.5.3.2"><span class="number">6.6 </span><span class="name">Running <code class="systemitem">rbd-mirror</code> daemons</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.15.3.1.2"><span class="number">6.7 </span><span class="name">Creating a pool with RBD application</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.15.3.2.2"><span class="number">6.8 </span><span class="name">Configuring the replication mode</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.15.3.3.2"><span class="number">6.9 </span><span class="name">Adding peer credentials</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.15.3.3.5"><span class="number">6.10 </span><span class="name">List of replicated pools</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.16.3.1.2"><span class="number">6.11 </span><span class="name">New RBD image</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.16.3.2.2"><span class="number">6.12 </span><span class="name">New RBD image synchronized</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.11.16.3.2.3.3"><span class="number">6.13 </span><span class="name">RBD images' replication status</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.12.6"><span class="number">6.14 </span><span class="name">List of iSCSI targets</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.12.8"><span class="number">6.15 </span><span class="name">iSCSI target details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.7.12.9.3"><span class="number">6.16 </span><span class="name">Adding a new target</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.8.7"><span class="number">7.1 </span><span class="name">List of NFS exports</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.8.9"><span class="number">7.2 </span><span class="name">NFS export details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.8.10.3"><span class="number">7.3 </span><span class="name">Adding a new NFS export</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.8.12.4"><span class="number">7.4 </span><span class="name">Editing an NFS export</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.9.4.4"><span class="number">8.1 </span><span class="name">CephFS details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.9.4.6"><span class="number">8.2 </span><span class="name">CephFS details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.10.5.4"><span class="number">9.1 </span><span class="name">Gateway's details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.10.6.4"><span class="number">9.2 </span><span class="name">Gateway users</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.10.6.5.3"><span class="number">9.3 </span><span class="name">Adding a new gateway user</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.10.7.5.3"><span class="number">9.4 </span><span class="name">Gateway bucket details</span></a></span></li><li><span class="figure"><a href="#id-1.4.3.10.7.6.4"><span class="number">9.5 </span><span class="name">Editing the bucket details</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.2.12.6"><span class="number">12.1 </span><span class="name">Ceph cluster</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.2.13.7.3"><span class="number">12.2 </span><span class="name">Peering schema</span></a></span></li><li><span class="figure"><a href="#id-1.4.4.2.13.8.18.1.2.2"><span class="number">12.3 </span><span class="name">Placement groups status</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.2.12.7.3.3"><span class="number">17.1 </span><span class="name">OSDs with mixed device classes</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-iterate-figure"><span class="number">17.2 </span><span class="name">Example tree</span></a></span></li><li><span class="figure"><a href="#datamgm-rules-step-mode-indep-figure"><span class="number">17.3 </span><span class="name">Node replacement methods</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.2.15.3.3"><span class="number">17.4 </span><span class="name">Placement groups in a pool</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.2.15.3.6"><span class="number">17.5 </span><span class="name">Placement groups and OSDs</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.11.5.3.2.5"><span class="number">18.1 </span><span class="name">Pools before migration</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.11.5.3.3.3"><span class="number">18.2 </span><span class="name">Cache tier setup</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.11.5.3.4.3"><span class="number">18.3 </span><span class="name">Data flushing</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.11.5.3.5.4"><span class="number">18.4 </span><span class="name">Setting overlay</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.3.11.5.3.6.3"><span class="number">18.5 </span><span class="name">Migration complete</span></a></span></li><li><span class="figure"><a href="#id-1.4.5.5.5"><span class="number">20.1 </span><span class="name">RADOS protocol</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.1.2"><span class="number">22.1 </span><span class="name">iSCSI initiator properties</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.2.2"><span class="number">22.2 </span><span class="name">Discover target portal</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.3.2"><span class="number">22.3 </span><span class="name">Target portals</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.4.2"><span class="number">22.4 </span><span class="name">Targets</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.6.2"><span class="number">22.5 </span><span class="name">iSCSI target properties</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.3.7.2"><span class="number">22.6 </span><span class="name">Device details</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.6.1.2"><span class="number">22.7 </span><span class="name">New volume wizard</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.6.2.2"><span class="number">22.8 </span><span class="name">Offline disk prompt</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.4.6.3.2"><span class="number">22.9 </span><span class="name">Confirm volume selections</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.2.2"><span class="number">22.10 </span><span class="name">iSCSI initiator properties</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.4.2"><span class="number">22.11 </span><span class="name">Add target server</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.5.2"><span class="number">22.12 </span><span class="name">Manage multipath devices</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.5.4"><span class="number">22.13 </span><span class="name">Paths listing for multipath</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.6.2"><span class="number">22.14 </span><span class="name">Add storage dialog</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.7.2"><span class="number">22.15 </span><span class="name">Custom space setting</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.3.4.5.3.7.5"><span class="number">22.16 </span><span class="name">iSCSI datastore overview</span></a></span></li><li><span class="figure"><a href="#id-1.4.6.6.8"><span class="number">25.1 </span><span class="name">NFS Ganesha structure</span></a></span></li><li><span class="figure"><a href="#id-1.4.8.4.5.6"><span class="number">30.1 </span><span class="name">Basic <code class="systemitem">cephx</code> authentication</span></a></span></li><li><span class="figure"><a href="#id-1.4.8.4.5.8"><span class="number">30.2 </span><span class="name"><code class="systemitem">cephx</code> authentication</span></a></span></li><li><span class="figure"><a href="#id-1.4.8.4.5.10"><span class="number">30.3 </span><span class="name"><code class="systemitem">cephx</code> authentication - MDS and OSD</span></a></span></li></ul></div><div class="list-of-tables"><div class="toc-title">List of Tables</div><ul><li><span class="table"><a href="#id-1.4.6.5.6.8.6.3"><span class="number">24.1 </span><span class="name">Default Users and Group ID Blocks</span></a></span></li><li><span class="table"><a href="#id-1.4.6.5.6.8.6.6"><span class="number">24.2 </span><span class="name">ID Ranges</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#id-1.4.4.2.13.9.4"><span class="number">12.1 </span><span class="name">Locating an object</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.12.4"><span class="number">13.1 </span><span class="name">Matching by disk size</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.13.3"><span class="number">13.2 </span><span class="name">Simple setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.13.4"><span class="number">13.3 </span><span class="name">Advanced setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.13.5"><span class="number">13.4 </span><span class="name">Advanced setup with non-uniform nodes</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.13.6"><span class="number">13.5 </span><span class="name">Expert setup</span></a></span></li><li><span class="example"><a href="#id-1.4.4.3.6.5.13.7"><span class="number">13.6 </span><span class="name">Complex (and unlikely) setup</span></a></span></li><li><span class="example"><a href="#id-1.4.5.2.12.7.7.4.1.2.6"><span class="number">17.1 </span><span class="name"><code class="command">crushtool --reclassify-root</code></span></a></span></li><li><span class="example"><a href="#id-1.4.5.2.12.7.7.4.3.2.2"><span class="number">17.2 </span><span class="name"><code class="command">crushtool --reclassify-bucket</code></span></a></span></li><li><span class="example"><a href="#id-1.4.6.2.11.6.5.3"><span class="number">21.1 </span><span class="name">Trivial configuration</span></a></span></li><li><span class="example"><a href="#id-1.4.6.2.11.6.5.4"><span class="number">21.2 </span><span class="name">Non-trivial configuration</span></a></span></li><li><span class="example"><a href="#id-1.4.8.2.8.4.2.3"><span class="number">28.1 </span><span class="name">Example Beast Configuration</span></a></span></li><li><span class="example"><a href="#id-1.4.8.2.8.4.3.3"><span class="number">28.2 </span><span class="name">Example Civetweb Configuration in <code class="filename">/etc/ceph/ceph.conf</code></span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.4.1.6"><p>
  Copyright © 2020–2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under Creative Commons Attribution-ShareAlike 4.0 International
  (CC-BY-SA 4.0): <a class="link" href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">https://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section xml:lang="en" class="preface" id="preface-admin" data-id-title="About this guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About this guide</span> <a title="Permalink" class="permalink" href="#preface-admin">#</a></h1></div></div></div><p>
  This guide focuses on routine tasks that you as an administrator need to take
  care of after the basic Ceph cluster has been deployed (day 2 operations).
  It also describes all the supported ways to access data stored in a Ceph
  cluster.
 </p><p>
 SUSE Enterprise Storage 7.1 is an extension to SUSE Linux Enterprise Server 15 SP3. It combines the
 capabilities of the Ceph
 (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>)
 storage project with the enterprise engineering and support of SUSE.
 SUSE Enterprise Storage 7.1 provides IT organizations with the ability to
 deploy a distributed storage architecture that can support a number of use
 cases using commodity hardware platforms.
</p><section xml:lang="en" class="sect1" id="id-1.4.2.5" data-id-title="Available documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available documentation</span> <a title="Permalink" class="permalink" href="#id-1.4.2.5">#</a></h2></div></div></div><div id="id-1.4.2.5.3" data-id-title="Online documentation and latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Online documentation and latest updates</h6><p>
   Documentation for our products is available at
   <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>,
   where you can also find the latest updates, and browse or download the
   documentation in various formats. The latest documentation updates can be
   found in the English language version.
  </p></div><p>
  In addition, the product documentation is available in your installed system
  under <code class="filename">/usr/share/doc/manual</code>. It is included in an RPM
  package named
  <span class="package">ses-manual_<em class="replaceable">LANG_CODE</em></span>. Install
  it if it is not already on your system, for example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper install ses-manual_en</pre></div><p>
  The following documentation is available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.5.7.1"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-deployment.html" target="_blank"><em class="citetitle">Deployment Guide</em></a></span></dt><dd><p>
     This guide focuses on deploying a basic Ceph cluster, and how to deploy
     additional services. It also cover the steps for upgrading to
     SUSE Enterprise Storage 7.1 from the previous product version.
    </p></dd><dt id="id-1.4.2.5.7.2"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-admin.html" target="_blank"><em class="citetitle">Administration and Operations Guide</em></a></span></dt><dd><p>
     This guide focuses on routine tasks that you as an administrator need to
     take care of after the basic Ceph cluster has been deployed (day 2
     operations). It also describes all the supported ways to access data
     stored in a Ceph cluster.
    </p></dd><dt id="id-1.4.2.5.7.3"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-security.html" target="_blank"><em class="citetitle">Security Hardening Guide</em></a></span></dt><dd><p>
     This guide focuses on how to ensure your cluster is secure.
    </p></dd><dt id="id-1.4.2.5.7.4"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-troubleshooting.html" target="_blank"><em class="citetitle">Troubleshooting Guide</em></a></span></dt><dd><p>
     This guide takes you through various common problems when running
     SUSE Enterprise Storage 7.1 and other related issues to relevant
     components such as Ceph or Object Gateway.
    </p></dd><dt id="id-1.4.2.5.7.5"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-windows.html" target="_blank"><em class="citetitle">SUSE Enterprise Storage for Windows Guide</em></a></span></dt><dd><p>
     This guide describes the integration, installation, and configuration of
     Microsoft Windows environments and SUSE Enterprise Storage using the Windows Driver.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.4.2.6" data-id-title="Giving feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Giving feedback</span> <a title="Permalink" class="permalink" href="#id-1.4.2.6">#</a></h2></div></div></div><p>
  We welcome feedback on, and contributions to, this documentation.
  There are several channels for this:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.6.4.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.4.2.6.4.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
     Reporting issues requires a Bugzilla account.
    </p><p>
     To simplify this process, you can use the <span class="guimenu">Report
     Documentation Bug</span> links next to headlines in the HTML
     version of this document. These preselect the right product and
     category in Bugzilla and add a link to the current section.
     You can start typing your bug report right away.
    </p></dd><dt id="id-1.4.2.6.4.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, use the <span class="guimenu">Edit Source</span>
     links next to headlines in the HTML version of this document. They
     take you to the source code on GitHub, where you can open a pull request.
     Contributing requires a GitHub account.
    </p><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README at
     <a class="link" href="https://github.com/SUSE/doc-ses" target="_blank">https://github.com/SUSE/doc-ses</a>.
    </p></dd><dt id="id-1.4.2.6.4.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.4.2.7" data-id-title="Documentation conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span> <a title="Permalink" class="permalink" href="#id-1.4.2.7">#</a></h2></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as non-privileged user.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.4.2.7.4.13.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Warning notice</h6><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.4.2.7.4.13.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Important notice</h6><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.4.2.7.4.13.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Note notice</h6><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.4.2.7.4.13.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Tip notice</h6><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.4.2.7.4.14.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.4.2.7.4.14.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.4.2.8" data-id-title="Support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Support</span> <a title="Permalink" class="permalink" href="#id-1.4.2.8">#</a></h2></div></div></div><p>
  Find the support statement for SUSE Enterprise Storage and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.4.2.8.5" data-id-title="Support statement for SUSE Enterprise Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#id-1.4.2.8.5">#</a></h3></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offerings available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.2.8.5.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.4.2.8.5.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.4.2.8.5.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Enterprise Storage is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.4.2.8.6" data-id-title="Technology previews"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span> <a title="Permalink" class="permalink" href="#id-1.4.2.8.6">#</a></h3></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback!
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or in other ways
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7.1" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7.1</a>.
  </p></section></section><section class="sect1" id="id-1.4.2.9" data-id-title="Ceph contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph contributors</span> <a title="Permalink" class="permalink" href="#id-1.4.2.9">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section><section class="sect1" id="id-1.4.2.10" data-id-title="Commands and command prompts used in this guide"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Commands and command prompts used in this guide</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect2" id="id-1.4.2.10.4" data-id-title="Salt-related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1 </span><span class="title-name">Salt-related commands</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10.4">#</a></h3></div></div></div><p>
   These commands help you to deploy Ceph cluster nodes, run commands on
   several (or all) cluster nodes at the same time, or assist you when adding
   or removing cluster nodes. The most frequently used commands are
   <code class="command">ceph-salt</code> and <code class="command">ceph-salt config</code>. You
   need to run Salt commands on the Salt Master node as <code class="systemitem">root</code>. These
   commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div></section><section class="sect2" id="id-1.4.2.10.5" data-id-title="Ceph related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2 </span><span class="title-name">Ceph related commands</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10.5">#</a></h3></div></div></div><p>
   These are lower-level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">cephadm</code>, <code class="command">rbd</code>,
   or <code class="command">radosgw-admin</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   The safer and recommended option is to create a more restrictive individual
   key for each administrator user and put it in a directory where the users
   can read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.4.2.10.5.6" data-id-title="Path to Ceph keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph-related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephuser</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><div id="id-1.4.2.10.5.11" data-id-title="Commands for specific nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for specific nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mon &gt; </code></pre></div></div><section class="sect3" id="id-1.4.2.10.5.12" data-id-title="Running ceph-volume"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Running <code class="command">ceph-volume</code></span> <a title="Permalink" class="permalink" href="#id-1.4.2.10.5.12">#</a></h4></div></div></div><p>
    Starting with SUSE Enterprise Storage 7, Ceph services are running containerized.
    If you need to run <code class="command">ceph-volume</code> on an OSD node, you need
    to prepend it with the <code class="command">cephadm</code> command, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm ceph-volume simple scan</pre></div></section></section><section class="sect2" id="id-1.4.2.10.6" data-id-title="General Linux commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3 </span><span class="title-name">General Linux commands</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10.6">#</a></h3></div></div></div><p>
   Linux commands not related to Ceph, such as <code class="command">mount</code>,
   <code class="command">cat</code>, or <code class="command">openssl</code>, are introduced either
   with the <code class="prompt user">cephuser@adm &gt; </code> or <code class="prompt root"># </code> prompts, depending on which
   privileges the related command requires.
  </p></section><section class="sect2" id="id-1.4.2.10.7" data-id-title="Additional information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4 </span><span class="title-name">Additional information</span> <a title="Permalink" class="permalink" href="#id-1.4.2.10.7">#</a></h3></div></div></div><p>
   For more information on Ceph key management, refer to
   <a class="xref" href="#storage-cephx-keymgmt" title="30.2. Key management">Section 30.2, “Key management”</a>.
  </p></section></section></section><div class="part" id="part-dashboard" data-id-title="Ceph Dashboard"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Ceph Dashboard </span><a title="Permalink" class="permalink" href="#part-dashboard">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#dashboard-about"><span class="title-number">1 </span><span class="title-name">About the Ceph Dashboard</span></a></span></li><dd class="toc-abstract"><p>The Ceph Dashboard is a built-in Web-based Ceph management and monitoring application that administers various aspects and objects of the cluster. The dashboard is automatically enabled after the basic cluster is deployed in Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using c…</p></dd><li><span class="chapter"><a href="#dashboard-webui-general"><span class="title-number">2 </span><span class="title-name">Dashboard's Web user interface</span></a></span></li><dd class="toc-abstract"><p>
   To log in to the Ceph Dashboard, point your browser to its URL including the
   port number. Run the following command to find the address:
  </p></dd><li><span class="chapter"><a href="#dashboard-user-mgmt"><span class="title-number">3 </span><span class="title-name">Manage Ceph Dashboard users and roles</span></a></span></li><dd class="toc-abstract"><p>
  Dashboard user management performed by Ceph commands on the command line
  was already introduced in <a class="xref" href="#dashboard-user-roles" title="Chapter 11. Manage users and roles on the command line">Chapter 11, <em>Manage users and roles on the command line</em></a>.
 </p></dd><li><span class="chapter"><a href="#dashboard-cluster"><span class="title-number">4 </span><span class="title-name">View cluster internals</span></a></span></li><dd class="toc-abstract"><p>
  The <span class="guimenu">Cluster</span> menu item lets you view detailed information
  about Ceph cluster hosts, inventory, Ceph Monitors, services, OSDs, configuration,
  CRUSH Map, Ceph Manager, logs, and monitoring files.
 </p></dd><li><span class="chapter"><a href="#dashboard-pools"><span class="title-number">5 </span><span class="title-name">Manage pools</span></a></span></li><dd class="toc-abstract"><p>
   For more general information about Ceph pools, refer to
   <a class="xref" href="#ceph-pools" title="Chapter 18. Manage storage pools">Chapter 18, <em>Manage storage pools</em></a>. For information specific to erasure code
   pools, refer to <a class="xref" href="#cha-ceph-erasure" title="Chapter 19. Erasure coded pools">Chapter 19, <em>Erasure coded pools</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-rbds"><span class="title-number">6 </span><span class="title-name">Manage RADOS Block Device</span></a></span></li><dd class="toc-abstract"><p>
  To list all available RADOS Block Devices (RBDs), click
  <span class="guimenu">Block</span> › <span class="guimenu">Images</span>
  from the main menu.
 </p></dd><li><span class="chapter"><a href="#dash-webui-nfs"><span class="title-number">7 </span><span class="title-name">Manage NFS Ganesha</span></a></span></li><dd class="toc-abstract"><p>
  NFS Ganesha supports NFS version 4.1 and newer.
  It does not support NFS version 3.
 </p></dd><li><span class="chapter"><a href="#dashboard-mds"><span class="title-number">8 </span><span class="title-name">Manage CephFS</span></a></span></li><dd class="toc-abstract"><p>
   To find detailed information about CephFS, refer to
   <a class="xref" href="#cha-ceph-cephfs" title="Chapter 23. Clustered file system">Chapter 23, <em>Clustered file system</em></a>.
  </p></dd><li><span class="chapter"><a href="#dashboard-ogw"><span class="title-number">9 </span><span class="title-name">Manage the Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
   Before you begin, you may encounter the following notification when trying
   to access the Object Gateway front-end on the Ceph Dashboard:
  </p></dd><li><span class="chapter"><a href="#dashboard-initial-configuration"><span class="title-number">10 </span><span class="title-name">Manual configuration</span></a></span></li><dd class="toc-abstract"><p>
  This section introduces advanced information for users that prefer
  configuring dashboard settings manually on the command line.
 </p></dd><li><span class="chapter"><a href="#dashboard-user-roles"><span class="title-number">11 </span><span class="title-name">Manage users and roles on the command line</span></a></span></li><dd class="toc-abstract"><p>
  This section describes how to manage user accounts used by the Ceph Dashboard.
  It helps you create or modify user accounts, as well as set proper user roles
  and permissions.
 </p></dd></ul></div><section class="chapter" id="dashboard-about" data-id-title="About the Ceph Dashboard"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">1 </span><span class="title-name">About the Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#dashboard-about">#</a></h1></div></div></div><p>
  The Ceph Dashboard is a built-in Web-based Ceph management and monitoring
  application that administers various aspects and objects of the cluster. The
  dashboard is automatically enabled after the basic cluster is deployed in
  <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”</span>.
 </p><p>
  The Ceph Dashboard for SUSE Enterprise Storage 7.1 has added more Web-based
  management capabilities to make it easier to administer Ceph, including
  monitoring and application administration to the Ceph Manager. You no longer need to
  know complex Ceph-related commands to manage and monitor your Ceph
  cluster. You can either use the Ceph Dashboard's intuitive interface, or its
  built-in REST API.
 </p><p>
  The Ceph Dashboard module visualizes information and statistics about the Ceph
  cluster using a Web server hosted by <code class="literal">ceph-mgr</code>. See
  <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.2.3 “Ceph nodes and daemons”</span> for more details on Ceph Manager.
 </p></section><section class="chapter" id="dashboard-webui-general" data-id-title="Dashboards Web user interface"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">2 </span><span class="title-name">Dashboard's Web user interface</span> <a title="Permalink" class="permalink" href="#dashboard-webui-general">#</a></h1></div></div></div><section class="sect1" id="dashboard-webui-login" data-id-title="Logging in"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Logging in</span> <a title="Permalink" class="permalink" href="#dashboard-webui-login">#</a></h2></div></div></div><p>
   To log in to the Ceph Dashboard, point your browser to its URL including the
   port number. Run the following command to find the address:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr services | grep dashboard
"dashboard": "https://host:port/",</pre></div><p>
   The command returns the URL where the Ceph Dashboard is located. If you are
   having issues with this command, see
   <span class="intraxref">Book “Troubleshooting Guide”, Chapter 10 “Troubleshooting the Ceph Dashboard”, Section 10.1 “Locating the Ceph Dashboard”</span>.
  </p><div class="figure" id="id-1.4.3.3.3.5"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_login.png"><img src="images/dashboard_login.png" width="100%" alt="Ceph Dashboard login screen" title="Ceph Dashboard login screen"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.1: </span><span class="title-name">Ceph Dashboard login screen </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.3.5">#</a></h6></div></div><p>
   Log in by using the credentials that you created during cluster deployment
   (see <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.9 “Configuring the Ceph Dashboard login credentials”</span>).
  </p><div id="id-1.4.3.3.3.7" data-id-title="Custom user account" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom user account</h6><p>
    If you do not want to use the default <span class="emphasis"><em>admin</em></span> account to
    access the Ceph Dashboard, create a custom user account with administrator
    privileges. Refer to <a class="xref" href="#dashboard-user-roles" title="Chapter 11. Manage users and roles on the command line">Chapter 11, <em>Manage users and roles on the command line</em></a> for more
    details.
   </p></div><div id="id-1.4.3.3.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    As soon as an upgrade to a new Ceph major release (code name: Pacific) is
    available, the Ceph Dashboard will display a relevant message in the top
    notification area. To perform the upgrade, follow instructions in
    <span class="intraxref">Book “Deployment Guide”, Chapter 11 “Upgrade from SUSE Enterprise Storage 7 to 7.1”</span>.
   </p><div class="figure" id="id-1.4.3.3.3.8.2"><div class="figure-contents"><div class="mediaobject"><a href="images/ses7-motd-pacific.png"><img src="images/ses7-motd-pacific.png" width="100%" alt="Notification about a new SUSE Enterprise Storage release" title="Notification about a new SUSE Enterprise Storage release"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.2: </span><span class="title-name">Notification about a new SUSE Enterprise Storage release </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.3.8.2">#</a></h6></div></div></div><p>
   The dashboard user interface is graphically divided into several
   <span class="emphasis"><em>blocks</em></span>: the <span class="emphasis"><em>utility menu</em></span> in the
   top right-hand side of the screen, the <span class="emphasis"><em>main menu</em></span> on the
   left-hand side, and the main <span class="emphasis"><em>content pane</em></span>.
  </p><div class="figure" id="id-1.4.3.3.3.10"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_homepage.png"><img src="images/dashboard_homepage.png" width="90%" alt="Ceph Dashboard home page" title="Ceph Dashboard home page"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.3: </span><span class="title-name">Ceph Dashboard home page </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.3.10">#</a></h6></div></div></section><section class="sect1" id="dashboard-util-menu" data-id-title="Utility menu"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Utility menu</span> <a title="Permalink" class="permalink" href="#dashboard-util-menu">#</a></h2></div></div></div><p>
   The top right-hand side of the screen contains a utility menu. It includes
   general tasks related more to the dashboard than to the Ceph cluster. By
   clicking the options, you can access the following topics:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Change the dashboard's language interface to: Czech, German, English,
     Spanish, French, Indonesian, Italian, Japanese, Korean, Polish, Portuguese
     (Brazilian), and Chinese.
    </p></li><li class="listitem"><p>
     Tasks and notifications
    </p></li><li class="listitem"><p>
     View the documentation, information about the REST API, or further
     information about the dashboard.
    </p></li><li class="listitem"><p>
     User management and telemetry configuration.
    </p><div id="id-1.4.3.3.4.3.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      For more detailed command line descriptions for user roles, see
      <a class="xref" href="#dashboard-user-roles" title="Chapter 11. Manage users and roles on the command line">Chapter 11, <em>Manage users and roles on the command line</em></a>.
     </p></div></li><li class="listitem"><p>
     Log in configuration; change the password or sign out.
    </p></li></ul></div></section><section class="sect1" id="dashboard-main-menu" data-id-title="Main menu"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Main menu</span> <a title="Permalink" class="permalink" href="#dashboard-main-menu">#</a></h2></div></div></div><p>
   The dashboard's main menu occupies the left-hand side of the screen. It
   covers the following topics:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.5.3.1"><span class="term"><span class="guimenu">Dashboard</span></span></dt><dd><p>
      Return to Ceph Dashboard's home page.
     </p></dd><dt id="id-1.4.3.3.5.3.2"><span class="term"><span class="guimenu">Cluster</span></span></dt><dd><p>
      View detailed information about hosts, inventory, Ceph Monitors, services,
      Ceph OSDs, cluster configuration, CRUSH Map, Ceph Manager modules, logs, and
      monitoring.
     </p></dd><dt id="id-1.4.3.3.5.3.3"><span class="term"><span class="guimenu">Pools</span></span></dt><dd><p>
      View and manage cluster pools.
     </p></dd><dt id="id-1.4.3.3.5.3.4"><span class="term"><span class="guimenu">Block</span></span></dt><dd><p>
      View detailed information and manage RADOS Block Device images, mirroring, and
      iSCSI.
     </p></dd><dt id="id-1.4.3.3.5.3.5"><span class="term"><span class="guimenu">NFS</span></span></dt><dd><p>
      View and manage NFS Ganesha deployments.
     </p><div id="id-1.4.3.3.5.3.5.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If NFS Ganesha is not deployed, an information notice appears. See
       <a class="xref" href="#dashboard-config-nfs-ganesha" title="11.6. Configuring NFS Ganesha in the Ceph Dashboard">Section 11.6, “Configuring NFS Ganesha in the Ceph Dashboard”</a>.
      </p></div></dd><dt id="id-1.4.3.3.5.3.6"><span class="term"><span class="guimenu">Filesystems</span></span></dt><dd><p>
      View and manage CephFSs.
     </p></dd><dt id="id-1.4.3.3.5.3.7"><span class="term"><span class="guimenu">Object Gateway</span></span></dt><dd><p>
      View and manage Object Gateway's daemons, users, and buckets.
     </p><div id="id-1.4.3.3.5.3.7.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If Object Gateway is not deployed, an informative notice appears. See
       <a class="xref" href="#dashboard-ogw-enabling" title="10.4. Enabling the Object Gateway management front-end">Section 10.4, “Enabling the Object Gateway management front-end”</a>.
      </p></div></dd></dl></div></section><section class="sect1" id="dashboard-cpane" data-id-title="Content pane"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Content pane</span> <a title="Permalink" class="permalink" href="#dashboard-cpane">#</a></h2></div></div></div><p>
   The content pane occupies the main part of the dashboard's screen. The
   dashboard home page shows plenty of helpful widgets to inform you briefly
   about the current status of the cluster, capacity, and performance
   information.
  </p></section><section class="sect1" id="dashboard-ui-common" data-id-title="Common Web UI features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Common Web UI features</span> <a title="Permalink" class="permalink" href="#dashboard-ui-common">#</a></h2></div></div></div><p>
   In Ceph Dashboard, you often work with <span class="emphasis"><em>lists</em></span>—for
   example, lists of pools, OSD nodes, or RBD devices. All lists will
   automatically refresh themselves by default every five seconds. The
   following common widgets help you manage or adjust these list:
  </p><p>
   Click <span class="inlinemediaobject"><a href="images/oa_widget_reload.png"><img src="images/oa_widget_reload.png" width="15" alt="Image" title="Image"/></a></span> to trigger a manual refresh of the list.
  </p><p>
   Click <span class="inlinemediaobject"><a href="images/oa_widget_columns.png"><img src="images/oa_widget_columns.png" width="16" alt="Image" title="Image"/></a></span> to display or hide individual table columns.
  </p><p>
   Click <span class="inlinemediaobject"><a href="images/oa_widget_rows.png"><img src="images/oa_widget_rows.png" width="23" alt="Image" title="Image"/></a></span> and enter (or select) how many rows to display on a
   single page.
  </p><p>
   Click inside <span class="inlinemediaobject"><a href="images/oa_widget_search.png"><img src="images/oa_widget_search.png" width="63" alt="Image" title="Image"/></a></span> and filter the rows by typing the string to search for.
  </p><p>
   Use <span class="inlinemediaobject"><a href="images/oa_widget_pager.png"><img src="images/oa_widget_pager.png" width="75" alt="Image" title="Image"/></a></span> to change the currently displayed page if the list
   spans across multiple pages.
  </p></section><section class="sect1" id="dashboard-widgets" data-id-title="Dashboard widgets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Dashboard widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets">#</a></h2></div></div></div><p>
   Each dashboard widget shows specific status information related to a
   specific aspect of a running Ceph cluster. Some widgets are active links
   and after clicking them, they will redirect you to a related detailed page
   of the topic they represent.
  </p><div id="id-1.4.3.3.8.3" data-id-title="More details on mouse over" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More details on mouse over</h6><p>
    Some graphical widgets show you more detail when you move the mouse over
    them.
   </p></div><section class="sect2" id="dashboard-widgets-status" data-id-title="Status widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.6.1 </span><span class="title-name">Status widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-status">#</a></h3></div></div></div><p>
    <span class="guimenu">Status</span> widgets give you a brief overview about the
    cluster's current status.
   </p><div class="figure" id="id-1.4.3.3.8.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_status.png"><img src="images/dashboard_widgets_status.png" width="100%" alt="Status widgets" title="Status widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.4: </span><span class="title-name">Status widgets </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.4.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.8.4.4.1"><span class="term"><span class="guimenu">Cluster Status</span></span></dt><dd><p>
       Presents basic information about the cluster's health.
      </p></dd><dt id="id-1.4.3.3.8.4.4.2"><span class="term"><span class="guimenu">Hosts</span></span></dt><dd><p>
       Shows the total number of cluster nodes.
      </p></dd><dt id="id-1.4.3.3.8.4.4.3"><span class="term"><span class="guimenu">Monitors</span></span></dt><dd><p>
       Shows the number of running monitors and their quorum.
      </p></dd><dt id="id-1.4.3.3.8.4.4.4"><span class="term"><span class="guimenu">OSDs</span></span></dt><dd><p>
       Shows the total number of OSDs, as well as the number of
       <span class="emphasis"><em>up</em></span> and <span class="emphasis"><em>in</em></span> OSDs.
      </p></dd><dt id="id-1.4.3.3.8.4.4.5"><span class="term"><span class="guimenu">Managers</span></span></dt><dd><p>
       Shows the number of active and standby Ceph Manager daemons.
      </p></dd><dt id="id-1.4.3.3.8.4.4.6"><span class="term"><span class="guimenu">Object Gateways</span></span></dt><dd><p>
       Shows the number of running Object Gateways.
      </p></dd><dt id="id-1.4.3.3.8.4.4.7"><span class="term"><span class="guimenu">Metadata Servers</span></span></dt><dd><p>
       Shows the number of Metadata Servers.
      </p></dd><dt id="id-1.4.3.3.8.4.4.8"><span class="term"><span class="guimenu">iSCSI Gateways</span></span></dt><dd><p>
       Shows the number of configured iSCSI gateways.
      </p></dd></dl></div></section><section class="sect2" id="dashboard-widgets-capacity" data-id-title="Capacity widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.6.2 </span><span class="title-name">Capacity widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-capacity">#</a></h3></div></div></div><p>
    <span class="guimenu">Capacity</span> widgets show brief information about the
    storage capacity.
   </p><div class="figure" id="id-1.4.3.3.8.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_capacity.png"><img src="images/dashboard_widgets_capacity.png" width="100%" alt="Capacity widgets" title="Capacity widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.5: </span><span class="title-name">Capacity widgets </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.5.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.8.5.4.1"><span class="term"><span class="guimenu">Raw Capacity</span></span></dt><dd><p>
       Shows the ratio of used and available raw storage capacity.
      </p></dd><dt id="id-1.4.3.3.8.5.4.2"><span class="term"><span class="guimenu">Objects</span></span></dt><dd><p>
       Shows the number of data objects stored in the cluster.
      </p></dd><dt id="id-1.4.3.3.8.5.4.3"><span class="term"><span class="guimenu">PG Status</span></span></dt><dd><p>
       Displays a chart of the placement groups according to their status.
      </p></dd><dt id="id-1.4.3.3.8.5.4.4"><span class="term"><span class="guimenu">Pools</span></span></dt><dd><p>
       Shows the number of pools in the cluster.
      </p></dd><dt id="id-1.4.3.3.8.5.4.5"><span class="term"><span class="guimenu">PGs per OSD</span></span></dt><dd><p>
       Shows the average number of placement groups per OSD.
      </p></dd></dl></div></section><section class="sect2" id="dashboard-widgets-performance" data-id-title="Performance widgets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.6.3 </span><span class="title-name">Performance widgets</span> <a title="Permalink" class="permalink" href="#dashboard-widgets-performance">#</a></h3></div></div></div><p>
    <span class="guimenu">Performance</span> widgets refer to basic performance data of
    Ceph clients.
   </p><div class="figure" id="id-1.4.3.3.8.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_widgets_perf.png"><img src="images/dashboard_widgets_perf.png" width="100%" alt="performance widgets" title="performance widgets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.6: </span><span class="title-name">performance widgets </span><a title="Permalink" class="permalink" href="#id-1.4.3.3.8.6.3">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.3.8.6.4.1"><span class="term"><span class="guimenu">Client Read/Write</span></span></dt><dd><p>
       The amount of clients' read and write operations per second.
      </p></dd><dt id="id-1.4.3.3.8.6.4.2"><span class="term"><span class="guimenu">Client Throughput</span></span></dt><dd><p>
       The amount of data transferred to and from Ceph clients in bytes per
       second.
      </p></dd><dt id="id-1.4.3.3.8.6.4.3"><span class="term"><span class="guimenu">Recovery Throughput</span></span></dt><dd><p>
       The throughput of data recovered per second.
      </p></dd><dt id="id-1.4.3.3.8.6.4.4"><span class="term"><span class="guimenu">Scrubbing</span></span></dt><dd><p>
       Shows the scrubbing (see <a class="xref" href="#op-pg-scrubpg" title="17.4.9. Scrubbing a placement group">Section 17.4.9, “Scrubbing a placement group”</a>) status. It is
       either <code class="option">inactive</code>, <code class="option">enabled</code>, or
       <code class="option">active</code>.
      </p></dd></dl></div></section></section></section><section class="chapter" id="dashboard-user-mgmt" data-id-title="Manage Ceph Dashboard users and roles"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">3 </span><span class="title-name">Manage Ceph Dashboard users and roles</span> <a title="Permalink" class="permalink" href="#dashboard-user-mgmt">#</a></h1></div></div></div><p>
  Dashboard user management performed by Ceph commands on the command line
  was already introduced in <a class="xref" href="#dashboard-user-roles" title="Chapter 11. Manage users and roles on the command line">Chapter 11, <em>Manage users and roles on the command line</em></a>.
 </p><p>
  This section describes how to manage user accounts by using the Dashboard Web
  user interface.
 </p><section class="sect1" id="dashboard-listing-users" data-id-title="Listing users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Listing users</span> <a title="Permalink" class="permalink" href="#dashboard-listing-users">#</a></h2></div></div></div><p>
   Click <span class="inlinemediaobject"><a href="images/dash_icon_gear.png"><img src="images/dash_icon_gear.png" width="16" alt="Image" title="Image"/></a></span> in the utility menu and select <span class="guimenu">User
   Management</span>.
  </p><p>
   The list contains each user's user name, full name, e-mail, a list of
   assigned roles, whether the role is enabled, and the password expiration
   date.
  </p><div class="figure" id="id-1.4.3.4.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_users.png"><img src="images/dash_users.png" width="100%" alt="User management" title="User management"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.1: </span><span class="title-name">User management </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.5.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-adding-users" data-id-title="Adding new users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Adding new users</span> <a title="Permalink" class="permalink" href="#dashboard-adding-users">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Create</span> in the top left of the table heading to add
   a new user. Enter their user name, password, and optionally a full name and
   an e-mail.
  </p><div class="figure" id="id-1.4.3.4.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_user_add.png"><img src="images/dash_user_add.png" width="100%" alt="Adding a user" title="Adding a user"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.2: </span><span class="title-name">Adding a user </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.6.3">#</a></h6></div></div><p>
   Click the little pen icon to assign predefined roles to the user. Confirm
   with <span class="guimenu">Create User</span>.
  </p></section><section class="sect1" id="dashboard-editing-users" data-id-title="Editing users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.3 </span><span class="title-name">Editing users</span> <a title="Permalink" class="permalink" href="#dashboard-editing-users">#</a></h2></div></div></div><p>
   Click a user's table row to highlight the selection Select
   <span class="guimenu">Edit</span> to edit details about the user. Confirm with
   <span class="guimenu">Edit User</span>.
  </p></section><section class="sect1" id="dashboard-deleting-users" data-id-title="Deleting users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.4 </span><span class="title-name">Deleting users</span> <a title="Permalink" class="permalink" href="#dashboard-deleting-users">#</a></h2></div></div></div><p>
   Click a user's table row to highlight the selection Select the drop-down
   box next to <span class="guimenu">Edit</span> and select <span class="guimenu">Delete</span>
   from the list to delete the user account. Activate the <span class="guimenu">Yes, I am
   sure</span> check box and confirm with <span class="guimenu">Delete User</span>.
  </p></section><section class="sect1" id="dashboard-listing-user-roles" data-id-title="Listing user roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.5 </span><span class="title-name">Listing user roles</span> <a title="Permalink" class="permalink" href="#dashboard-listing-user-roles">#</a></h2></div></div></div><p>
   Click <span class="inlinemediaobject"><a href="images/dash_icon_gear.png"><img src="images/dash_icon_gear.png" width="16" alt="Image" title="Image"/></a></span> in the utility menu and select <span class="guimenu">User
   Management</span>. Then click the <span class="guimenu">Roles</span> tab.
  </p><p>
   The list contains each role's name, description, and whether it is a system
   role.
  </p><div class="figure" id="id-1.4.3.4.9.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_roles.png"><img src="images/dash_roles.png" width="100%" alt="User roles" title="User roles"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.3: </span><span class="title-name">User roles </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.9.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-adding-roles" data-id-title="Adding custom roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.6 </span><span class="title-name">Adding custom roles</span> <a title="Permalink" class="permalink" href="#dashboard-adding-roles">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Create</span> in the top left of the table heading to add
   a new custom role. Enter the <span class="guimenu">Name</span> and
   <span class="guimenu">Description</span> and next to <span class="guimenu">Permissions</span>,
   select the appropriate permissions.
  </p><div id="id-1.4.3.4.10.3" data-id-title="Purging custom roles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Purging custom roles</h6><p>
    If you create custom user roles and intend to remove the Ceph cluster
    with the <code class="command">ceph-salt purge</code> command later on, you need to
    purge the custom roles first. Find more details in
    <a class="xref" href="#ceph-cluster-purge" title="13.9. Removing an entire Ceph cluster">Section 13.9, “Removing an entire Ceph cluster”</a>.
   </p></div><div class="figure" id="id-1.4.3.4.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_roles_add.png"><img src="images/dash_roles_add.png" width="70%" alt="Adding a role" title="Adding a role"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.4: </span><span class="title-name">Adding a role </span><a title="Permalink" class="permalink" href="#id-1.4.3.4.10.4">#</a></h6></div></div><div id="id-1.4.3.4.10.5" data-id-title="Multiple activation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple activation</h6><p>
    By activating the check box that precedes the topic name, you activate all
    permissions for that topic. By activating the <span class="guimenu">All</span> check
    box, you activate all permissions for all the topics.
   </p></div><p>
   Confirm with <span class="guimenu">Create Role</span>.
  </p></section><section class="sect1" id="dashboard-editing-roles" data-id-title="Editing custom roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.7 </span><span class="title-name">Editing custom roles</span> <a title="Permalink" class="permalink" href="#dashboard-editing-roles">#</a></h2></div></div></div><p>
   Click a user's table row to highlight the selection Select
   <span class="guimenu">Edit</span> in the top left of the table heading to edit a
   description and permissions of the custom role. Confirm with <span class="guimenu">Edit
   Role</span>.
  </p></section><section class="sect1" id="dashboard-deleting-roles" data-id-title="Deleting custom roles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.8 </span><span class="title-name">Deleting custom roles</span> <a title="Permalink" class="permalink" href="#dashboard-deleting-roles">#</a></h2></div></div></div><p>
   Click a role's table row to highlight the selection Select the drop-down
   box next to <span class="guimenu">Edit</span> and select <span class="guimenu">Delete</span>
   from the list to delete the role. Activate the <span class="guimenu">Yes, I am
   sure</span> check box and confirm with <span class="guimenu">Delete Role</span>.
  </p></section></section><section class="chapter" id="dashboard-cluster" data-id-title="View cluster internals"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">4 </span><span class="title-name">View cluster internals</span> <a title="Permalink" class="permalink" href="#dashboard-cluster">#</a></h1></div></div></div><p>
  The <span class="guimenu">Cluster</span> menu item lets you view detailed information
  about Ceph cluster hosts, inventory, Ceph Monitors, services, OSDs, configuration,
  CRUSH Map, Ceph Manager, logs, and monitoring files.
 </p><section class="sect1" id="dashboard-cluster-hosts" data-id-title="Viewing cluster nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Viewing cluster nodes</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-hosts">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Hosts</span>
   to view a list of cluster nodes.
  </p><div class="figure" id="id-1.4.3.5.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_hosts.png"><img src="images/dash_hosts.png" width="100%" alt="Hosts" title="Hosts"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.1: </span><span class="title-name">Hosts </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.4.3">#</a></h6></div></div><p>
   Click the drop-down arrow next to a node name in the
   <span class="guimenu">Hostname</span> column to view the performance details of the
   node.
  </p><p>
   The <span class="guimenu">Services</span> column lists all daemons that are running on
   each related node. Click a daemon name to view its detailed configuration.
  </p></section><section class="sect1" id="dashboard-cluster-inventory" data-id-title="Listing physical disks"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Listing physical disks</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-inventory">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Cluster</span> › <span class="guimenu">Physical
   Disks</span> to view a list of physical disks. The list
   includes the device path, type, availability, vendor, model, size, and the
   OSDs.
  </p><p>
   Click to select a node name in the <span class="guimenu">Hostname</span> column. When
   selected, click <span class="guimenu">Identify</span> to identify the device the host
   is running on. This tells the device to blink its LEDs. Select the duration
   of this action between 1, 2, 5, 10, or 15 minutes. Click
   <span class="guimenu">Execute</span>.
  </p><div class="figure" id="id-1.4.3.5.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_inventory.png"><img src="images/dash_inventory.png" width="100%" alt="Physical disks" title="Physical disks"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.2: </span><span class="title-name">Physical disks </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.5.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-monitors" data-id-title="Viewing Ceph Monitors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">Viewing Ceph Monitors</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-monitors">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Monitors</span>
   to view a list of cluster nodes with running Ceph monitors. The content
   pane is split into two views: <code class="literal">Status</code>, and <code class="literal">In
   Quorum</code> or <code class="literal">Not In Quorum</code>.
  </p><p>
   The <span class="guimenu">Status</span> table shows general statistics about the
   running Ceph Monitors, including the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Cluster ID
    </p></li><li class="listitem"><p>
     monmap modified
    </p></li><li class="listitem"><p>
     monmap epoch
    </p></li><li class="listitem"><p>
     quorum con
    </p></li><li class="listitem"><p>
     quorum mon
    </p></li><li class="listitem"><p>
     required con
    </p></li><li class="listitem"><p>
     required mon
    </p></li></ul></div><p>
   The <code class="literal">In Quorum</code> and <code class="literal">Not In Quorum</code> panes
   include each monitor's name, rank number, public IP address, and number of
   open sessions.
  </p><p>
   Click a node name in the <span class="guimenu">Name</span> column to view the related
   Ceph Monitor configuration.
  </p><div class="figure" id="id-1.4.3.5.6.7"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_mons.png"><img src="images/dash_mons.png" width="100%" alt="Ceph Monitors" title="Ceph Monitors"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.3: </span><span class="title-name">Ceph Monitors </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.6.7">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-services" data-id-title="Displaying services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.4 </span><span class="title-name">Displaying services</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-services">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Services</span>
   to view details on each of the available services: <code class="literal">crash</code>,
   Ceph Manager, and Ceph Monitors. The list includes the container image name, container
   image ID, status of what is running, size, and when it was last refreshed.
  </p><p>
   Click the drop-down arrow next to a service name in the
   <span class="guimenu">Service</span> column to view details of the daemon. The detail
   list includes the host name, daemon type, daemon ID, container ID, container
   image name, container image ID, version number, status, and when it was last
   refreshed.
  </p><div class="figure" id="id-1.4.3.5.7.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_services.png"><img src="images/dash_services.png" width="100%" alt="Services" title="Services"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.4: </span><span class="title-name">Services </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.7.4">#</a></h6></div></div><section class="sect2" id="dashboard-cluster-services-add" data-id-title="Adding new cluster services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.4.1 </span><span class="title-name">Adding new cluster services</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-services-add">#</a></h3></div></div></div><p>
    To add a new service to a cluster, click the <span class="guimenu">Create</span>
    button in the top left corner of the <span class="guimenu">Services</span> table.
   </p><p>
    In the <span class="guimenu">Create Service</span> window, specify the type of the
    service and then fill the required options that are relevant for the
    service you previously selected. Confirm with <span class="guimenu">Create
    Service</span>.
   </p><div class="figure" id="id-1.4.3.5.7.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_services_add.png"><img src="images/dash_services_add.png" width="75%" alt="An overlay window with the new service specification" title="An overlay window with the new service specification"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.5: </span><span class="title-name">Creating a new cluster service </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.7.5.4">#</a></h6></div></div></section></section><section class="sect1" id="dashboard-cluster-osds" data-id-title="Displaying Ceph OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.5 </span><span class="title-name">Displaying Ceph OSDs</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-osds">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">OSDs</span>
   to view a list of nodes with running OSD daemons. The list includes each
   node's name, ID, status, device class, number of placement groups, size,
   usage, reads/writes chart in time, and the rate of read/write operations per
   second.
  </p><div class="figure" id="id-1.4.3.5.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds.png"><img src="images/dash_osds.png" width="100%" alt="Ceph OSDs" title="Ceph OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.6: </span><span class="title-name">Ceph OSDs </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.3">#</a></h6></div></div><p>
   Select <span class="guimenu">Flags</span> from the <span class="guimenu">Cluster-wide
   configuration</span> drop-down menu in the table heading to open a pop-up
   window. This has a list of flags that apply to the whole cluster. You can
   activate or deactivate individual flags, and confirm with
   <span class="guimenu">Submit</span>.
  </p><div class="figure" id="id-1.4.3.5.8.5"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds_flags.png"><img src="images/dash_osds_flags.png" width="70%" alt="OSD flags" title="OSD flags"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.7: </span><span class="title-name">OSD flags </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.5">#</a></h6></div></div><p>
   Select <span class="guimenu">Recovery Priority</span> from the <span class="guimenu">Cluster-wide
   configuration</span> drop-down menu in the table heading to open a pop-up
   window. This has a list of OSD recovery priorities that apply to the whole
   cluster. You can activate the preferred priority profile and fine-tune the
   individual values below. Confirm with <span class="guimenu">Submit</span>.
  </p><div class="figure" id="id-1.4.3.5.8.7"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osds_recover.png"><img src="images/dash_osds_recover.png" width="70%" alt="OSD recovery priority" title="OSD recovery priority"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.8: </span><span class="title-name">OSD recovery priority </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.7">#</a></h6></div></div><p>
   Click the drop-down arrow next to a node name in the <span class="guimenu">Host</span>
   column to view an extended table with details about the device settings and
   performance. Browsing through several tabs, you can see lists of
   <span class="guimenu">Attributes</span>, <span class="guimenu">Metadata</span>, <span class="guimenu">Device
   health</span>, <span class="guimenu">Performance counter</span>, a graphical
   <span class="guimenu">Histogram</span> of reads and writes, and <span class="guimenu">Performance
   details</span>.
  </p><div class="figure" id="id-1.4.3.5.8.9"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_details.png"><img src="images/dash_osd_details.png" width="100%" alt="OSD details" title="OSD details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.9: </span><span class="title-name">OSD details </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.9">#</a></h6></div></div><div id="id-1.4.3.5.8.10" data-id-title="Performing specific tasks on OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Performing specific tasks on OSDs</h6><p>
    After you click an OSD node name, the table row is highlighted. This means
    that you can now perform a task on the node. You can choose to perform any
    of the following actions: <span class="guimenu">Edit</span>,
    <span class="guimenu">Create</span>, <span class="guimenu">Scrub</span>, <span class="guimenu">Deep
    Scrub</span>, <span class="guimenu">Reweight</span>, <span class="guimenu">Mark out</span>,
    <span class="guimenu">Mark In</span>, <span class="guimenu">Mark Down</span>, <span class="guimenu">Mark
    Lost</span>, <span class="guimenu">Purge</span>, <span class="guimenu">Destroy</span>, or
    <span class="guimenu">Delete</span>.
   </p><p>
    Click the down arrow in the top left of the table heading next to the
    <span class="guimenu">Create</span> button and select the task you want to perform.
   </p></div><section class="sect2" id="dashboard-cluster-osds-add" data-id-title="Adding OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.5.1 </span><span class="title-name">Adding OSDs</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-osds-add">#</a></h3></div></div></div><p>
    To add new OSDs, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Verify that some cluster nodes have storage devices whose status is
      <code class="literal">available</code>. Then click the down arrow in the top left
      of the table heading and select <span class="guimenu">Create</span>. This opens the
      <span class="guimenu">Create OSDs</span> window.
     </p><div class="figure" id="id-1.4.3.5.8.11.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_add.png"><img src="images/dash_osd_add.png" width="75%" alt="Create OSDs" title="Create OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.10: </span><span class="title-name">Create OSDs </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.11.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      To add primary storage devices for OSDs, click <span class="guimenu">Add</span>.
      Before you can add storage devices, you need to specify filtering
      criteria in the top right of the <span class="guimenu">Primary devices</span>
      table—for example <span class="guimenu">Type</span> <span class="guimenu">hdd</span>.
      Confirm with <span class="guimenu">Add</span>.
     </p><div class="figure" id="id-1.4.3.5.8.11.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_add_primary.png"><img src="images/dash_osd_add_primary.png" width="100%" alt="Adding primary devices" title="Adding primary devices"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.11: </span><span class="title-name">Adding primary devices </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.11.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      In the updated <span class="guimenu">Create OSDs</span> window, optionally add
      shared WAL and BD devices, or enable device encryption.
     </p><div class="figure" id="id-1.4.3.5.8.11.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_add_primary_created.png"><img src="images/dash_osd_add_primary_created.png" width="75%" alt="Create OSDs with primary devices added" title="Create OSDs with primary devices added"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.12: </span><span class="title-name">Create OSDs with primary devices added </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.11.3.3.2">#</a></h6></div></div></li><li class="step"><p>
      Click <span class="guimenu">Preview</span> to view the preview of DriveGroups
      specification for the devices you previously added. Confirm with
      <span class="guimenu">Create</span>.
     </p><div class="figure" id="id-1.4.3.5.8.11.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_add_primary_preview.png"><img src="images/dash_osd_add_primary_preview.png" width="75%" alt="" title=""/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.13: </span><span class="title-name"> </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.11.3.4.2">#</a></h6></div></div></li><li class="step"><p>
      New devices will be added to the list of OSDs.
     </p><div class="figure" id="id-1.4.3.5.8.11.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_osd_add_primary_added.png"><img src="images/dash_osd_add_primary_added.png" width="100%" alt="Newly added OSDs" title="Newly added OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.14: </span><span class="title-name">Newly added OSDs </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.8.11.3.5.2">#</a></h6></div></div><div id="id-1.4.3.5.8.11.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       There is no progress visualization of the OSD creation process. It takes
       some time before they are actually created. The OSDs will appear in the
       list when they have been deployed. If you want to check the deployment
       status, view the logs by clicking
       <span class="guimenu">Cluster</span> › <span class="guimenu">Logs</span>.
      </p></div></li></ol></div></div></section></section><section class="sect1" id="dashboard-cluster-config" data-id-title="Viewing cluster configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.6 </span><span class="title-name">Viewing cluster configuration</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-config">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Configuration</span>
   to view a complete list of Ceph cluster configuration options. The list
   contains the name of the option, its short description, and its current and
   default values, and whether the option is editable.
  </p><div class="figure" id="id-1.4.3.5.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_cluster_config.png"><img src="images/dash_cluster_config.png" width="100%" alt="Cluster configuration" title="Cluster configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.15: </span><span class="title-name">Cluster configuration </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.9.3">#</a></h6></div></div><p>
   Click the drop-down arrow next to a configuration option in the
   <span class="guimenu">Name</span> column to view an extended table with detailed
   information about the option, such as its type of value, minimum and maximum
   permitted values, whether it can be updated at runtime, and many more.
  </p><p>
   After highlighting a specific option, you can edit its value(s) by clicking
   the <span class="guimenu">Edit</span> button in the top left of the table heading.
   Confirm changes with <span class="guimenu">Save</span>.
  </p></section><section class="sect1" id="dashboard-cluster-crushmap" data-id-title="Viewing the CRUSH Map"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.7 </span><span class="title-name">Viewing the CRUSH Map</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-crushmap">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Cluster</span> › <span class="guimenu">CRUSH
   map</span> to view a CRUSH Map of the cluster. For more
   general information on CRUSH Maps, refer to <a class="xref" href="#op-crush" title="17.5. CRUSH Map manipulation">Section 17.5, “CRUSH Map manipulation”</a>.
  </p><p>
   Click the root, nodes, or individual OSDs to view more detailed information,
   such as crush weight, depth in the map tree, device class of the OSD, and
   many more.
  </p><div class="figure" id="id-1.4.3.5.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_crushmap.png"><img src="images/dash_crushmap.png" width="100%" alt="CRUSH Map" title="CRUSH Map"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.16: </span><span class="title-name">CRUSH Map </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.10.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-mgr-plugins" data-id-title="Viewing manager modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.8 </span><span class="title-name">Viewing manager modules</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-mgr-plugins">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Cluster</span> › <span class="guimenu">Manager
   modules</span> to view a list of available Ceph Manager modules.
   Each line consists of a module name and information on whether it is
   currently enabled or not.
  </p><div class="figure" id="id-1.4.3.5.11.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_mgr_modules.png"><img src="images/dash_mgr_modules.png" width="70%" alt="Manager modules" title="Manager modules"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.17: </span><span class="title-name">Manager modules </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.11.3">#</a></h6></div></div><p>
   Click the drop-down arrow next to a module in the <span class="guimenu">Name</span>
   column to view an extended table with detailed settings in the
   <span class="guimenu">Details</span> table below. Edit them by clicking the
   <span class="guimenu">Edit</span> button in the top left of the table heading. Confirm
   changes with <span class="guimenu">Update</span>.
  </p><p>
   Click the drop-down arrow next to the <span class="guimenu">Edit</span> button in the
   top left of the table heading to <span class="guimenu">Enable</span> or
   <span class="guimenu">Disable</span> a module.
  </p></section><section class="sect1" id="dashboard-cluster-logs" data-id-title="Viewing logs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.9 </span><span class="title-name">Viewing logs</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-logs">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Logs</span>
   to view a list of cluster's recent log entries. Each line consists of a time
   stamp, the type of the log entry, and the logged message itself.
  </p><p>
   Click the <span class="guimenu">Audit Logs</span> tab to view log entries of the
   auditing subsystem. Refer to <a class="xref" href="#dashboard-auditing" title="11.5. Auditing API requests">Section 11.5, “Auditing API requests”</a> for
   commands to enable or disable auditing.
  </p><div class="figure" id="id-1.4.3.5.12.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_logs.png"><img src="images/dash_logs.png" width="80%" alt="Logs" title="Logs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 4.18: </span><span class="title-name">Logs </span><a title="Permalink" class="permalink" href="#id-1.4.3.5.12.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-cluster-monitoring" data-id-title="Viewing monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.10 </span><span class="title-name">Viewing monitoring</span> <a title="Permalink" class="permalink" href="#dashboard-cluster-monitoring">#</a></h2></div></div></div><p>
   Click
   <span class="guimenu">Cluster</span> › <span class="guimenu">Monitoring</span>
   to manage and view details on Prometheus alerts.
  </p><p>
   If you have Prometheus active, in this content pane you can view detailed
   information on <span class="guimenu">Active Alerts</span>, <span class="guimenu">All
   Alerts</span>, or <span class="guimenu">Silences</span>.
  </p><div id="id-1.4.3.5.13.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    If you do not have Prometheus deployed, an information banner will appear
    and link to relevant documentation.
   </p></div></section></section><section class="chapter" id="dashboard-pools" data-id-title="Manage pools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">5 </span><span class="title-name">Manage pools</span> <a title="Permalink" class="permalink" href="#dashboard-pools">#</a></h1></div></div></div><div id="id-1.4.3.6.3" data-id-title="More information on pools" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More information on pools</h6><p>
   For more general information about Ceph pools, refer to
   <a class="xref" href="#ceph-pools" title="Chapter 18. Manage storage pools">Chapter 18, <em>Manage storage pools</em></a>. For information specific to erasure code
   pools, refer to <a class="xref" href="#cha-ceph-erasure" title="Chapter 19. Erasure coded pools">Chapter 19, <em>Erasure coded pools</em></a>.
  </p></div><p>
  To list all available pools, click <span class="guimenu">Pools</span> from the main
  menu.
 </p><p>
  The list shows each pool's name, type, related application, placement group
  status, replica size, last change, erasure coded profile, crush ruleset,
  usage, and read/write statistics.
 </p><div class="figure" id="id-1.4.3.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools.png"><img src="images/oa_pools.png" width="100%" alt="List of pools" title="List of pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.1: </span><span class="title-name">List of pools </span><a title="Permalink" class="permalink" href="#id-1.4.3.6.6">#</a></h6></div></div><p>
  Click the drop-down arrow next to a pool name in the <span class="guimenu">Name</span>
  column to view an extended table with detailed information on the pool, such
  as the general details, performance details, and configuration.
 </p><section class="sect1" id="dashboard-pools-create" data-id-title="Adding a new pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Adding a new pool</span> <a title="Permalink" class="permalink" href="#dashboard-pools-create">#</a></h2></div></div></div><p>
   To add a new pool, click <span class="guimenu">Create</span> in the top left of the
   pools table. In the pool form you can enter the pool's name, type, its
   applications, compression mode, and quotas including maximum byes and
   maximum objects. The pool form itself pre-calculates the number of placement
   groups that best suited to this specific pool. The calculation is based on
   the amount of OSDs in the cluster and the selected pool type with its
   specific settings. As soon as a placement groups number is set manually, it
   will be replaced by a calculated number. Confirm with <span class="guimenu">Create
   Pool</span>.
  </p><div class="figure" id="id-1.4.3.6.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_pools_add.png"><img src="images/oa_pools_add.png" width="70%" alt="Adding a new pool" title="Adding a new pool"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 5.2: </span><span class="title-name">Adding a new pool </span><a title="Permalink" class="permalink" href="#id-1.4.3.6.8.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-pools-delete" data-id-title="Deleting pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.2 </span><span class="title-name">Deleting pools</span> <a title="Permalink" class="permalink" href="#dashboard-pools-delete">#</a></h2></div></div></div><p>
   To delete a pool, select the pool in the table row. Click the drop-down
   arrow next to the <span class="guimenu">Create</span> button and click
   <span class="guimenu">Delete</span>.
  </p></section><section class="sect1" id="dashboard-pools-edit" data-id-title="Editing a pools options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.3 </span><span class="title-name">Editing a pool's options</span> <a title="Permalink" class="permalink" href="#dashboard-pools-edit">#</a></h2></div></div></div><p>
   To edit a pool's options, select the pool in the table row and click
   <span class="guimenu">Edit</span> in the top left of the pools table.
  </p><p>
   You can change the name of the pool, increase the number of placement
   groups, change the list of the pool's applications and compression settings.
   Confirm with <span class="guimenu">Edit Pool</span>.
  </p></section></section><section class="chapter" id="dashboard-rbds" data-id-title="Manage RADOS Block Device"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">6 </span><span class="title-name">Manage RADOS Block Device</span> <a title="Permalink" class="permalink" href="#dashboard-rbds">#</a></h1></div></div></div><p>
  To list all available RADOS Block Devices (RBDs), click
  <span class="guimenu">Block</span> › <span class="guimenu">Images</span>
  from the main menu.
 </p><p>
  The list shows brief information about the device, such as the device's name,
  the related pool name, namespace, size of the device, number and size of
  objects on the device, details on the provisioning of the details, and the
  parent.
 </p><div class="figure" id="id-1.4.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images.png"><img src="images/dash_rbd_images.png" width="100%" alt="List of RBD images" title="List of RBD images"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.1: </span><span class="title-name">List of RBD images </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.5">#</a></h6></div></div><section class="sect1" id="dashboard-rbds-details" data-id-title="Viewing details about RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Viewing details about RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-details">#</a></h2></div></div></div><p>
   To view more detailed information about a device, click its row in the
   table:
  </p><div class="figure" id="id-1.4.3.7.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_details.png"><img src="images/dash_rbd_images_details.png" width="100%" alt="RBD details" title="RBD details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.2: </span><span class="title-name">RBD details </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.6.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-rbds-configuration" data-id-title="Viewing RBDs configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Viewing RBD's configuration</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-configuration">#</a></h2></div></div></div><p>
   To view detailed configuration of a device, click its row in the table and
   then the <span class="guimenu">Configuration</span> tab in the lower table:
  </p><div class="figure" id="id-1.4.3.7.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_config.png"><img src="images/dash_rbd_images_config.png" width="100%" alt="RBD configuration" title="RBD configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.3: </span><span class="title-name">RBD configuration </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.7.3">#</a></h6></div></div></section><section class="sect1" id="dashboard-rbds-create" data-id-title="Creating RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Creating RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-create">#</a></h2></div></div></div><p>
   To add a new device, click <span class="guimenu">Create</span> in the top left of the
   table heading and do the following on the <span class="guimenu">Create RBD</span>
   screen:
  </p><div class="figure" id="id-1.4.3.7.8.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_rbd_add.png"><img src="images/oa_rbd_add.png" width="100%" alt="Adding a new RBD" title="Adding a new RBD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.4: </span><span class="title-name">Adding a new RBD </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.8.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Enter the name of the new device. Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware requirements and recommendations”, Section 2.11 “Name limitations”</span>
     for naming limitations.
    </p></li><li class="step"><p>
     Select the pool with the <code class="option">rbd</code> application assigned from
     which the new RBD device will be created.
    </p></li><li class="step"><p>
     Specify the size of the new device.
    </p></li><li class="step"><p>
     Specify additional options for the device. To fine-tune the device
     parameters, click <span class="guimenu">Advanced</span> and enter values for object
     size, stripe unit, or stripe count. To enter Quality of Service (QoS)
     limits, click <span class="guimenu">Quality of Service</span> and enter them.
    </p></li><li class="step"><p>
     Confirm with <span class="guimenu">Create RBD</span>.
    </p></li></ol></div></div></section><section class="sect1" id="dashboard-rbd-delete" data-id-title="Deleting RBDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.4 </span><span class="title-name">Deleting RBDs</span> <a title="Permalink" class="permalink" href="#dashboard-rbd-delete">#</a></h2></div></div></div><p>
   To delete a device, select the device in the table row. Click the drop-down
   arrow next to the <span class="guimenu">Create</span> button and click
   <span class="guimenu">Delete</span>. Confirm the deletion with <span class="guimenu">Delete
   RBD</span>.
  </p><div id="id-1.4.3.7.9.3" data-id-title="Moving RBDs to trash" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Moving RBDs to trash</h6><p>
    Deleting an RBD is an irreversible action. If you <span class="guimenu">Move to
    Trash</span> instead, you can restore the device later on by selecting
    it on the <span class="guimenu">Trash</span> tab of the main table and clicking
    <span class="guimenu">Restore</span> in the top left of the table heading.
   </p></div></section><section class="sect1" id="dashboard-rbds-snapshots" data-id-title="Creating RADOS Block Device snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.5 </span><span class="title-name">Creating RADOS Block Device snapshots</span> <a title="Permalink" class="permalink" href="#dashboard-rbds-snapshots">#</a></h2></div></div></div><p>
   To create a RADOS Block Device snapshot, select the device in the table row and the
   detailed configuration content pane appears. Select the
   <span class="guimenu">Snapshots</span> tab and click <span class="guimenu">Create</span> in the
   top left of the table heading. Enter the snapshot's name and confirm with
   <span class="guimenu">Create RBD Snapshot</span>.
  </p><p>
   After selecting a snapshot, you can perform additional actions on the
   device, such as rename, protect, clone, copy, or delete.
   <span class="guimenu">Rollback</span> restores the device's state from the current
   snapshot.
  </p><div class="figure" id="id-1.4.3.7.10.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_rbd_images_snapshots.png"><img src="images/dash_rbd_images_snapshots.png" width="100%" alt="RBD snapshots" title="RBD snapshots"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.5: </span><span class="title-name">RBD snapshots </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.10.4">#</a></h6></div></div></section><section class="sect1" id="dash-rbd-mirror" data-id-title="RBD mirroring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.6 </span><span class="title-name">RBD mirroring</span> <a title="Permalink" class="permalink" href="#dash-rbd-mirror">#</a></h2></div></div></div><p>
   RADOS Block Device images can be asynchronously mirrored between two Ceph clusters. You
   can use the Ceph Dashboard to configure replication of RBD images between two
   or more clusters. This capability is available in two modes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.7.11.3.1"><span class="term">Journal-based</span></dt><dd><p>
      This mode uses the RBD journaling image feature to ensure point-in-time,
      crash-consistent replication between clusters.
     </p></dd><dt id="id-1.4.3.7.11.3.2"><span class="term">Snapshot-based</span></dt><dd><p>
      This mode uses periodically scheduled or manually created RBD image
      mirror-snapshots to replicate crash-consistent RBD images between
      clusters.
     </p></dd></dl></div><p>
   Mirroring is configured on a per-pool basis within peer clusters and can be
   configured on a specific subset of images within the pool or configured to
   automatically mirror all images within a pool when using journal-based
   mirroring only.
  </p><p>
   Mirroring is configured using the <code class="command">rbd</code> command, which is
   installed by default in SUSE Enterprise Storage 7.1. The
   <code class="systemitem">rbd-mirror</code> daemon is responsible for
   pulling image updates from the remote, peer cluster and applying them to the
   image within the local cluster. See
   <a class="xref" href="#rbd-mirror-daemon-enable" title="6.6.2. Enabling the rbd-mirror daemon">Section 6.6.2, “Enabling the <code class="systemitem">rbd-mirror</code> daemon”</a> for more information on enabling
   the <code class="systemitem">rbd-mirror</code> daemon.
  </p><p>
   Depending on the need for replication, RADOS Block Device mirroring can be configured for
   either one- or two-way replication:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.7.11.7.1"><span class="term">One-way Replication</span></dt><dd><p>
      When data is only mirrored from a primary cluster to a secondary cluster,
      the <code class="systemitem">rbd-mirror</code> daemon runs only
      on the secondary cluster.
     </p></dd><dt id="id-1.4.3.7.11.7.2"><span class="term">Two-way Replication</span></dt><dd><p>
      When data is mirrored from primary images on one cluster to non-primary
      images on another cluster (and vice-versa), the
      <code class="systemitem">rbd-mirror</code> daemon runs on both
      clusters.
     </p></dd></dl></div><div id="id-1.4.3.7.11.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    Each instance of the <code class="systemitem">rbd-mirror</code>
    daemon must be able to connect to both the local and remote Ceph clusters
    simultaneously, for example all monitor and OSD hosts. Additionally, the
    network must have sufficient bandwidth between the two data centers to
    handle mirroring workload.
   </p></div><div id="id-1.4.3.7.11.9" data-id-title="General information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: General information</h6><p>
    For general information and the command line approach to RADOS Block Device mirroring,
    refer to <a class="xref" href="#ceph-rbd-mirror" title="20.4. RBD image mirrors">Section 20.4, “RBD image mirrors”</a>.
   </p></div><section class="sect2" id="rbd-mirror-primary-secondary" data-id-title="Configuring primary and secondary clusters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.1 </span><span class="title-name">Configuring primary and secondary clusters</span> <a title="Permalink" class="permalink" href="#rbd-mirror-primary-secondary">#</a></h3></div></div></div><p>
    A <span class="emphasis"><em>primary</em></span> cluster is where the original pool with
    images is created. A <span class="emphasis"><em>secondary</em></span> cluster is where the
    pool or images are replicated from the <span class="emphasis"><em>primary</em></span>
    cluster.
   </p><div id="id-1.4.3.7.11.10.3" data-id-title="Relative naming" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Relative naming</h6><p>
     The <span class="emphasis"><em>primary</em></span> and <span class="emphasis"><em>secondary</em></span> terms
     can be relative in the context of replication because they relate more to
     individual pools than to clusters. For example, in two-way replication,
     one pool can be mirrored from the <span class="emphasis"><em>primary</em></span> cluster to
     the <span class="emphasis"><em>secondary</em></span> one, while another pool can be mirrored
     from the <span class="emphasis"><em>secondary</em></span> cluster to the
     <span class="emphasis"><em>primary</em></span> one.
    </p></div></section><section class="sect2" id="rbd-mirror-daemon-enable" data-id-title="Enabling the rbd-mirror daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.2 </span><span class="title-name">Enabling the <code class="systemitem">rbd-mirror</code> daemon</span> <a title="Permalink" class="permalink" href="#rbd-mirror-daemon-enable">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <code class="command">rbd</code> command. Mirroring is configured on a per-pool basis
    within the Ceph clusters.
   </p><p>
    The pool configuration steps should be performed on both peer clusters.
    These procedures assume two clusters, named “primary” and
    “secondary”, are accessible from a single host for clarity.
   </p><p>
    The <code class="systemitem">rbd-mirror</code> daemon performs the
    actual cluster data replication.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Rename <code class="filename">ceph.conf</code> and keyring files and copy them
      from the primary host to the secondary host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@secondary &gt; </code>cp /etc/ceph/ceph.conf /etc/ceph/primary.conf
<code class="prompt user">cephuser@secondary &gt; </code>cp /etc/ceph/ceph.admin.client.keyring \
 /etc/ceph/primary.client.admin.keyring
<code class="prompt user">cephuser@secondary &gt; </code>scp <em class="replaceable">PRIMARY_HOST</em>:/etc/ceph/ceph.conf \
 /etc/ceph/secondary.conf
<code class="prompt user">cephuser@secondary &gt; </code>scp  <em class="replaceable">PRIMARY_HOST</em>:/etc/ceph/ceph.client.admin.keyring \
 /etc/ceph/secondary.client.admin.keyring</pre></div></li><li class="step"><p>
      To enable mirroring on a pool with <code class="command">rbd</code>, specify the
      <code class="command">mirror pool enable</code>, the pool name, and the mirroring
      mode:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool enable <em class="replaceable">POOL_NAME</em> <em class="replaceable">MODE</em></pre></div><div id="id-1.4.3.7.11.11.5.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The mirroring mode can either be <code class="literal">image</code> or
       <code class="literal">pool</code>. For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@secondary &gt; </code>rbd --cluster primary mirror pool enable image-pool image
<code class="prompt user">cephuser@secondary &gt; </code>rbd --cluster secondary mirror pool enable image-pool image</pre></div></div></li><li class="step"><p>
      On the Ceph Dashboard, navigate to
      <span class="guimenu">Block</span> › <span class="guimenu">Mirroring</span>.
      The <span class="guimenu">Daemons</span> table to the left shows actively running
      <code class="systemitem">rbd-mirror</code> daemons and their
      health.
     </p><div class="figure" id="id-1.4.3.7.11.11.5.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-daemon.png"><img src="images/rbd-mirror-daemon.png" width="100%" alt="Running rbd-mirror daemons" title="Running rbd-mirror daemons"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.6: </span><span class="title-name">Running <code class="systemitem">rbd-mirror</code> daemons </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.11.5.3.2">#</a></h6></div></div></li></ol></div></div></section><section class="sect2" id="rbd-mirror-disable" data-id-title="Disabling mirroring"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.3 </span><span class="title-name">Disabling mirroring</span> <a title="Permalink" class="permalink" href="#rbd-mirror-disable">#</a></h3></div></div></div><p>
    To disable mirroring on a pool with <code class="command">rbd</code>, specify the
    <code class="command">mirror pool disable</code> command and the pool name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool disable <em class="replaceable">POOL_NAME</em></pre></div><p>
    When mirroring is disabled on a pool in this way, mirroring will also be
    disabled on any images (within the pool) for which mirroring was enabled
    explicitly.
   </p></section><section class="sect2" id="rbd-mirror-peer-bootstrap" data-id-title="Bootstrapping peers"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.4 </span><span class="title-name">Bootstrapping peers</span> <a title="Permalink" class="permalink" href="#rbd-mirror-peer-bootstrap">#</a></h3></div></div></div><p>
    In order for the <code class="systemitem">rbd-mirror</code> to
    discover its peer cluster, the peer needs to be registered to the pool and
    a user account needs to be created. This process can be automated with
    <code class="command">rbd</code> by using the <code class="command">mirror pool peer bootstrap
    create</code> and <code class="command">mirror pool peer bootstrap import</code>
    commands.
   </p><p>
    To manually create a new bootstrap token with <code class="command">rbd</code>,
    specify the <code class="command">mirror pool peer bootstrap create</code> command, a
    pool name, along with an optional site name to describe the local cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool peer bootstrap create [--site-name <em class="replaceable">local-site-name</em>] <em class="replaceable">pool-name</em></pre></div><p>
    The output of <code class="command">mirror pool peer bootstrap create</code> will be
    a token that should be provided to the <code class="command">mirror pool peer bootstrap
    import</code> command. For example, on the primary cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster primary mirror pool peer bootstrap create --site-name primary
  image-pool eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkL \
  W1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0I \
  joiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</pre></div><p>
    To manually import the bootstrap token created by another cluster with the
    <code class="command">rbd</code> command, specify the <code class="command">mirror pool peer
    bootstrap import</code> command, the pool name, a file path to the
    created token (or ‘-‘ to read from standard input), along with an
    optional site name to describe the local cluster and a mirroring direction
    (defaults to <code class="literal">rx-tx</code> for bidirectional mirroring, but can
    also be set to <code class="literal">rx-only</code> for unidirectional mirroring):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool peer bootstrap import [--site-name <em class="replaceable">local-site-name</em>] \
[--direction <em class="replaceable">rx-only or rx-tx</em>] <em class="replaceable">pool-name</em> <em class="replaceable">token-path</em></pre></div><p>
    For example, on the secondary cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat &gt;&gt;EOF &lt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcn \
Jvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0I \
joiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster secondary mirror pool peer bootstrap import --site-name secondary image-pool token</pre></div></section><section class="sect2" id="rbd-mirror-remove-peers" data-id-title="Removing cluster peer"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.5 </span><span class="title-name">Removing cluster peer</span> <a title="Permalink" class="permalink" href="#rbd-mirror-remove-peers">#</a></h3></div></div></div><p>
    To remove a mirroring peer Ceph cluster with the <code class="command">rbd</code>
    command, specify the <code class="command">mirror pool peer remove</code> command,
    the pool name, and the peer UUID (available from the <code class="command">rbd mirror
    pool info</code> command):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool peer remove <em class="replaceable">pool-name</em> <em class="replaceable">peer-uuid</em></pre></div></section><section class="sect2" id="rbd-mirror-gui-config" data-id-title="Configuring pool replication in the Ceph Dashboard"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.6 </span><span class="title-name">Configuring pool replication in the Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#rbd-mirror-gui-config">#</a></h3></div></div></div><p>
    The <code class="systemitem">rbd-mirror</code> daemon needs to have
    access to the primary cluster to be able to mirror RBD images. Ensure you
    have followed the steps in <a class="xref" href="#rbd-mirror-peer-bootstrap" title="6.6.4. Bootstrapping peers">Section 6.6.4, “Bootstrapping peers”</a>
    before continuing.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On both the <span class="emphasis"><em>primary</em></span> and
      <span class="emphasis"><em>secondary</em></span> cluster, create pools with an identical
      name and assign the <code class="literal">rbd</code> application to them. Refer to
      <a class="xref" href="#dashboard-pools-create" title="5.1. Adding a new pool">Section 5.1, “Adding a new pool”</a> for more details on creating a
      new pool.
     </p><div class="figure" id="id-1.4.3.7.11.15.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool.png"><img src="images/rbd-mirror-pool.png" width="100%" alt="Creating a pool with RBD application" title="Creating a pool with RBD application"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.7: </span><span class="title-name">Creating a pool with RBD application </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.15.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      On both the <span class="emphasis"><em>primary</em></span> and
      <span class="emphasis"><em>secondary</em></span> cluster's dashboards, navigate to
      <span class="guimenu">Block</span> › <span class="guimenu">Mirroring</span>.
      In the <span class="guimenu">Pools</span> table on the right, click the name of the
      pool to replicate, and after clicking <span class="guimenu">Edit Mode</span>,
      select the replication mode. In this example, we will work with a
      <span class="emphasis"><em>pool</em></span> replication mode, which means that all images
      within a given pool will be replicated. Confirm with
      <span class="guimenu">Update</span>.
     </p><div class="figure" id="id-1.4.3.7.11.15.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-mode.png"><img src="images/rbd-mirror-pool-mode.png" width="100%" alt="Configuring the replication mode" title="Configuring the replication mode"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.8: </span><span class="title-name">Configuring the replication mode </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.15.3.2.2">#</a></h6></div></div><div id="id-1.4.3.7.11.15.3.2.3" data-id-title="Error or warning on the primary cluster" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Error or warning on the primary cluster</h6><p>
       After updating the replication mode, an error or warning flag will
       appear in the corresponding right column. That is because the pool has
       no peer user for replication assigned yet. Ignore this flag for the
       <span class="emphasis"><em>primary</em></span> cluster as we assign a peer user to the
       <span class="emphasis"><em>secondary</em></span> cluster only.
      </p></div></li><li class="step"><p>
      On the <span class="emphasis"><em>secondary</em></span> cluster's Dashboard, navigate to
      <span class="guimenu">Block</span> › <span class="guimenu">Mirroring</span>.
      Add the pool mirror peer by selecting <span class="guimenu">Add Peer</span>.
      Provide the <span class="emphasis"><em>primary</em></span> cluster's details:
     </p><div class="figure" id="id-1.4.3.7.11.15.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-peer.png"><img src="images/rbd-mirror-pool-peer.png" width="100%" alt="Adding peer credentials" title="Adding peer credentials"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.9: </span><span class="title-name">Adding peer credentials </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.15.3.3.2">#</a></h6></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.7.11.15.3.3.3.1"><span class="term"><span class="guimenu">Cluster Name</span></span></dt><dd><p>
         An arbitrary unique string that identifies the primary cluster, such
         as 'primary'. The cluster name needs to be different from the real
         secondary cluster's name.
        </p></dd><dt id="id-1.4.3.7.11.15.3.3.3.2"><span class="term"><span class="guimenu">CephX ID</span></span></dt><dd><p>
         The Ceph user ID that you created as a mirroring peer. In this
         example it is 'rbd-mirror-peer'.
        </p></dd><dt id="id-1.4.3.7.11.15.3.3.3.3"><span class="term"><span class="guimenu">Monitor Addresses</span></span></dt><dd><p>
         Comma-separated list of IP addresses of the primary cluster's Ceph Monitor
         nodes.
        </p></dd><dt id="id-1.4.3.7.11.15.3.3.3.4"><span class="term"><span class="guimenu">CephX Key</span></span></dt><dd><p>
         The key related to the peer user ID. You can retrieve it by running
         the following example command on the primary cluster:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth print_key <em class="replaceable">pool-mirror-peer-name</em></pre></div></dd></dl></div><p>
      Confirm with <span class="guimenu">Submit</span>.
     </p><div class="figure" id="id-1.4.3.7.11.15.3.3.5"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-list.png"><img src="images/rbd-mirror-pool-list.png" width="100%" alt="List of replicated pools" title="List of replicated pools"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.10: </span><span class="title-name">List of replicated pools </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.15.3.3.5">#</a></h6></div></div></li></ol></div></div></section><section class="sect2" id="rbd-mirror-test" data-id-title="Verifying that RBD image replication works"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.6.7 </span><span class="title-name">Verifying that RBD image replication works</span> <a title="Permalink" class="permalink" href="#rbd-mirror-test">#</a></h3></div></div></div><p>
    When the <code class="systemitem">rbd-mirror</code> daemon is
    running and RBD image replication is configured on the Ceph Dashboard, it is
    time to verify whether the replication actually works:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster's Ceph Dashboard, create an RBD
      image so that its parent pool is the pool that you already created for
      replication purposes. Enable the <code class="option">Exclusive lock</code> and
      <code class="option">Journaling</code> features for the image. Refer to
      <a class="xref" href="#dashboard-rbds-create" title="6.3. Creating RBDs">Section 6.3, “Creating RBDs”</a> for details on how to create RBD
      images.
     </p><div class="figure" id="id-1.4.3.7.11.16.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-image.png"><img src="images/rbd-mirror-image.png" width="100%" alt="New RBD image" title="New RBD image"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.11: </span><span class="title-name">New RBD image </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.16.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      After you create the image that you want to replicate, open the
      <span class="emphasis"><em>secondary</em></span> cluster's Ceph Dashboard and navigate to
      <span class="guimenu">Block</span> › <span class="guimenu">Mirroring</span>.
      The <span class="guimenu">Pools</span> table on the right will reflect the change
      in the number of <span class="guimenu"># Remote</span> images and synchronize the
      number of <span class="guimenu"># Local</span> images.
     </p><div class="figure" id="id-1.4.3.7.11.16.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-pool-list-1.png"><img src="images/rbd-mirror-pool-list-1.png" width="100%" alt="New RBD image synchronized" title="New RBD image synchronized"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.12: </span><span class="title-name">New RBD image synchronized </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.16.3.2.2">#</a></h6></div></div><div id="id-1.4.3.7.11.16.3.2.3" data-id-title="Replication progress" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Replication progress</h6><p>
       The <span class="guimenu">Images</span> table at the bottom of the page shows the
       status of replication of RBD images. The <span class="guimenu">Issues</span> tab
       includes possible problems, the <span class="guimenu">Syncing</span> tab displays
       the progress of image replication, and the <span class="guimenu">Ready</span> tab
       lists all images with successful replication.
      </p><div class="figure" id="id-1.4.3.7.11.16.3.2.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/rbd-mirror-images-status.png"><img src="images/rbd-mirror-images-status.png" width="100%" alt="RBD images' replication status" title="RBD images' replication status"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.13: </span><span class="title-name">RBD images' replication status </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.11.16.3.2.3.3">#</a></h6></div></div></div></li><li class="step"><p>
      On the <span class="emphasis"><em>primary</em></span> cluster, write data to the RBD image.
      On the <span class="emphasis"><em>secondary</em></span> cluster's Ceph Dashboard, navigate to
      <span class="guimenu">Block</span> › <span class="guimenu">Images</span>
      and monitor whether the corresponding image's size is growing as the data
      on the primary cluster is written.
     </p></li></ol></div></div></section></section><section class="sect1" id="dashboard-iscsi" data-id-title="Managing iSCSI Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.7 </span><span class="title-name">Managing iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi">#</a></h2></div></div></div><div id="id-1.4.3.7.12.2" data-id-title="More information on iSCSI Gateways" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More information on iSCSI Gateways</h6><p>
    For more general information about iSCSI Gateways, refer to
    <a class="xref" href="#cha-ceph-iscsi" title="Chapter 22. Ceph iSCSI gateway">Chapter 22, <em>Ceph iSCSI gateway</em></a>.
   </p></div><p>
   To list all available gateways and mapped images, click
   <span class="guimenu">Block</span> › <span class="guimenu">iSCSI</span>
   from the main menu. An <span class="guimenu">Overview</span> tab opens, listing
   currently configured iSCSI Gateways and mapped RBD images.
  </p><p>
   The <span class="guimenu">Gateways</span> table lists each gateway's state, number of
   iSCSI targets, and number of sessions. The <span class="guimenu">Images</span> table
   lists each mapped image's name, related pool name backstore type, and other
   statistical details.
  </p><p>
   The <span class="guimenu">Targets</span> tab lists currently configured iSCSI
   targets.
  </p><div class="figure" id="id-1.4.3.7.12.6"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws.png"><img src="images/dash_igws.png" width="100%" alt="List of iSCSI targets" title="List of iSCSI targets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.14: </span><span class="title-name">List of iSCSI targets </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.12.6">#</a></h6></div></div><p>
   To view more detailed information about a target, click the drop-down arrow
   on the target table row. A tree-structured schema opens, listing disks,
   portals, initiators, and groups. Click an item to expand it and view its
   detailed contents, optionally with a related configuration in the table on
   the right.
  </p><div class="figure" id="id-1.4.3.7.12.8"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws_status.png"><img src="images/dash_igws_status.png" width="100%" alt="iSCSI target details" title="iSCSI target details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.15: </span><span class="title-name">iSCSI target details </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.12.8">#</a></h6></div></div><section class="sect2" id="dashboard-iscsi-create" data-id-title="Adding iSCSI targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.1 </span><span class="title-name">Adding iSCSI targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-create">#</a></h3></div></div></div><p>
    To add a new iSCSI target, click <span class="guimenu">Create</span> in the top
    left of the <span class="guimenu">Targets</span> table and enter the required
    information.
   </p><div class="figure" id="id-1.4.3.7.12.9.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_igws_add.png"><img src="images/dash_igws_add.png" width="100%" alt="Adding a new target" title="Adding a new target"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 6.16: </span><span class="title-name">Adding a new target </span><a title="Permalink" class="permalink" href="#id-1.4.3.7.12.9.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Enter the target address of the new gateway.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add portal</span> and select one or multiple iSCSI
      portals from the list.
     </p></li><li class="step"><p>
      Click <span class="guimenu">Add image</span> and select one or multiple RBD images
      for the gateway.
     </p></li><li class="step"><p>
      If you need to use authentication to access the gateway, activate the
      <span class="guimenu">ACL Authentication</span> check box and enter the
      credentials. You can find more advanced authentication options after
      activating <span class="guimenu">Mutual authentication</span> and
      <span class="guimenu">Discovery authentication</span>.
     </p></li><li class="step"><p>
      Confirm with <span class="guimenu">Create Target</span>.
     </p></li></ol></div></div></section><section class="sect2" id="dashboard-iscsi-edit" data-id-title="Editing iSCSI targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.2 </span><span class="title-name">Editing iSCSI targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-edit">#</a></h3></div></div></div><p>
    To edit an existing iSCSI target, click its row in the
    <span class="guimenu">Targets</span> table and click <span class="guimenu">Edit</span> in the
    top left of the table.
   </p><p>
    You can then modify the iSCSI target, add or delete portals, and add or
    delete related RBD images. You can also adjust authentication information
    for the gateway.
   </p></section><section class="sect2" id="dashboard-iscsi-delete" data-id-title="Deleting iSCSI targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.7.3 </span><span class="title-name">Deleting iSCSI targets</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-delete">#</a></h3></div></div></div><p>
    To delete an iSCSI target, select the table row and click the drop-down
    arrow next to the <span class="guimenu">Edit</span> button and select
    <span class="guimenu">Delete</span>. Activate <span class="guimenu">Yes, I am sure</span> and
    confirm with <span class="guimenu">Delete iSCSI target</span>.
   </p></section></section><section class="sect1" id="dash-rbd-qos" data-id-title="RBD Quality of Service (QoS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.8 </span><span class="title-name">RBD Quality of Service (QoS)</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos">#</a></h2></div></div></div><div id="id-1.4.3.7.13.2" data-id-title="For more information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: For more information</h6><p>
    For more general information and a description of RBD QoS configuration
    options, refer to <a class="xref" href="#rbd-qos" title="20.6. QoS settings">Section 20.6, “QoS settings”</a>.
   </p></div><p>
   The QoS options can be configured at different levels.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Globally
    </p></li><li class="listitem"><p>
     On a per-pool basis
    </p></li><li class="listitem"><p>
     On a per-image basis
    </p></li></ul></div><p>
   The <span class="emphasis"><em>global</em></span> configuration is at the top of the list and
   will be used for all newly created RBD images and for those images that do
   not override these values on the pool or RBD image layer. An option value
   specified globally can be overridden on a per-pool or per-image basis.
   Options specified on a pool will be applied to all RBD images of that pool
   unless overridden by a configuration option set on an image. Options
   specified on an image will override options specified on a pool and will
   override options specified globally.
  </p><p>
   This way it is possible to define defaults globally, adapt them for all RBD
   images of a specific pool, and override the pool configuration for
   individual RBD images.
  </p><section class="sect2" id="dash-rbd-qos-global" data-id-title="Configuring options globally"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.1 </span><span class="title-name">Configuring options globally</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-global">#</a></h3></div></div></div><p>
    To configure the RADOS Block Device options globally, select
    <span class="guimenu">Cluster</span> › <span class="guimenu">Configuration</span> from the main menu.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To list all available global configuration options, next to
      <span class="guimenu">Level</span>, choose
      <span class="guimenu">Advanced</span> from the drop-down
      menu.
     </p></li><li class="step"><p>
      Filter the results of the table by filtering for
      <code class="literal">rbd_qos</code> in the search field. This lists all available
      configuration options for QoS.
     </p></li><li class="step"><p>
      To change a value, click the row in the table, then select
      <span class="guimenu">Edit</span> at the top left of the table. The
      <span class="guimenu">Edit</span> dialog contains six different fields for
      specifying values. The RBD configuration option values are required in
      the <span class="guimenu">mgr</span> text box.
     </p><div id="id-1.4.3.7.13.7.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       Unlike the other dialogs, this one does not allow you to specify the
       value in convenient units. You need to set these values in either bytes
       or IOPS, depending on the option you are editing.
      </p></div></li></ol></div></div></section><section class="sect2" id="dash-rbd-qos-pool-create" data-id-title="Configuring options on a new pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.2 </span><span class="title-name">Configuring options on a new pool</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-pool-create">#</a></h3></div></div></div><p>
    To create a new pool and configure RBD configuration options on it, click
    <span class="guimenu">Pools</span> › <span class="guimenu">Create</span>. Select
    <span class="guimenu">replicated</span> as pool type. You will then need to add the
    <code class="literal">rbd</code> application tag to the pool to be able to configure
    the RBD QoS options.
   </p><div id="id-1.4.3.7.13.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     It is not possible to configure RBD QoS configuration options on an
     erasure coded pool. To configure the RBD QoS options for erasure coded
     pools, you need to edit the replicated metadata pool of an RBD image. The
     configuration will then be applied to the erasure coded data pool of that
     image.
    </p></div></section><section class="sect2" id="dash-rbd-qos-pool-edit" data-id-title="Configuring options on an existing pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.3 </span><span class="title-name">Configuring options on an existing pool</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-pool-edit">#</a></h3></div></div></div><p>
    To configure RBD QoS options on an existing pool, click
    <span class="guimenu">Pools</span>, then click the pool's table row and select
    <span class="guimenu">Edit</span> at the top left of the table.
   </p><p>
    You should see the <span class="guimenu">RBD Configuration</span> section in the
    dialog, followed by a <span class="guimenu">Quality of Service</span> section.
   </p><div id="id-1.4.3.7.13.9.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If you see neither the <span class="guimenu">RBD Configuration</span> nor the
     <span class="guimenu">Quality of Service</span> section, you are likely either
     editing an <span class="emphasis"><em>erasure coded</em></span> pool, which cannot be used
     to set RBD configuration options, or the pool is not configured to be used
     by RBD images. In the latter case, assign the <span class="guimenu">rbd</span>
     application tag to the pool and the corresponding configuration sections
     will show up.
    </p></div></section><section class="sect2" id="dash-rbd-qos-qos" data-id-title="Configuration options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.4 </span><span class="title-name">Configuration options</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-qos">#</a></h3></div></div></div><p>
    Click <span class="guimenu">Quality of Service +</span> to expand the configuration
    options. A list of all available options will show up. The units of the
    configuration options are already shown in the text boxes. In case of any
    bytes per second (BPS) option, you are free to use shortcuts such as '1M'
    or '5G'. They will be automatically converted to '1 MB/s' and
    '5 GB/s' respectively.
   </p><p>
    By clicking the reset button to the right of each text box, any value set
    on the pool will be removed. This does not remove configuration values of
    options configured globally or on an RBD image.
   </p></section><section class="sect2" id="dash-rbd-qos-image-create" data-id-title="Creating RBD QoS options with a new RBD image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.5 </span><span class="title-name">Creating RBD QoS options with a new RBD image</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-create">#</a></h3></div></div></div><p>
    To create an RBD image with RBD QoS options set on that image, select
    <span class="guimenu">Block</span> › <span class="guimenu">Images</span>
    and then click <span class="guimenu">Create</span>. Click
    <span class="guimenu">Advanced...</span> to expand the advanced configuration
    section. Click <span class="guimenu">Quality of Service +</span> to open all
    available configuration options.
   </p></section><section class="sect2" id="dash-rbd-qos-image-edit" data-id-title="Editing RBD QoS options on existing images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.6 </span><span class="title-name">Editing RBD QoS options on existing images</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-edit">#</a></h3></div></div></div><p>
    To edit RBD QoS options on an existing image, select
    <span class="guimenu">Block</span> › <span class="guimenu">Images</span>, then click the pool's table row,
    and lastly click <span class="guimenu">Edit</span>. The edit dialog will show up.
    Click <span class="guimenu">Advanced...</span> to expand the advanced configuration
    section. Click <span class="guimenu">Quality of Service +</span> to open all
    available configuration options.
   </p></section><section class="sect2" id="dash-rbd-qos-image-copy" data-id-title="Changing configuration options when copying or cloning images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.8.7 </span><span class="title-name">Changing configuration options when copying or cloning images</span> <a title="Permalink" class="permalink" href="#dash-rbd-qos-image-copy">#</a></h3></div></div></div><p>
    If an RBD image is cloned or copied, the values set on that particular
    image will be copied too, by default. If you want to change them while
    copying or cloning, you can do so by specifying the updated configuration
    values in the copy/clone dialog, the same way as when creating or editing
    an RBD image. Doing so will only set (or reset) the values for the RBD
    image that is copied or cloned. This operation changes neither the source
    RBD image configuration, nor the global configuration.
   </p><p>
    If you choose to reset the option value on copying/cloning, no value for
    that option will be set on that image. This means that any value of that
    option specified for the parent pool will be used if the parent pool has
    the value configured. Otherwise, the global default will be used.
   </p></section></section></section><section class="chapter" id="dash-webui-nfs" data-id-title="Manage NFS Ganesha"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">7 </span><span class="title-name">Manage NFS Ganesha</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs">#</a></h1></div></div></div><div id="id-1.4.3.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
  NFS Ganesha supports NFS version 4.1 and newer.
  It does not support NFS version 3.
 </p></div><div id="id-1.4.3.8.4" data-id-title="More information on NFS Ganesha" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More information on NFS Ganesha</h6><p>
   For more general information about NFS Ganesha, refer to
   <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 25. NFS Ganesha">Chapter 25, <em>NFS Ganesha</em></a>.
  </p></div><p>
  To list all available NFS exports, click <span class="guimenu">NFS</span> from the main
  menu.
 </p><p>
  The list shows each export's directory, daemon host name, type of storage
  back-end, and access type.
 </p><div class="figure" id="id-1.4.3.8.7"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs.png"><img src="images/oa_nfs.png" width="100%" alt="List of NFS exports" title="List of NFS exports"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.1: </span><span class="title-name">List of NFS exports </span><a title="Permalink" class="permalink" href="#id-1.4.3.8.7">#</a></h6></div></div><p>
  To view more detailed information about an NFS export, click its table row.
 </p><div class="figure" id="id-1.4.3.8.9"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_status.png"><img src="images/oa_nfs_status.png" width="100%" alt="NFS export details" title="NFS export details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.2: </span><span class="title-name">NFS export details </span><a title="Permalink" class="permalink" href="#id-1.4.3.8.9">#</a></h6></div></div><section class="sect1" id="dash-webui-nfs-create" data-id-title="Creating NFS exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Creating NFS exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-create">#</a></h2></div></div></div><p>
   To add a new NFS export, click <span class="guimenu">Create</span> in the top left of
   the exports table and enter the required information.
  </p><div class="figure" id="id-1.4.3.8.10.3"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_add.png"><img src="images/oa_nfs_add.png" width="100%" alt="Adding a new NFS export" title="Adding a new NFS export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.3: </span><span class="title-name">Adding a new NFS export </span><a title="Permalink" class="permalink" href="#id-1.4.3.8.10.3">#</a></h6></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Select one or more NFS Ganesha daemons that will run the export.
    </p></li><li class="step"><p>
     Select a storage back-end.
    </p><div id="id-1.4.3.8.10.4.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      At this time, only NFS exports backed by CephFS are supported.
     </p></div></li><li class="step"><p>
     Select a user ID and other back-end related options.
    </p></li><li class="step"><p>
     Enter the directory path for the NFS export. If the directory does not
     exist on the server, it will be created.
    </p></li><li class="step"><p>
     Specify other NFS related options, such as supported NFS protocol version,
     pseudo, access type, squashing, or transport protocol.
    </p></li><li class="step"><p>
     If you need to limit access to specific clients only, click <span class="guimenu">Add
     clients</span> and add their IP addresses together with access type and
     squashing options.
    </p></li><li class="step"><p>
     Confirm with <span class="guimenu">Create NFS export</span>.
    </p></li></ol></div></div></section><section class="sect1" id="dash-webui-nfs-delete" data-id-title="Deleting NFS exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Deleting NFS exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-delete">#</a></h2></div></div></div><p>
   To delete an export, select and highlight the export in the table row. Click
   the drop-down arrow next to the <span class="guimenu">Edit</span> button and select
   <span class="guimenu">Delete</span>. Activate the <span class="guimenu">Yes, I am sure</span>
   check box and confirm with <span class="guimenu">Delete NFS export</span>.
  </p></section><section class="sect1" id="dash-webui-nfs-edit" data-id-title="Editing NFS exports"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.3 </span><span class="title-name">Editing NFS exports</span> <a title="Permalink" class="permalink" href="#dash-webui-nfs-edit">#</a></h2></div></div></div><p>
   To edit an existing export, select and highlight the export in the table row
   and click <span class="guimenu">Edit</span> in the top left of the exports table.
  </p><p>
   You can then adjust all the details of the NFS export.
  </p><div class="figure" id="id-1.4.3.8.12.4"><div class="figure-contents"><div class="mediaobject"><a href="images/oa_nfs_edit.png"><img src="images/oa_nfs_edit.png" width="100%" alt="Editing an NFS export" title="Editing an NFS export"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.4: </span><span class="title-name">Editing an NFS export </span><a title="Permalink" class="permalink" href="#id-1.4.3.8.12.4">#</a></h6></div></div></section></section><section class="chapter" id="dashboard-mds" data-id-title="Manage CephFS"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">8 </span><span class="title-name">Manage CephFS</span> <a title="Permalink" class="permalink" href="#dashboard-mds">#</a></h1></div></div></div><div id="id-1.4.3.9.3" data-id-title="For more information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: For more information</h6><p>
   To find detailed information about CephFS, refer to
   <a class="xref" href="#cha-ceph-cephfs" title="Chapter 23. Clustered file system">Chapter 23, <em>Clustered file system</em></a>.
  </p></div><section class="sect1" id="dashboard-mds-overview" data-id-title="Viewing CephFS overview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">Viewing CephFS overview</span> <a title="Permalink" class="permalink" href="#dashboard-mds-overview">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Filesystems</span> from the main menu to view the overview
   of configured file systems. The main table shows each file system's name,
   date of creation, and whether it is enabled or not.
  </p><p>
   By clicking a file system's table row, you reveal details about its rank and
   pools added to the file system.
  </p><div class="figure" id="id-1.4.3.9.4.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_cephfs_details.png"><img src="images/dash_cephfs_details.png" width="100%" alt="CephFS details" title="CephFS details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.1: </span><span class="title-name">CephFS details </span><a title="Permalink" class="permalink" href="#id-1.4.3.9.4.4">#</a></h6></div></div><p>
   At the bottom of the screen, you can see statistics counting the number of
   related MDS inodes and client requests, collected in real time.
  </p><div class="figure" id="id-1.4.3.9.4.6"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_cephfs_perf_counter.png"><img src="images/dash_cephfs_perf_counter.png" width="100%" alt="CephFS details" title="CephFS details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 8.2: </span><span class="title-name">CephFS details </span><a title="Permalink" class="permalink" href="#id-1.4.3.9.4.6">#</a></h6></div></div></section></section><section class="chapter" id="dashboard-ogw" data-id-title="Manage the Object Gateway"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">9 </span><span class="title-name">Manage the Object Gateway</span> <a title="Permalink" class="permalink" href="#dashboard-ogw">#</a></h1></div></div></div><div id="id-1.4.3.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
   Before you begin, you may encounter the following notification when trying
   to access the Object Gateway front-end on the Ceph Dashboard:
  </p><div class="verbatim-wrap"><pre class="screen">Information
No RGW credentials found, please consult the documentation on how to enable RGW for the dashboard.
Please consult the documentation on how to configure and enable the Object Gateway management functionality.</pre></div><p>
   This is because the Object Gateway has not been automatically configured by cephadm
   for the Ceph Dashboard. If you encounter this notification, follow the
   instructions at <a class="xref" href="#dashboard-ogw-enabling" title="10.4. Enabling the Object Gateway management front-end">Section 10.4, “Enabling the Object Gateway management front-end”</a> to manually enable
   the Object Gateway front-end for the Ceph Dashboard.
  </p></div><div id="id-1.4.3.10.4" data-id-title="More information on Object Gateway" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More information on Object Gateway</h6><p>
   For more general information about Object Gateway, refer to
   <a class="xref" href="#cha-ceph-gw" title="Chapter 21. Ceph Object Gateway">Chapter 21, <em>Ceph Object Gateway</em></a>.
  </p></div><section class="sect1" id="dashboard-ogw-view" data-id-title="Viewing Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Viewing Object Gateways</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-view">#</a></h2></div></div></div><p>
   To view a list of configured Object Gateways, click <span class="guimenu">Object
   Gateway</span> › <span class="guimenu">Daemons</span>. The list includes
   the ID of the gateway, host name of the cluster node where the gateway
   daemon is running, and the gateway's version number.
  </p><p>
   Click the drop-down arrow next to the gateway's name to view detailed
   information about the gateway. The <span class="guimenu">Performance Counters</span>
   tab shows details about read/write operations and cache statistics.
  </p><div class="figure" id="id-1.4.3.10.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_details.png"><img src="images/dash_ogw_details.png" width="100%" alt="Gateway's details" title="Gateway's details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.1: </span><span class="title-name">Gateway's details </span><a title="Permalink" class="permalink" href="#id-1.4.3.10.5.4">#</a></h6></div></div></section><section class="sect1" id="dashboard-ogw-user" data-id-title="Managing Object Gateway users"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.2 </span><span class="title-name">Managing Object Gateway users</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user">#</a></h2></div></div></div><p>
   Click <span class="guimenu">Object
   Gateway</span> › <span class="guimenu">Users</span> to view a list of
   existing Object Gateway users.
  </p><p>
   Click the drop-down arrow next to the user name to view details about the
   user account, such as status information or the user and bucket quota
   details.
  </p><div class="figure" id="id-1.4.3.10.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_users_details.png"><img src="images/dash_ogw_users_details.png" width="100%" alt="Gateway users" title="Gateway users"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.2: </span><span class="title-name">Gateway users </span><a title="Permalink" class="permalink" href="#id-1.4.3.10.6.4">#</a></h6></div></div><section class="sect2" id="dashboard-ogw-user-create" data-id-title="Adding a new gateway user"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.2.1 </span><span class="title-name">Adding a new gateway user</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-create">#</a></h3></div></div></div><p>
    To add a new gateway user, click <span class="guimenu">Create</span> in the top left
    of the table heading. Fill in their credentials, details about the S3 key
    and user and bucket quotas, then confirm with <span class="guimenu">Create
    User</span>.
   </p><div class="figure" id="id-1.4.3.10.6.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_user_add.png"><img src="images/dash_ogw_user_add.png" width="100%" alt="Adding a new gateway user" title="Adding a new gateway user"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.3: </span><span class="title-name">Adding a new gateway user </span><a title="Permalink" class="permalink" href="#id-1.4.3.10.6.5.3">#</a></h6></div></div></section><section class="sect2" id="dashboard-ogw-user-delete" data-id-title="Deleting gateway users"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.2.2 </span><span class="title-name">Deleting gateway users</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-delete">#</a></h3></div></div></div><p>
    To delete a gateway user, select and highlight the user. Click the
    drop-down button next to <span class="guimenu">Edit</span> and select
    <span class="guimenu">Delete</span> from the list to delete the user account.
    Activate the <span class="guimenu">Yes, I am sure</span> check box and confirm with
    <span class="guimenu">Delete user</span>.
   </p></section><section class="sect2" id="dashboard-ogw-user-edit" data-id-title="Editing gateway user details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.2.3 </span><span class="title-name">Editing gateway user details</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-user-edit">#</a></h3></div></div></div><p>
    To change gateway user details, select and highlight the user. Click
    <span class="guimenu">Edit</span> in the top left of the table heading.
   </p><p>
    Modify basic or additional user information, such as their capabilities,
    keys, sub-users, and quota information. Confirm with <span class="guimenu">Edit
    User</span>.
   </p><p>
    The <span class="guimenu">Keys</span> tab includes a read-only list of the gateway's
    users and their access and secret keys. To view the keys, click a user name
    in the list and then select <span class="guimenu">Show</span> in the top left of the
    table heading. In the <span class="guimenu">S3 Key</span> dialog, click the 'eye'
    icon to unveil the keys, or click the clipboard icon to copy the related
    key to the clipboard.
   </p></section></section><section class="sect1" id="dashboard-ogw-bucket" data-id-title="Managing the Object Gateway buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.3 </span><span class="title-name">Managing the Object Gateway buckets</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket">#</a></h2></div></div></div><p>
   Object Gateway (OGW) buckets implement the functionality of OpenStack Swift
   containers. Object Gateway buckets serve as containers for storing data objects.
  </p><p>
   Click <span class="guimenu">Object
   Gateway</span> › <span class="guimenu">Buckets</span> to view a list of
   Object Gateway buckets.
  </p><section class="sect2" id="dashboard-ogw-bucket-create" data-id-title="Adding a new bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.1 </span><span class="title-name">Adding a new bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-create">#</a></h3></div></div></div><p>
    To add a new Object Gateway bucket, click <span class="guimenu">Create</span> in the top left
    of the table heading. Enter the bucket's name, select the owner, and set
    the placement target. Confirm with <span class="guimenu">Create Bucket</span>.
   </p><div id="id-1.4.3.10.7.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     At this stage you can also enable locking by selecting
     <span class="guimenu">Enabled</span>; however, this is configurable after creation.
     See <a class="xref" href="#dashboard-ogw-bucket-edit" title="9.3.3. Editing the bucket">Section 9.3.3, “Editing the bucket”</a> for more information.
    </p></div></section><section class="sect2" id="dashboard-ogw-bucket-view" data-id-title="Viewing bucket details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.2 </span><span class="title-name">Viewing bucket details</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-view">#</a></h3></div></div></div><p>
    To view detailed information about an Object Gateway bucket, click the drop-down
    arrow next to the bucket name.
   </p><div class="figure" id="id-1.4.3.10.7.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_bucket_details.png"><img src="images/dash_ogw_bucket_details.png" width="100%" alt="Gateway bucket details" title="Gateway bucket details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.4: </span><span class="title-name">Gateway bucket details </span><a title="Permalink" class="permalink" href="#id-1.4.3.10.7.5.3">#</a></h6></div></div><div id="id-1.4.3.10.7.5.4" data-id-title="Bucket quota" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Bucket quota</h6><p>
     Below the <span class="guimenu">Details</span> table, you can find details about the
     bucket quota and locking settings.
    </p></div></section><section class="sect2" id="dashboard-ogw-bucket-edit" data-id-title="Editing the bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.3 </span><span class="title-name">Editing the bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-edit">#</a></h3></div></div></div><p>
    Select and highlight a bucket, then click <span class="guimenu">Edit</span> in the
    top left of the table heading.
   </p><p>
    You can update the owner of the bucket or enable versioning, multi-factor
    authentication or locking. Confirm any changes with <span class="guimenu">Edit
    Bucket</span>.
   </p><div class="figure" id="id-1.4.3.10.7.6.4"><div class="figure-contents"><div class="mediaobject"><a href="images/dash_ogw_bucket_edit.png"><img src="images/dash_ogw_bucket_edit.png" width="100%" alt="Editing the bucket details" title="Editing the bucket details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.5: </span><span class="title-name">Editing the bucket details </span><a title="Permalink" class="permalink" href="#id-1.4.3.10.7.6.4">#</a></h6></div></div></section><section class="sect2" id="dashboard-ogw-bucket-delete" data-id-title="Deleting a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.3.4 </span><span class="title-name">Deleting a bucket</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-bucket-delete">#</a></h3></div></div></div><p>
    To delete an Object Gateway bucket, select and highlight the bucket. Click the
    drop-down button next to <span class="guimenu">Edit</span> and select
    <span class="guimenu">Delete</span> from the list to delete the bucket. Activate the
    <span class="guimenu">Yes, I am sure</span> check box and confirm with
    <span class="guimenu">Delete bucket</span>.
   </p></section></section></section><section class="chapter" id="dashboard-initial-configuration" data-id-title="Manual configuration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">10 </span><span class="title-name">Manual configuration</span> <a title="Permalink" class="permalink" href="#dashboard-initial-configuration">#</a></h1></div></div></div><p>
  This section introduces advanced information for users that prefer
  configuring dashboard settings manually on the command line.
 </p><section class="sect1" id="dashboard-ssl" data-id-title="Configuring TLS/SSL support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Configuring TLS/SSL support</span> <a title="Permalink" class="permalink" href="#dashboard-ssl">#</a></h2></div></div></div><p>
   All HTTP connections to the dashboard are secured with TLS/SSL by default. A
   secure connection requires an SSL certificate. You can either use a
   self-signed certificate, or generate a certificate and have a well known
   certificate authority (CA) sign it.
  </p><div id="id-1.4.3.11.4.3" data-id-title="Disabling SSL" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling SSL</h6><p>
    You may want to disable the SSL support for a specific reason. For example,
    if the dashboard is running behind a proxy that does not support SSL.
   </p><p>
    Use caution when disabling SSL as <span class="bold"><strong>user names and
    passwords</strong></span> will be sent to the dashboard
    <span class="bold"><strong>unencrypted</strong></span>.
   </p><p>
    To disable SSL, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/ssl false</pre></div></div><div id="id-1.4.3.11.4.4" data-id-title="Restarting the Ceph Manager processes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Restarting the Ceph Manager processes</h6><p>
    You need to restart the Ceph Manager processes manually after changing the SSL
    certificate and key. You can do so by either running
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr fail <em class="replaceable">ACTIVE-MANAGER-NAME</em></pre></div><p>
    or by disabling and re-enabling the dashboard module, which also triggers
    the manager to respawn itself:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module disable dashboard
<code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable dashboard</pre></div></div><section class="sect2" id="self-sign-certificates" data-id-title="Creating self-signed certificates"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.1 </span><span class="title-name">Creating self-signed certificates</span> <a title="Permalink" class="permalink" href="#self-sign-certificates">#</a></h3></div></div></div><p>
    Creating a self-signed certificate for secure communication is simple. This
    way you can get the dashboard running quickly.
   </p><div id="id-1.4.3.11.4.5.3" data-id-title="Web browsers complaint" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Web browsers complaint</h6><p>
     Most Web browsers will complain about a self-signed certificate and
     require explicit confirmation before establishing a secure connection to
     the dashboard.
    </p></div><p>
    To generate and install a self-signed certificate, use the following
    built-in command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard create-self-signed-cert</pre></div></section><section class="sect2" id="cert-sign-CA" data-id-title="Using certificates signed by CA"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.2 </span><span class="title-name">Using certificates signed by CA</span> <a title="Permalink" class="permalink" href="#cert-sign-CA">#</a></h3></div></div></div><p>
    To properly secure the connection to the dashboard and to eliminate Web
    browser complaints about a self-signed certificate, we recommend using a
    certificate that is signed by a CA.
   </p><p>
    You can generate a certificate key pair with a command similar to the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>openssl req -new -nodes -x509 \
  -subj "/O=IT/CN=ceph-mgr-dashboard" -days 3650 \
  -keyout dashboard.key -out dashboard.crt -extensions v3_ca</pre></div><p>
    The above command outputs <code class="filename">dashboard.key</code> and
    <code class="filename">dashboard.crt</code> files. After you get the
    <code class="filename">dashboard.crt</code> file signed by a CA, enable it for all
    Ceph Manager instances by running the following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ssl-certificate -i dashboard.crt
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ssl-certificate-key -i dashboard.key</pre></div><div id="id-1.4.3.11.4.6.7" data-id-title="Different certificates for each manager instance" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Different certificates for each manager instance</h6><p>
     If you require different certificates for each Ceph Manager instance, modify the
     commands and include the name of the instance as follows. Replace
     <em class="replaceable">NAME</em> with the name of the Ceph Manager instance
     (usually the related host name):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ssl-certificate <em class="replaceable">NAME</em> -i dashboard.crt
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ssl-certificate-key <em class="replaceable">NAME</em> -i dashboard.key</pre></div></div></section></section><section class="sect1" id="dashboard-hostname-port" data-id-title="Changing host name and port number"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">Changing host name and port number</span> <a title="Permalink" class="permalink" href="#dashboard-hostname-port">#</a></h2></div></div></div><p>
   The Ceph Dashboard binds to a specific TCP/IP address and TCP port. By default,
   the currently active Ceph Manager that hosts the dashboard binds to TCP port 8443
   (or 8080 when SSL is disabled).
  </p><div id="id-1.4.3.11.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    If a firewall is enabled on the hosts running Ceph Manager (and thus the
    Ceph Dashboard), you may need to change the configuration to enable access to
    these ports. For more information on firewall settings for Ceph, see
    <span class="intraxref">Book “Troubleshooting Guide”, Chapter 13 “Hints and tips”, Section 13.7 “Firewall settings for Ceph”</span>.
   </p></div><p>
   The Ceph Dashboard binds to "::" by default, which corresponds to all available
   IPv4 and IPv6 addresses. You can change the IP address and port number of
   the Web application so that they apply to all Ceph Manager instances by using the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/server_addr <em class="replaceable">IP_ADDRESS</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/server_port <em class="replaceable">PORT_NUMBER</em></pre></div><div id="id-1.4.3.11.5.6" data-id-title="Configuring Ceph Manager instances separately" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Configuring Ceph Manager instances separately</h6><p>
    Since each <code class="systemitem">ceph-mgr</code> daemon hosts
    its own instance of the dashboard, you may need to configure them
    separately. Change the IP address and port number for a specific manager
    instance by using the following commands (replace
    <em class="replaceable">NAME</em> with the ID of the
    <code class="systemitem">ceph-mgr</code> instance):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/<em class="replaceable">NAME</em>/server_addr <em class="replaceable">IP_ADDRESS</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/<em class="replaceable">NAME</em>/server_port <em class="replaceable">PORT_NUMBER</em></pre></div></div><div id="id-1.4.3.11.5.7" data-id-title="Listing configured endpoints" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Listing configured endpoints</h6><p>
    The <code class="command">ceph mgr services</code> command displays all endpoints
    that are currently configured. Look for the <code class="literal">dashboard</code>
    key to obtain the URL for accessing the dashboard.
   </p></div></section><section class="sect1" id="dashboard-username-password" data-id-title="Adjusting user names and passwords"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Adjusting user names and passwords</span> <a title="Permalink" class="permalink" href="#dashboard-username-password">#</a></h2></div></div></div><p>
   If you do not want to use the default administrator account, create a
   different user account and associate it with at least one role. We provide a
   set of predefined system roles that you can use. For more details refer to
   <a class="xref" href="#dashboard-user-roles" title="Chapter 11. Manage users and roles on the command line">Chapter 11, <em>Manage users and roles on the command line</em></a>.
  </p><p>
   To create a user with administrator privileges, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-create <em class="replaceable">USER_NAME</em> <em class="replaceable">PASSWORD</em> administrator</pre></div></section><section class="sect1" id="dashboard-ogw-enabling" data-id-title="Enabling the Object Gateway management front-end"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Enabling the Object Gateway management front-end</span> <a title="Permalink" class="permalink" href="#dashboard-ogw-enabling">#</a></h2></div></div></div><p>
   To use the Object Gateway management functionality of the dashboard, you need to
   provide the login credentials of a user with the <code class="option">system</code>
   flag enabled:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If you do not have a user with the <code class="option">system</code> flag, create
     one:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USER_ID</em> --display-name=<em class="replaceable">DISPLAY_NAME</em> --system</pre></div><p>
     Take note of the <em class="replaceable">access_key</em> and
     <em class="replaceable">secret_key</em> keys in the output of the command.
    </p></li><li class="step"><p>
     You can also obtain the credentials of an existing user by using the
     <code class="command">radosgw-admin</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user info --uid=<em class="replaceable">USER_ID</em></pre></div></li><li class="step"><p>
     Provide the received credentials to the dashboard in separate files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-access-key <em class="replaceable">ACCESS_KEY_FILE</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-secret-key <em class="replaceable">SECRET_KEY_FILE</em></pre></div></li></ol></div></div><div id="id-1.4.3.11.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    By default the firewall is enabled in SUSE Linux Enterprise Server 15 SP3. For more information on
    firewall configuration, see <span class="intraxref">Book “Troubleshooting Guide”, Chapter 13 “Hints and tips”, Section 13.7 “Firewall settings for Ceph”</span>.
   </p></div><p>
   There are several points to consider:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The host name and port number of the Object Gateway are determined automatically.
    </p></li><li class="listitem"><p>
     If multiple zones are used, it will automatically determine the host
     within the master zonegroup and master zone. This is sufficient for most
     setups, but in some circumstances you may want to set the host name and
     port manually:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-host <em class="replaceable">HOST</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-port <em class="replaceable">PORT</em></pre></div></li><li class="listitem"><p>
     These are additional settings that you may need:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-scheme <em class="replaceable">SCHEME</em>  # http or https
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-admin-resource <em class="replaceable">ADMIN_RESOURCE</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-user-id <em class="replaceable">USER_ID</em></pre></div></li><li class="listitem"><p>
     If you are using a self-signed certificate
     (<a class="xref" href="#dashboard-ssl" title="10.1. Configuring TLS/SSL support">Section 10.1, “Configuring TLS/SSL support”</a>) in your Object Gateway setup, disable
     certificate verification in the dashboard to avoid refused connections
     caused by certificates signed by an unknown CA or not matching the host
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rgw-api-ssl-verify False</pre></div></li><li class="listitem"><p>
     If the Object Gateway takes too long to process requests and the dashboard runs
     into timeouts, the timeout value can be adjusted (default is 45 seconds):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-rest-requests-timeout <em class="replaceable">SECONDS</em></pre></div></li></ul></div></section><section class="sect1" id="dashboard-iscsi-management" data-id-title="Enabling iSCSI management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.5 </span><span class="title-name">Enabling iSCSI management</span> <a title="Permalink" class="permalink" href="#dashboard-iscsi-management">#</a></h2></div></div></div><p>
   The Ceph Dashboard manages iSCSI targets using the REST API provided by the
   <code class="systemitem">rbd-target-api</code> service of the
   Ceph iSCSI gateway. Ensure it is installed and enabled on iSCSI
   gateways.
  </p><div id="id-1.4.3.11.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The iSCSI management functionality of the Ceph Dashboard depends on the
    latest version 3 of the <code class="literal">ceph-iscsi</code> project. Ensure that
    your operating system provides the correct version, otherwise the
    Ceph Dashboard will not enable the management features.
   </p></div><p>
   If the <code class="literal">ceph-iscsi</code> REST API is configured in HTTPS mode
   and it is using a self-signed certificate, configure the dashboard to avoid
   SSL certificate verification when accessing ceph-iscsi API.
  </p><p>
   Disable API SSL verification:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-iscsi-api-ssl-verification false</pre></div><p>
   Define the available iSCSI gateways:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard iscsi-gateway-list
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard iscsi-gateway-add <em class="replaceable">scheme</em>://<em class="replaceable">username</em>:<em class="replaceable">password</em>@<em class="replaceable">host</em>[:port]
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard iscsi-gateway-rm <em class="replaceable">gateway_name</em></pre></div></section><section class="sect1" id="dashboard-sso" data-id-title="Enabling Single Sign-On"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.6 </span><span class="title-name">Enabling Single Sign-On</span> <a title="Permalink" class="permalink" href="#dashboard-sso">#</a></h2></div></div></div><p>
   <span class="emphasis"><em>Single Sign-On</em></span> (SSO) is an access control method that enables
   users to log in with a single ID and password to multiple applications
   simultaneously.
  </p><p>
   The Ceph Dashboard supports external authentication of users via the SAML 2.0
   protocol. Because <span class="emphasis"><em>authorization</em></span> is still performed by
   the dashboard, you first need to create user accounts and associate them
   with the desired roles. However, the <span class="emphasis"><em>authentication</em></span>
   process can be performed by an existing <span class="emphasis"><em>Identity
   Provider</em></span> (IdP).
  </p><p>
   To configure Single Sign-On, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard sso setup saml2 <em class="replaceable">CEPH_DASHBOARD_BASE_URL</em> \
 <em class="replaceable">IDP_METADATA</em> <em class="replaceable">IDP_USERNAME_ATTRIBUTE</em> \
 <em class="replaceable">IDP_ENTITY_ID</em> <em class="replaceable">SP_X_509_CERT</em> \
 <em class="replaceable">SP_PRIVATE_KEY</em></pre></div><p>
   Parameters:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.11.9.7.1"><span class="term"><em class="replaceable">CEPH_DASHBOARD_BASE_URL</em></span></dt><dd><p>
      Base URL where Ceph Dashboard is accessible (for example,
      'https://cephdashboard.local').
     </p></dd><dt id="id-1.4.3.11.9.7.2"><span class="term"><em class="replaceable">IDP_METADATA</em></span></dt><dd><p>
      URL, file path, or content of the IdP metadata XML (for example,
      'https://myidp/metadata').
     </p></dd><dt id="id-1.4.3.11.9.7.3"><span class="term"><em class="replaceable">IDP_USERNAME_ATTRIBUTE</em></span></dt><dd><p>
      Optional. Attribute that will be used to get the user name from the
      authentication response. Defaults to 'uid'.
     </p></dd><dt id="id-1.4.3.11.9.7.4"><span class="term"><em class="replaceable">IDP_ENTITY_ID</em></span></dt><dd><p>
      Optional. Use when more than one entity ID exists on the IdP metadata.
     </p></dd><dt id="id-1.4.3.11.9.7.5"><span class="term"><em class="replaceable">SP_X_509_CERT</em> / <em class="replaceable">SP_PRIVATE_KEY</em></span></dt><dd><p>
      Optional. File path or content of the certificate that will be used by
      Ceph Dashboard (Service Provider) for signing and encryption. These file
      paths need to be accessible from the active Ceph Manager instance.
     </p></dd></dl></div><div id="id-1.4.3.11.9.8" data-id-title="SAML requests" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: SAML requests</h6><p>
    The issuer value of SAML requests will follow this pattern:
   </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">CEPH_DASHBOARD_BASE_URL</em>/auth/saml2/metadata</pre></div></div><p>
   To display the current SAML 2.0 configuration, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard sso show saml2</pre></div><p>
   To disable Single Sign-On, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard sso disable</pre></div><p>
   To check if SSO is enabled, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard sso status</pre></div><p>
   To enable SSO, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard sso enable saml2</pre></div></section></section><section class="chapter" id="dashboard-user-roles" data-id-title="Manage users and roles on the command line"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">11 </span><span class="title-name">Manage users and roles on the command line</span> <a title="Permalink" class="permalink" href="#dashboard-user-roles">#</a></h1></div></div></div><p>
  This section describes how to manage user accounts used by the Ceph Dashboard.
  It helps you create or modify user accounts, as well as set proper user roles
  and permissions.
 </p><section class="sect1" id="dashboard-password-policy" data-id-title="Managing the password policy"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Managing the password policy</span> <a title="Permalink" class="permalink" href="#dashboard-password-policy">#</a></h2></div></div></div><p>
   By default the password policy feature is enabled including the following
   checks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Is the password longer than <span class="emphasis"><em>N</em></span> characters?
    </p></li><li class="listitem"><p>
     Are the old and new password the same?
    </p></li></ul></div><p>
   The password policy feature can be switched on or off completely:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-enabled <em class="replaceable">true|false</em></pre></div><p>
   The following individual checks can be switched on or off:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-length-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-oldpwd-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-username-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-exclusion-list-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-complexity-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-sequential-chars-enabled <em class="replaceable">true|false</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-check-repetitive-chars-enabled <em class="replaceable">true|false</em></pre></div><p>
   In addition, the following options are available to configure the password
   policy behaviour.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The minimum password length (defaults to 8):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-min-length <em class="replaceable">N</em></pre></div></li><li class="listitem"><p>
     The minimum password complexity (defaults to 10):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-min-complexity <em class="replaceable">N</em></pre></div><p>
     The password complexity is calculated by classifying each character in the
     password.
    </p></li><li class="listitem"><p>
     A list of comma-separated words that are not allowed to be used in a
     password:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-pwd-policy-exclusion-list <em class="replaceable">word</em>[,...]</pre></div></li></ul></div></section><section class="sect1" id="dashboard-user-accounts" data-id-title="Managing user accounts"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.2 </span><span class="title-name">Managing user accounts</span> <a title="Permalink" class="permalink" href="#dashboard-user-accounts">#</a></h2></div></div></div><p>
   The Ceph Dashboard supports managing multiple user accounts. Each user account
   consists of a user name, a password (stored in encrypted form using
   <code class="literal">bcrypt</code>), an optional name, and an optional e-mail
   address.
  </p><p>
   User accounts are stored in Ceph Monitor's configuration database and are shared
   globally across all Ceph Manager instances.
  </p><p>
   Use the following commands to manage user accounts:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.5.5.1"><span class="term">Show existing users:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-show [<em class="replaceable">USERNAME</em>]</pre></div></dd><dt id="id-1.4.3.12.5.5.2"><span class="term">Create a new user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-create <em class="replaceable">USERNAME</em> -i [<em class="replaceable">PASSWORD_FILE</em>] [<em class="replaceable">ROLENAME</em>] [<em class="replaceable">NAME</em>] [<em class="replaceable">EMAIL</em>]</pre></div></dd><dt id="id-1.4.3.12.5.5.3"><span class="term">Delete a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-delete <em class="replaceable">USERNAME</em></pre></div></dd><dt id="id-1.4.3.12.5.5.4"><span class="term">Change a user's password:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-set-password <em class="replaceable">USERNAME</em> -i <em class="replaceable">PASSWORD_FILE</em></pre></div></dd><dt id="id-1.4.3.12.5.5.5"><span class="term">Modify a user's name and email:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-set-info <em class="replaceable">USERNAME</em> <em class="replaceable">NAME</em> <em class="replaceable">EMAIL</em></pre></div></dd><dt id="id-1.4.3.12.5.5.6"><span class="term">Disable user</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-disable <em class="replaceable">USERNAME</em></pre></div></dd><dt id="id-1.4.3.12.5.5.7"><span class="term">Enable User</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-enable <em class="replaceable">USERNAME</em></pre></div></dd></dl></div></section><section class="sect1" id="dashboard-permissions" data-id-title="User roles and permissions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.3 </span><span class="title-name">User roles and permissions</span> <a title="Permalink" class="permalink" href="#dashboard-permissions">#</a></h2></div></div></div><p>
   This section describes what security scopes you can assign to a user role,
   how to manage user roles and assign them to user accounts.
  </p><section class="sect2" id="dashboard-security-scopes" data-id-title="Defining security scopes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.1 </span><span class="title-name">Defining security scopes</span> <a title="Permalink" class="permalink" href="#dashboard-security-scopes">#</a></h3></div></div></div><p>
    User accounts are associated with a set of roles that define which parts of
    the dashboard can be accessed by the user. The dashboard parts are grouped
    within a <span class="emphasis"><em>security</em></span> scope. Security scopes are
    predefined and static. The following security scopes are currently
    available:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.6.3.3.1"><span class="term">hosts</span></dt><dd><p>
       Includes all features related to the <span class="guimenu">Hosts</span> menu
       entry.
      </p></dd><dt id="id-1.4.3.12.6.3.3.2"><span class="term">config-opt</span></dt><dd><p>
       Includes all features related to the management of Ceph configuration
       options.
      </p></dd><dt id="id-1.4.3.12.6.3.3.3"><span class="term">pool</span></dt><dd><p>
       Includes all features related to pool management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.4"><span class="term">osd</span></dt><dd><p>
       Includes all features related to the Ceph OSD management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.5"><span class="term">monitor</span></dt><dd><p>
       Includes all features related to the Ceph Monitor management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.6"><span class="term">rbd-image</span></dt><dd><p>
       Includes all features related to the RADOS Block Device image management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.7"><span class="term">rbd-mirroring</span></dt><dd><p>
       Includes all features related to the RADOS Block Device mirroring management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.8"><span class="term">iscsi</span></dt><dd><p>
       Includes all features related to iSCSI management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.9"><span class="term">rgw</span></dt><dd><p>
       Includes all features related to the Object Gateway management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.10"><span class="term">cephfs</span></dt><dd><p>
       Includes all features related to CephFS management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.11"><span class="term">manager</span></dt><dd><p>
       Includes all features related to the Ceph Manager management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.12"><span class="term">log</span></dt><dd><p>
       Includes all features related to Ceph logs management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.13"><span class="term">grafana</span></dt><dd><p>
       Includes all features related to the Grafana proxy.
      </p></dd><dt id="id-1.4.3.12.6.3.3.14"><span class="term">prometheus</span></dt><dd><p>
       Include all features related to Prometheus alert management.
      </p></dd><dt id="id-1.4.3.12.6.3.3.15"><span class="term">dashboard-settings</span></dt><dd><p>
       Allows changing dashboard settings.
      </p></dd></dl></div></section><section class="sect2" id="dashboard-user-roles-security-scope" data-id-title="Specifying user roles"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.3.2 </span><span class="title-name">Specifying user roles</span> <a title="Permalink" class="permalink" href="#dashboard-user-roles-security-scope">#</a></h3></div></div></div><p>
    A <span class="emphasis"><em>role</em></span> specifies a set of mappings between a
    <span class="emphasis"><em>security scope</em></span> and a set of
    <span class="emphasis"><em>permissions</em></span>. There are four types of permissions:
    'read', 'create', 'update', and 'delete'.
   </p><p>
    The following example specifies a role where a user has 'read' and 'create'
    permissions for features related to pool management, and has full
    permissions for features related to RBD image management:
   </p><div class="verbatim-wrap"><pre class="screen">{
  'role': 'my_new_role',
  'description': 'My new role',
  'scopes_permissions': {
    'pool': ['read', 'create'],
    'rbd-image': ['read', 'create', 'update', 'delete']
  }
}</pre></div><p>
    The dashboard already provides a set of predefined roles that we call
    <span class="emphasis"><em>system roles</em></span>. You can instantly use them after a fresh
    Ceph Dashboard installation:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.6.4.6.1"><span class="term">administrator</span></dt><dd><p>
       Provides full permissions for all security scopes.
      </p></dd><dt id="id-1.4.3.12.6.4.6.2"><span class="term">read-only</span></dt><dd><p>
       Provides read permission for all security scopes except the dashboard
       settings.
      </p></dd><dt id="id-1.4.3.12.6.4.6.3"><span class="term">block-manager</span></dt><dd><p>
       Provides full permissions for 'rbd-image', 'rbd-mirroring', and 'iscsi'
       scopes.
      </p></dd><dt id="id-1.4.3.12.6.4.6.4"><span class="term">rgw-manager</span></dt><dd><p>
       Provides full permissions for the 'rgw' scope.
      </p></dd><dt id="id-1.4.3.12.6.4.6.5"><span class="term">cluster-manager</span></dt><dd><p>
       Provides full permissions for the 'hosts', 'osd', 'monitor', 'manager',
       and 'config-opt' scopes.
      </p></dd><dt id="id-1.4.3.12.6.4.6.6"><span class="term">pool-manager</span></dt><dd><p>
       Provides full permissions for the 'pool' scope.
      </p></dd><dt id="id-1.4.3.12.6.4.6.7"><span class="term">cephfs-manager</span></dt><dd><p>
       Provides full permissions for the 'cephfs' scope.
      </p></dd></dl></div><section class="sect3" id="dashboard-managing-custom-roles" data-id-title="Managing custom roles"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.2.1 </span><span class="title-name">Managing custom roles</span> <a title="Permalink" class="permalink" href="#dashboard-managing-custom-roles">#</a></h4></div></div></div><p>
     You can create new user roles by using the following commands:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.6.4.7.3.1"><span class="term">Create a new role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-create <em class="replaceable">ROLENAME</em> [<em class="replaceable">DESCRIPTION</em>]</pre></div></dd><dt id="id-1.4.3.12.6.4.7.3.2"><span class="term">Delete a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-delete <em class="replaceable">ROLENAME</em></pre></div></dd><dt id="id-1.4.3.12.6.4.7.3.3"><span class="term">Add scope permissions to a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-add-scope-perms <em class="replaceable">ROLENAME</em> <em class="replaceable">SCOPENAME</em> <em class="replaceable">PERMISSION</em> [<em class="replaceable">PERMISSION</em>...]</pre></div></dd><dt id="id-1.4.3.12.6.4.7.3.4"><span class="term">Delete scope permissions from a role:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-del-perms <em class="replaceable">ROLENAME</em> <em class="replaceable">SCOPENAME</em></pre></div></dd></dl></div></section><section class="sect3" id="dashboard-assigning-roles-user-accounts" data-id-title="Assigning roles to user accounts"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.2.2 </span><span class="title-name">Assigning roles to user accounts</span> <a title="Permalink" class="permalink" href="#dashboard-assigning-roles-user-accounts">#</a></h4></div></div></div><p>
     Use the following commands to assign roles to users:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.6.4.8.3.1"><span class="term">Set user roles:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-set-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd><dt id="id-1.4.3.12.6.4.8.3.2"><span class="term">Add additional roles to a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-add-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd><dt id="id-1.4.3.12.6.4.8.3.3"><span class="term">Delete roles from a user:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-del-roles <em class="replaceable">USERNAME</em> <em class="replaceable">ROLENAME</em> [<em class="replaceable">ROLENAME</em> ...]</pre></div></dd></dl></div><div id="id-1.4.3.12.6.4.8.4" data-id-title="Purging custom roles" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Purging custom roles</h6><p>
      If you create custom user roles and intend to remove the Ceph cluster
      with the <code class="command">ceph.purge</code> runner later on, you need to purge
      the custom roles first. Find more details in
      <a class="xref" href="#ceph-cluster-purge" title="13.9. Removing an entire Ceph cluster">Section 13.9, “Removing an entire Ceph cluster”</a>.
     </p></div></section><section class="sect3" id="example-create-user-custom-role" data-id-title="Example: Creating a user and a custom role"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.3.2.3 </span><span class="title-name">Example: Creating a user and a custom role</span> <a title="Permalink" class="permalink" href="#example-create-user-custom-role">#</a></h4></div></div></div><p>
     This section illustrates a procedure for creating a user account capable
     of managing RBD images, viewing and creating Ceph pools, and having
     read-only access to any other scopes.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create a new user named <code class="literal">tux</code>:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-create tux <em class="replaceable">PASSWORD</em></pre></div></li><li class="step"><p>
       Create a role and specify scope permissions:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-create rbd/pool-manager
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-add-scope-perms rbd/pool-manager \
 rbd-image read create update delete
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-role-add-scope-perms rbd/pool-manager pool read create</pre></div></li><li class="step"><p>
       Associate the roles with the <code class="literal">tux</code> user:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard ac-user-set-roles tux rbd/pool-manager read-only</pre></div></li></ol></div></div></section></section></section><section class="sect1" id="dashboard-proxy-config" data-id-title="Proxy configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.4 </span><span class="title-name">Proxy configuration</span> <a title="Permalink" class="permalink" href="#dashboard-proxy-config">#</a></h2></div></div></div><p>
   If you want to establish a fixed URL to reach the Ceph Dashboard or if you do
   not want to allow direct connections to the manager nodes, you can set up a
   proxy that automatically forwards incoming requests to the currently active
   <code class="literal">ceph-mgr</code> instance.
  </p><section class="sect2" id="dashboard-proxy" data-id-title="Accessing the dashboard with reverse proxies"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.4.1 </span><span class="title-name">Accessing the dashboard with reverse proxies</span> <a title="Permalink" class="permalink" href="#dashboard-proxy">#</a></h3></div></div></div><p>
    If you are accessing the dashboard via a reverse proxy configuration, you
    may need to service it under a URL prefix. To get the dashboard to use
    hyperlinks that include your prefix, you can set the
    <code class="option">url_prefix</code> setting:
   </p><div class="verbatim-wrap"><pre class="screen">  <code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/url_prefix <em class="replaceable">URL_PREFIX</em></pre></div><p>
    Then you can access the dashboard at
    <code class="literal">http://<em class="replaceable">HOST_NAME</em>:<em class="replaceable">PORT_NUMBER</em>/<em class="replaceable">URL_PREFIX</em>/</code>.
   </p></section><section class="sect2" id="dashboard-disable-redirection" data-id-title="Disabling re-directions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.4.2 </span><span class="title-name">Disabling re-directions</span> <a title="Permalink" class="permalink" href="#dashboard-disable-redirection">#</a></h3></div></div></div><p>
    If the Ceph Dashboard is behind a load-balancing proxy such as HAProxy,
    disable the redirection behaviour to prevent situations where the internal
    (unresolvable) URLs are published to the front-end client. Use the
    following command to get the dashboard to respond with an HTTP error
    (<code class="literal">500</code> by default) instead of redirecting to the active
    dashboard:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/standby_behaviour "error"</pre></div><p>
    To reset the setting to the default redirection behaviour, use the
    following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/standby_behaviour "redirect"</pre></div></section><section class="sect2" id="dashboard-config-error-status" data-id-title="Configuring error status codes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.4.3 </span><span class="title-name">Configuring error status codes</span> <a title="Permalink" class="permalink" href="#dashboard-config-error-status">#</a></h3></div></div></div><p>
    If the redirection behaviour is disabled, then you should customize the
    HTTP status code of standby dashboards. To do so, run the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/dashboard/standby_error_status_code 503</pre></div></section><section class="sect2" id="dashboard-haproxy-config" data-id-title="HAProxy example configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.4.4 </span><span class="title-name">HAProxy example configuration</span> <a title="Permalink" class="permalink" href="#dashboard-haproxy-config">#</a></h3></div></div></div><p>
    The following example configuration is for TLS/SSL pass through using
    HAProxy.
   </p><div id="id-1.4.3.12.7.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The configuration works under the following conditions: If the dashboard
     fails over, the front-end client might receive an HTTP redirect
     (<code class="literal">303</code>) response and will be redirected to an
     unresolvable host.
    </p><p>
     This happens when the failover occurs during two HAProxy health checks. In
     this situation the previously active dashboard node will now respond with
     a 303 which points to the new active node. To prevent that situation you
     should consider disabling the redirection behaviour on standby nodes.
    </p></div><div class="verbatim-wrap"><pre class="screen">  defaults
    log global
    option log-health-checks
    timeout connect 5s
    timeout client 50s
    timeout server 450s

  frontend dashboard_front
    mode http
    bind *:80
    option httplog
    redirect scheme https code 301 if !{ ssl_fc }

  frontend dashboard_front_ssl
    mode tcp
    bind *:443
    option tcplog
    default_backend dashboard_back_ssl

  backend dashboard_back_ssl
    mode tcp
    option httpchk GET /
    http-check expect status 200
    server x <em class="replaceable">HOST</em>:<em class="replaceable">PORT</em> ssl check verify none
    server y <em class="replaceable">HOST</em>:<em class="replaceable">PORT</em> ssl check verify none
    server z <em class="replaceable">HOST</em>:<em class="replaceable">PORT</em> ssl check verify none</pre></div></section></section><section class="sect1" id="dashboard-auditing" data-id-title="Auditing API requests"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.5 </span><span class="title-name">Auditing API requests</span> <a title="Permalink" class="permalink" href="#dashboard-auditing">#</a></h2></div></div></div><p>
   The Ceph Dashboard's REST API can log PUT, POST, and DELETE requests to the
   Ceph audit log. Logging is disabled by default, but you can enable it with
   the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-audit-api-enabled true</pre></div><p>
   If enabled, the following parameters are logged per each request:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.3.12.8.5.1"><span class="term">from</span></dt><dd><p>
      The origin of the request, for example 'https://[::1]:44410'.
     </p></dd><dt id="id-1.4.3.12.8.5.2"><span class="term">path</span></dt><dd><p>
      The REST API path, for example <code class="filename">/api/auth</code>.
     </p></dd><dt id="id-1.4.3.12.8.5.3"><span class="term">method</span></dt><dd><p>
      'PUT', 'POST', or 'DELETE'.
     </p></dd><dt id="id-1.4.3.12.8.5.4"><span class="term">user</span></dt><dd><p>
      The name of the user (or ‘None’).
     </p></dd></dl></div><p>
   An example log entry looks like this:
  </p><div class="verbatim-wrap"><pre class="screen">2019-02-06 10:33:01.302514 mgr.x [INF] [DASHBOARD] \
 from='https://[::ffff:127.0.0.1]:37022' path='/api/rgw/user/exu' method='PUT' \
 user='admin' params='{"max_buckets": "1000", "display_name": "Example User", "uid": "exu", "suspended": "0", "email": "user@example.com"}'</pre></div><div id="id-1.4.3.12.8.8" data-id-title="Disable logging of request payload" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disable logging of request payload</h6><p>
    The logging of the request payload (the list of arguments and their values)
    is enabled by default. You can disable it as follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-audit-api-log-payload false</pre></div></div></section><section class="sect1" id="dashboard-config-nfs-ganesha" data-id-title="Configuring NFS Ganesha in the Ceph Dashboard"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.6 </span><span class="title-name">Configuring NFS Ganesha in the Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#dashboard-config-nfs-ganesha">#</a></h2></div></div></div><p>
   Ceph Dashboard can manage NFS Ganesha exports that use CephFS or Object Gateway as their
   backstore. The dashboard manages NFS Ganesha configuration files stored in
   RADOS objects on the CephFS cluster. NFS Ganesha must store part of their
   configuration in the Ceph cluster.
  </p><p>
   Run the following command to configure the NFS Ganesha configuration object's
   location:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ganesha-clusters-rados-pool-namespace <em class="replaceable">pool_name</em>[/<em class="replaceable">namespace</em>]</pre></div><p>
   You can now manage NFS Ganesha exports using the Ceph Dashboard.
  </p><section class="sect2" id="dashboard-multi-config-nfs-ganesha" data-id-title="Configuring multiple NFS Ganesha clusters"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.6.1 </span><span class="title-name">Configuring multiple NFS Ganesha clusters</span> <a title="Permalink" class="permalink" href="#dashboard-multi-config-nfs-ganesha">#</a></h3></div></div></div><p>
    The Ceph Dashboard supports the management of NFS Ganesha exports belonging to
    different NFS Ganesha clusters. We recommend each NFS Ganesha cluster store its
    configuration objects in a different RADOS pool/namespace to isolate the
    configurations from each other.
   </p><p>
    Use the following command to specify the locations of the configuration of
    each NFS Ganesha cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-ganesha-clusters-rados-pool-namespace <em class="replaceable">cluster_id</em>:<em class="replaceable">pool_name</em>[/<em class="replaceable">namespace</em>](,<em class="replaceable">cluster_id</em>:<em class="replaceable">pool_name</em>[/<em class="replaceable">namespace</em>])*</pre></div><p>
    The <em class="replaceable">cluster_id</em> is an arbitrary string that
    uniquely identifies the NFS Ganesha cluster.
   </p><p>
    When configuring the Ceph Dashboard with multiple NFS Ganesha clusters, the Web
    UI automatically allows you to choose to which cluster an export belongs.
   </p></section></section><section class="sect1" id="dashboard-debug-plugin" data-id-title="Debugging plugins"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.7 </span><span class="title-name">Debugging plugins</span> <a title="Permalink" class="permalink" href="#dashboard-debug-plugin">#</a></h2></div></div></div><p>
   Ceph Dashboard plugins extend the functionality of the dashboard. The debug
   plugin allows the customization of the behaviour of the dashboard according
   to the debug mode. It can be enabled, disabled, or checked with the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard debug status
Debug: 'disabled'
<code class="prompt user">cephuser@adm &gt; </code>ceph dashboard debug enable
Debug: 'enabled'
<code class="prompt user">cephuser@adm &gt; </code>dashboard debug disable
Debug: 'disabled'</pre></div><p>
   By default, this is disabled. This is the recommended setting for production
   deployments. If required, debug mode can be enabled without need of
   restarting.
  </p></section></section></div><div class="part" id="part-cluster-operation" data-id-title="Cluster Operation"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Cluster Operation </span><a title="Permalink" class="permalink" href="#part-cluster-operation">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#ceph-monitor"><span class="title-number">12 </span><span class="title-name">Determine the cluster state</span></a></span></li><dd class="toc-abstract"><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups, and Metadata Servers.
 </p></dd><li><span class="chapter"><a href="#storage-salt-cluster"><span class="title-number">13 </span><span class="title-name">Operational tasks</span></a></span></li><dd class="toc-abstract"><p>
   To modify the configuration of an existing Ceph cluster, follow these
   steps:
  </p></dd><li><span class="chapter"><a href="#cha-ceph-operating"><span class="title-number">14 </span><span class="title-name">Operation of Ceph services</span></a></span></li><dd class="toc-abstract"><p>
  You can operate Ceph services on a daemon, node, or cluster level.
  Depending on which approach you need, use cephadm or the
  <code class="command">systemctl</code> command.
 </p></dd><li><span class="chapter"><a href="#cha-deployment-backup"><span class="title-number">15 </span><span class="title-name">Backup and restore</span></a></span></li><dd class="toc-abstract"><p>
  This chapter explains which parts of the Ceph cluster you should back up in
  order to be able to restore its functionality.
 </p></dd><li><span class="chapter"><a href="#monitoring-alerting"><span class="title-number">16 </span><span class="title-name">Monitoring and alerting</span></a></span></li><dd class="toc-abstract"><p>In SUSE Enterprise Storage 7.1, cephadm deploys a monitoring and alerting stack. Users need to either define the services (such as Prometheus, Alertmanager, and Grafana) that they want to deploy with cephadm in a YAML configuration file, or they can use the CLI to deploy them. When multiple services…</p></dd></ul></div><section class="chapter" id="ceph-monitor" data-id-title="Determine the cluster state"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">12 </span><span class="title-name">Determine the cluster state</span> <a title="Permalink" class="permalink" href="#ceph-monitor">#</a></h1></div></div></div><p>
  When you have a running cluster, you may use the <code class="command">ceph</code> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of Ceph OSDs, Ceph Monitors, placement groups, and Metadata Servers.
 </p><div id="id-1.4.4.2.4" data-id-title="Interactive mode" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Interactive mode</h6><p>
   To run the <code class="command">ceph</code> tool in an interactive mode, type
   <code class="command">ceph</code> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <code class="command">ceph</code> commands in a row. For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon stat</pre></div></div><section class="sect1" id="monitor-status" data-id-title="Checking a clusters status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Checking a cluster's status</span> <a title="Permalink" class="permalink" href="#monitor-status">#</a></h2></div></div></div><p>
   You can find the immediate state of the cluster using <code class="command">ceph
   status</code> or <code class="command">ceph -s</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph -s
cluster:
    id:     b4b30c6e-9681-11ea-ac39-525400d7702d
    health: HEALTH_OK

  services:
    mon: 5 daemons, quorum ses-min1,ses-master,ses-min2,ses-min4,ses-min3 (age 2m)
    mgr: ses-min1.gpijpm(active, since 3d), standbys: ses-min2.oopvyh
    mds: my_cephfs:1 {0=my_cephfs.ses-min1.oterul=up:active}
    osd: 3 osds: 3 up (since 3d), 3 in (since 11d)
    rgw: 2 daemons active (myrealm.myzone.ses-min1.kwwazo, myrealm.myzone.ses-min2.jngabw)

  task status:
    scrub status:
        mds.my_cephfs.ses-min1.oterul: idle

  data:
    pools:   7 pools, 169 pgs
    objects: 250 objects, 10 KiB
    usage:   3.1 GiB used, 27 GiB / 30 GiB avail
    pgs:     169 active+clean</pre></div><p>
   The output provides the following information:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Cluster ID
    </p></li><li class="listitem"><p>
     Cluster health status
    </p></li><li class="listitem"><p>
     The monitor map epoch and the status of the monitor quorum
    </p></li><li class="listitem"><p>
     The OSD map epoch and the status of OSDs
    </p></li><li class="listitem"><p>
     The status of Ceph Managers
    </p></li><li class="listitem"><p>
     The status of Object Gateways
    </p></li><li class="listitem"><p>
     The placement group map version
    </p></li><li class="listitem"><p>
     The number of placement groups and pools
    </p></li><li class="listitem"><p>
     The <span class="emphasis"><em>notional</em></span> amount of data stored and the number of
     objects stored
    </p></li><li class="listitem"><p>
     The total amount of data stored.
    </p></li></ul></div><div id="id-1.4.4.2.5.6" data-id-title="How Ceph calculates data usage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: How Ceph calculates data usage</h6><p>
    The <code class="literal">used</code> value reflects the actual amount of raw storage
    used. The <code class="literal">xxx GB / xxx GB</code> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because Ceph creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </p></div><p>
   Other commands that display immediate status information are:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="command">ceph pg stat</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph osd pool stats</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df</code>
    </p></li><li class="listitem"><p>
     <code class="command">ceph df detail</code>
    </p></li></ul></div><p>
   To get the information updated in real time, put any of these commands
   (including <code class="command">ceph -s</code>) as an argument of the
   <code class="command">watch</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>watch -n 10 'ceph -s'</pre></div><p>
   Press <span class="keycap">Ctrl</span><span class="key-connector">–</span><span class="keycap">C</span>
   when you are tired of watching.
  </p></section><section class="sect1" id="monitor-health" data-id-title="Checking cluster health"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Checking cluster health</span> <a title="Permalink" class="permalink" href="#monitor-health">#</a></h2></div></div></div><p>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</pre></div><div id="id-1.4.4.2.6.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph -c <em class="replaceable">/path/to/conf</em> -k <em class="replaceable">/path/to/keyring</em> health</pre></div></div><p>
   The Ceph cluster returns one of the following health codes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.6.6.1"><span class="term">OSD_DOWN</span></dt><dd><p>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </p><p>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<code class="filename">/var/log/ceph/ceph-osd.*</code>) may contain debugging
      information.
     </p></dd><dt id="id-1.4.4.2.6.6.2"><span class="term">OSD_<em class="replaceable">crush type</em>_DOWN, for example OSD_HOST_DOWN</span></dt><dd><p>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </p></dd><dt id="id-1.4.4.2.6.6.3"><span class="term">OSD_ORPHAN</span></dt><dd><p>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rm osd.<em class="replaceable">ID</em></pre></div></dd><dt id="id-1.4.4.2.6.6.4"><span class="term">OSD_OUT_OF_ORDER_FULL</span></dt><dd><p>
      The usage thresholds for <span class="emphasis"><em>backfillfull</em></span> (defaults to
      0.90), <span class="emphasis"><em>nearfull</em></span> (defaults to 0.85),
      <span class="emphasis"><em>full</em></span> (defaults to 0.95), and/or
      <span class="emphasis"><em>failsafe_full</em></span> are not ascending. In particular, we
      expect <span class="emphasis"><em>backfillfull</em></span> &lt;
      <span class="emphasis"><em>nearfull</em></span>, <span class="emphasis"><em>nearfull</em></span> &lt;
      <span class="emphasis"><em>full</em></span>, and <span class="emphasis"><em>full</em></span> &lt;
      <span class="emphasis"><em>failsafe_full</em></span>.
     </p><p>
      To read the current values, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%</pre></div><p>
      The thresholds can be adjusted with the following commands:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set-backfillfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd set-nearfull-ratio <em class="replaceable">ratio</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div></dd><dt id="id-1.4.4.2.6.6.5"><span class="term">OSD_FULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>full</em></span> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph df</pre></div><p>
      The currently defined <span class="emphasis"><em>full</em></span> ratio can be seen with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd dump | grep full_ratio</pre></div><p>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set-full-ratio <em class="replaceable">ratio</em></pre></div><p>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </p></dd><dt id="id-1.4.4.2.6.6.6"><span class="term">OSD_BACKFILLFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>backfillfull</em></span>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.4.4.2.6.6.7"><span class="term">OSD_NEARFULL</span></dt><dd><p>
      One or more OSDs has exceeded the <span class="emphasis"><em>nearfull</em></span>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph df</pre></div></dd><dt id="id-1.4.4.2.6.6.8"><span class="term">OSDMAP_FLAGS</span></dt><dd><p>
      One or more cluster flags of interest has been set. With the exception of
      <span class="emphasis"><em>full</em></span>, these flags can be set or cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set <em class="replaceable">flag</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd unset <em class="replaceable">flag</em></pre></div><p>
      These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.6.6.8.2.4.1"><span class="term">full</span></dt><dd><p>
         The cluster is flagged as full and cannot service writes.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.2"><span class="term">pauserd, pausewr</span></dt><dd><p>
         Paused reads or writes.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.3"><span class="term">noup</span></dt><dd><p>
         OSDs are not allowed to start.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.4"><span class="term">nodown</span></dt><dd><p>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <span class="emphasis"><em>down</em></span>.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.5"><span class="term">noin</span></dt><dd><p>
         OSDs that were previously marked <span class="emphasis"><em>out</em></span> will not be
         marked back <span class="emphasis"><em>in</em></span> when they start.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.6"><span class="term">noout</span></dt><dd><p>
         <span class="emphasis"><em>Down</em></span> OSDs will not automatically be marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.7"><span class="term">nobackfill, norecover, norebalance</span></dt><dd><p>
         Recovery or data rebalancing is suspended.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.8"><span class="term">noscrub, nodeep_scrub</span></dt><dd><p>
         Scrubbing (see <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) is disabled.
        </p></dd><dt id="id-1.4.4.2.6.6.8.2.4.9"><span class="term">notieragent</span></dt><dd><p>
         Cache tiering activity is suspended.
        </p></dd></dl></div></dd><dt id="id-1.4.4.2.6.6.9"><span class="term">OSD_FLAGS</span></dt><dd><p>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.6.6.9.2.2.1"><span class="term">noup</span></dt><dd><p>
         OSD is not allowed to start.
        </p></dd><dt id="id-1.4.4.2.6.6.9.2.2.2"><span class="term">nodown</span></dt><dd><p>
         Failure reports for this OSD will be ignored.
        </p></dd><dt id="id-1.4.4.2.6.6.9.2.2.3"><span class="term">noin</span></dt><dd><p>
         If this OSD was previously marked <span class="emphasis"><em>out</em></span>
         automatically after a failure, it will not be marked
         <span class="emphasis"><em>in</em></span> when it starts.
        </p></dd><dt id="id-1.4.4.2.6.6.9.2.2.4"><span class="term">noout</span></dt><dd><p>
         If this OSD is down, it will not be automatically marked
         <span class="emphasis"><em>out</em></span> after the configured interval.
        </p></dd></dl></div><p>
      Per-OSD flags can be set and cleared with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd add-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd rm-<em class="replaceable">flag</em> <em class="replaceable">osd-ID</em></pre></div></dd><dt id="id-1.4.4.2.6.6.10"><span class="term">OLD_CRUSH_TUNABLES</span></dt><dd><p>
      The CRUSH Map is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <code class="option">mon_crush_min_required_version</code>
      configuration option.
     </p></dd><dt id="id-1.4.4.2.6.6.11"><span class="term">OLD_CRUSH_STRAW_CALC_VERSION</span></dt><dd><p>
      The CRUSH Map is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The CRUSH Map should be
      updated to use the newer method (<code class="option">straw_calc_version</code>=1).
     </p></dd><dt id="id-1.4.4.2.6.6.12"><span class="term">CACHE_POOL_NO_HIT_SET</span></dt><dd><p>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_type <em class="replaceable">type</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_period <em class="replaceable">period-in-seconds</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_count <em class="replaceable">number-of-hitsets</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> hit_set_fpp <em class="replaceable">target-false-positive-rate</em></pre></div></dd><dt id="id-1.4.4.2.6.6.13"><span class="term">OSD_NO_SORTBITWISE</span></dt><dd><p>
      No pre-Luminous v12 OSDs are running but the <code class="option">sortbitwise</code>
      flag has not been set. You need to set the <code class="option">sortbitwise</code>
      flag before Luminous v12 or newer OSDs can start:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set sortbitwise</pre></div></dd><dt id="id-1.4.4.2.6.6.14"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph df detail</pre></div><p>
      You can either raise the pool quota with
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_objects <em class="replaceable">num-objects</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">poolname</em> max_bytes <em class="replaceable">num-bytes</em></pre></div><p>
      or delete some existing data to reduce usage.
     </p></dd><dt id="id-1.4.4.2.6.6.15"><span class="term">PG_AVAILABILITY</span></dt><dd><p>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow I/O
      requests to be serviced. Problematic PG states include
      <span class="emphasis"><em>peering</em></span>, <span class="emphasis"><em>stale</em></span>,
      <span class="emphasis"><em>incomplete</em></span>, and the lack of
      <span class="emphasis"><em>active</em></span> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.4.4.2.6.6.16"><span class="term">PG_DEGRADED</span></dt><dd><p>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <span class="emphasis"><em>degraded</em></span> or
      <span class="emphasis"><em>undersized</em></span> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <span class="emphasis"><em>clean</em></span> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health detail</pre></div><p>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.4.4.2.6.6.17"><span class="term">PG_DEGRADED_FULL</span></dt><dd><p>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <span class="emphasis"><em>backfill_toofull</em></span> or
      <span class="emphasis"><em>recovery_toofull</em></span> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <span class="emphasis"><em>backfillfull</em></span> threshold.
     </p></dd><dt id="id-1.4.4.2.6.6.18"><span class="term">PG_DAMAGED</span></dt><dd><p>
      Data scrubbing (see <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <span class="emphasis"><em>inconsistent</em></span> or
      <span class="emphasis"><em>snaptrim_error</em></span> flag is set, indicating an earlier
      scrub operation found a problem, or that the <span class="emphasis"><em>repair</em></span>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </p></dd><dt id="id-1.4.4.2.6.6.19"><span class="term">OSD_SCRUB_ERRORS</span></dt><dd><p>
      Recent OSD scrubs have uncovered inconsistencies.
     </p></dd><dt id="id-1.4.4.2.6.6.20"><span class="term">CACHE_POOL_NEAR_FULL</span></dt><dd><p>
      A cache tier pool is nearly full. Full in this context is determined by
      the <span class="emphasis"><em>target_max_bytes</em></span> and
      <span class="emphasis"><em>target_max_objects</em></span> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">cache-pool-name</em> target_max_objects <em class="replaceable">objects</em></pre></div><p>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </p></dd><dt id="id-1.4.4.2.6.6.21"><span class="term">TOO_FEW_PGS</span></dt><dd><p>
      The number of PGs in use is below the configurable threshold of
      <code class="option">mon_pg_warn_min_per_osd</code> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </p></dd><dt id="id-1.4.4.2.6.6.22"><span class="term">TOO_MANY_PGS</span></dt><dd><p>
      The number of PGs in use is above the configurable threshold of
      <code class="option">mon_pg_warn_max_per_osd</code> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the Ceph Managers and Ceph Monitors.
     </p><p>
      While the <code class="option">pg_num</code> value for existing pools cannot be
      reduced, the <code class="option">pgp_num</code> value can. This effectively
      co-locates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <code class="option">pgp_num</code> value can
      be adjusted with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">value</em></pre></div></dd><dt id="id-1.4.4.2.6.6.23"><span class="term">SMALLER_PGP_NUM</span></dt><dd><p>
      One or more pools has a <code class="option">pgp_num</code> value less than
      <code class="option">pg_num</code>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <code class="option">pgp_num</code> to match
      <code class="option">pg_num</code>, triggering the data migration, with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">pool</em> pgp_num <em class="replaceable">pg_num_value</em></pre></div></dd><dt id="id-1.4.4.2.6.6.24"><span class="term">MANY_OBJECTS_PER_PG</span></dt><dd><p>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <code class="option">mon_pg_warn_max_object_skew</code> configuration option on the
      monitors.
     </p></dd><dt id="id-1.4.4.2.6.6.25"><span class="term">POOL_APP_NOT_ENABLED¶</span></dt><dd><p>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd pool init <em class="replaceable">pool_name</em></pre></div><p>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool application enable foo</pre></div></dd><dt id="id-1.4.4.2.6.6.26"><span class="term">POOL_FULL</span></dt><dd><p>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <code class="option">mon_pool_quota_crit_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.4.4.2.6.6.27"><span class="term">POOL_NEAR_FULL</span></dt><dd><p>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <code class="option">mon_pool_quota_warn_threshold</code> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_bytes <em class="replaceable">bytes</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd osd pool set-quota <em class="replaceable">pool</em> max_objects <em class="replaceable">objects</em></pre></div><p>
      Setting the quota value to 0 will disable the quota.
     </p></dd><dt id="id-1.4.4.2.6.6.28"><span class="term">OBJECT_MISPLACED</span></dt><dd><p>
      One or more objects in the cluster are not stored on the node where the
      cluster wants them to be. This is an indication that data migration
      caused by a recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in itself. Data consistency is never at
      risk, and old copies of objects are never removed until the desired
      number of new copies (in the desired locations) are present.
     </p></dd><dt id="id-1.4.4.2.6.6.29"><span class="term">OBJECT_UNFOUND</span></dt><dd><p>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on the OSDs that
      are currently up. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, the down OSD that has the most recent copy of the
      unfound object can be brought back up. Candidate OSDs can be identified
      from the peering state for the PG(s) responsible for the unfound object:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell <em class="replaceable">pgid</em> query</pre></div></dd><dt id="id-1.4.4.2.6.6.30"><span class="term">REQUEST_SLOW</span></dt><dd><p>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.<em class="replaceable">ID</em> -- ceph daemon osd.<em class="replaceable">ID</em> ops</pre></div><p>
      You can see a summary of the slowest recent requests:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.<em class="replaceable">ID</em> -- ceph daemon osd.<em class="replaceable">ID</em> dump_historic_ops</pre></div><p>
      You can find the location of an OSD with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd find osd.<em class="replaceable">id</em></pre></div></dd><dt id="id-1.4.4.2.6.6.31"><span class="term">REQUEST_STUCK</span></dt><dd><p>
      One or more OSD requests have been blocked for a relatively long time,
      for example 4096 seconds. This is an indication that either the cluster
      has been unhealthy for an extended period of time (for example, not
      enough running OSDs or inactive PGs) or there is some internal problem
      with the OSD.
     </p></dd><dt id="id-1.4.4.2.6.6.32"><span class="term">PG_NOT_SCRUBBED</span></dt><dd><p>
      One or more PGs have not been scrubbed (see
      <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) recently. PGs are normally scrubbed
      every <code class="option">mon_scrub_interval</code> seconds, and this warning
      triggers when <code class="option">mon_warn_not_scrubbed</code> such intervals have
      elapsed without a scrub. PGs will not scrub if they are not flagged as
      clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg scrub <em class="replaceable">pgid</em></pre></div></dd><dt id="id-1.4.4.2.6.6.33"><span class="term">PG_NOT_DEEP_SCRUBBED</span></dt><dd><p>
      One or more PGs has not been deep scrubbed (see
      <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) recently. PGs are normally scrubbed
      every <code class="option">osd_deep_mon_scrub_interval</code> seconds, and this
      warning triggers when <code class="option">mon_warn_not_deep_scrubbed</code> seconds
      have elapsed without a scrub. PGs will not (deep) scrub if they are not
      flagged as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg deep-scrub <em class="replaceable">pgid</em></pre></div></dd></dl></div><div id="id-1.4.4.2.6.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph -c <em class="replaceable">/path/to/conf</em> -k <em class="replaceable">/path/to/keyring</em> health</pre></div></div></section><section class="sect1" id="monitor-stats" data-id-title="Checking a clusters usage stats"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.3 </span><span class="title-name">Checking a cluster's usage stats</span> <a title="Permalink" class="permalink" href="#monitor-stats">#</a></h2></div></div></div><p>
   To check a cluster’s data usage and distribution among pools, use the
   <code class="command">ceph df</code> command. To get more details, use <code class="command">ceph
   df detail</code>.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph df
--- RAW STORAGE ---
CLASS  SIZE    AVAIL   USED     RAW USED  %RAW USED
hdd    30 GiB  27 GiB  121 MiB   3.1 GiB      10.40
TOTAL  30 GiB  27 GiB  121 MiB   3.1 GiB      10.40

--- POOLS ---
POOL                   ID  STORED   OBJECTS  USED     %USED  MAX AVAIL
device_health_metrics   1      0 B        0      0 B      0    8.5 GiB
cephfs.my_cephfs.meta   2  1.0 MiB       22  4.5 MiB   0.02    8.5 GiB
cephfs.my_cephfs.data   3      0 B        0      0 B      0    8.5 GiB
.rgw.root               4  1.9 KiB       13  2.2 MiB      0    8.5 GiB
myzone.rgw.log          5  3.4 KiB      207    6 MiB   0.02    8.5 GiB
myzone.rgw.control      6      0 B        8      0 B      0    8.5 GiB
myzone.rgw.meta         7      0 B        0      0 B      0    8.5 GiB</pre></div><p>
   The <code class="literal">RAW STORAGE</code> section of the output provides an
   overview of the amount of storage your cluster uses for your data.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">CLASS</code>: The storage class of the device. Refer to
     <a class="xref" href="#crush-devclasses" title="17.1.1. Device classes">Section 17.1.1, “Device classes”</a> for more details on device classes.
    </p></li><li class="listitem"><p>
     <code class="literal">SIZE</code>: The overall storage capacity of the cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">AVAIL</code>: The amount of free space available in the
     cluster.
    </p></li><li class="listitem"><p>
     <code class="literal">USED</code>: The space (accumulated over all OSDs) allocated
     purely for data objects kept at block device.
    </p></li><li class="listitem"><p>
     <code class="literal">RAW USED</code>: The sum of 'USED' space and space
     allocated/reserved at block device for Ceph purposes, for example BlueFS
     part for BlueStore.
    </p></li><li class="listitem"><p>
     <code class="literal">% RAW USED</code>: The percentage of raw storage used. Use
     this number in conjunction with the <code class="literal">full ratio</code> and
     <code class="literal">near full ratio</code> to ensure that you are not reaching
     your cluster’s capacity. See <a class="xref" href="#storage-capacity" title="12.8. Storage capacity">Section 12.8, “Storage capacity”</a> for
     additional details.
    </p><div id="id-1.4.4.2.7.5.6.2" data-id-title="Cluster fill level" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Cluster fill level</h6><p>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </p><p>
      Use the command <code class="command">ceph osd df tree</code> to list the fill
      level of all OSDs.
     </p></div></li></ul></div><p>
   The <code class="literal">POOLS</code> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <span class="emphasis"><em>does not</em></span> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <code class="literal">POOL</code>: The name of the pool.
    </p></li><li class="listitem"><p>
     <code class="literal">ID</code>: The pool ID.
    </p></li><li class="listitem"><p>
     <code class="literal">STORED</code>: The amount of data stored by the user.
    </p></li><li class="listitem"><p>
     <code class="literal">OBJECTS</code>: The notional number of objects stored per
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">USED</code>: The amount of space allocated purely for data by
     all OSD nodes in kB.
    </p></li><li class="listitem"><p>
     <code class="literal">%USED</code>: The notional percentage of storage used per
     pool.
    </p></li><li class="listitem"><p>
     <code class="literal">MAX AVAIL</code>: The maximum available space in the given
     pool.
    </p></li></ul></div><div id="id-1.4.4.2.7.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    <code class="literal">USED</code>and %<code class="literal">USED</code> amounts will not add up
    to the <code class="literal">RAW USED</code> and <code class="literal">%RAW USED</code> amounts
    in the <code class="literal">RAW STORAGE</code> section of the output.
   </p></div></section><section class="sect1" id="monitor-osdstatus" data-id-title="Checking OSD status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.4 </span><span class="title-name">Checking OSD status</span> <a title="Permalink" class="permalink" href="#monitor-osdstatus">#</a></h2></div></div></div><p>
   You can check OSDs to ensure they are up and on by executing:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd dump</pre></div><p>
   You can also view OSDs according to their position in the CRUSH map.
  </p><p>
   <code class="command">ceph osd tree</code> will print a CRUSH tree with a host, its
   OSDs, whether they are up, and their weight:
  </p><div class="verbatim-wrap"><pre class="screen">   <code class="prompt user">cephuser@adm &gt; </code>ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME              STATUS  REWEIGHT  PRI-AFF
-1      3  0.02939  root default
-3      3  0.00980    rack mainrack
-2      3  0.00980            host osd-host
0       1  0.00980                    osd.0   up   1.00000   1.00000
1       1  0.00980                    osd.1   up   1.00000   1.00000
2       1  0.00980                    osd.2   up   1.00000   1.00000</pre></div></section><section class="sect1" id="storage-bp-monitoring-fullosd" data-id-title="Checking for full OSDs"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.5 </span><span class="title-name">Checking for full OSDs</span> <a title="Permalink" class="permalink" href="#storage-bp-monitoring-fullosd">#</a></h2></div></div></div><p>
   Ceph prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <code class="command">mon osd full ratio</code>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <code class="command">mon osd nearfull ratio</code> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </p><p>
   Full OSD nodes will be reported by <code class="command">ceph health</code>:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</pre></div><p>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </p><div id="id-1.4.4.2.9.8" data-id-title="Preventing full OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Preventing full OSDs</h6><p>
    After an OSD becomes full—it uses 100% of its disk space—it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Each OSD's disk space (usually mounted under
      <code class="filename">/var/lib/ceph/osd/osd-{1,2..}</code>) needs to be placed on
      a dedicated underlying disk or partition.
     </p></li><li class="listitem"><p>
      Check the Ceph configuration files and make sure that Ceph does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </p></li><li class="listitem"><p>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </p></li></ul></div></div></section><section class="sect1" id="monitor-monstatus" data-id-title="Checking the monitor status"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.6 </span><span class="title-name">Checking the monitor status</span> <a title="Permalink" class="permalink" href="#monitor-monstatus">#</a></h2></div></div></div><p>
   After you start the cluster and before first reading and/or writing data,
   check the Ceph Monitors' quorum status. When the cluster is already serving
   requests, check the Ceph Monitors' status periodically to ensure that they are
   running.
  </p><p>
   To display the monitor map, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mon stat</pre></div><p>
   or
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mon dump</pre></div><p>
   To check the quorum status for the monitor cluster, execute the following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph quorum_status</pre></div><p>
   Ceph will return the quorum status. For example, a Ceph cluster
   consisting of three monitors may return the following:
  </p><div class="verbatim-wrap"><pre class="screen">{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</pre></div></section><section class="sect1" id="monitor-pgroupstatus" data-id-title="Checking placement group states"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.7 </span><span class="title-name">Checking placement group states</span> <a title="Permalink" class="permalink" href="#monitor-pgroupstatus">#</a></h2></div></div></div><p>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <code class="literal">active</code> and
   <code class="literal">clean</code>. For a detailed discussion, refer to
   <a class="xref" href="#op-mon-osd-pg" title="12.9. Monitoring OSDs and placement groups">Section 12.9, “Monitoring OSDs and placement groups”</a>.
  </p></section><section class="sect1" id="storage-capacity" data-id-title="Storage capacity"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.8 </span><span class="title-name">Storage capacity</span> <a title="Permalink" class="permalink" href="#storage-capacity">#</a></h2></div></div></div><p>
   When a Ceph storage cluster gets close to its maximum capacity, Ceph
   prevents you from writing to or reading from Ceph OSDs as a safety measure to
   prevent data loss. Therefore, letting a production cluster approach its full
   ratio is not a good practice, because it sacrifices high availability. The
   default full ratio is set to .95, meaning 95% of capacity. This a very
   aggressive setting for a test cluster with a small number of OSDs.
  </p><div id="id-1.4.4.2.12.3" data-id-title="Increase Storage Capacity" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increase Storage Capacity</h6><p>
    When monitoring your cluster, be alert to warnings related to the
    <code class="literal">nearfull</code> ratio. It means that a failure of some OSDs
    could result in a temporary service disruption if one or more OSDs fails.
    Consider adding more OSDs to increase storage capacity.
   </p></div><p>
   A common scenario for test clusters involves a system administrator removing
   a Ceph OSD from the Ceph storage cluster to watch the cluster rebalance. Then
   removing another Ceph OSD, and so on until the cluster eventually reaches the
   full ratio and locks up. We recommend a bit of capacity planning even with a
   test cluster. Planning enables you to estimate how much spare capacity you
   will need in order to maintain high availability. Ideally, you want to plan
   for a series of Ceph OSD failures where the cluster can recover to an
   <code class="literal">active + clean</code> state without replacing those Ceph OSDs
   immediately. You can run a cluster in an <code class="literal">active +
   degraded</code> state, but this is not ideal for normal operating
   conditions.
  </p><p>
   The following diagram depicts a simplistic Ceph storage cluster containing
   33 Ceph nodes with one Ceph OSD per host, each of them reading from and
   writing to a 3 TB drive. This exemplary cluster has a maximum actual
   capacity of 99 TB. The <code class="option">mon osd full ratio</code> option is set to
   0.95. If the cluster falls to 5 TB of the remaining capacity, it will not
   allow the clients to read and write data. Therefore the storage cluster’s
   operating capacity is 95 TB, not 99 TB.
  </p><div class="figure" id="id-1.4.4.2.12.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_cluster.png"><img src="images/ceph_cluster.png" width="85%" alt="Ceph cluster" title="Ceph cluster"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.1: </span><span class="title-name">Ceph cluster </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.12.6">#</a></h6></div></div><p>
   It is normal in such a cluster for one or two OSDs to fail. A less frequent
   but reasonable scenario involves a rack’s router or power supply failing,
   which brings down multiple OSDs simultaneously (for example, OSDs 7-12). In
   such a scenario, you should still strive for a cluster that can remain
   operational and achieve an <code class="literal">active + clean</code>
   state—even if that means adding a few hosts with additional OSDs in
   short order. If your capacity usage is too high, you may not lose data. But
   you could still sacrifice data availability while resolving an outage within
   a failure domain if capacity usage of the cluster exceeds the full ratio.
   For this reason, we recommend at least some rough capacity planning.
  </p><p>
   Identify two numbers for your cluster:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     The number of OSDs.
    </p></li><li class="listitem"><p>
     The total capacity of the cluster.
    </p></li></ol></div><p>
   If you divide the total capacity of your cluster by the number of OSDs in
   your cluster, you will find the mean average capacity of an OSD within your
   cluster. Consider multiplying that number by the number of OSDs you expect
   will fail simultaneously during normal operations (a relatively small
   number). Finally, multiply the capacity of the cluster by the full ratio to
   arrive at a maximum operating capacity. Then, subtract the number of the
   amount of data from the OSDs you expect to fail to arrive at a reasonable
   full ratio. Repeat the foregoing process with a higher number of OSD
   failures (a rack of OSDs) to arrive at a reasonable number for a near full
   ratio.
  </p><p>
   The following settings only apply on cluster creation and are then stored in
   the OSD map:
  </p><div class="verbatim-wrap"><pre class="screen">[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70</pre></div><div id="id-1.4.4.2.12.13" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    These settings only apply during cluster creation. Afterward they need to
    be changed in the OSD Map using the <code class="command">ceph osd
    set-nearfull-ratio</code> and <code class="command">ceph osd set-full-ratio</code>
    commands.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.12.14.1"><span class="term">mon osd full ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered
      <code class="literal">full</code>. Default is .95
     </p></dd><dt id="id-1.4.4.2.12.14.2"><span class="term">mon osd backfillfull ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered too
      <code class="literal">full</code> to backfill. Default is .90
     </p></dd><dt id="id-1.4.4.2.12.14.3"><span class="term">mon osd nearfull ratio</span></dt><dd><p>
      The percentage of disk space used before an OSD is considered
      <code class="literal">nearfull</code>. Default is .85
     </p></dd></dl></div><div id="id-1.4.4.2.12.15" data-id-title="Check OSD weight" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Check OSD weight</h6><p>
    If some OSDs are <code class="literal">nearfull</code>, but others have plenty of
    capacity, you may have a problem with the CRUSH weight for the
    <code class="literal">nearfull</code> OSDs.
   </p></div></section><section class="sect1" id="op-mon-osd-pg" data-id-title="Monitoring OSDs and placement groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.9 </span><span class="title-name">Monitoring OSDs and placement groups</span> <a title="Permalink" class="permalink" href="#op-mon-osd-pg">#</a></h2></div></div></div><p>
   High availability and high reliability require a fault-tolerant approach to
   managing hardware and software issues. Ceph has no single
   point-of-failure, and can service requests for data in a 'degraded' mode.
   Ceph’s data placement introduces a layer of indirection to ensure that
   data does not bind directly to particular OSD addresses. This means that
   tracking down system faults requires finding the placement group and the
   underlying OSDs at root of the problem.
  </p><div id="id-1.4.4.2.13.3" data-id-title="Access in case of failure" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Access in case of failure</h6><p>
    A fault in one part of the cluster may prevent you from accessing a
    particular object. That does not mean that you cannot access other objects.
    When you run into a fault, follow the steps for monitoring your OSDs and
    placement groups. Then begin troubleshooting.
   </p></div><p>
   Ceph is generally self-repairing. However, when problems persist,
   monitoring OSDs and placement groups will help you identify the problem.
  </p><section class="sect2" id="op-mon-osds" data-id-title="Monitoring OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.9.1 </span><span class="title-name">Monitoring OSDs</span> <a title="Permalink" class="permalink" href="#op-mon-osds">#</a></h3></div></div></div><p>
    An OSD’s status is either <span class="emphasis"><em>in the cluster</em></span> ('in') or
    <span class="emphasis"><em>out of the cluster</em></span> ('out'). At the same time, it is
    either <span class="emphasis"><em>up and running</em></span> ('up') or it is <span class="emphasis"><em>down
    and not running</em></span> ('down'). If an OSD is 'up', it may be either in
    the cluster (you can read and write data) or out of the cluster. If it was
    in the cluster and recently moved out of the cluster, Ceph will migrate
    placement groups to other OSDs. If an OSD is out of the cluster, CRUSH will
    not assign placement groups to it. If an OSD is 'down', it should also be
    'out'.
   </p><div id="id-1.4.4.2.13.5.3" data-id-title="Unhealthy state" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Unhealthy state</h6><p>
     If an OSD is 'down' and 'in', there is a problem and the cluster will not
     be in a healthy state.
    </p></div><p>
    If you execute a command such as <code class="command">ceph health</code>,
    <code class="command">ceph -s</code> or <code class="command">ceph -w</code>, you may notice
    that the cluster does not always echo back <code class="literal">HEALTH OK</code>.
    With regard to OSDs, you should expect that the cluster will
    <span class="emphasis"><em>not</em></span> echo <code class="literal">HEALTH OK</code> under the
    following circumstances:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You have not started the cluster yet (it will not respond).
     </p></li><li class="listitem"><p>
      You have started or restarted the cluster and it is not ready yet,
      because the placement groups are being created and the OSDs are in the
      process of peering.
     </p></li><li class="listitem"><p>
      You have added or removed an OSD.
     </p></li><li class="listitem"><p>
      You have modified your cluster map.
     </p></li></ul></div><p>
    An important aspect of monitoring OSDs is to ensure that when the cluster
    is up and running, all the OSDs in the cluster are up and running, too. To
    see if all the OSDs are running, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd stat
x osds: y up, z in; epoch: eNNNN</pre></div><p>
    The result should tell you the total number of OSDs (x), how many are 'up'
    (y), how many are 'in' (z), and the map epoch (eNNNN). If the number of
    OSDs that are 'in' the cluster is more than the number of OSDs that are
    'up', execute the following command to identify the
    <code class="literal">ceph-osd</code> daemons that are not running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000</pre></div><p>
    For example, if an OSD with ID 1 is down, start it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@osd &gt; </code>sudo systemctl start ceph-<em class="replaceable">CLUSTER_ID</em>@osd.0.service</pre></div><p>
    See <span class="intraxref">Book “Troubleshooting Guide”, Chapter 4 “Troubleshooting OSDs”, Section 4.3 “OSDs not running”</span> for problems
    associated with OSDs that have stopped or that will not restart.
   </p></section><section class="sect2" id="op-pgsets" data-id-title="Assigning placement group sets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.9.2 </span><span class="title-name">Assigning placement group sets</span> <a title="Permalink" class="permalink" href="#op-pgsets">#</a></h3></div></div></div><p>
    When CRUSH assigns placement groups to OSDs, it looks at the number of
    replicas for the pool and assigns the placement group to OSDs such that
    each replica of the placement group gets assigned to a different OSD. For
    example, if the pool requires three replicas of a placement group, CRUSH
    may assign them to <code class="literal">osd.1</code>, <code class="literal">osd.2</code> and
    <code class="literal">osd.3</code> respectively. CRUSH actually seeks a pseudo-random
    placement that will take into account failure domains you set in your
    CRUSH Map, so you will rarely see placement groups assigned to nearest
    neighbor OSDs in a large cluster. We refer to the set of OSDs that should
    contain the replicas of a particular placement group as the <span class="emphasis"><em>acting set</em></span>. In
    some cases, an OSD in the acting set is down or otherwise not able to
    service requests for objects in the placement group. When these situations
    arise, it may match one of the following scenarios:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You added or removed an OSD. Then, CRUSH reassigned the placement group
      to other OSDs and therefore changed the composition of the <span class="emphasis"><em>acting set</em></span>,
      causing the migration of data with a 'backfill' process.
     </p></li><li class="listitem"><p>
      An OSD was 'down', was restarted, and is now recovering.
     </p></li><li class="listitem"><p>
      An OSD in the <span class="emphasis"><em>acting set</em></span> is 'down' or unable to service requests, and
      another OSD has temporarily assumed its duties.
     </p><p>
      Ceph processes a client request using the <span class="emphasis"><em>up set</em></span>, which is the set of
      OSDs that will actually handle the requests. In most cases, the <span class="emphasis"><em>up set</em></span>
      and the <span class="emphasis"><em>acting set</em></span> are virtually identical. When they are not, it may
      indicate that Ceph is migrating data, an OSD is recovering, or that
      there is a problem (for example, Ceph usually echoes a <code class="literal">HEALTH
      WARN</code> state with a 'stuck stale' message in such scenarios).
     </p></li></ul></div><p>
    To retrieve a list of placement groups, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg dump</pre></div><p>
    To view which OSDs are within the <span class="emphasis"><em>acting set</em></span> or the <span class="emphasis"><em>up set</em></span> for a given
    placement group, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg map <em class="replaceable">PG_NUM</em>
osdmap eNNN pg <em class="replaceable">RAW_PG_NUM</em> (<em class="replaceable">PG_NUM</em>) -&gt; up [0,1,2] acting [0,1,2]</pre></div><p>
    The result should tell you the osdmap epoch (eNNN), the placement group
    number (<em class="replaceable">PG_NUM</em>), the OSDs in the <span class="emphasis"><em>up set</em></span> ('up'),
    and the OSDs in the <span class="emphasis"><em>acting set</em></span> ('acting'):
   </p><div id="id-1.4.4.2.13.6.9" data-id-title="Cluster problem indicator" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Cluster problem indicator</h6><p>
     If the <span class="emphasis"><em>up set</em></span> and <span class="emphasis"><em>acting set</em></span> do not match, this may be an indicator
     either of the cluster rebalancing itself, or of a potential problem with
     the cluster.
    </p></div></section><section class="sect2" id="op-peering" data-id-title="Peering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.9.3 </span><span class="title-name">Peering</span> <a title="Permalink" class="permalink" href="#op-peering">#</a></h3></div></div></div><p>
    Before you can write data to a placement group, it must be in an
    <code class="literal">active</code> state, and it should be in a
    <code class="literal">clean</code> state. For Ceph to determine the current state
    of a placement group, the primary OSD of the placement group (the first OSD
    in the <span class="emphasis"><em>acting set</em></span>), peers with the secondary and tertiary OSDs to
    establish agreement on the current state of the placement group (assuming a
    pool with three replicas of the PG).
   </p><div class="figure" id="id-1.4.4.2.13.7.3"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_peering.png"><img src="images/ceph_peering.png" width="70%" alt="Peering schema" title="Peering schema"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.2: </span><span class="title-name">Peering schema </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.13.7.3">#</a></h6></div></div></section><section class="sect2" id="op-mon-pg-states" data-id-title="Monitoring placement group states"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.9.4 </span><span class="title-name">Monitoring placement group states</span> <a title="Permalink" class="permalink" href="#op-mon-pg-states">#</a></h3></div></div></div><p>
    If you execute a command such as <code class="command">ceph health</code>,
    <code class="command">ceph -s</code> or <code class="command">ceph -w</code>, you may notice
    that the cluster does not always echo back the <code class="literal">HEALTH OK</code>
    message. After you check to see if the OSDs are running, you should also
    check placement group states.
   </p><p>
    Expect that the cluster will <span class="bold"><strong>not</strong></span> echo
    <code class="literal">HEALTH OK</code> in a number of placement group peering-related
    circumstances:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You have created a pool and placement groups have not peered yet.
     </p></li><li class="listitem"><p>
      The placement groups are recovering.
     </p></li><li class="listitem"><p>
      You have added an OSD to or removed an OSD from the cluster.
     </p></li><li class="listitem"><p>
      You have modified your CRUSH Map and your placement groups are
      migrating.
     </p></li><li class="listitem"><p>
      There is inconsistent data in different replicas of a placement group.
     </p></li><li class="listitem"><p>
      Ceph is scrubbing a placement group’s replicas.
     </p></li><li class="listitem"><p>
      Ceph does not have enough storage capacity to complete backfilling
      operations.
     </p></li></ul></div><p>
    If one of the above mentioned circumstances causes Ceph to echo
    <code class="literal">HEALTH WARN</code>, do not panic. In many cases, the cluster
    will recover on its own. In some cases, you may need to take action. An
    important aspect of monitoring placement groups is to ensure that when the
    cluster is up and running, all placement groups are 'active' and preferably
    in the 'clean state'. To see the status of all placement groups, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail</pre></div><p>
    The result should tell you the total number of placement groups (x), how
    many placement groups are in a particular state such as 'active+clean' (y)
    and the amount of data stored (z).
   </p><p>
    In addition to the placement group states, Ceph will also echo back the
    amount of storage capacity used (aa), the amount of storage capacity
    remaining (bb), and the total storage capacity for the placement group.
    These numbers can be important in a few cases:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You are reaching your <code class="option">near full ratio</code> or <code class="option">full
      ratio</code>.
     </p></li><li class="listitem"><p>
      Your data is not getting distributed across the cluster because of an
      error in your CRUSH configuration.
     </p></li></ul></div><div id="id-1.4.4.2.13.8.10" data-id-title="Placement group IDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Placement group IDs</h6><p>
     Placement group IDs consist of the pool number (not pool name) followed by
     a period (.) and the placement group ID—a hexadecimal number. You
     can view pool numbers and their names from the output of <code class="command">ceph osd
     lspools</code>. For example, the default pool <code class="literal">rbd</code>
     corresponds to pool number 0. A fully qualified placement group ID has the
     following form:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">POOL_NUM</em>.<em class="replaceable">PG_ID</em></pre></div><p>
     And it typically looks like this:
    </p><div class="verbatim-wrap"><pre class="screen">0.1f</pre></div></div><p>
    To retrieve a list of placement groups, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg dump</pre></div><p>
    You can also format the output in JSON format and save it to a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg dump -o <em class="replaceable">FILE_NAME</em> --format=json</pre></div><p>
    To query a particular placement group, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph pg <em class="replaceable">POOL_NUM</em>.<em class="replaceable">PG_ID</em> query</pre></div><p>
    The following list describes the common placement group states in detail.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.2.13.8.18.1"><span class="term">CREATING</span></dt><dd><p>
       When you create a pool, it will create the number of placement groups
       you specified. Ceph will echo 'creating' when it is creating one or
       more placement groups. When they are created, the OSDs that are part of
       the placement group’s <span class="emphasis"><em>acting set</em></span> will peer. When peering is complete,
       the placement group status should be 'active+clean', which means that a
       Ceph client can begin writing to the placement group.
      </p><div class="figure" id="id-1.4.4.2.13.8.18.1.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pg_creating.png"><img src="images/ceph_pg_creating.png" width="80%" alt="Placement groups status" title="Placement groups status"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 12.3: </span><span class="title-name">Placement groups status </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.13.8.18.1.2.2">#</a></h6></div></div></dd><dt id="id-1.4.4.2.13.8.18.2"><span class="term">PEERING</span></dt><dd><p>
       When Ceph is peering a placement group, it is bringing the OSDs that
       store the replicas of the placement group into agreement about the state
       of the objects and metadata in the placement group. When Ceph
       completes peering, this means that the OSDs that store the placement
       group agree about the current state of the placement group. However,
       completion of the peering process does
       <span class="bold"><strong>not</strong></span> mean that each replica has the
       latest contents.
      </p><div id="id-1.4.4.2.13.8.18.2.2.2" data-id-title="Authoritative history" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Authoritative history</h6><p>
        Ceph will <span class="bold"><strong>not</strong></span> acknowledge a write
        operation to a client until all OSDs of the <span class="emphasis"><em>acting set</em></span> persist the
        write operation. This practice ensures that at least one member of the
        <span class="emphasis"><em>acting set</em></span> will have a record of every acknowledged write operation
        since the last successful peering operation.
       </p><p>
        With an accurate record of each acknowledged write operation, Ceph
        can construct and enlarge a new authoritative history of the placement
        group—a complete and fully ordered set of operations that, if
        performed, would bring an OSD’s copy of a placement group up to date.
       </p></div></dd><dt id="id-1.4.4.2.13.8.18.3"><span class="term">ACTIVE</span></dt><dd><p>
       When Ceph completes the peering process, a placement group may become
       <code class="literal">active</code>. The <code class="literal">active</code> state means
       that the data in the placement group is generally available in the
       primary placement group and the replicas for read and write operations.
      </p></dd><dt id="id-1.4.4.2.13.8.18.4"><span class="term">CLEAN</span></dt><dd><p>
       When a placement group is in the <code class="literal">clean</code> state, the
       primary OSD and the replica OSDs have successfully peered and there are
       no stray replicas for the placement group. Ceph replicated all objects
       in the placement group the correct number of times.
      </p></dd><dt id="id-1.4.4.2.13.8.18.5"><span class="term">DEGRADED</span></dt><dd><p>
       When a client writes an object to the primary OSD, the primary OSD is
       responsible for writing the replicas to the replica OSDs. After the
       primary OSD writes the object to storage, the placement group will
       remain in a 'degraded' state until the primary OSD has received an
       acknowledgement from the replica OSDs that Ceph created the replica
       objects successfully.
      </p><p>
       The reason a placement group can be 'active+degraded' is that an OSD may
       be 'active' even though it does not hold all of the objects yet. If an
       OSD goes down, Ceph marks each placement group assigned to the OSD as
       'degraded'. The OSDs must peer again when the OSD comes back up.
       However, a client can still write a new object to a degraded placement
       group if it is 'active'.
      </p><p>
       If an OSD is 'down' and the 'degraded' condition persists, Ceph may
       mark the down OSD as 'out' of the cluster and remap the data from the
       'down' OSD to another OSD. The time between being marked 'down' and
       being marked 'out' is controlled by the <code class="option">mon osd down out
       interval</code> option, which is set to 600 seconds by default.
      </p><p>
       A placement group can also be 'degraded' because Ceph cannot find one
       or more objects that should be in the placement group. While you cannot
       read or write to unfound objects, you can still access all of the other
       objects in the 'degraded' placement group.
      </p></dd><dt id="id-1.4.4.2.13.8.18.6"><span class="term">RECOVERING</span></dt><dd><p>
       Ceph was designed for fault-tolerance at a scale where hardware and
       software problems are ongoing. When an OSD goes 'down', its contents may
       fall behind the current state of other replicas in the placement groups.
       When the OSD is back 'up', the contents of the placement groups must be
       updated to reflect the current state. During that time period, the OSD
       may reflect a 'recovering' state.
      </p><p>
       Recovery is not always trivial, because a hardware failure may cause a
       cascading failure of multiple OSDs. For example, a network switch for a
       rack or cabinet may fail, which can cause the OSDs of a number of host
       machines to fall behind the current state of the cluster. Each of the
       OSDs must recover when the fault is resolved.
      </p><p>
       Ceph provides a number of settings to balance the resource contention
       between new service requests and the need to recover data objects and
       restore the placement groups to the current state. The <code class="option">osd
       recovery delay start</code> setting allows an OSD to restart, re-peer
       and even process some replay requests before starting the recovery
       process. The <code class="option">osd recovery thread timeout</code> sets a thread
       timeout, because multiple OSDs may fail, restart and re-peer at
       staggered rates. The <code class="option">osd recovery max active</code> setting
       limits the number of recovery requests an OSD will process
       simultaneously to prevent the OSD from failing to serve. The <code class="option">osd
       recovery max chunk</code> setting limits the size of the recovered
       data chunks to prevent network congestion.
      </p></dd><dt id="id-1.4.4.2.13.8.18.7"><span class="term">BACK FILLING</span></dt><dd><p>
       When a new OSD joins the cluster, CRUSH will reassign placement groups
       from OSDs in the cluster to the newly added OSD. Forcing the new OSD to
       accept the reassigned placement groups immediately can put excessive
       load on the new OSD. Backfilling the OSD with the placement groups
       allows this process to begin in the background. When backfilling is
       complete, the new OSD will begin serving requests when it is ready.
      </p><p>
       During the backfill operations, you may see one of several states:
       'backfill_wait' indicates that a backfill operation is pending, but is
       not yet in progress; 'backfill' indicates that a backfill operation is
       in progress; 'backfill_too_full' indicates that a backfill operation was
       requested, but could not be completed because of insufficient storage
       capacity. When a placement group cannot be backfilled, it may be
       considered 'incomplete'.
      </p><p>
       Ceph provides a number of settings to manage the load associated with
       reassigning placement groups to an OSD (especially a new OSD). By
       default, <code class="option">osd max backfills</code> sets the maximum number of
       concurrent backfills to or from an OSD to 10. The <code class="option">backfill full
       ratio</code> enables an OSD to refuse a backfill request if the OSD is
       approaching its full ratio (90%, by default) and change with
       <code class="command">ceph osd set-backfillfull-ratio</code> command. If an OSD
       refuses a backfill request, the <code class="option">osd backfill retry
       interval</code> enables an OSD to retry the request (after 10 seconds,
       by default). OSDs can also set <code class="option">osd backfill scan min</code>
       and <code class="option">osd backfill scan max</code> to manage scan intervals (64
       and 512, by default).
      </p></dd><dt id="id-1.4.4.2.13.8.18.8"><span class="term">REMAPPED</span></dt><dd><p>
       When the <span class="emphasis"><em>acting set</em></span> that services a placement group changes, the data
       migrates from the old <span class="emphasis"><em>acting set</em></span> to the new <span class="emphasis"><em>acting set</em></span>. It may take
       some time for a new primary OSD to service requests. So it may ask the
       old primary to continue to service requests until the placement group
       migration is complete. When data migration completes, the mapping uses
       the primary OSD of the new <span class="emphasis"><em>acting set</em></span>.
      </p></dd><dt id="id-1.4.4.2.13.8.18.9"><span class="term">STALE</span></dt><dd><p>
       While Ceph uses heartbeats to ensure that hosts and daemons are
       running, the <code class="literal">ceph-osd</code> daemons may also get into a
       'stuck' state where they are not reporting statistics in a timely manner
       (for example, a temporary network fault). By default, OSD daemons report
       their placement group, boot and failure statistics every half second
       (0.5), which is more frequent than the heartbeat thresholds. If the
       primary OSD of a placement group’s <span class="emphasis"><em>acting set</em></span> fails to report to the
       monitor or if other OSDs have reported the primary OSD as 'down', the
       monitors will mark the placement group as 'stale'.
      </p><p>
       When you start your cluster, it is common to see the 'stale' state until
       the peering process completes. After your cluster has been running for a
       while, seeing placement groups in the 'stale' state indicates that the
       primary OSD for those placement groups is down or not reporting
       placement group statistics to the monitor.
      </p></dd></dl></div></section><section class="sect2" id="op-pg-objectfinding" data-id-title="Finding an object location"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.9.5 </span><span class="title-name">Finding an object location</span> <a title="Permalink" class="permalink" href="#op-pg-objectfinding">#</a></h3></div></div></div><p>
    To store object data in the Ceph Object Store, a Ceph client needs to
    set an object name and specify a related pool. The Ceph client retrieves
    the latest cluster map and the CRUSH algorithm calculates how to map the
    object to a placement group, and then calculates how to assign the
    placement group to an OSD dynamically. To find the object location, all you
    need is the object name and the pool name. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd map <em class="replaceable">POOL_NAME</em> <em class="replaceable">OBJECT_NAME</em> [<em class="replaceable">NAMESPACE</em>]</pre></div><div class="complex-example"><div class="example" id="id-1.4.4.2.13.9.4" data-id-title="Locating an object"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 12.1: </span><span class="title-name">Locating an object </span><a title="Permalink" class="permalink" href="#id-1.4.4.2.13.9.4">#</a></h6></div><div class="example-contents"><p>
     As an example, let us create an object. Specify an object name
     'test-object-1', a path to an example file 'testfile.txt' containing some
     object data, and a pool name 'data' using the <code class="command">rados put</code>
     command on the command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados put test-object-1 testfile.txt --pool=data</pre></div><p>
     To verify that the Ceph Object Store stored the object, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados -p data ls</pre></div><p>
     Now, identify the object location. Ceph will output the object’s
     location:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)</pre></div><p>
     To remove the example object, simply delete it using the <code class="command">rados
     rm</code> command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados rm test-object-1 --pool=data</pre></div></div></div></div></section></section></section><section class="chapter" id="storage-salt-cluster" data-id-title="Operational tasks"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">13 </span><span class="title-name">Operational tasks</span> <a title="Permalink" class="permalink" href="#storage-salt-cluster">#</a></h1></div></div></div><section class="sect1" id="modifying-cluster-configuration" data-id-title="Modifying the cluster configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">Modifying the cluster configuration</span> <a title="Permalink" class="permalink" href="#modifying-cluster-configuration">#</a></h2></div></div></div><p>
   To modify the configuration of an existing Ceph cluster, follow these
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Export the current configuration of the cluster to a file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ls --export --format yaml &gt; cluster.yaml</pre></div></li><li class="step"><p>
     Edit the file with the configuration and update the relevant lines. Find
     specification examples in <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”</span> and
     <a class="xref" href="#drive-groups" title="13.4.3. Adding OSDs using DriveGroups specification">Section 13.4.3, “Adding OSDs using DriveGroups specification”</a>.
    </p></li><li class="step"><p>
     Apply the new configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i cluster.yaml</pre></div></li></ol></div></div></section><section class="sect1" id="adding-node" data-id-title="Adding nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.2 </span><span class="title-name">Adding nodes</span> <a title="Permalink" class="permalink" href="#adding-node">#</a></h2></div></div></div><p>
   To add a new node to a Ceph cluster, follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install SUSE Linux Enterprise Server and SUSE Enterprise Storage on the new host. Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Installing and configuring SUSE Linux Enterprise Server”</span> for more information.
    </p></li><li class="step"><p>
     Configure the host as a Salt Minion of an already existing Salt Master. Refer
     to <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Deploying Salt”</span> for more information.
    </p></li><li class="step"><p>
     Add the new host to <code class="systemitem">ceph-salt</code> and make cephadm aware of it, for
     example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com</pre></div><p>
     Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.2 “Adding Salt Minions”</span> for more
     information.
    </p></li><li class="step"><p>
     Verify that the node was added to <code class="systemitem">ceph-salt</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]</pre></div></li><li class="step"><p>
     Apply the configuration to the new cluster host:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply ses-min5.example.com</pre></div></li><li class="step"><p>
     Verify that the newly added host now belongs to the cephadm environment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com</pre></div></li></ol></div></div></section><section class="sect1" id="salt-node-removing" data-id-title="Removing nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.3 </span><span class="title-name">Removing nodes</span> <a title="Permalink" class="permalink" href="#salt-node-removing">#</a></h2></div></div></div><div id="id-1.4.4.3.5.2" data-id-title="Remove OSDs" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Remove OSDs</h6><p>
    If the node that you are going to remove runs OSDs, remove the OSDs from it
    first and check that no OSDs are running on that node. Refer to
    <a class="xref" href="#removing-node-osds" title="13.4.4. Removing OSDs">Section 13.4.4, “Removing OSDs”</a> for more details on removing
    OSDs.
   </p></div><p>
   To remove a node from a cluster, do the following:
  </p><div class="procedure" id="removing-node"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     For all Ceph service types except for <code class="literal">node-exporter</code>
     and <code class="literal">crash</code>, remove the node's host name from the cluster
     placement specification file (for example,
     <code class="filename">cluster.yml</code>). Refer to
     <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.2 “Service and placement specification”</span> for more details.
     For example, if you are removing the host named
     <code class="literal">ses-min2</code>, remove all occurrences of <code class="literal">-
     ses-min2</code> from all <code class="literal">placement:</code> sections:
    </p><p>
     Update
    </p><div class="verbatim-wrap"><pre class="screen">service_type: rgw
service_id: <em class="replaceable">EXAMPLE_NFS</em>
placement:
  hosts:
  - ses-min2
  - ses-min3</pre></div><p>
     to
    </p><div class="verbatim-wrap"><pre class="screen">service_type: rgw
service_id: <em class="replaceable">EXAMPLE_NFS</em>
placement:
  hosts:
  - ses-min3</pre></div><p>
     Apply your changes to the configuration file:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">rgw-example.yaml</em></pre></div></li><li class="step"><p>
     Remove the node from cephadm's environment:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch host rm ses-min2</pre></div></li><li class="step"><p>
     If the node is running <code class="literal">crash.osd.1</code> and
     <code class="literal">crash.osd.2</code> services, remove them by running the
     following command on the host:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>cephadm rm-daemon --fsid <em class="replaceable">CLUSTER_ID</em> --name <em class="replaceable">SERVICE_NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<code class="prompt user">root@minion &gt; </code>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2</pre></div></li><li class="step"><p>
     Remove all the roles from the minion you want to delete:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</pre></div><p>
     If the minion you want to remove is the bootstrap minion, you also need to
     remove the bootstrap role:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/roles/bootstrap reset</pre></div></li><li class="step"><p>
     After removing all OSDs on a single host, remove the host from the CRUSH
     map:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove <em class="replaceable">bucket-name</em></pre></div><div id="id-1.4.4.3.5.4.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The bucket name should be the same as the host name.
     </p></div></li><li class="step"><p>
     You can now remove the minion from the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /ceph_cluster/minions remove ses-min2</pre></div></li></ol></div></div><div id="id-1.4.4.3.5.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    In the event of a failure and the minion you are trying to remove is in a
    permanently powered-off state, you will need to remove the node from the
    Salt Master:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -d <em class="replaceable">minion_id</em></pre></div><p>
    Then, manually remove the node from
    <code class="filename"><em class="replaceable">pillar_root</em>/ceph-salt.sls</code>.
    This is typically located in
    <code class="filename">/srv/pillar/ceph-salt.sls</code>.
   </p></div></section><section class="sect1" id="osd-management" data-id-title="OSD management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.4 </span><span class="title-name">OSD management</span> <a title="Permalink" class="permalink" href="#osd-management">#</a></h2></div></div></div><p>
   This section describes how to add, erase, or remove OSDs in a Ceph
   cluster.
  </p><section class="sect2" id="osd-management-listing" data-id-title="Listing disk devices"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.1 </span><span class="title-name">Listing disk devices</span> <a title="Permalink" class="permalink" href="#osd-management-listing">#</a></h3></div></div></div><p>
    To identify used and unused disk devices on all cluster nodes, list them by
    running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True</pre></div></section><section class="sect2" id="osd-management-erasing" data-id-title="Erasing disk devices"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.2 </span><span class="title-name">Erasing disk devices</span> <a title="Permalink" class="permalink" href="#osd-management-erasing">#</a></h3></div></div></div><p>
    To re-use a disk device, you need to erase (or <span class="emphasis"><em>zap</em></span>) it
    first:
   </p><div class="verbatim-wrap"><pre class="screen">ceph orch device zap <em class="replaceable">HOST_NAME</em> <em class="replaceable">DISK_DEVICE</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch device zap ses-min2 /dev/vdc</pre></div><div id="id-1.4.4.3.6.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If you previously deployed OSDs by using DriveGroups or the
     <code class="option">--all-available-devices</code> option while the
     <code class="literal">unmanaged</code> flag was not set, cephadm will deploy these
     OSDs automatically after you erase them.
    </p></div></section><section class="sect2" id="drive-groups" data-id-title="Adding OSDs using DriveGroups specification"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.3 </span><span class="title-name">Adding OSDs using DriveGroups specification</span> <a title="Permalink" class="permalink" href="#drive-groups">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>DriveGroups</em></span> specify the layouts of OSDs in the Ceph
    cluster. They are defined in a single YAML file. In this section, we will
    use <code class="filename">drive_groups.yml</code> as an example.
   </p><p>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on a mixture of HDDs and SDDs)
    or share identical deployment options (for example, the same object store,
    same encryption option, stand-alone OSDs). To avoid explicitly listing
    devices, DriveGroups use a list of filter items that correspond to a few
    selected fields of <code class="command">ceph-volume</code>'s inventory reports.
    cephadm will provide code that translates these DriveGroups into actual
    device lists for inspection by the user.
   </p><p>
    The command to apply the OSD specification to the cluster is:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply osd -i <code class="filename">drive_groups.yml</code></pre></div><p>
    To see a preview of actions and test your application, you can use the
    <code class="option">--dry-run</code> option together with the <code class="command">ceph orch
    apply osd</code> command. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply osd -i <code class="filename">drive_groups.yml</code> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</pre></div><p>
    If the <code class="option">--dry-run</code> output matches your expectations, then
    simply re-run the command without the <code class="option">--dry-run</code> option.
   </p><section class="sect3" id="unmanaged-osds" data-id-title="Unmanaged OSDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.3.1 </span><span class="title-name">Unmanaged OSDs</span> <a title="Permalink" class="permalink" href="#unmanaged-osds">#</a></h4></div></div></div><p>
     All available clean disk devices that match the DriveGroups specification
     will be used as OSDs automatically after you add them to the cluster. This
     behavior is called a <span class="emphasis"><em>managed</em></span> mode.
    </p><p>
     To disable the <span class="emphasis"><em>managed</em></span> mode, add the
     <code class="literal">unmanaged: true</code> line to the relevant specifications,
     for example:
    </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true</pre></div><div id="id-1.4.4.3.6.5.9.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      To change already deployed OSDs from the <span class="emphasis"><em>managed</em></span> to
      <span class="emphasis"><em>unmanaged</em></span> mode, add the <code class="literal">unmanaged:
      true</code> lines where applicable during the procedure described in
      <a class="xref" href="#modifying-cluster-configuration" title="13.1. Modifying the cluster configuration">Section 13.1, “Modifying the cluster configuration”</a>.
     </p></div></section><section class="sect3" id="drive-groups-specs" data-id-title="DriveGroups specification"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.3.2 </span><span class="title-name">DriveGroups specification</span> <a title="Permalink" class="permalink" href="#drive-groups-specs">#</a></h4></div></div></div><p>
     Following is an example DriveGroups specification file:
    </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
db_devices:
  drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
wal_devices:
  drive_spec: <em class="replaceable">DEVICE_SPECIFICATION</em>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')</pre></div><div id="id-1.4.4.3.6.5.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The option previously called "encryption" in DeepSea has been renamed
      to "encrypted". When applying DriveGroups in SUSE Enterprise Storage 7, ensure you
      use this new terminology in your service specification, otherwise the
      <code class="command">ceph orch apply</code> operation will fail.
     </p></div></section><section class="sect3" id="matching-disk-devices" data-id-title="Matching disk devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.3.3 </span><span class="title-name">Matching disk devices</span> <a title="Permalink" class="permalink" href="#matching-disk-devices">#</a></h4></div></div></div><p>
     You can describe the specification using the following filters:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       By a disk model:
      </p><div class="verbatim-wrap"><pre class="screen">model: <em class="replaceable">DISK_MODEL_STRING</em></pre></div></li><li class="listitem"><p>
       By a disk vendor:
      </p><div class="verbatim-wrap"><pre class="screen">vendor: <em class="replaceable">DISK_VENDOR_STRING</em></pre></div><div id="id-1.4.4.3.6.5.11.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
        Always enter the <em class="replaceable">DISK_VENDOR_STRING</em> in
        lowercase.
       </p></div><p>
       To obtain details about disk model and vendor, examine the output of the
       following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]</pre></div></li><li class="listitem"><p>
       Whether a disk is rotational or not. SSDs and NVMe drives are not
       rotational.
      </p><div class="verbatim-wrap"><pre class="screen">rotational: 0</pre></div></li><li class="listitem"><p>
       Deploy a node using <span class="emphasis"><em>all</em></span> available drives for OSDs:
      </p><div class="verbatim-wrap"><pre class="screen">data_devices:
  all: true</pre></div></li><li class="listitem"><p>
       Additionally, by limiting the number of matching disks:
      </p><div class="verbatim-wrap"><pre class="screen">limit: 10</pre></div></li></ul></div></section><section class="sect3" id="filtering-devices-size" data-id-title="Filtering devices by size"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.3.4 </span><span class="title-name">Filtering devices by size</span> <a title="Permalink" class="permalink" href="#filtering-devices-size">#</a></h4></div></div></div><p>
     You can filter disk devices by their size—either by an exact size,
     or a size range. The <code class="option">size:</code> parameter accepts arguments in
     the following form:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       '10G' - Includes disks of an exact size.
      </p></li><li class="listitem"><p>
       '10G:40G' - Includes disks whose size is within the range.
      </p></li><li class="listitem"><p>
       ':10G' - Includes disks less than or equal to 10 GB in size.
      </p></li><li class="listitem"><p>
       '40G:' - Includes disks equal to or greater than 40 GB in size.
      </p></li></ul></div><div class="example" id="id-1.4.4.3.6.5.12.4" data-id-title="Matching by disk size"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.1: </span><span class="title-name">Matching by disk size </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.12.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'</pre></div></div></div><div id="id-1.4.4.3.6.5.12.5" data-id-title="Quotes required" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quotes required</h6><p>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </p></div><div id="id-1.4.4.3.6.5.12.6" data-id-title="Unit shortcuts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Unit shortcuts</h6><p>
      Instead of Gigabytes (G), you can specify the sizes in Megabytes (M) or
      Terabytes (T).
     </p></div></section><section class="sect3" id="ds-drive-groups-examples" data-id-title="DriveGroups examples"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.3.5 </span><span class="title-name">DriveGroups examples</span> <a title="Permalink" class="permalink" href="#ds-drive-groups-examples">#</a></h4></div></div></div><p>
     This section includes examples of different OSD setups.
    </p><div class="complex-example"><div class="example" id="id-1.4.4.3.6.5.13.3" data-id-title="Simple setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.2: </span><span class="title-name">Simple setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.13.3">#</a></h6></div><div class="example-contents"><p>
      This example describes two nodes with the same setup:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      The corresponding <code class="filename">drive_groups.yml</code> file will be as
      follows:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ</pre></div><p>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0</pre></div><p>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </p><p>
      If you know that drives with more than 2 TB will always be the
      slower data devices, you can filter by size:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.3.6.5.13.4" data-id-title="Advanced setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.3: </span><span class="title-name">Advanced setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.13.4">#</a></h6></div><div class="example-contents"><p>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        12 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      Such a setup can be defined with two layouts as follows:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ</pre></div><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.3.6.5.13.5" data-id-title="Advanced setup with non-uniform nodes"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.4: </span><span class="title-name">Advanced setup with non-uniform nodes </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.13.5">#</a></h6></div><div class="example-contents"><p>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </p><p>
      Nodes 1-5:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      Nodes 6-10:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        5 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        20 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li></ul></div><p>
      You can use the 'target' key in the layout to target specific nodes.
      Salt target notation helps to keep things simple:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0</pre></div><p>
      followed by
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.3.6.5.13.6" data-id-title="Expert setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.5: </span><span class="title-name">Expert setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.13.6">#</a></h6></div><div class="example-contents"><p>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        2 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        2 NVMes
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.4.4.3.6.5.13.7" data-id-title="Complex (and unlikely) setup"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 13.6: </span><span class="title-name">Complex (and unlikely) setup </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.5.13.7">#</a></h6></div><div class="example-contents"><p>
      In the following setup, we are trying to define:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        20 HDDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 HDDs backed by 1 SSD(db) and 1 NVMe (wal)
       </p></li><li class="listitem"><p>
        8 SSDs backed by 1 NVMe
       </p></li><li class="listitem"><p>
        2 SSDs stand-alone (encrypted)
       </p></li><li class="listitem"><p>
        1 HDD is spare and should not be deployed
       </p></li></ul></div><p>
      The summary of used drives is as follows:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        23 HDDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Intel
         </p></li><li class="listitem"><p>
          Model: SSD-123-foo
         </p></li><li class="listitem"><p>
          Size: 4 TB
         </p></li></ul></div></li><li class="listitem"><p>
        10 SSDs
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Micron
         </p></li><li class="listitem"><p>
          Model: MC-55-44-ZX
         </p></li><li class="listitem"><p>
          Size: 512 GB
         </p></li></ul></div></li><li class="listitem"><p>
        1 NVMe
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          Vendor: Samsung
         </p></li><li class="listitem"><p>
          Model: NVME-QQQQ-987
         </p></li><li class="listitem"><p>
          Size: 256 GB
         </p></li></ul></div></li></ul></div><p>
      The DriveGroups definition will be the following:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987</pre></div><div class="verbatim-wrap"><pre class="screen">service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True</pre></div><p>
      One HDD will remain as the file is being parsed from top to bottom.
     </p></div></div></div></section></section><section class="sect2" id="removing-node-osds" data-id-title="Removing OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.4 </span><span class="title-name">Removing OSDs</span> <a title="Permalink" class="permalink" href="#removing-node-osds">#</a></h3></div></div></div><p>
    Before removing an OSD node from the cluster, verify that the cluster has
    more free disk space than the OSD disk you are going to remove. Be aware
    that removing an OSD results in rebalancing of the whole cluster.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Identify which OSD to remove by getting its ID:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...</pre></div></li><li class="step"><p>
      Remove one or more OSDs from the cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm <em class="replaceable">OSD1_ID</em> <em class="replaceable">OSD2_ID</em> ...</pre></div><p>
      For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm 1 2</pre></div></li><li class="step"><p>
      You can query the state of the removal operation:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158</pre></div></li></ol></div></div><section class="sect3" id="removing-node-osds-stop" data-id-title="Stopping OSD removal"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.4.4.1 </span><span class="title-name">Stopping OSD removal</span> <a title="Permalink" class="permalink" href="#removing-node-osds-stop">#</a></h4></div></div></div><p>
     After you have scheduled an OSD removal, you can stop the removal if
     needed. The following command will reset the initial state of the OSD and
     remove it from the queue:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm stop <em class="replaceable">OSD_SERVICE_ID</em></pre></div></section></section><section class="sect2" id="removing-node-osds-replace" data-id-title="Replacing OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.5 </span><span class="title-name">Replacing OSDs</span> <a title="Permalink" class="permalink" href="#removing-node-osds-replace">#</a></h3></div></div></div><p>
    There are several reasons why you may need to replace an OSD disk. For
    example:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The OSD disk failed or is soon going to fail based on SMART information,
      and can no longer be used to store data safely.
     </p></li><li class="listitem"><p>
      You need to upgrade the OSD disk, for example to increase its size.
     </p></li><li class="listitem"><p>
      You need to change the OSD disk layout.
     </p></li><li class="listitem"><p>
      You plan to move from a non-LVM to a LVM-based layout.
     </p></li></ul></div><p>
    To replace an OSD while preserving its ID, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm <em class="replaceable">OSD_SERVICE_ID</em> --replace</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch osd rm 4 --replace</pre></div><p>
    Replacing an OSD is identical to removing an OSD (see
    <a class="xref" href="#removing-node-osds" title="13.4.4. Removing OSDs">Section 13.4.4, “Removing OSDs”</a> for more details) with the exception
    that the OSD is not permanently removed from the CRUSH hierarchy and is
    assigned a <code class="literal">destroyed</code> flag instead.
   </p><p>
    The <code class="literal">destroyed</code> flag is used to determined OSD IDs that
    will be reused during the next OSD deployment. Newly added disks that match
    the DriveGroups specification (see <a class="xref" href="#drive-groups" title="13.4.3. Adding OSDs using DriveGroups specification">Section 13.4.3, “Adding OSDs using DriveGroups specification”</a> for more
    details) will be assigned OSD IDs of their replaced counterpart.
   </p><div id="id-1.4.4.3.6.7.10" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Appending the <code class="option">--dry-run</code> option will not execute the
     actual replacement, but will preview the steps that would normally happen.
    </p></div><div id="id-1.4.4.3.6.7.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     In the case of replacing an OSD after a failure, we highly recommend
     triggering a deep scrub of the placement groups. See
     <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a> for more details.
    </p><p>
     Run the following command to initiate a deep scrub:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd deep-scrub osd.<em class="replaceable">OSD_NUMBER</em></pre></div></div><div id="id-1.4.4.3.6.7.12" data-id-title="Shared device failure" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Shared device failure</h6><p>
     If a shared device for DB/WAL fails you will need to perform the
     replacement procedure for all OSDs that share the failed device.
    </p></div></section><section class="sect2" id="migrating-db-device" data-id-title="Migrating OSDs DB device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.4.6 </span><span class="title-name">Migrating OSD's DB device</span> <a title="Permalink" class="permalink" href="#migrating-db-device">#</a></h3></div></div></div><p>
    DB device belongs to an OSD and stores its metadata (see <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.4 “BlueStore”</span> for
    more details). There are several reasons why you may want to migrate an existing DB device to a
    new one—for example, when OSDs have different DB sizes and you need to 
    align them.
   </p><div id="id-1.4.4.3.6.8.3" data-id-title="ceph-volume naming convention" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: <code class="command">ceph-volume</code> naming convention</h6><p>
     Some clusters may have old volume group (VG) or logical volume (LV) names prefixed with
     <code class="literal">ceph-block-dbs</code> and <code class="literal">osd-block-db</code>, for example:
    </p><div class="verbatim-wrap"><pre class="screen">ceph-block-dbs-c3dc9227-ca3e-49bc-992c-00602cb3eec7/osd-block-db-b346b9ff-dbbe-40db-a95e-2419ccd31f2c</pre></div><p>
     The current naming convention is as follows:
    </p><div class="verbatim-wrap"><pre class="screen">ceph-c3dc9227-ca3e-49bc-992c-00602cb3eec7/osd-db-b346b9ff-dbbe-40db-a95e-2419ccd31f2c</pre></div></div><div class="procedure" id="id-1.4.4.3.6.8.4" data-id-title="Migrating a DB device to a new device"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 13.1: </span><span class="title-name">Migrating a DB device to a new device </span><a title="Permalink" class="permalink" href="#id-1.4.4.3.6.8.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Identify the <code class="option">db device</code> and <code class="option">osd fsid</code> values by running
      the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm ceph-volume lvm list
[...]
====== osd.0 =======

[block]       /dev/ceph-b03b5ad4-98e8-446a-9a9f-840ecd90215c/osd-block-c276d2a4-5578-4847-94c6-8e2e6abf81c4

block device              /dev/ceph-b03b5ad4-98e8-446a-9a9f-840ecd90215c/osd-block-c276d2a4-5578-4847-94c6-8e2e6abf81c4
block uuid                Kg3ySP-ykP8-adFE-UrHY-OSiv-0WQ5-uuUEJ9
cephx lockbox secret
cluster fsid              9c8d3126-9faf-11ec-a2cf-52540035cdc1
cluster name              ceph
crush device class
db device                 /dev/ceph-block-dbs-c3dc9227-ca3e-49bc-992c-00602cb3eec7/osd-block-db-b346b9ff-dbbe-40db-a95e-2419ccd31f2c
encrypted                 0
osd fsid                  c276d2a4-5578-4847-94c6-8e2e6abf81c4
osd id                    0
osdspec affinity          sesdev_osd_deployment
type                      block
vdo                       0
devices                   /dev/vdb
[...]</pre></div></li><li class="step"><p>
      Create a new logical volume (LV) for the new DB device. Refer to
      <span class="intraxref">Book “Deployment Guide”, Chapter 2 “Hardware requirements and recommendations”, Section 2.4.3 “Recommended size for the BlueStore's WAL and DB device”</span> when determining the right size for the DB device. For
      example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>lvcreate -n osd-db-$(cat /proc/sys/kernel/random/uuid) \
 ceph-c3dc9227-ca3e-49bc-992c-00602cb3eec7 --size <em class="replaceable">DB_SIZE</em></pre></div></li><li class="step"><p>
      Stop the OSD. Run the following command on the OSD node where the OSD daemon runs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@osd &gt; </code>cephadm unit --name osd.0</pre></div></li><li class="step"><p>
      Enter the shell on the stopped OSD container:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@osd &gt; </code>cephadm shell --name osd.0</pre></div></li><li class="step"><p>
      Migrate the DB device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-volume lvm migrate --osd-id 0 \
 --osd-fsid c276d2a4-5578-4847-94c6-8e2e6abf81c4 --from db \
 --target ceph-c3dc9227-ca3e-49bc-992c-00602cb3eec7/osd-db-b346b9ff-dbbe-40db-a95e-2419ccd31f2c</pre></div></li><li class="step"><p>
      Exit the cephadm shell:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>exit</pre></div></li><li class="step"><p>
      Start the OSD. Run the following command on the OSD node where the OSD daemon runs:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@osd &gt; </code>cephadm unit --name osd.0 start</pre></div></li><li class="step"><p>
      Remove the old DB logical volume.
     </p></li></ol></div></div></section></section><section class="sect1" id="moving-saltmaster" data-id-title="Moving the Salt Master to a new node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.5 </span><span class="title-name">Moving the Salt Master to a new node</span> <a title="Permalink" class="permalink" href="#moving-saltmaster">#</a></h2></div></div></div><p>
   If you need to replace the Salt Master host with a new one, follow these
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Export the cluster configuration and back up the exported JSON file. Find
     more details in <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.14 “Exporting cluster configurations”</span>.
    </p></li><li class="step"><p>
     If the old Salt Master is also the only administration node in the cluster,
     then manually move
     <code class="filename">/etc/ceph/ceph.client.admin.keyring</code> and
     <code class="filename">/etc/ceph/ceph.conf</code> to the new Salt Master.
    </p></li><li class="step"><p>
     Stop and disable the Salt Master <code class="systemitem">systemd</code> service on the old Salt Master
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-master.service
<code class="prompt user">root@master # </code>systemctl disable salt-master.service</pre></div></li><li class="step"><p>
     If the old Salt Master node is no longer in the cluster, also stop and
     disable the Salt Minion <code class="systemitem">systemd</code> service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl stop salt-minion.service
<code class="prompt user">root@master # </code>systemctl disable salt-minion.service</pre></div><div id="id-1.4.4.3.7.3.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Do not stop or disable the <code class="literal">salt-minion.service</code> if the
      old Salt Master node has any Ceph daemons (MON, MGR, OSD, MDS, gateway,
      monitoring) running on it.
     </p></div></li><li class="step"><p>
     Install SUSE Linux Enterprise Server 15 SP3 on the new Salt Master following the procedure described in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Installing and configuring SUSE Linux Enterprise Server”</span>.
    </p><div id="id-1.4.4.3.7.3.5.2" data-id-title="Transition of Salt Minion" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Transition of Salt Minion</h6><p>
      To simplify the transition of Salt Minions to the new Salt Master, remove the
      original Salt Master's public key from each of them:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>rm /etc/salt/pki/minion/minion_master.pub
<code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></div></li><li class="step"><p>
     Install the <span class="package">salt-master</span> package and, if applicable, the
     <span class="package">salt-minion</span> package on the new Salt Master.
    </p></li><li class="step"><p>
     Install <code class="systemitem">ceph-salt</code> on the new Salt Master node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install ceph-salt
<code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt '*' saltutil.sync_all</pre></div><div id="id-1.4.4.3.7.3.7.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Make sure to run all three commands before continuing. The commands are
      idempotent; it does not matter if they get repeated.
     </p></div></li><li class="step"><p>
     Include the new Salt Master in the cluster as described in
     <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.1 “Installing <code class="systemitem">ceph-salt</code>”</span>,
     <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.2 “Adding Salt Minions”</span> and
     <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.4 “Specifying Admin Node”</span>.
    </p></li><li class="step"><p>
     Import the backed up cluster configuration and apply it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt import <em class="replaceable">CLUSTER_CONFIG</em>.json
<code class="prompt user">root@master # </code>ceph-salt apply</pre></div><div id="id-1.4.4.3.7.3.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Rename the Salt Master's <code class="literal">minion id</code> in the exported
      <code class="filename"><em class="replaceable">CLUSTER_CONFIG</em>.json</code> file
      before importing it.
     </p></div></li></ol></div></div></section><section class="sect1" id="cephadm-rolling-updates" data-id-title="Updating the cluster nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.6 </span><span class="title-name">Updating the cluster nodes</span> <a title="Permalink" class="permalink" href="#cephadm-rolling-updates">#</a></h2></div></div></div><p>
   Keep the Ceph cluster nodes up-to-date by applying rolling updates
   regularly.
  </p><section class="sect2" id="rolling-updates-repos" data-id-title="Software repositories"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.1 </span><span class="title-name">Software repositories</span> <a title="Permalink" class="permalink" href="#rolling-updates-repos">#</a></h3></div></div></div><p>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <span class="intraxref">Book “Deployment Guide”, Chapter 10 “Upgrade from SUSE Enterprise Storage 6 to 7.1”, Section 10.1.5.1 “Software repositories”</span> for a complete
    list of the required repositories.
   </p></section><section class="sect2" id="rolling-upgrades-staging" data-id-title="Repository staging"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.2 </span><span class="title-name">Repository staging</span> <a title="Permalink" class="permalink" href="#rolling-upgrades-staging">#</a></h3></div></div></div><p>
    If you use a staging tool—for example, SUSE Manager, Subscription Management Tool, or
    RMT—that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for SUSE Linux Enterprise Server and SUSE Enterprise Storage are
    created at the same point in time.
   </p><p>
    We strongly recommend to use a staging tool to apply patches which have
    <code class="literal">frozen</code> or <code class="literal">staged</code> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </p></section><section class="sect2" id="id-1.4.4.3.8.5" data-id-title="Downtime of Ceph services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.3 </span><span class="title-name">Downtime of Ceph services</span> <a title="Permalink" class="permalink" href="#id-1.4.4.3.8.5">#</a></h3></div></div></div><p>
    Depending on the configuration, cluster nodes may be rebooted during the
    update. If there is a single point of failure for services such as Object Gateway,
    Samba Gateway, NFS Ganesha, or iSCSI, the client machines may be temporarily
    disconnected from services whose nodes are being rebooted.
   </p></section><section class="sect2" id="rolling-updates-running" data-id-title="Running the update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.6.4 </span><span class="title-name">Running the update</span> <a title="Permalink" class="permalink" href="#rolling-updates-running">#</a></h3></div></div></div><p>
    To update the software packages on all cluster nodes to the latest version,
    run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt update</pre></div></section></section><section class="sect1" id="deploy-cephadm-day2-cephupdate" data-id-title="Updating Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.7 </span><span class="title-name">Updating Ceph</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-cephupdate">#</a></h2></div></div></div><p>
   You can instruct cephadm to update Ceph from one bugfix release to
   another. The automated update of Ceph services respects the recommended
   order—it starts with Ceph Managers, Ceph Monitors, and then continues on to other
   services such as Ceph OSDs, Metadata Servers, and Object Gateways. Each daemon is restarted only
   after Ceph indicates that the cluster will remain available.
  </p><div id="id-1.4.4.3.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The following update procedure uses the <code class="command">ceph orch
    upgrade</code> command. Keep in mind that the following instructions
    detail how to update your Ceph cluster with a product version (for
    example, a maintenance update), and <span class="emphasis"><em>does not</em></span> provide
    instructions on how to upgrade your cluster from one product version to
    another.
   </p></div><section class="sect2" id="deploy-cephadm-day2-cephupdate-start" data-id-title="Starting the update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.7.1 </span><span class="title-name">Starting the update</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-cephupdate-start">#</a></h3></div></div></div><p>
    Before you start the update, verify that all nodes are currently online and
    your cluster is healthy:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm shell -- ceph -s</pre></div><p>
    To update to a specific Ceph release:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch upgrade start --image <em class="replaceable">REGISTRY_URL</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch upgrade start --image registry.suse.com/ses/7.1/ceph/ceph:latest</pre></div><p>
    Upgrade packages on the hosts:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt update</pre></div></section><section class="sect2" id="deploy-cephadm-day2-cephupdate-monitor" data-id-title="Monitoring the update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.7.2 </span><span class="title-name">Monitoring the update</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-cephupdate-monitor">#</a></h3></div></div></div><p>
    Run the following command to determine whether an update is in progress:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch upgrade status</pre></div><p>
    While the update is in progress, you will see a progress bar in the Ceph
    status output:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7.1/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</pre></div><p>
    You can also watch the cephadm log:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph -W cephadm</pre></div></section><section class="sect2" id="deploy-cephadm-day2-cephupdate-stop" data-id-title="Cancelling an update"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.7.3 </span><span class="title-name">Cancelling an update</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-cephupdate-stop">#</a></h3></div></div></div><p>
    You can stop the update process at any time:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch upgrade stop</pre></div></section></section><section class="sect1" id="sec-salt-cluster-reboot" data-id-title="Halting or rebooting cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.8 </span><span class="title-name">Halting or rebooting cluster</span> <a title="Permalink" class="permalink" href="#sec-salt-cluster-reboot">#</a></h2></div></div></div><p>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Tell the Ceph cluster not to mark OSDs as out:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> osd set noout</pre></div></li><li class="step"><p>
     Stop daemons and nodes in the following order:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Storage clients
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph Monitor
      </p></li></ol></div></li><li class="step"><p>
     If required, perform maintenance tasks.
    </p></li><li class="step"><p>
     Start the nodes and servers in the reverse order of the shutdown process:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Ceph Monitor
      </p></li><li class="listitem"><p>
       Ceph Manager
      </p></li><li class="listitem"><p>
       Ceph OSD
      </p></li><li class="listitem"><p>
       Metadata Server
      </p></li><li class="listitem"><p>
       Gateways, for example NFS Ganesha or Object Gateway
      </p></li><li class="listitem"><p>
       Storage clients
      </p></li></ol></div></li><li class="step"><p>
     Remove the noout flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> osd unset noout</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-cluster-purge" data-id-title="Removing an entire Ceph cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.9 </span><span class="title-name">Removing an entire Ceph cluster</span> <a title="Permalink" class="permalink" href="#ceph-cluster-purge">#</a></h2></div></div></div><p>
   The <code class="command">ceph-salt purge</code> command removes the entire Ceph
   cluster. If there are more Ceph clusters deployed, the one reported by
   <code class="command">ceph -s</code> is purged. This way you can clean the cluster
   environment when testing different setups.
  </p><p>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the Ceph
   cluster by running:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt disengage-safety
<code class="prompt user">root@master # </code>ceph-salt purge</pre></div></section></section><section class="chapter" id="cha-ceph-operating" data-id-title="Operation of Ceph services"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">14 </span><span class="title-name">Operation of Ceph services</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating">#</a></h1></div></div></div><p>
  You can operate Ceph services on a daemon, node, or cluster level.
  Depending on which approach you need, use cephadm or the
  <code class="command">systemctl</code> command.
 </p><section class="sect1" id="cha-ceph-operating-individual" data-id-title="Operating individual services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.1 </span><span class="title-name">Operating individual services</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating-individual">#</a></h2></div></div></div><p>
   If you need to operate an individual service, identify it first:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps
NAME                                HOST        STATUS         REFRESHED  [...]
mds.my_cephfs.ses-min1.oterul       ses-min1    running (5d)   8m ago
mgr.ses-min1.gpijpm                 ses-min1    running (5d)   8m ago
mgr.ses-min2.oopvyh                 ses-min2    running (5d)   8m ago
mon.ses-min1                        ses-min1    running (5d)   8m ago
mon.ses-min2                        ses-min2    running (5d)   8m ago
mon.ses-min4                        ses-min4    running (5d)   7m ago
osd.0                               ses-min2    running (61m)  8m ago
osd.1                               ses-min3    running (61m)  7m ago
osd.2                               ses-min4    running (61m)  7m ago
rgw.myrealm.myzone.ses-min1.kwwazo  ses-min1    running (5d)   8m ago
rgw.myrealm.myzone.ses-min2.jngabw  ses-min2    error          8m ago</pre></div><p>
   To identify a service on a specific node, run:
  </p><div class="verbatim-wrap"><pre class="screen">ceph orch ps <em class="replaceable">NODE_HOST_NAME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps ses-min2
NAME                                HOST      STATUS         REFRESHED
mgr.ses-min2.oopvyh                 ses-min2  running (5d)   3m ago
mon.ses-min2                        ses-min2  running (5d)   3m ago
osd.0                               ses-min2  running (67m)  3m ago</pre></div><div id="id-1.4.4.4.4.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    The <code class="command">ceph orch ps</code> command supports several output
    formats. To change it, append the <code class="option">--format
    <em class="replaceable">FORMAT</em></code> option where
    <em class="replaceable">FORMAT</em> is one of <code class="literal">json</code>,
    <code class="literal">json-pretty</code>, or <code class="literal">yaml</code>. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --format yaml</pre></div></div><p>
   Once you know the name of the service you can start, restart, or stop it:
  </p><div class="verbatim-wrap"><pre class="screen">ceph orch daemon <em class="replaceable">COMMAND</em> <em class="replaceable">SERVICE_NAME</em></pre></div><p>
   For example, to restart the OSD service with ID 0, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch daemon restart osd.0</pre></div></section><section class="sect1" id="cha-ceph-operating-service-types" data-id-title="Operating service types"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.2 </span><span class="title-name">Operating service types</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating-service-types">#</a></h2></div></div></div><p>
   If you need to operate a specific type of service across the whole Ceph
   cluster, use the following command:
  </p><div class="verbatim-wrap"><pre class="screen">ceph orch <em class="replaceable">COMMAND</em> <em class="replaceable">SERVICE_TYPE</em></pre></div><p>
   Replace <em class="replaceable">COMMAND</em> with either
   <code class="literal">start</code>, <code class="literal">stop</code>, or
   <code class="literal">restart</code>.
  </p><p>
   For example, the following command restarts all MONs in the cluster,
   regardless of which nodes they actually run on:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart mon</pre></div></section><section class="sect1" id="cha-ceph-operating-node" data-id-title="Operating services on a single node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.3 </span><span class="title-name">Operating services on a single node</span> <a title="Permalink" class="permalink" href="#cha-ceph-operating-node">#</a></h2></div></div></div><p>
   By using the <code class="command">systemctl</code> command, you can operate Ceph
   related <code class="systemitem">systemd</code> services and targets on a single node.
  </p><section class="sect2" id="ceph-operating-services-finding-names" data-id-title="Identifying services and targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.3.1 </span><span class="title-name">Identifying services and targets</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-finding-names">#</a></h3></div></div></div><p>
    Before operating Ceph related <code class="systemitem">systemd</code> services and targets, you need to
    identify the file names of their unit files. File names of the services
    have the following pattern:
   </p><div class="verbatim-wrap"><pre class="screen">ceph-<em class="replaceable">FSID</em>@<em class="replaceable">SERVICE_TYPE</em>.<em class="replaceable">ID</em>.service</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@mon.doc-ses-min1.service</pre></div><div class="verbatim-wrap"><pre class="screen">ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@rgw.myrealm.myzone.doc-ses-min1.kwwazo.service</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.4.4.6.3.7.1"><span class="term">FSID</span></dt><dd><p>
       Unique ID of the Ceph cluster. You can find it in the output of the
       <code class="command">ceph fsid</code> command.
      </p></dd><dt id="id-1.4.4.4.6.3.7.2"><span class="term">SERVICE_TYPE</span></dt><dd><p>
       Type of the service, for example <code class="literal">osd</code>,
       <code class="literal">mon</code>, or <code class="literal">rgw</code>.
      </p></dd><dt id="id-1.4.4.4.6.3.7.3"><span class="term">ID</span></dt><dd><p>
       Identification string of the service. For OSDs, it is the ID number of
       the service. For other services, it can be either a host name of the
       node, or additional strings relevant for the service type.
      </p></dd></dl></div><div id="id-1.4.4.4.6.3.8" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     The <em class="replaceable">SERVICE_TYPE</em>.<em class="replaceable">ID</em>
     part is identical to the content of the <code class="literal">NAME</code> column in
     the output of the <code class="command">ceph orch ps</code> command.
    </p></div></section><section class="sect2" id="ceph-operating-services-targets" data-id-title="Operating all services on a node"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.3.2 </span><span class="title-name">Operating all services on a node</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-targets">#</a></h3></div></div></div><p>
    By using Ceph's <code class="systemitem">systemd</code> targets, you can simultaneously operate either
    <span class="emphasis"><em>all</em></span> services on a node, or all services that
    <span class="emphasis"><em>belong to a cluster</em></span> identified by its
    <em class="replaceable">FSID</em>.
   </p><p>
    For example, to stop all Ceph services on a node regardless to which
    cluster the services belong to, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl stop ceph.target</pre></div><p>
    To restart all services that belong to a Ceph cluster with ID
    <code class="literal">b4b30c6e-9681-11ea-ac39-525400d7702d</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart ceph-b4b30c6e-9681-11ea-ac39-525400d7702d.target</pre></div></section><section class="sect2" id="ceph-operating-services-single" data-id-title="Operating an individual service on a node"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.3.3 </span><span class="title-name">Operating an individual service on a node</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-single">#</a></h3></div></div></div><p>
    After you have identified the name of a specific service, operate it the
    following way:
   </p><div class="verbatim-wrap"><pre class="screen">systemctl <em class="replaceable">COMMAND</em> <em class="replaceable">SERVICE_NAME</em></pre></div><p>
    For example, to restart a single OSD service with ID 1 on a cluster with ID
    <code class="literal">b4b30c6e-9681-11ea-ac39-525400d7702d</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@osd.1.service</pre></div></section><section class="sect2" id="ceph-operating-services-status" data-id-title="Querying the service status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.3.4 </span><span class="title-name">Querying the service status</span> <a title="Permalink" class="permalink" href="#ceph-operating-services-status">#</a></h3></div></div></div><p>
    You can query <code class="systemitem">systemd</code> for the status of services. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl status ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@osd.0.service</pre></div></section></section><section class="sect1" id="ceph-cluster-shutdown" data-id-title="Shutting down and restarting the whole Ceph cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.4 </span><span class="title-name">Shutting down and restarting the whole Ceph cluster</span> <a title="Permalink" class="permalink" href="#ceph-cluster-shutdown">#</a></h2></div></div></div><p>
   Shutting down and restarting the cluster may be necessary in the case of a
   planned power outage. To stop all Ceph related services and restart
   without issue, follow the steps below.
  </p><div class="procedure" id="id-1.4.4.4.7.3" data-id-title="Shutting down the whole Ceph cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 14.1: </span><span class="title-name">Shutting down the whole Ceph cluster </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.7.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Shut down or disconnect any clients accessing the cluster.
    </p></li><li class="step"><p>
     To prevent CRUSH from automatically rebalancing the cluster, set the
     cluster to <code class="literal">noout</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set noout</pre></div></li><li class="step"><p>
     Stop all Ceph services on all cluster nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt stop</pre></div></li><li class="step"><p>
     Power off all cluster nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt -G 'ceph-salt:member' cmd.run "shutdown -h"</pre></div></li></ol></div></div><div class="procedure" id="id-1.4.4.4.7.4" data-id-title="Starting the whole Ceph cluster"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 14.2: </span><span class="title-name">Starting the whole Ceph cluster </span><a title="Permalink" class="permalink" href="#id-1.4.4.4.7.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Power on the Admin Node.
    </p></li><li class="step"><p>
     Power on the Ceph Monitor nodes.
    </p></li><li class="step"><p>
     Power on the Ceph OSD nodes.
    </p></li><li class="step"><p>
     Unset the previously set <code class="literal">noout</code> flag:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd unset noout</pre></div></li><li class="step"><p>
     Power on all configured gateways.
    </p></li><li class="step"><p>
     Power on or connect cluster clients.
    </p></li></ol></div></div></section></section><section class="chapter" id="cha-deployment-backup" data-id-title="Backup and restore"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">15 </span><span class="title-name">Backup and restore</span> <a title="Permalink" class="permalink" href="#cha-deployment-backup">#</a></h1></div></div></div><p>
  This chapter explains which parts of the Ceph cluster you should back up in
  order to be able to restore its functionality.
 </p><section class="sect1" id="backrest-ceph" data-id-title="Back Up Cluster Configuration and Data"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.1 </span><span class="title-name">Back Up Cluster Configuration and Data</span> <a title="Permalink" class="permalink" href="#backrest-ceph">#</a></h2></div></div></div><section class="sect2" id="backrest-ceph-cephsalt" data-id-title="Back up ceph-salt configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.1 </span><span class="title-name">Back up <code class="systemitem">ceph-salt</code> configuration</span> <a title="Permalink" class="permalink" href="#backrest-ceph-cephsalt">#</a></h3></div></div></div><p>
    Export the cluster configuration. Find more information in
    <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.14 “Exporting cluster configurations”</span>.
   </p></section><section class="sect2" id="backup-ceph" data-id-title="Back up Ceph configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.2 </span><span class="title-name">Back up Ceph configuration</span> <a title="Permalink" class="permalink" href="#backup-ceph">#</a></h3></div></div></div><p>
    Back up the <code class="filename">/etc/ceph</code> directory. It contains crucial
    cluster configuration. For example, you will need a backup of
    <code class="filename">/etc/ceph</code> when you need to replace the Admin Node.
   </p></section><section class="sect2" id="sec-deployment-backup-salt" data-id-title="Back up Salt configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.3 </span><span class="title-name">Back up Salt configuration</span> <a title="Permalink" class="permalink" href="#sec-deployment-backup-salt">#</a></h3></div></div></div><p>
    You need to back up the <code class="filename">/etc/salt/</code> directory. It
    contains the Salt configuration files, for example the Salt Master key and
    accepted client keys.
   </p><p>
    The Salt files are not strictly required for backing up the Admin Node, but
    make redeploying the Salt cluster easier. If there is no backup of these
    files, the Salt minions need to be registered again at the new Admin Node.
   </p><div id="id-1.4.4.5.4.4.4" data-id-title="Security of the Salt Master Private Key" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Security of the Salt Master Private Key</h6><p>
     Make sure that the backup of the Salt Master private key is stored in a safe
     location. The Salt Master key can be used to manipulate all cluster nodes.
    </p></div></section><section class="sect2" id="backup-config-files" data-id-title="Back up custom configurations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.4 </span><span class="title-name">Back up custom configurations</span> <a title="Permalink" class="permalink" href="#backup-config-files">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Prometheus data and customization.
     </p></li><li class="listitem"><p>
      Grafana customization.
     </p></li><li class="listitem"><p>
      Manual changes to the iSCSI configuration.
     </p></li><li class="listitem"><p>
      Ceph keys.
     </p></li><li class="listitem"><p>
      CRUSH Map and CRUSH rules. Save the decompiled CRUSH Map including
      CRUSH rules into <code class="filename">crushmap-backup.txt</code> by running the
      following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</pre></div></li><li class="listitem"><p>
      Samba Gateway configuration. If you are using a single gateway, backup
      <code class="filename">/etc/samba/smb.conf</code>. If you are using an HA setup,
      also back up the CTDB and Pacemaker configuration files. Refer to
      <a class="xref" href="#cha-ses-cifs" title="Chapter 24. Export Ceph data via Samba">Chapter 24, <em>Export Ceph data via Samba</em></a> for details on what configuration is used
      by Samba Gateways.
     </p></li><li class="listitem"><p>
      NFS Ganesha configuration. Only needed when using an HA setup. Refer to
      <a class="xref" href="#cha-ceph-nfsganesha" title="Chapter 25. NFS Ganesha">Chapter 25, <em>NFS Ganesha</em></a> for details on what configuration
      is used by NFS Ganesha.
     </p></li></ul></div></section></section><section class="sect1" id="restore-ceph" data-id-title="Restoring a Ceph node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.2 </span><span class="title-name">Restoring a Ceph node</span> <a title="Permalink" class="permalink" href="#restore-ceph">#</a></h2></div></div></div><p>
   The procedure to recover a node from backup is to reinstall the node,
   replace its configuration files, and then re-orchestrate the cluster so that
   the replacement node is re-added.
  </p><p>
   If you need to redeploy the Admin Node, refer to
   <a class="xref" href="#moving-saltmaster" title="13.5. Moving the Salt Master to a new node">Section 13.5, “Moving the Salt Master to a new node”</a>.
  </p><p>
   For minions, it is usually easier to simply rebuild and redeploy.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Re-install the node. Find more information in
     <span class="intraxref">Book “Deployment Guide”, Chapter 5 “Installing and configuring SUSE Linux Enterprise Server”</span>
    </p></li><li class="listitem"><p>
     Install Salt Find more information in <span class="intraxref">Book “Deployment Guide”, Chapter 6 “Deploying Salt”</span>
    </p></li><li class="listitem"><p>
     After restoring the <code class="filename">/etc/salt</code> directory from a
     backup, enable and restart applicable Salt services, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">systemctl</code> enable salt-master
<code class="prompt user">root@master # </code><code class="command">systemctl</code> start salt-master
<code class="prompt user">root@master # </code><code class="command">systemctl</code> enable salt-minion
<code class="prompt user">root@master # </code><code class="command">systemctl</code> start salt-minion</pre></div></li><li class="listitem"><p>
     Remove the public master key for the old Salt Master node from all the
     minions.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code><code class="command">rm</code> /etc/salt/pki/minion/minion_master.pub
<code class="prompt user">root@master # </code><code class="command">systemctl</code> restart salt-minion</pre></div></li><li class="listitem"><p>
     Restore anything that was local to the Admin Node.
    </p></li><li class="listitem"><p>
     Import the cluster configuration from the previously exported JSON file.
     Refer to <span class="intraxref">Book “Deployment Guide”, Chapter 7 “Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code>”, Section 7.2.14 “Exporting cluster configurations”</span> for more
     details.
    </p></li><li class="listitem"><p>
     Apply the imported cluster configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply</pre></div></li></ol></div></section></section><section class="chapter" id="monitoring-alerting" data-id-title="Monitoring and alerting"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">16 </span><span class="title-name">Monitoring and alerting</span> <a title="Permalink" class="permalink" href="#monitoring-alerting">#</a></h1></div></div></div><p>
  In SUSE Enterprise Storage 7.1, cephadm deploys a monitoring and alerting
  stack. Users need to either define the services (such as Prometheus,
  Alertmanager, and Grafana) that they want to deploy with cephadm in a
  YAML configuration file, or they can use the CLI to deploy them. When
  multiple services of the same type are deployed, a highly-available setup is
  deployed. The node exporter is an exception to this rule.
 </p><p>
  The following monitoring services can be deployed with cephadm:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="bold"><strong>Prometheus</strong></span> is the monitoring and
    alerting toolkit. It collects the data provided by Prometheus exporters and
    fires preconfigured alerts if predefined thresholds have been reached.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Alertmanager</strong></span> handles alerts sent by the
    Prometheus server. It deduplicates, groups, and routes the alerts to the
    correct receiver. By default, the Ceph Dashboard will automatically be
    configured as the receiver.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Grafana</strong></span> is the visualization and
    alerting software. The alerting functionality of Grafana is not used by
    this monitoring stack. For alerting, the Alertmanager is used.
   </p></li><li class="listitem"><p>
    <span class="bold"><strong>Node exporter</strong></span> is an exporter for
    Prometheus which provides data about the node it is installed on. It is
    recommended to install the node exporter on all nodes.
   </p></li></ul></div><p>
  The Prometheus Manager Module provides a Prometheus exporter to pass on Ceph
  performance counters from the collection point in
  <code class="literal">ceph-mgr</code>.
 </p><p>
  The Prometheus configuration, including <span class="emphasis"><em>scrape</em></span> targets
  (metrics providing daemons), is set up automatically by cephadm. cephadm
  also deploys a list of default alerts, for example <code class="literal">health
  error</code>, <code class="literal">10% OSDs down</code>, or <code class="literal">pgs
  inactive</code>.
 </p><p>
  By default, traffic to Grafana is encrypted with TLS. You can either supply
  your own TLS certificate or use a self-signed one. If no custom certificate
  has been configured before Grafana has been deployed, then a self-signed
  certificate is automatically created and configured for Grafana.
 </p><p>
  You can configure custom certificates for Grafana by following these steps:
 </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Configure certificate files:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code> ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
<code class="prompt user">cephuser@adm &gt; </code> ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem</pre></div></li><li class="step"><p>
    Restart the Ceph Manager service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart mgr</pre></div></li><li class="step"><p>
    Reconfigure the Grafana service to reflect the new certificate paths and
    set the right URL for the Ceph Dashboard:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch reconfig grafana</pre></div></li></ol></div></div><p>
  The Alertmanager handles alerts sent by the Prometheus server. It takes
  care of deduplicating, grouping, and routing them to the correct receiver.
  Alerts can be silenced using the Alertmanager, but silences can also be
  managed using the Ceph Dashboard.
 </p><p>
  We recommend that the <code class="systemitem">Node exporter</code>
  is deployed on all nodes. This can be done using the
  <code class="filename">monitoring.yaml</code> file with the
  <code class="literal">node-exporter</code> service type. See
  <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.8 “Deploying the monitoring stack”</span> for more
  information on deploying services.
 </p><section class="sect1" id="monitoring-custom-images" data-id-title="Configuring custom or local images"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.1 </span><span class="title-name">Configuring custom or local images</span> <a title="Permalink" class="permalink" href="#monitoring-custom-images">#</a></h2></div></div></div><div id="id-1.4.4.6.13.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    This section describes how to change the configuration of container images
    which are used when services are deployed or updated. It does not include
    the commands necessary to deploy or re-deploy services.
   </p><p>
    The recommended method to deploy the monitoring stack is by applying its
    specification as described in
    <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.8 “Deploying the monitoring stack”</span>.
   </p></div><p>
   To deploy custom or local container images, the images need to be set in
   cephadm. To do so, you will need to run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/cephadm/<em class="replaceable">OPTION_NAME</em> <em class="replaceable">VALUE</em></pre></div><p>
   Where <em class="replaceable">OPTION_NAME</em> is any of the following names:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     container_image_prometheus
    </p></li><li class="listitem"><p>
     container_image_node_exporter
    </p></li><li class="listitem"><p>
     container_image_alertmanager
    </p></li><li class="listitem"><p>
     container_image_grafana
    </p></li></ul></div><p>
   If no option is set or if the setting has been removed, the following images
   are used as <em class="replaceable">VALUE</em>:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1
    </p></li><li class="listitem"><p>
     registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2
    </p></li><li class="listitem"><p>
     registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0
    </p></li><li class="listitem"><p>
     registry.suse.com/ses/7.1/ceph/grafana:7.5.12
    </p></li></ul></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1</pre></div><div id="id-1.4.4.6.13.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    By setting a custom image, the default value will be overridden (but not
    overwritten). The default value changes when updates become available. By
    setting a custom image, you will not be able to update the component you
    have set the custom image for automatically. You will need to manually
    update the configuration (image name and tag) to be able to install
    updates.
   </p><p>
    If you choose to go with the recommendations instead, you can reset the
    custom image you have set before. After that, the default value will be
    used again. Use <code class="command">ceph config rm</code> to reset the
    configuration option:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config rm mgr mgr/cephadm/<em class="replaceable">OPTION_NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config rm mgr mgr/cephadm/container_image_prometheus</pre></div></div></section><section class="sect1" id="monitoring-applying-updates" data-id-title="Updating monitoring services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.2 </span><span class="title-name">Updating monitoring services</span> <a title="Permalink" class="permalink" href="#monitoring-applying-updates">#</a></h2></div></div></div><p>
   As mentioned in <a class="xref" href="#monitoring-custom-images" title="16.1. Configuring custom or local images">Section 16.1, “Configuring custom or local images”</a>, cephadm is
   shipped with the URLs of the recommended and tested container images, and
   they are used by default.
  </p><p>
   By updating the Ceph packages, new versions of these URLs may be shipped.
   This just updates where the container images are pulled from but does not
   update any services.
  </p><p>
   After the URLs to the new container images have been updated, either
   manually as described in <a class="xref" href="#monitoring-custom-images" title="16.1. Configuring custom or local images">Section 16.1, “Configuring custom or local images”</a>, or
   automatically through an update of the Ceph package, the monitoring
   services can be updated.
  </p><p>
   To do so, use <code class="command">ceph orch reconfig</code> like so:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch reconfig node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch reconfig prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch reconfig alertmanager
<code class="prompt user">cephuser@adm &gt; </code>ceph orch reconfig grafana</pre></div><p>
   Currently no single command to update all monitoring services exists. The
   order in which these services are updated is not important.
  </p><div id="id-1.4.4.6.14.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    If you use custom container images, the URLs specified for the monitoring
    services will not change automatically if the Ceph packages are updated.
    If you have specified custom container images, you will need to specify the
    URLs of the new container images manually. This may be the case if you use
    a local container registry.
   </p><p>
    You can find the URLs of the recommended container images to be used in the
    <a class="xref" href="#monitoring-custom-images" title="16.1. Configuring custom or local images">Section 16.1, “Configuring custom or local images”</a> section.
   </p></div></section><section class="sect1" id="monitoring-stack-disable" data-id-title="Disabling monitoring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.3 </span><span class="title-name">Disabling monitoring</span> <a title="Permalink" class="permalink" href="#monitoring-stack-disable">#</a></h2></div></div></div><p>
   To disable the monitoring stack, run the following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch rm grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch rm prometheus --force   # this will delete metrics data collected so far
<code class="prompt user">cephuser@adm &gt; </code>ceph orch rm node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch rm alertmanager
<code class="prompt user">cephuser@adm &gt; </code>ceph mgr module disable prometheus</pre></div></section><section class="sect1" id="monitoring-grafana-config" data-id-title="Configuring Grafana"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.4 </span><span class="title-name">Configuring Grafana</span> <a title="Permalink" class="permalink" href="#monitoring-grafana-config">#</a></h2></div></div></div><p>
   The Ceph Dashboard back-end requires the Grafana URL to be able to verify the
   existence of Grafana Dashboards before the front-end even loads them.
   Because of the nature of how Grafana is implemented in Ceph Dashboard, this
   means that two working connections are required in order to be able to see
   Grafana graphs in Ceph Dashboard:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The back-end (Ceph MGR module) needs to verify the existence of the
     requested graph. If this request succeeds, it lets the front-end know that
     it can safely access Grafana.
    </p></li><li class="listitem"><p>
     The front-end then requests the Grafana graphs directly from the user's
     browser using an <code class="literal">iframe</code>. The Grafana instance is
     accessed directly without any detour through Ceph Dashboard.
    </p></li></ul></div><p>
   Now, it might be the case that your environment makes it difficult for the
   user's browser to directly access the URL configured in Ceph Dashboard. To
   solve this issue, a separate URL can be configured which will solely be used
   to tell the front-end (the user's browser) which URL it should use to access
   Grafana.
  </p><p>
   To change the URL that is returned to the front-end issue the following
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard set-grafana-frontend-api-url <em class="replaceable">GRAFANA-SERVER-URL</em></pre></div><p>
   If no value is set for that option, it will simply fall back to the value of
   the <em class="replaceable">GRAFANA_API_URL</em> option, which is set
   automatically and periodically updated by cephadm. If set, it will
   instruct the browser to use this URL to access Grafana.
  </p></section><section class="sect1" id="monitoring-cephadm-config" data-id-title="Configuring the Prometheus Manager Module"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.5 </span><span class="title-name">Configuring the Prometheus Manager Module</span> <a title="Permalink" class="permalink" href="#monitoring-cephadm-config">#</a></h2></div></div></div><p>
   The Prometheus Manager Module is a module inside Ceph that extends Ceph's
   functionality. The module reads (meta-)data from Ceph about its state and
   health, providing the (scraped) data in a consumable format to Prometheus.
  </p><div id="id-1.4.4.6.17.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The Prometheus Manager Module needs to be restarted for the configuration changes to
    be applied.
   </p></div><section class="sect2" id="monitoring-http-requests" data-id-title="Configuring the network interface"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.5.1 </span><span class="title-name">Configuring the network interface</span> <a title="Permalink" class="permalink" href="#monitoring-http-requests">#</a></h3></div></div></div><p>
    By default, the Prometheus Manager Module accepts HTTP requests on port 9283 on all
    IPv4 and IPv6 addresses on the host. The port and listen address are both
    configurable with <code class="option">ceph config-key set</code> , with keys
    <code class="option">mgr/prometheus/server_addr</code> and
    <code class="option">mgr/prometheus/server_port</code> . This port is registered
    withPrometheus's registry.
   </p><p>
    To update the <code class="literal">server_addr</code> execute the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/server_addr <em class="replaceable">0.0.0.0</em></pre></div><p>
    To update the <code class="literal">server_port</code> execute the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/server_port <em class="replaceable">9283</em></pre></div></section><section class="sect2" id="monitoring-scrape-intervals" data-id-title="Configuring scrape_interval"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.5.2 </span><span class="title-name">Configuring <code class="literal">scrape_interval</code></span> <a title="Permalink" class="permalink" href="#monitoring-scrape-intervals">#</a></h3></div></div></div><p>
    By default, the Prometheus Manager Module is configured with a scrape interval of 15
    seconds. We do not recommend using a scrape interval below 10 seconds. To
    set a different scrape interval in the Prometheus module, set
    <code class="literal">scrape_interval</code> to the desired value:
   </p><div id="id-1.4.4.6.17.5.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     To work properly and not cause any issues, the
     <code class="literal">scrape_interval</code> of this module should always be set to
     match the Prometheus scrape interval .
    </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/scrape_interval <em class="replaceable">15</em></pre></div></section><section class="sect2" id="monitoring-stale-cache" data-id-title="Configuring the cache"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.5.3 </span><span class="title-name">Configuring the cache</span> <a title="Permalink" class="permalink" href="#monitoring-stale-cache">#</a></h3></div></div></div><p>
    On large clusters (more than 1000 OSDs), the time to fetch the metrics may
    become significant. Without the cache, the Prometheus Manager Module can overload the
    manager and lead to unresponsive or crashing Ceph Manager instances. As a result,
    the cache is enabled by default and cannot be disabled, but this does mean
    that the cache can become stale. The cache is considered stale when the
    time to fetch the metrics from Ceph exceeds the configured
    <code class="literal">scrape_interval</code>.
   </p><p>
    If this is the case, a warning will be logged and the module will either:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Respond with a 503 HTTP status code (service unavailable).
     </p></li><li class="listitem"><p>
      Return the content of the cache, even though it might be stale.
     </p></li></ul></div><p>
    This behavior can be configured using the <code class="command">ceph config
    set</code> commands.
   </p><p>
    To tell the module to respond with possibly-stale data, set it to
    <code class="literal">return</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/stale_cache_strategy return</pre></div><p>
    To tell the module to respond with <code class="literal">service unavailable</code>,
    set it to <code class="literal">fail</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/stale_cache_strategy fail</pre></div></section><section class="sect2" id="monitoring-rbd-image" data-id-title="Enabling RBD-image monitoring"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">16.5.4 </span><span class="title-name">Enabling RBD-image monitoring</span> <a title="Permalink" class="permalink" href="#monitoring-rbd-image">#</a></h3></div></div></div><p>
    The Prometheus Manager Module can optionally collect RBD per-image IO statistics by
    enabling dynamic OSD performance counters. The statistics are gathered for
    all images in the pools that are specified in the
    <code class="literal">mgr/prometheus/rbd_stats_pools</code> configuration parameter.
   </p><p>
    The parameter is a comma- or space-separated list of
    <code class="literal">pool[/namespace]</code> entries. If the namespace is not
    specified, the statistics are collected for all namespaces in the pool.
   </p><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/rbd_stats_pools "<em class="replaceable">pool1,pool2,poolN</em>"</pre></div><p>
    The module scans the specified pools and namespaces and makes a list of all
    available images, and refreshes it periodically. The interval is
    configurable via the
    <code class="literal">mgr/prometheus/rbd_stats_pools_refresh_interval</code>
    parameter (in seconds), and is 300 seconds (five minutes) by default.
   </p><p>
    For example, if you changed the synchronization interval to 10 minutes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <em class="replaceable">600</em></pre></div></section></section><section class="sect1" id="prometheus-security-model" data-id-title="Prometheus security model"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.6 </span><span class="title-name">Prometheus security model</span> <a title="Permalink" class="permalink" href="#prometheus-security-model">#</a></h2></div></div></div><p>
   Prometheus' security model presumes that untrusted users have access to
   the Prometheus HTTP endpoint and logs. Untrusted users have access to all
   the (meta-)data Prometheus collects that is contained in the database,
   plus a variety of operational and debugging information.
  </p><p>
   However, Prometheus' HTTP API is limited to read-only operations.
   Configurations cannot be changed using the API, and secrets are not exposed.
   Moreover, Prometheus has some built-in measures to mitigate the impact of
   denial-of-service attacks.
  </p></section><section class="sect1" id="prometheus-webhook-snmp" data-id-title="Prometheus Alertmanager SNMP gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">16.7 </span><span class="title-name">Prometheus Alertmanager SNMP gateway</span> <a title="Permalink" class="permalink" href="#prometheus-webhook-snmp">#</a></h2></div></div></div><p>
   If you want to get notified about Prometheus alerts via SNMP traps, then
   you can install the Prometheus Alertmanager SNMP gateway via cephadm
   or the Ceph Dashboard. To do so for SNMPv2c, for example, you need to create a
   service and placement specification file with the following content:
  </p><div id="id-1.4.4.6.19.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    For more information on service and placement files, see
    <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.2 “Service and placement specification”</span>.
   </p></div><div class="verbatim-wrap"><pre class="screen">service_type: snmp-gateway
service_name: snmp-gateway
placement:
    <em class="replaceable">ADD_PLACEMENT_HERE</em>
spec:
  credentials:
    snmp_community: <em class="replaceable">ADD_COMMUNITY_STRING_HERE</em>
  snmp_destination: <em class="replaceable">ADD_FQDN_HERE</em>:<em class="replaceable">ADD_PORT_HERE</em>
  snmp_version: V2c</pre></div><p>
   Alternatively, you can use the Ceph Dashboard to deploy the SNMP gateway
   service for SNMPv2c and SNMPv3. For more details, refer to
   <a class="xref" href="#dashboard-cluster-services" title="4.4. Displaying services">Section 4.4, “Displaying services”</a>.
  </p></section></section></div><div class="part" id="part-storing-data" data-id-title="Storing Data in a Cluster"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Storing Data in a Cluster </span><a title="Permalink" class="permalink" href="#part-storing-data">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-storage-datamgm"><span class="title-number">17 </span><span class="title-name">Stored data management</span></a></span></li><dd class="toc-abstract"><p>The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OSDs directly rather than through a centralized server or broker. With an algorithmically determined method of storing and retrieving data, Ceph avoids a…</p></dd><li><span class="chapter"><a href="#ceph-pools"><span class="title-number">18 </span><span class="title-name">Manage storage pools</span></a></span></li><dd class="toc-abstract"><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p></dd><li><span class="chapter"><a href="#cha-ceph-erasure"><span class="title-number">19 </span><span class="title-name">Erasure coded pools</span></a></span></li><dd class="toc-abstract"><p>Ceph provides an alternative to the normal replication of data in pools, called erasure or erasure coded pool. Erasure pools do not provide all functionality of replicated pools (for example, they cannot store metadata for RBD pools), but require less raw storage. A default erasure pool capable of s…</p></dd><li><span class="chapter"><a href="#ceph-rbd"><span class="title-number">20 </span><span class="title-name">RADOS Block Device</span></a></span></li><dd class="toc-abstract"><p>A block is a sequence of bytes, for example a 4 MB block of data. Block-based storage interfaces are the most common way to store data with rotating media, such as hard disks, CDs, floppy disks. The ubiquity of block device interfaces makes a virtual block device an ideal candidate to interact with …</p></dd></ul></div><section class="chapter" id="cha-storage-datamgm" data-id-title="Stored data management"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">17 </span><span class="title-name">Stored data management</span> <a title="Permalink" class="permalink" href="#cha-storage-datamgm">#</a></h1></div></div></div><p>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers Ceph clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, Ceph
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </p><p>
  CRUSH requires a map of your cluster, and uses the CRUSH Map to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </p><p>
  CRUSH maps contain a list of OSDs, a list of 'buckets' for aggregating the
  devices into physical locations, and a list of rules that tell CRUSH how it
  should replicate data in a Ceph cluster's pools. By reflecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </p><p>
  After you deploy a Ceph cluster, a default CRUSH Map is generated. It is
  fine for your Ceph sandbox environment. However, when you deploy a
  large-scale data cluster, you should give significant consideration to
  developing a custom CRUSH Map, because it will help you manage your Ceph
  cluster, improve performance and ensure data safety.
 </p><p>
  For example, if an OSD goes down, a CRUSH Map can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </p><p>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </p><p>
  A custom CRUSH Map can also help you identify the physical locations where
  Ceph stores redundant copies of data when the placement group(s) (refer to
  <a class="xref" href="#op-pgs" title="17.4. Placement groups">Section 17.4, “Placement groups”</a>) associated with a failed host are in a degraded
  state.
 </p><p>
  There are three main sections to a CRUSH Map.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="#datamgm-devices" title="17.1. OSD devices">OSD devices</a> consist of any
    object storage device corresponding to a <code class="systemitem">ceph-osd</code>
    daemon.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-buckets" title="17.2. Buckets">Buckets</a> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </p></li><li class="listitem"><p>
    <a class="xref" href="#datamgm-rules" title="17.3. Rule sets">Rule sets</a> consist of the
    manner of selecting buckets.
   </p></li></ul></div><section class="sect1" id="datamgm-devices" data-id-title="OSD devices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.1 </span><span class="title-name">OSD devices</span> <a title="Permalink" class="permalink" href="#datamgm-devices">#</a></h2></div></div></div><p>
   To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   CRUSH Map.
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device <em class="replaceable">NUM</em> osd.<em class="replaceable">OSD_NAME</em> class <em class="replaceable">CLASS_NAME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3 class ssd</pre></div><p>
   As a general rule, an OSD daemon maps to a single disk.
  </p><section class="sect2" id="crush-devclasses" data-id-title="Device classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.1 </span><span class="title-name">Device classes</span> <a title="Permalink" class="permalink" href="#crush-devclasses">#</a></h3></div></div></div><p>
    The flexibility of the CRUSH Map in controlling data placement is one of
    the Ceph's strengths. It is also one of the most difficult parts of the
    cluster to manage. <span class="emphasis"><em>Device classes</em></span> automate the most
    common changes to CRUSH Maps that the administrator needed to do manually
    previously.
   </p><section class="sect3" id="crush-management-problem" data-id-title="The CRUSH management problem"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.1 </span><span class="title-name">The CRUSH management problem</span> <a title="Permalink" class="permalink" href="#crush-management-problem">#</a></h4></div></div></div><p>
     Ceph clusters are frequently built with multiple types of storage
     devices: HDD, SSD, NVMe, or even mixed classes of the above. We call these
     different types of storage devices <span class="emphasis"><em>device classes</em></span> to
     avoid confusion between the <span class="emphasis"><em>type</em></span> property of CRUSH
     buckets (for example, host, rack, row, see
     <a class="xref" href="#datamgm-buckets" title="17.2. Buckets">Section 17.2, “Buckets”</a> for more details). Ceph OSDs backed by
     SSDs are much faster than those backed by spinning disks, making them
     better suited for certain workloads. Ceph makes it easy to create RADOS
     pools for different data sets or workloads and to assign different CRUSH
     rules to control data placement for those pools.
    </p><div class="figure" id="id-1.4.5.2.12.7.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/device_classes.png"><img src="images/device_classes.png" width="70%" alt="OSDs with mixed device classes" title="OSDs with mixed device classes"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.1: </span><span class="title-name">OSDs with mixed device classes </span><a title="Permalink" class="permalink" href="#id-1.4.5.2.12.7.3.3">#</a></h6></div></div><p>
     However, setting up the CRUSH rules to place data only on a certain class
     of device is tedious. Rules work in terms of the CRUSH hierarchy, but if
     the devices are mixed into the same hosts or racks (as in the sample
     hierarchy above), they will (by default) be mixed together and appear in
     the same sub-trees of the hierarchy. Manually separating them out into
     separate trees involved creating multiple versions of each intermediate
     node for each device class in previous versions of SUSE Enterprise Storage.
    </p></section><section class="sect3" id="osd-crush-device-classes" data-id-title="Device classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.2 </span><span class="title-name">Device classes</span> <a title="Permalink" class="permalink" href="#osd-crush-device-classes">#</a></h4></div></div></div><p>
     An elegant solution that Ceph offers is to add a property called
     <span class="emphasis"><em>device class</em></span> to each OSD. By default, OSDs will
     automatically set their device classes to either 'hdd', 'ssd', or 'nvme'
     based on the hardware properties exposed by the Linux kernel. These device
     classes are reported in a new column of the <code class="command">ceph osd
     tree</code> command output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000</pre></div><p>
     If the automatic device class detection fails, for example because the
     device driver is not properly exposing information about the device via
     <code class="filename">/sys/block</code>, you can adjust device classes from the
     command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
<code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set-device-class ssd osd.2 osd.3
set osd(s) 2,3 to class 'ssd'</pre></div></section><section class="sect3" id="crush-placement-rules" data-id-title="Setting CRUSH placement rules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.3 </span><span class="title-name">Setting CRUSH placement rules</span> <a title="Permalink" class="permalink" href="#crush-placement-rules">#</a></h4></div></div></div><p>
     CRUSH rules can restrict placement to a specific device class. For
     example, you can create a 'fast'
     <span class="bold"><strong>replicated</strong></span> pool that distributes data
     only over SSD disks by running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule create-replicated <em class="replaceable">RULE_NAME</em> <em class="replaceable">ROOT</em> <em class="replaceable">FAILURE_DOMAIN_TYPE</em> <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule create-replicated fast default host ssd</pre></div><p>
     Create a pool named 'fast_pool' and assign it to the 'fast' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create fast_pool 128 128 replicated fast</pre></div><p>
     The process for creating <span class="bold"><strong>erasure code</strong></span>
     rules is slightly different. First, you create an erasure code profile
     that includes a property for your desired device class. Then, use that
     profile when creating the erasure coded pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd erasure-code-profile set myprofile \
 k=4 m=2 crush-device-class=ssd crush-failure-domain=host
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create mypool 64 erasure myprofile</pre></div><p>
     In case you need to manually edit the CRUSH Map to customize your rule,
     the syntax has been extended to allow the device class to be specified.
     For example, the CRUSH rule generated by the above commands looks as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rule ecpool {
  id 2
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default <span class="bold"><strong>class ssd</strong></span>
  step chooseleaf indep 0 type host
  step emit
}</pre></div><p>
     The important difference here is that the 'take' command includes the
     additional 'class <em class="replaceable">CLASS_NAME</em>' suffix.
    </p></section><section class="sect3" id="crush-additional-commands" data-id-title="Additional commands"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.4 </span><span class="title-name">Additional commands</span> <a title="Permalink" class="permalink" href="#crush-additional-commands">#</a></h4></div></div></div><p>
     To list device classes used in a CRUSH Map, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush class ls
[
  "hdd",
  "ssd"
]</pre></div><p>
     To list existing CRUSH rules, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule ls
replicated_rule
fast</pre></div><p>
     To view details of the CRUSH rule named 'fast', run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule dump fast
{
		"rule_id": 1,
		"rule_name": "fast",
		"ruleset": 1,
		"type": 1,
		"min_size": 1,
		"max_size": 10,
		"steps": [
						{
										"op": "take",
										"item": -21,
										"item_name": "default~ssd"
						},
						{
										"op": "chooseleaf_firstn",
										"num": 0,
										"type": "host"
						},
						{
										"op": "emit"
						}
		]
}</pre></div><p>
     To list OSDs that belong to an 'ssd' class, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush class ls-osd ssd
0
1</pre></div></section><section class="sect3" id="device-classes-reclassify" data-id-title="Migrating from a legacy SSD rule to device classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.5 </span><span class="title-name">Migrating from a legacy SSD rule to device classes</span> <a title="Permalink" class="permalink" href="#device-classes-reclassify">#</a></h4></div></div></div><p>
     In SUSE Enterprise Storage prior to version 5, you needed to manually edit the
     CRUSH Map and maintain a parallel hierarchy for each specialized device
     type (such as SSD) in order to write rules that apply to these devices.
     Since SUSE Enterprise Storage 5, the device class feature has enabled this
     transparently.
    </p><p>
     You can transform a legacy rule and hierarchy to the new class-based rules
     by using the <code class="command">crushtool</code> command. There are several types
     of transformation possible:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.12.7.7.4.1"><span class="term"><code class="command">crushtool --reclassify-root <em class="replaceable">ROOT_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This command takes everything in the hierarchy beneath
        <em class="replaceable">ROOT_NAME</em> and adjusts any rules that
        reference that root via
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em></pre></div><p>
        to instead
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em> class <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
        It renumbers the buckets so that the old IDs are used for the specified
        class's 'shadow tree'. As a consequence, no data movement occurs.
       </p><div class="complex-example"><div class="example" id="id-1.4.5.2.12.7.7.4.1.2.6" data-id-title="crushtool --reclassify-root"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 17.1: </span><span class="title-name"><code class="command">crushtool --reclassify-root</code> </span><a title="Permalink" class="permalink" href="#id-1.4.5.2.12.7.7.4.1.2.6">#</a></h6></div><div class="example-contents"><p>
         Consider the following existing rule:
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div><p>
         If you reclassify the root 'default' as class 'hdd', the rule will
         become
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default class hdd
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div></div></div></div></dd><dt id="id-1.4.5.2.12.7.7.4.2"><span class="term"><code class="command">crushtool --set-subtree-class <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This method marks every device in the subtree rooted at
        <em class="replaceable">BUCKET_NAME</em> with the specified device class.
       </p><p>
        <code class="option">--set-subtree-class</code> is normally used in conjunction
        with the <code class="option">--reclassify-root</code> option to ensure that all
        devices in that root are labeled with the correct class. However, some
        of those devices may intentionally have a different class, and
        therefore you do not want to relabel them. In such cases, exclude the
        <code class="option">--set-subtree-class</code> option. Keep in mind that such
        remapping will not be perfect, because the previous rule is distributed
        across devices of multiple classes but the adjusted rules will only map
        to devices of the specified device class.
       </p></dd><dt id="id-1.4.5.2.12.7.7.4.3"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">MATCH_PATTERN</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">DEFAULT_PATTERN</em></code></span></dt><dd><p>
        This method allows merging a parallel type-specific hierarchy with the
        normal hierarchy. For example, many users have CRUSH Maps similar to
        the following one:
       </p><div class="example" id="id-1.4.5.2.12.7.7.4.3.2.2" data-id-title="crushtool --reclassify-bucket"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 17.2: </span><span class="title-name"><code class="command">crushtool --reclassify-bucket</code> </span><a title="Permalink" class="permalink" href="#id-1.4.5.2.12.7.7.4.3.2.2">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">host node1 {
   id -2           # do not change unnecessarily
   # weight 109.152
   alg straw
   hash 0  # rjenkins1
   item osd.0 weight 9.096
   item osd.1 weight 9.096
   item osd.2 weight 9.096
   item osd.3 weight 9.096
   item osd.4 weight 9.096
   item osd.5 weight 9.096
   [...]
}

host node1-ssd {
   id -10          # do not change unnecessarily
   # weight 2.000
   alg straw
   hash 0  # rjenkins1
   item osd.80 weight 2.000
   [...]
}

root default {
   id -1           # do not change unnecessarily
   alg straw
   hash 0  # rjenkins1
   item node1 weight 110.967
   [...]
}

root ssd {
   id -18          # do not change unnecessarily
   # weight 16.000
   alg straw
   hash 0  # rjenkins1
   item node1-ssd weight 2.000
   [...]
}</pre></div></div></div><p>
        This function reclassifies each bucket that matches a given pattern.
        The pattern can look like <code class="literal">%suffix</code> or
        <code class="literal">prefix%</code>. In the above example, you would use the
        pattern <code class="literal">%-ssd</code>. For each matched bucket, the
        remaining portion of the name that matches the '%' wild card specifies
        the base bucket. All devices in the matched bucket are labeled with the
        specified device class and then moved to the base bucket. If the base
        bucket does not exist (for example, if 'node12-ssd' exists but 'node12'
        does not), then it is created and linked underneath the specified
        default parent bucket. The old bucket IDs are preserved for the new
        shadow buckets to prevent data movement. Rules with the
        <code class="literal">take</code> steps that reference the old buckets are
        adjusted.
       </p></dd><dt id="id-1.4.5.2.12.7.7.4.4"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">BASE_BUCKET</em></code></span></dt><dd><p>
        You can use the <code class="option">--reclassify-bucket</code> option without a
        wild card to map a single bucket. For example, in the previous example,
        we want the 'ssd' bucket to be mapped to the default bucket.
       </p><p>
        The final command to convert the map comprised of the above fragments
        would be as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap -o original
<code class="prompt user">cephuser@adm &gt; </code>crushtool -i original --reclassify \
  --set-subtree-class default hdd \
  --reclassify-root default hdd \
  --reclassify-bucket %-ssd ssd default \
  --reclassify-bucket ssd ssd default \
  -o adjusted</pre></div><p>
        In order to verify that the conversion is correct, there is a
        <code class="option">--compare</code> option that tests a large sample of inputs
        to the CRUSH Map and compares if the same result comes back out. These
        inputs are controlled by the same options that apply to the
        <code class="option">--test</code>. For the above example, the command would be as
        follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent</pre></div><div id="id-1.4.5.2.12.7.7.4.4.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
         If there were differences, you would see what ratio of inputs are
         remapped in the parentheses.
        </p></div><p>
        If you are satisfied with the adjusted CRUSH Map, you can apply it to
        the cluster:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd setcrushmap -i adjusted</pre></div></dd></dl></div></section><section class="sect3" id="id-1.4.5.2.12.7.8" data-id-title="For more information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.6 </span><span class="title-name">For more information</span> <a title="Permalink" class="permalink" href="#id-1.4.5.2.12.7.8">#</a></h4></div></div></div><p>
     Find more details on CRUSH Maps in <a class="xref" href="#op-crush" title="17.5. CRUSH Map manipulation">Section 17.5, “CRUSH Map manipulation”</a>.
    </p><p>
     Find more details on Ceph pools in general in
     <a class="xref" href="#ceph-pools" title="Chapter 18. Manage storage pools">Chapter 18, <em>Manage storage pools</em></a>.
    </p><p>
     Find more details about erasure coded pools in
     <a class="xref" href="#cha-ceph-erasure" title="Chapter 19. Erasure coded pools">Chapter 19, <em>Erasure coded pools</em></a>.
    </p></section></section></section><section class="sect1" id="datamgm-buckets" data-id-title="Buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.2 </span><span class="title-name">Buckets</span> <a title="Permalink" class="permalink" href="#datamgm-buckets">#</a></h2></div></div></div><p>
   CRUSH maps contain a list of OSDs, which can be organized into a
   tree-structured arrangement of buckets for aggregating the devices into
   physical locations. Individual OSDs comprise the leaves on the tree.
  </p><div class="informaltable"><table style="border: none;"><colgroup><col/><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        0
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        osd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A specific device or OSD (<code class="literal">osd.1</code>,
        <code class="literal">osd.2</code>, etc.).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        host
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The name of a host containing one or more OSDs.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        2
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        chassis
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Identifier for which chassis in the rack contains the
        <code class="literal">host</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        rack
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A computer rack. The default is <code class="literal">unknownrack</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        row
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A row in a series of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        5
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pdu
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Abbreviation for "Power Distribution Unit".
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        6
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pod
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Abbreviation for "Point of Delivery": in this context, a group of PDUs,
        or a group of rows of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        7
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        room
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A room containing rows of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        datacenter
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A physical data center containing one or more rooms.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        9
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        region
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Geographical region of the world (for example, NAM, LAM, EMEA, APAC
        etc.)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        10
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        root
       </p>
      </td><td>
       <p>
        The root node of the tree of OSD buckets (normally set to
        <code class="literal">default</code>).
       </p>
      </td></tr></tbody></table></div><div id="id-1.4.5.2.13.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    You can modify the existing types and create your own bucket types.
   </p></div><p>
   Ceph's deployment tools generate a CRUSH Map that contains a bucket for
   each host, and a root named 'default', which is useful for the default
   <code class="literal">rbd</code> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </p><p>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <code class="literal">straw2</code> by default), and
   the hash (<code class="literal">0</code> by default, reflecting CRUSH Hash
   <code class="literal">rjenkins1</code>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </p><div class="verbatim-wrap"><pre class="screen">[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</pre></div><p>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </p><div class="verbatim-wrap"><pre class="screen">host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</pre></div></section><section class="sect1" id="datamgm-rules" data-id-title="Rule sets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.3 </span><span class="title-name">Rule sets</span> <a title="Permalink" class="permalink" href="#datamgm-rules">#</a></h2></div></div></div><p>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default CRUSH Map has a rule for the default root. If you want more
   roots and more rules, you need to create them later or they will be created
   automatically when new pools are created.
  </p><div id="id-1.4.5.2.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </p></div><p>
   A rule takes the following form:
  </p><div class="verbatim-wrap"><pre class="screen">rule <em class="replaceable">rulename</em> {

        ruleset <em class="replaceable">ruleset</em>
        type <em class="replaceable">type</em>
        min_size <em class="replaceable">min-size</em>
        max_size <em class="replaceable">max-size</em>
        step <em class="replaceable">step</em>

}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.14.6.1"><span class="term">ruleset</span></dt><dd><p>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <code class="literal">0</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.2"><span class="term">type</span></dt><dd><p>
      A string. Describes a rule for either a 'replicated' or 'erasure' coded
      pool. This option is required. Default is <code class="literal">replicated</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.3"><span class="term">min_size</span></dt><dd><p>
      An integer. If a pool group makes fewer replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">2</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.4"><span class="term">max_size</span></dt><dd><p>
      An integer. If a pool group makes more replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">10</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.5"><span class="term">step take <em class="replaceable">bucket</em></span></dt><dd><p>
      Takes a bucket specified by a name, and begins iterating down the tree.
      This option is required. For an explanation about iterating through the
      tree, see <a class="xref" href="#datamgm-rules-step-iterate" title="17.3.1. Iterating the node tree">Section 17.3.1, “Iterating the node tree”</a>.
     </p></dd><dt id="id-1.4.5.2.14.6.6"><span class="term">step <em class="replaceable">target</em><em class="replaceable">mode</em><em class="replaceable">num</em> type <em class="replaceable">bucket-type</em></span></dt><dd><p>
      <em class="replaceable">target</em> can either be <code class="literal">choose</code>
      or <code class="literal">chooseleaf</code>. When set to <code class="literal">choose</code>,
      a number of buckets is selected. <code class="literal">chooseleaf</code> directly
      selects the OSDs (leaf nodes) from the sub-tree of each bucket in the set
      of buckets.
     </p><p>
      <em class="replaceable">mode</em> can either be <code class="literal">firstn</code>
      or <code class="literal">indep</code>. See
      <a class="xref" href="#datamgm-rules-step-mode" title="17.3.2. firstn and indep">Section 17.3.2, “<code class="literal">firstn</code> and <code class="literal">indep</code>”</a>.
     </p><p>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <em class="replaceable">num</em> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <em class="replaceable">num</em> &lt;
      0, it means N - <em class="replaceable">num</em>; and, if
      <em class="replaceable">num</em> == 0, choose N buckets (all available).
      Follows <code class="literal">step take</code> or <code class="literal">step choose</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.7"><span class="term">step emit</span></dt><dd><p>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <code class="literal">step choose</code>.
     </p></dd></dl></div><section class="sect2" id="datamgm-rules-step-iterate" data-id-title="Iterating the node tree"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.3.1 </span><span class="title-name">Iterating the node tree</span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate">#</a></h3></div></div></div><p>
    The structure defined with the buckets can be viewed as a node tree.
    Buckets are nodes and OSDs are leafs in this tree.
   </p><p>
    Rules in the CRUSH Map define how OSDs are selected from this tree. A rule
    starts with a node and then iterates down the tree to return a set of OSDs.
    It is not possible to define which branch needs to be selected. Instead the
    CRUSH algorithm assures that the set of OSDs fulfills the replication
    requirements and evenly distributes the data.
   </p><p>
    With <code class="literal">step take</code> <em class="replaceable">bucket</em> the
    iteration through the node tree begins at the given bucket (not bucket
    type). If OSDs from all branches in the tree are to be returned, the bucket
    must be the root bucket. Otherwise the following steps are only iterating
    through a sub-tree.
   </p><p>
    After <code class="literal">step take</code> one or more <code class="literal">step
    choose</code> entries follow in the rule definition. Each <code class="literal">step
    choose</code> chooses a defined number of nodes (or branches) from the
    previously selected upper node.
   </p><p>
    In the end the selected OSDs are returned with <code class="literal">step
    emit</code>.
   </p><p>
    <code class="literal">step chooseleaf</code> is a convenience function that directly
    selects OSDs from branches of the given bucket.
   </p><p>
    <a class="xref" href="#datamgm-rules-step-iterate-figure" title="Example tree">Figure 17.2, “Example tree”</a> provides an example of
    how <code class="literal">step</code> is used to iterate through a tree. The orange
    arrows and numbers correspond to <code class="literal">example1a</code> and
    <code class="literal">example1b</code>, while blue corresponds to
    <code class="literal">example2</code> in the following rule definitions.
   </p><div class="figure" id="datamgm-rules-step-iterate-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-step.png"><img src="images/crush-step.png" width="100%" alt="Example tree" title="Example tree"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.2: </span><span class="title-name">Example tree </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-iterate-figure">#</a></h6></div></div><div class="verbatim-wrap"><pre class="screen"># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</pre></div></section><section class="sect2" id="datamgm-rules-step-mode" data-id-title="firstn and indep"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.3.2 </span><span class="title-name"><code class="literal">firstn</code> and <code class="literal">indep</code></span> <a title="Permalink" class="permalink" href="#datamgm-rules-step-mode">#</a></h3></div></div></div><p>
    A CRUSH rule defines replacements for failed nodes or OSDs (see
    <a class="xref" href="#datamgm-rules" title="17.3. Rule sets">Section 17.3, “Rule sets”</a>). The keyword <code class="literal">step</code>
    requires either <code class="literal">firstn</code> or <code class="literal">indep</code> as
    parameter. <a class="xref" href="#datamgm-rules-step-mode-indep-figure" title="Node replacement methods">Figure 17.3, “Node replacement methods”</a> provides
    an example.
   </p><p>
    <code class="literal">firstn</code> adds replacement nodes to the end of the list of
    active nodes. In case of a failed node, the following healthy nodes are
    shifted to the left to fill the gap of the failed node. This is the default
    and desired method for <span class="emphasis"><em>replicated pools</em></span>, because a
    secondary node already has all data and therefore can take over the duties
    of the primary node immediately.
   </p><p>
    <code class="literal">indep</code> selects fixed replacement nodes for each active
    node. The replacement of a failed node does not change the order of the
    remaining nodes. This is desired for <span class="emphasis"><em>erasure coded
    pools</em></span>. In erasure coded pools the data stored on a node depends
    on its position in the node selection. When the order of nodes changes, all
    data on affected nodes needs to be relocated.
   </p><div class="figure" id="datamgm-rules-step-mode-indep-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-firstn-indep.png"><img src="images/crush-firstn-indep.png" width="100%" alt="Node replacement methods" title="Node replacement methods"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.3: </span><span class="title-name">Node replacement methods </span><a title="Permalink" class="permalink" href="#datamgm-rules-step-mode-indep-figure">#</a></h6></div></div></section></section><section class="sect1" id="op-pgs" data-id-title="Placement groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.4 </span><span class="title-name">Placement groups</span> <a title="Permalink" class="permalink" href="#op-pgs">#</a></h2></div></div></div><p>
   Ceph maps objects to placement groups (PGs). Placement groups are shards
   or fragments of a logical object pool that place objects as a group into
   OSDs. Placement groups reduce the amount of per-object metadata when Ceph
   stores the data in OSDs. A larger number of placement groups—for
   example, 100 per OSD—leads to better balancing.
  </p><section class="sect2" id="op-pgs-usage" data-id-title="Using placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.1 </span><span class="title-name">Using placement groups</span> <a title="Permalink" class="permalink" href="#op-pgs-usage">#</a></h3></div></div></div><p>
    A placement group (PG) aggregates objects within a pool. The main reason is
    that tracking object placement and metadata on a per-object basis is
    computationally expensive. For example, a system with millions of objects
    cannot track placement of each of its objects directly.
   </p><div class="figure" id="id-1.4.5.2.15.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_schema.png"><img src="images/ceph_pgs_schema.png" width="70%" alt="Placement groups in a pool" title="Placement groups in a pool"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.4: </span><span class="title-name">Placement groups in a pool </span><a title="Permalink" class="permalink" href="#id-1.4.5.2.15.3.3">#</a></h6></div></div><p>
    The Ceph client will calculate to which placement group an object will
    belong to. It does this by hashing the object ID and applying an operation
    based on the number of PGs in the defined pool and the ID of the pool.
   </p><p>
    The object's contents within a placement group are stored in a set of OSDs.
    For example, in a replicated pool of size two, each placement group will
    store objects on two OSDs:
   </p><div class="figure" id="id-1.4.5.2.15.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_osds.png"><img src="images/ceph_pgs_osds.png" width="70%" alt="Placement groups and OSDs" title="Placement groups and OSDs"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 17.5: </span><span class="title-name">Placement groups and OSDs </span><a title="Permalink" class="permalink" href="#id-1.4.5.2.15.3.6">#</a></h6></div></div><p>
    If OSD #2 fails, another OSD will be assigned to placement group #1 and
    will be filled with copies of all objects in OSD #1. If the pool size is
    changed from two to three, an additional OSD will be assigned to the
    placement group and will receive copies of all objects in the placement
    group.
   </p><p>
    Placement groups do not own the OSD, they share it with other placement
    groups from the same pool or even other pools. If OSD #2 fails, the
    placement group #2 will also need to restore copies of objects, using OSD
    #3.
   </p><p>
    When the number of placement groups increases, the new placement groups
    will be assigned OSDs. The result of the CRUSH function will also change
    and some objects from the former placement groups will be copied over to
    the new placement groups and removed from the old ones.
   </p></section><section class="sect2" id="op-pgs-pg-num" data-id-title="Determining the value of PG_NUM"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.2 </span><span class="title-name">Determining the value of <em class="replaceable">PG_NUM</em></span> <a title="Permalink" class="permalink" href="#op-pgs-pg-num">#</a></h3></div></div></div><div id="id-1.4.5.2.15.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Since Ceph Nautilus (v14.x), you can use the Ceph Manager
     <code class="literal">pg_autoscaler</code> module to auto-scale the PGs as needed.
     If you want to enable this feature, refer to
     <span class="intraxref">Book “Deploying and Administering SUSE Enterprise Storage with Rook”, Chapter 8 “Configuration”, Section 8.1.1.1 “Default PG and PGP counts”</span>.
    </p></div><p>
    When creating a new pool, you can still choose the value of
    <em class="replaceable">PG_NUM</em> manually:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool create <em class="replaceable">POOL_NAME</em> <em class="replaceable">PG_NUM</em></pre></div><p>
    <em class="replaceable">PG_NUM</em> cannot be calculated automatically.
    Following are a few commonly used values, depending on the number of OSDs
    in the cluster:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.15.4.6.1"><span class="term">Less than 5 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 128.
      </p></dd><dt id="id-1.4.5.2.15.4.6.2"><span class="term">Between 5 and 10 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 512.
      </p></dd><dt id="id-1.4.5.2.15.4.6.3"><span class="term">Between 10 and 50 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 1024.
      </p></dd></dl></div><p>
    As the number of OSDs increases, choosing the right value for
    <em class="replaceable">PG_NUM</em> becomes more important.
    <em class="replaceable">PG_NUM</em> strongly affects the behavior of the
    cluster as well as the durability of the data in case of OSD failure.
   </p><section class="sect3" id="op-pgs-choosing" data-id-title="Calculating placement groups for more than 50 OSDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.2.1 </span><span class="title-name">Calculating placement groups for more than 50 OSDs</span> <a title="Permalink" class="permalink" href="#op-pgs-choosing">#</a></h4></div></div></div><p>
     If you have less than 50 OSDs, use the preselection described in
     <a class="xref" href="#op-pgs-pg-num" title="17.4.2. Determining the value of PG_NUM">Section 17.4.2, “Determining the value of <em class="replaceable">PG_NUM</em>”</a>. If you have more than 50 OSDs, we
     recommend approximately 50-100 placement groups per OSD to balance out
     resource usage, data durability, and distribution. For a single pool of
     objects, you can use the following formula to get a baseline:
    </p><div class="verbatim-wrap"><pre class="screen">total PGs = (OSDs * 100) / <em class="replaceable">POOL_SIZE</em></pre></div><p>
     Where <em class="replaceable">POOL_SIZE</em> is either the number of
     replicas for replicated pools, or the 'k'+'m' sum for erasure coded pools
     as returned by the <code class="command">ceph osd erasure-code-profile get</code>
     command. You should round the result up to the nearest power of 2.
     Rounding up is recommended for the CRUSH algorithm to evenly balance the
     number of objects among placement groups.
    </p><p>
     As an example, for a cluster with 200 OSDs and a pool size of 3 replicas,
     you would estimate the number of PGs as follows:
    </p><div class="verbatim-wrap"><pre class="screen">          (200 * 100) / 3 = 6667</pre></div><p>
     The nearest power of 2 is <span class="bold"><strong>8192</strong></span>.
    </p><p>
     When using multiple data pools for storing objects, you need to ensure
     that you balance the number of placement groups per pool with the number
     of placement groups per OSD. You need to reach a reasonable total number
     of placement groups that provides reasonably low variance per OSD without
     taxing system resources or making the peering process too slow.
    </p><p>
     For example, a cluster of 10 pools, each with 512 placement groups on 10
     OSDs, is a total of 5,120 placement groups spread over 10 OSDs, that is
     512 placement groups per OSD. Such a setup does not use too many
     resources. However, if 1000 pools were created with 512 placement groups
     each, the OSDs would handle approximately 50,000 placement groups each and
     it would require significantly more resources and time for peering.
    </p></section></section><section class="sect2" id="op-pg-set" data-id-title="Setting the number of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.3 </span><span class="title-name">Setting the number of placement groups</span> <a title="Permalink" class="permalink" href="#op-pg-set">#</a></h3></div></div></div><div id="id-1.4.5.2.15.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Since Ceph Nautilus (v14.x), you can use the Ceph Manager
     <code class="literal">pg_autoscaler</code> module to auto-scale the PGs as needed.
     If you want to enable this feature, refer to
     <span class="intraxref">Book “Deploying and Administering SUSE Enterprise Storage with Rook”, Chapter 8 “Configuration”, Section 8.1.1.1 “Default PG and PGP counts”</span>.
    </p></div><p>
    If you still need to specify the number of placement groups in a pool
    manually, you need to specify them at the time of pool creation (see
    <a class="xref" href="#ceph-pools-operate-add-pool" title="18.1. Creating a pool">Section 18.1, “Creating a pool”</a>). Once you have set
    placement groups for a pool, you may increase the number of placement
    groups by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">PG_NUM</em></pre></div><p>
    After you increase the number of placement groups, you also need to
    increase the number of placement groups for placement
    (<code class="option">PGP_NUM</code>) before your cluster will rebalance.
    <code class="option">PGP_NUM</code> will be the number of placement groups that will
    be considered for placement by the CRUSH algorithm. Increasing
    <code class="option">PG_NUM</code> splits the placement groups but data will not be
    migrated to the newer placement groups until <code class="option">PGP_NUM</code> is
    increased. <code class="option">PGP_NUM</code> should be equal to
    <code class="option">PG_NUM</code>. To increase the number of placement groups for
    placement, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pgp_num <em class="replaceable">PGP_NUM</em></pre></div></section><section class="sect2" id="op-pg-get" data-id-title="Finding the number of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.4 </span><span class="title-name">Finding the number of placement groups</span> <a title="Permalink" class="permalink" href="#op-pg-get">#</a></h3></div></div></div><p>
    To find out the number of placement groups in a pool, run the following
    <code class="command">get</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> pg_num</pre></div></section><section class="sect2" id="op-pg-getpgstat" data-id-title="Finding a clusters PG statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.5 </span><span class="title-name">Finding a cluster's PG statistics</span> <a title="Permalink" class="permalink" href="#op-pg-getpgstat">#</a></h3></div></div></div><p>
    To find out the statistics for the placement groups in your cluster, run
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg dump [--format <em class="replaceable">FORMAT</em>]</pre></div><p>
    Valid formats are 'plain' (default) and 'json'.
   </p></section><section class="sect2" id="op-pg-getstuckstat" data-id-title="Finding statistics for stuck PGs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.6 </span><span class="title-name">Finding statistics for stuck PGs</span> <a title="Permalink" class="permalink" href="#op-pg-getstuckstat">#</a></h3></div></div></div><p>
    To find out the statistics for all placement groups stuck in a specified
    state, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg dump_stuck <em class="replaceable">STATE</em> \
 [--format <em class="replaceable">FORMAT</em>] [--threshold <em class="replaceable">THRESHOLD</em>]</pre></div><p>
    <em class="replaceable">STATE</em> is one of 'inactive' (PGs cannot process
    reads or writes because they are waiting for an OSD with the most
    up-to-date data to come up), 'unclean' (PGs contain objects that are not
    replicated the desired number of times), 'stale' (PGs are in an unknown
    state—the OSDs that host them have not reported to the monitor
    cluster in a time interval specified by the
    <code class="option">mon_osd_report_timeout</code> option), 'undersized', or
    'degraded'.
   </p><p>
    Valid formats are 'plain' (default) and 'json'.
   </p><p>
    The threshold defines the minimum number of seconds the placement group is
    stuck before including it in the returned statistics (300 seconds by
    default).
   </p></section><section class="sect2" id="op-pgs-pgmap" data-id-title="Searching a placement group map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.7 </span><span class="title-name">Searching a placement group map</span> <a title="Permalink" class="permalink" href="#op-pgs-pgmap">#</a></h3></div></div></div><p>
    To search for the placement group map for a particular placement group, run
    the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg map <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph will return the placement group map, the placement group, and the
    OSD status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg map 1.6c
osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</pre></div></section><section class="sect2" id="op-pg-pgstats" data-id-title="Retrieving a placement groups statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.8 </span><span class="title-name">Retrieving a placement groups statistics</span> <a title="Permalink" class="permalink" href="#op-pg-pgstats">#</a></h3></div></div></div><p>
    To retrieve statistics for a particular placement group, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg <em class="replaceable">PG_ID</em> query</pre></div></section><section class="sect2" id="op-pg-scrubpg" data-id-title="Scrubbing a placement group"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.9 </span><span class="title-name">Scrubbing a placement group</span> <a title="Permalink" class="permalink" href="#op-pg-scrubpg">#</a></h3></div></div></div><p>
    To scrub (<a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) a placement group, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg scrub <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph checks the primary and replica nodes, generates a catalog of all
    objects in the placement group, and compares them to ensure that no objects
    are missing or mismatched and their contents are consistent. Assuming the
    replicas all match, a final semantic sweep ensures that all of the
    snapshot-related object metadata is consistent. Errors are reported via
    logs.
   </p></section><section class="sect2" id="op-pg-backfill" data-id-title="Prioritizing backfill and recovery of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.10 </span><span class="title-name">Prioritizing backfill and recovery of placement groups</span> <a title="Permalink" class="permalink" href="#op-pg-backfill">#</a></h3></div></div></div><p>
    You may run into a situation where several placement groups require
    recovery and/or back-fill, while some groups hold data more important than
    others. For example, those PGs may hold data for images used by running
    machines and other PGs may be used by inactive machines or less relevant
    data. In that case, you may want to prioritize recovery of those groups so
    that performance and availability of data stored on those groups is
    restored earlier. To mark particular placement groups as prioritized during
    backfill or recovery, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt root"># </code>ceph pg force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    This will cause Ceph to perform recovery or backfill on specified
    placement groups first, before other placement groups. This does not
    interrupt currently ongoing backfills or recovery, but causes specified PGs
    to be processed as soon as possible. If you change your mind or prioritize
    wrong groups, cancel the prioritization:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg cancel-force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt root"># </code>ceph pg cancel-force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    The <code class="command">cancel-*</code> commands remove the 'force' flag from the
    PGs so that they are processed in default order. Again, this does not
    affect placement groups currently being processed, only those that are
    still queued. The 'force' flag is cleared automatically after recovery or
    backfill of the group is done.
   </p></section><section class="sect2" id="op-pgs-revert" data-id-title="Reverting lost objects"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.11 </span><span class="title-name">Reverting lost objects</span> <a title="Permalink" class="permalink" href="#op-pgs-revert">#</a></h3></div></div></div><p>
    If the cluster has lost one or more objects and you have decided to abandon
    the search for the lost data, you need to mark the unfound objects as
    'lost'.
   </p><p>
    If the objects are still lost after having queried all possible locations,
    you may need to give up on the lost objects. This is possible given unusual
    combinations of failures that allow the cluster to learn about writes that
    were performed before the writes themselves are recovered.
   </p><p>
    Currently the only supported option is 'revert', which will either roll
    back to a previous version of the object, or forget about it entirely in
    case of a new object. To mark the 'unfound' objects as 'lost', run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">  <code class="prompt user">cephuser@adm &gt; </code>ceph pg <em class="replaceable">PG_ID</em> mark_unfound_lost revert|delete</pre></div></section><section class="sect2" id="op-pgs-autoscaler" data-id-title="Enabling the PG auto-scaler"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.12 </span><span class="title-name">Enabling the PG auto-scaler</span> <a title="Permalink" class="permalink" href="#op-pgs-autoscaler">#</a></h3></div></div></div><p>
    Placement groups (PGs) are an internal implementation detail of how Ceph
    distributes data. By enabling pg-autoscaling, you can allow the cluster to
    either make or automatically tune PGs based on how the cluster is used.
   </p><p>
    Each pool in the system has a <code class="option">pg_autoscale_mode</code> property
    that can be set to <code class="literal">off</code>, <code class="literal">on</code>, or
    <code class="literal">warn</code>:
   </p><p>
    The autoscaler is configured on a per-pool basis, and can run in three
    modes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.15.14.5.1"><span class="term">off</span></dt><dd><p>
       Disable autoscaling for this pool. It is up to the administrator to
       choose an appropriate PG number for each pool.
      </p></dd><dt id="id-1.4.5.2.15.14.5.2"><span class="term">on</span></dt><dd><p>
       Enable automated adjustments of the PG count for the given pool.
      </p></dd><dt id="id-1.4.5.2.15.14.5.3"><span class="term">warn</span></dt><dd><p>
       Raise health alerts when the PG count should be adjusted.
      </p></dd></dl></div><p>
    To set the autoscaling mode for existing pools:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_autoscale_mode <em class="replaceable">mode</em></pre></div><p>
    You can also configure the default <code class="option">pg_autoscale_mode</code> that
    is applied to any pools that are created in the future with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set global osd_pool_default_pg_autoscale_mode <em class="replaceable">MODE</em></pre></div><p>
    You can view each pool, its relative utilization, and any suggested changes
    to the PG count with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool autoscale-status</pre></div></section></section><section class="sect1" id="op-crush" data-id-title="CRUSH Map manipulation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.5 </span><span class="title-name">CRUSH Map manipulation</span> <a title="Permalink" class="permalink" href="#op-crush">#</a></h2></div></div></div><p>
   This section introduces ways to basic CRUSH Map manipulation, such as
   editing a CRUSH Map, changing CRUSH Map parameters, and
   adding/moving/removing an OSD.
  </p><section class="sect2" id="id-1.4.5.2.16.3" data-id-title="Editing a CRUSH Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.1 </span><span class="title-name">Editing a CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.4.5.2.16.3">#</a></h3></div></div></div><p>
    To edit an existing CRUSH map, do the following:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Get a CRUSH Map. To get the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap -o <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will output (<code class="option">-o</code>) a compiled CRUSH Map to the
      file name you specified. Since the CRUSH Map is in a compiled form, you
      must decompile it first before you can edit it.
     </p></li><li class="step"><p>
      Decompile a CRUSH Map. To decompile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -d <em class="replaceable">compiled-crushmap-filename</em> \
 -o <em class="replaceable">decompiled-crushmap-filename</em></pre></div><p>
      Ceph will decompile (<code class="option">-d</code>) the compiled CRUSH Mapand
      output (<code class="option">-o</code>) it to the file name you specified.
     </p></li><li class="step"><p>
      Edit at least one of Devices, Buckets and Rules parameters.
     </p></li><li class="step"><p>
      Compile a CRUSH Map. To compile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -c <em class="replaceable">decompiled-crush-map-filename</em> \
 -o <em class="replaceable">compiled-crush-map-filename</em></pre></div><p>
      Ceph will store a compiled CRUSH Mapto the file name you specified.
     </p></li><li class="step"><p>
      Set a CRUSH Map. To set the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd setcrushmap -i <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will input the compiled CRUSH Map of the file name you specified
      as the CRUSH Map for the cluster.
     </p></li></ol></div></div><div id="id-1.4.5.2.16.3.4" data-id-title="Use versioning system" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use versioning system</h6><p>
     Use a versioning system—such as git or svn—for the exported
     and modified CRUSH Map files. It makes a possible rollback simple.
    </p></div><div id="id-1.4.5.2.16.3.5" data-id-title="Test the new CRUSH Map" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Test the new CRUSH Map</h6><p>
     Test the new adjusted CRUSH Map using the <code class="command">crushtool
     --test</code> command, and compare to the state before applying the new
     CRUSH Map. You may find the following command switches useful:
     <code class="option">--show-statistics</code>, <code class="option">--show-mappings</code>,
     <code class="option">--show-bad-mappings</code>, <code class="option">--show-utilization</code>,
     <code class="option">--show-utilization-all</code>,
     <code class="option">--show-choose-tries</code>
    </p></div></section><section class="sect2" id="op-crush-addosd" data-id-title="Adding or moving an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.2 </span><span class="title-name">Adding or moving an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-addosd">#</a></h3></div></div></div><p>
    To add or move an OSD in the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set <em class="replaceable">id_or_name</em> <em class="replaceable">weight</em> root=<em class="replaceable">pool-name</em>
<em class="replaceable">bucket-type</em>=<em class="replaceable">bucket-name</em> ...</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.16.4.4.1"><span class="term">id</span></dt><dd><p>
       An integer. The numeric ID of the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.2"><span class="term">name</span></dt><dd><p>
       A string. The full name of the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.3"><span class="term">weight</span></dt><dd><p>
       A double. The CRUSH weight for the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.4"><span class="term">root</span></dt><dd><p>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.5"><span class="term">bucket-type</span></dt><dd><p>
       Key/value pairs. You may specify the OSD's location in the CRUSH
       hierarchy.
      </p></dd></dl></div><p>
    The following example adds <code class="literal">osd.0</code> to the hierarchy, or
    moves the OSD from a previous location.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-osdweight" data-id-title="Difference between ceph osd reweight and ceph osd crush reweight"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.3 </span><span class="title-name">Difference between <code class="command">ceph osd reweight</code> and <code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#op-crush-osdweight">#</a></h3></div></div></div><p>
    There are two similar commands that change the 'weight' of a Ceph OSD. The
    context of their usage is different and may cause confusion.
   </p><section class="sect3" id="ceph-osd-reweight" data-id-title="ceph osd reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.5.3.1 </span><span class="title-name"><code class="command">ceph osd reweight</code></span> <a title="Permalink" class="permalink" href="#ceph-osd-reweight">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd reweight</code> sets an override weight on the Ceph OSD.
     This value is in the range of 0 to 1, and forces CRUSH to reposition the
     data that would otherwise live on this drive. It does
     <span class="bold"><strong>not</strong></span> change the weights
     assigned to the buckets above the OSD, and is a corrective measure in case
     the normal CRUSH distribution is not working out quite right. For example,
     if one of your OSDs is at 90% and the others are at 40%, you could reduce
     this weight to try and compensate for it.
    </p><div id="id-1.4.5.2.16.5.3.5" data-id-title="OSD weight is temporary" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: OSD weight is temporary</h6><p>
      Note that <code class="command">ceph osd reweight</code> is not a persistent
      setting. When an OSD gets marked out, its weight will be set to 0 and
      when it gets marked in again, the weight will be changed to 1.
     </p></div></section><section class="sect3" id="ceph-osd-crush-reweight" data-id-title="ceph osd crush reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.5.3.2 </span><span class="title-name"><code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="#ceph-osd-crush-reweight">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd crush reweight</code> sets the
     <span class="bold"><strong>CRUSH</strong></span> weight of the OSD. This
     weight is an arbitrary value—generally the size of the disk in
     TB—and controls how much data the system tries to allocate to the
     OSD.
    </p></section></section><section class="sect2" id="op-crush-osdremove" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.4 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="#op-crush-osdremove">#</a></h3></div></div></div><p>
    To remove an OSD from the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove <em class="replaceable">OSD_NAME</em></pre></div></section><section class="sect2" id="op-crush-addbaucket" data-id-title="Adding a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.5 </span><span class="title-name">Adding a bucket</span> <a title="Permalink" class="permalink" href="#op-crush-addbaucket">#</a></h3></div></div></div><p>
    To add a bucket to the CRUSH Map of a running cluster, execute the
    <code class="command">ceph osd crush add-bucket</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush add-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em></pre></div></section><section class="sect2" id="op-crush-movebucket" data-id-title="Moving a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.6 </span><span class="title-name">Moving a bucket</span> <a title="Permalink" class="permalink" href="#op-crush-movebucket">#</a></h3></div></div></div><p>
    To move a bucket to a different location or position in the CRUSH Map
    hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush move <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em>=<em class="replaceable">BUCKET_NAME</em> [...]</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-rmbucket" data-id-title="Removing a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.7 </span><span class="title-name">Removing a bucket</span> <a title="Permalink" class="permalink" href="#op-crush-rmbucket">#</a></h3></div></div></div><p>
    To remove a bucket from the CRUSH Map hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove <em class="replaceable">BUCKET_NAME</em></pre></div><div id="id-1.4.5.2.16.9.4" data-id-title="Empty bucket only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Empty bucket only</h6><p>
     A bucket must be empty before removing it from the CRUSH hierarchy.
    </p></div></section></section><section class="sect1" id="scrubbing-pgs" data-id-title="Scrubbing placement groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.6 </span><span class="title-name">Scrubbing placement groups</span> <a title="Permalink" class="permalink" href="#scrubbing-pgs">#</a></h2></div></div></div><p>
   In addition to making multiple copies of objects, Ceph ensures data
   integrity by <span class="emphasis"><em>scrubbing</em></span> placement groups (find more
   information about placement groups in
   <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.3.2 “Placement groups”</span>). Ceph scrubbing is analogous
   to running <code class="command">fsck</code> on the object storage layer. For each
   placement group, Ceph generates a catalog of all objects and compares each
   primary object and its replicas to ensure that no objects are missing or
   mismatched. Daily light scrubbing checks the object size and attributes,
   while weekly deep scrubbing reads the data and uses checksums to ensure data
   integrity.
  </p><p>
   Scrubbing is important for maintaining data integrity, but it can reduce
   performance. You can adjust the following settings to increase or decrease
   scrubbing operations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.17.4.1"><span class="term"><code class="option">osd max scrubs</code></span></dt><dd><p>
      The maximum number of simultaneous scrub operations for a Ceph OSD. Default
      is 1.
     </p></dd><dt id="id-1.4.5.2.17.4.2"><span class="term"><code class="option">osd scrub begin hour</code>, <code class="option">osd scrub end hour</code></span></dt><dd><p>
      The hours of day (0 to 24) that define a time window during which the
      scrubbing can happen. By default, begins at 0 and ends at 24.
     </p><div id="id-1.4.5.2.17.4.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       If the placement group's scrub interval exceeds the <code class="option">osd scrub
       max interval</code> setting, the scrub will happen no matter what time
       window you define for scrubbing.
      </p></div></dd><dt id="id-1.4.5.2.17.4.3"><span class="term"><code class="option">osd scrub during recovery</code></span></dt><dd><p>
      Allows scrubs during recovery. Setting this to 'false' will disable
      scheduling new scrubs while there is an active recovery. Already running
      scrubs will continue. This option is useful for reducing load on busy
      clusters. Default is 'true'.
     </p></dd><dt id="id-1.4.5.2.17.4.4"><span class="term"><code class="option">osd scrub thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub thread times out. Default is
      60.
     </p></dd><dt id="id-1.4.5.2.17.4.5"><span class="term"><code class="option">osd scrub finalize thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub finalize thread times out.
      Default is 60*10.
     </p></dd><dt id="id-1.4.5.2.17.4.6"><span class="term"><code class="option">osd scrub load threshold</code></span></dt><dd><p>
      The normalized maximum load. Ceph will not scrub when the system load
      (as defined by the ratio of <code class="literal">getloadavg()</code> / number of
      <code class="literal">online cpus</code>) is higher than this number. Default is
      0.5.
     </p></dd><dt id="id-1.4.5.2.17.4.7"><span class="term"><code class="option">osd scrub min interval</code></span></dt><dd><p>
      The minimal interval in seconds for scrubbing Ceph OSD when the Ceph
      cluster load is low. Default is 60*60*24 (once a day).
     </p></dd><dt id="id-1.4.5.2.17.4.8"><span class="term"><code class="option">osd scrub max interval</code></span></dt><dd><p>
      The maximum interval in seconds for scrubbing Ceph OSD, irrespective of
      cluster load. Default is 7*60*60*24 (once a week).
     </p></dd><dt id="id-1.4.5.2.17.4.9"><span class="term"><code class="option">osd scrub chunk min</code></span></dt><dd><p>
      The minimum number of object store chunks to scrub during a single
      operation. Ceph blocks writes to a single chunk during a scrub. Default
      is 5.
     </p></dd><dt id="id-1.4.5.2.17.4.10"><span class="term"><code class="option">osd scrub chunk max</code></span></dt><dd><p>
      The maximum number of object store chunks to scrub during a single
      operation. Default is 25.
     </p></dd><dt id="id-1.4.5.2.17.4.11"><span class="term"><code class="option">osd scrub sleep</code></span></dt><dd><p>
      Time to sleep before scrubbing the next group of chunks. Increasing this
      value slows down the whole scrub operation, while client operations are
      less impacted. Default is 0.
     </p></dd><dt id="id-1.4.5.2.17.4.12"><span class="term"><code class="option">osd deep scrub interval</code></span></dt><dd><p>
      The interval for 'deep' scrubbing (fully reading all data). The
      <code class="option">osd scrub load threshold</code> option does not affect this
      setting. Default is 60*60*24*7 (once a week).
     </p></dd><dt id="id-1.4.5.2.17.4.13"><span class="term"><code class="option">osd scrub interval randomize ratio</code></span></dt><dd><p>
      Add a random delay to the <code class="option">osd scrub min interval</code> value
      when scheduling the next scrub job for a placement group. The delay is a
      random value smaller than the result of <code class="option">osd scrub min
      interval</code> * <code class="option">osd scrub interval randomized ratio</code>.
      Therefore, the default setting practically randomly spreads the scrubs
      out in the allowed time window of [1, 1.5] * <code class="option">osd scrub min
      interval</code>. Default is 0.5.
     </p></dd><dt id="id-1.4.5.2.17.4.14"><span class="term"><code class="option">osd deep scrub stride</code></span></dt><dd><p>
      Read size when doing a deep scrub. Default is 524288 (512 kB).
     </p></dd></dl></div></section></section><section class="chapter" id="ceph-pools" data-id-title="Manage storage pools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">18 </span><span class="title-name">Manage storage pools</span> <a title="Permalink" class="permalink" href="#ceph-pools">#</a></h1></div></div></div><p>
  Ceph stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, Ceph uses
  the default pools for storing data. The following important highlights relate
  to Ceph pools:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Resilience</em></span>: Ceph pools provide resilience by
    replicating or encoding the data contained within them. Each pool can be
    set to either <code class="literal">replicated</code> or <code class="literal">erasure
    coding</code>. For replicated pools, you further set the number of
    replicas, or copies, which each data object within the pool will have. The
    number of copies (OSDs, CRUSH buckets/leaves) that can be lost is one less
    than the number of replicas. With erasure coding, you set the values of
    <code class="option">k</code> and <code class="option">m</code>, where <code class="option">k</code> is the
    number of data chunks and <code class="option">m</code> is the number of coding
    chunks. For erasure coded pools, it is the number of coding chunks that
    determines how many OSDs (CRUSH buckets/leaves) can be lost without losing
    data.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Placement Groups</em></span>: You can set the number of placement
    groups for the pool. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>CRUSH Rules</em></span>: When you store data in a pool, objects
    and its replicas (or chunks in case of erasure coded pools) are placed
    according to the CRUSH ruleset mapped to the pool. You can create a custom
    CRUSH rule for your pool.
   </p></li><li class="listitem"><p>
    <span class="emphasis"><em>Snapshots</em></span>: When you create snapshots with
    <code class="command">ceph osd pool mksnap</code>, you effectively take a snapshot of
    a particular pool.
   </p></li></ul></div><p>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </p><section class="sect1" id="ceph-pools-operate-add-pool" data-id-title="Creating a pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.1 </span><span class="title-name">Creating a pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-add-pool">#</a></h2></div></div></div><p>
   A pool can be created as either <code class="literal">replicated</code> to recover
   from lost OSDs by keeping multiple copies of the objects or
   <code class="literal">erasure</code> to have generalized RAID 5 or 6 capability.
   Replicated pools require more raw storage, while erasure coded pools require
   less raw storage. The default setting is <code class="literal">replicated</code>. For
   more information on erasure coded pools, see
   <a class="xref" href="#cha-ceph-erasure" title="Chapter 19. Erasure coded pools">Chapter 19, <em>Erasure coded pools</em></a>.
  </p><p>
   To create a replicated pool, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.4.5.3.6.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The autoscaler will take care of the remaining optional arguments. For more
    information, see <a class="xref" href="#op-pgs-autoscaler" title="17.4.12. Enabling the PG auto-scaler">Section 17.4.12, “Enabling the PG auto-scaler”</a>.
   </p></div><p>
   To create an erasure coded pool, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create <em class="replaceable">POOL_NAME</em> erasure <em class="replaceable">CRUSH_RULESET_NAME</em> \
<em class="replaceable">EXPECTED_NUM_OBJECTS</em></pre></div><p>
   The <code class="command">ceph osd pool create</code> command can fail if you exceed
   the limit of placement groups per OSD. The limit is set with the option
   <code class="option">mon_max_pg_per_osd</code>.
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.6.9.1"><span class="term">POOL_NAME</span></dt><dd><p>
      The name of the pool. It must be unique. This option is required.
     </p></dd><dt id="id-1.4.5.3.6.9.2"><span class="term">POOL_TYPE</span></dt><dd><p>
      The pool type which may either be <code class="literal">replicated</code> to
      recover from lost OSDs by keeping multiple copies of the objects or
      <code class="literal">erasure</code> to get a kind of generalized RAID 5
      capability. The replicated pools require more raw storage but implement
      all Ceph operations. The erasure pools require less raw storage but only
      implement a subset of the available operations. The default
      <code class="literal">POOL_TYPE</code> is <code class="literal">replicated</code>.
     </p></dd><dt id="id-1.4.5.3.6.9.3"><span class="term">CRUSH_RULESET_NAME</span></dt><dd><p>
      The name of the CRUSH ruleset for this pool. If the specified ruleset
      does not exist, the creation of replicated pools will fail with -ENOENT.
      For replicated pools it is the ruleset specified by the <code class="varname">osd pool
      default CRUSH replicated ruleset</code> configuration variable. This
      ruleset must exist. For erasure pools it is 'erasure-code' if the default
      erasure code profile is used or <em class="replaceable">POOL_NAME</em>
      otherwise. This ruleset will be created implicitly if it does not exist
      already.
     </p></dd><dt id="id-1.4.5.3.6.9.4"><span class="term">erasure_code_profile=profile</span></dt><dd><p>
      For erasure coded pools only. Use the erasure code profile. It must be an
      existing profile as defined by <code class="command">osd erasure-code-profile
      set</code>.
     </p><div id="id-1.4.5.3.6.9.4.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       If for any reason the autoscaler has been disabled
       (<code class="literal">pg_autoscale_mode</code> set to off) on a pool, you can
       calculate and set the PG numbers manually. See <a class="xref" href="#op-pgs" title="17.4. Placement groups">Section 17.4, “Placement groups”</a>
       for details on calculating an appropriate number of placement groups for
       your pool.
      </p></div></dd><dt id="id-1.4.5.3.6.9.5"><span class="term">EXPECTED_NUM_OBJECTS</span></dt><dd><p>
      The expected number of objects for this pool. By setting this value
      (together with a negative <code class="option">filestore merge threshold</code>),
      the PG folder splitting happens at the pool creation time. This avoids
      the latency impact with a runtime folder splitting.
     </p></dd></dl></div></section><section class="sect1" id="ceph-listing-pools" data-id-title="Listing pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.2 </span><span class="title-name">Listing pools</span> <a title="Permalink" class="permalink" href="#ceph-listing-pools">#</a></h2></div></div></div><p>
   To list your cluster’s pools, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool ls</pre></div></section><section class="sect1" id="ceph-renaming-pool" data-id-title="Renaming a pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.3 </span><span class="title-name">Renaming a pool</span> <a title="Permalink" class="permalink" href="#ceph-renaming-pool">#</a></h2></div></div></div><p>
   To rename a pool, execute:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rename <em class="replaceable">CURRENT_POOL_NAME</em> <em class="replaceable">NEW_POOL_NAME</em></pre></div><p>
   If you rename a pool and you have per-pool capabilities for an authenticated
   user, you must update the user’s capabilities with the new pool name.
  </p></section><section class="sect1" id="ceph-pools-operate-del-pool" data-id-title="Deleting a pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.4 </span><span class="title-name">Deleting a pool</span> <a title="Permalink" class="permalink" href="#ceph-pools-operate-del-pool">#</a></h2></div></div></div><div id="id-1.4.5.3.9.2" data-id-title="Pool deletion is not reversible" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Pool deletion is not reversible</h6><p>
    Pools may contain important data. Deleting a pool causes all data in the
    pool to disappear, and there is no way to recover it.
   </p></div><p>
   Because inadvertent pool deletion is a real danger, Ceph implements two
   mechanisms that prevent pools from being deleted. Both mechanisms must be
   disabled before a pool can be deleted.
  </p><p>
   The first mechanism is the <code class="literal">NODELETE</code> flag. Each pool has
   this flag, and its default value is 'false'. To find out the value of this
   flag on a pool, run the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool get <em class="replaceable">pool_name</em> nodelete</pre></div><p>
   If it outputs <code class="literal">nodelete: true</code>, it is not possible to
   delete the pool until you change the flag using the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">pool_name</em> nodelete false</pre></div><p>
   The second mechanism is the cluster-wide configuration parameter <code class="option">mon
   allow pool delete</code>, which defaults to 'false'. This means that, by
   default, it is not possible to delete a pool. The error message displayed
   is:
  </p><div class="verbatim-wrap"><pre class="screen">Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</pre></div><p>
   To delete the pool in spite of this safety setting, you can temporarily set
   <code class="option">mon allow pool delete</code> to 'true', delete the pool, and then
   return the parameter to 'false':
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph tell mon.* injectargs --mon-allow-pool-delete=false</pre></div><p>
   The <code class="command">injectargs</code> command displays the following message:
  </p><div class="verbatim-wrap"><pre class="screen">injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</pre></div><p>
   This is merely confirming that the command was executed successfully. It is
   not an error.
  </p><p>
   If you created your own rulesets and rules for a pool you created, you
   should consider removing them when you no longer need your pool.
  </p></section><section class="sect1" id="ceph-pool-other-operations" data-id-title="Other operations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.5 </span><span class="title-name">Other operations</span> <a title="Permalink" class="permalink" href="#ceph-pool-other-operations">#</a></h2></div></div></div><section class="sect2" id="ceph-pools-associate" data-id-title="Associating pools with an application"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.1 </span><span class="title-name">Associating pools with an application</span> <a title="Permalink" class="permalink" href="#ceph-pools-associate">#</a></h3></div></div></div><p>
    Before using pools, you need to associate them with an application. Pools
    that will be used with CephFS, or pools that are automatically created by
    Object Gateway are automatically associated.
   </p><p>
    For other cases, you can manually associate a free-form application name
    with a pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool application enable <em class="replaceable">POOL_NAME</em> <em class="replaceable">APPLICATION_NAME</em></pre></div><div id="id-1.4.5.3.10.2.5" data-id-title="Default application names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Default application names</h6><p>
     CephFS uses the application name <code class="literal">cephfs</code>, RADOS Block Device uses
     <code class="literal">rbd</code>, and Object Gateway uses <code class="literal">rgw</code>.
    </p></div><p>
    A pool can be associated with multiple applications, and each application
    can have its own metadata. To list the application (or applications)
    associated with a pool, issue the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool application get <em class="replaceable">pool_name</em></pre></div></section><section class="sect2" id="ceph-set-pool-quotas" data-id-title="Setting pool quotas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.2 </span><span class="title-name">Setting pool quotas</span> <a title="Permalink" class="permalink" href="#ceph-set-pool-quotas">#</a></h3></div></div></div><p>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota <em class="replaceable">POOL_NAME</em> <em class="replaceable">MAX_OBJECTS</em> <em class="replaceable">OBJ_COUNT</em> <em class="replaceable">MAX_BYTES</em> <em class="replaceable">BYTES</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set-quota data max_objects 10000</pre></div><p>
    To remove a quota, set its value to 0.
   </p></section><section class="sect2" id="ceph-showing-pool-statistics" data-id-title="Showing pool statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.3 </span><span class="title-name">Showing pool statistics</span> <a title="Permalink" class="permalink" href="#ceph-showing-pool-statistics">#</a></h3></div></div></div><p>
    To show a pool’s usage statistics, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados df
 POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
 .rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
 cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
 cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
 default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
 default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
 default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
 default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
 example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
 iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
 mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
 pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
 pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
 pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B</pre></div><p>
    A description of individual columns follow:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.10.4.5.1"><span class="term">USED</span></dt><dd><p>
       Number of bytes used by the pool.
      </p></dd><dt id="id-1.4.5.3.10.4.5.2"><span class="term">OBJECTS</span></dt><dd><p>
       Number of objects stored in the pool.
      </p></dd><dt id="id-1.4.5.3.10.4.5.3"><span class="term">CLONES</span></dt><dd><p>
       Number of clones stored in the pool. When a snapshot is created and one
       writes to an object, instead of modifying the original object its clone
       is created so the original snapshotted object content is not modified.
      </p></dd><dt id="id-1.4.5.3.10.4.5.4"><span class="term">COPIES</span></dt><dd><p>
       Number of object replicas. For example, if a replicated pool with the
       replication factor 3 has 'x' objects, it will normally have 3 * x
       copies.
      </p></dd><dt id="id-1.4.5.3.10.4.5.5"><span class="term">MISSING_ON_PRIMARY</span></dt><dd><p>
       Number of objects in the degraded state (not all copies exist) while the
       copy is missing on the primary OSD.
      </p></dd><dt id="id-1.4.5.3.10.4.5.6"><span class="term">UNFOUND</span></dt><dd><p>
       Number of unfound objects.
      </p></dd><dt id="id-1.4.5.3.10.4.5.7"><span class="term">DEGRADED</span></dt><dd><p>
       Number of degraded objects.
      </p></dd><dt id="id-1.4.5.3.10.4.5.8"><span class="term">RD_OPS</span></dt><dd><p>
       Total number of read operations requested for this pool.
      </p></dd><dt id="id-1.4.5.3.10.4.5.9"><span class="term">RD</span></dt><dd><p>
       Total number of bytes read from this pool.
      </p></dd><dt id="id-1.4.5.3.10.4.5.10"><span class="term">WR_OPS</span></dt><dd><p>
       Total number of write operations requested for this pool.
      </p></dd><dt id="id-1.4.5.3.10.4.5.11"><span class="term">WR</span></dt><dd><p>
       Total number of bytes written to the pool. Note that it is not the same
       as the pool's usage because you can write to the same object many times.
       The result is that the pool's usage will remain the same but the number
       of bytes written to the pool will grow.
      </p></dd><dt id="id-1.4.5.3.10.4.5.12"><span class="term">USED COMPR</span></dt><dd><p>
       Number of bytes allocated for compressed data.
      </p></dd><dt id="id-1.4.5.3.10.4.5.13"><span class="term">UNDER COMPR</span></dt><dd><p>
       Number of bytes that the compressed data occupy when it is not
       compressed.
      </p></dd></dl></div></section><section class="sect2" id="ceph-getting-pool-values" data-id-title="Getting pool values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.4 </span><span class="title-name">Getting pool values</span> <a title="Permalink" class="permalink" href="#ceph-getting-pool-values">#</a></h3></div></div></div><p>
    To get a value from a pool, run the following <code class="command">get</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> <em class="replaceable">KEY</em></pre></div><p>
    You can get values for keys listed in <a class="xref" href="#ceph-pools-values" title="18.5.5. Setting pool values">Section 18.5.5, “Setting pool values”</a>
    plus the following keys:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.10.5.5.1"><span class="term">PG_NUM</span></dt><dd><p>
       The number of placement groups for the pool.
      </p></dd><dt id="id-1.4.5.3.10.5.5.2"><span class="term">PGP_NUM</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than <code class="option">PG_NUM</code>.
      </p></dd></dl></div><div id="id-1.4.5.3.10.5.6" data-id-title="All of a pools values" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: All of a pool's values</h6><p>
     To list all values related to a specific pool, run:
    </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> all</pre></div></div></section><section class="sect2" id="ceph-pools-values" data-id-title="Setting pool values"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.5 </span><span class="title-name">Setting pool values</span> <a title="Permalink" class="permalink" href="#ceph-pools-values">#</a></h3></div></div></div><p>
    To set a value to a pool, execute:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> <em class="replaceable">KEY</em> <em class="replaceable">VALUE</em></pre></div><p>
    The following is a list of pool values sorted by a pool type:
   </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Common pool values </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.10.6.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.3.10.6.5.2"><span class="term">crash_replay_interval</span></dt><dd><p>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </p></dd><dt id="id-1.4.5.3.10.6.5.3"><span class="term">pg_num</span></dt><dd><p>
       The number of placement groups for the pool. If you add new OSDs to the
       cluster, verify the value for placement groups on all pools targeted for
       the new OSDs.
      </p></dd><dt id="id-1.4.5.3.10.6.5.4"><span class="term">pgp_num</span></dt><dd><p>
       The effective number of placement groups to use when calculating data
       placement.
      </p></dd><dt id="id-1.4.5.3.10.6.5.5"><span class="term">crush_ruleset</span></dt><dd><p>
       The ruleset to use for mapping object placement in the cluster.
      </p></dd><dt id="id-1.4.5.3.10.6.5.6"><span class="term">hashpspool</span></dt><dd><p>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to the
       default 0, the cluster starts backfilling to have a correct placement of
       all PGs again. Be aware that this can create quite substantial I/O load
       on a cluster, therefore do not enable the flag from 0 to 1 on highly
       loaded production clusters.
      </p></dd><dt id="id-1.4.5.3.10.6.5.7"><span class="term">nodelete</span></dt><dd><p>
       Prevents the pool from being removed.
      </p></dd><dt id="id-1.4.5.3.10.6.5.8"><span class="term">nopgchange</span></dt><dd><p>
       Prevents the pool's <code class="option">pg_num</code> and <code class="option">pgp_num</code>
       from being changed.
      </p></dd><dt id="id-1.4.5.3.10.6.5.9"><span class="term">noscrub,nodeep-scrub</span></dt><dd><p>
       Disables (deep) scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </p></dd><dt id="id-1.4.5.3.10.6.5.10"><span class="term">write_fadvise_dontneed</span></dt><dd><p>
       Set or unset the <code class="literal">WRITE_FADVISE_DONTNEED</code> flag on a
       given pool's read/write requests to bypass putting data into cache.
       Default is <code class="literal">false</code>. Applies to both replicated and EC
       pools.
      </p></dd><dt id="id-1.4.5.3.10.6.5.11"><span class="term">scrub_min_interval</span></dt><dd><p>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_min_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.4.5.3.10.6.5.12"><span class="term">scrub_max_interval</span></dt><dd><p>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <code class="literal">0</code> means that the
       <code class="option">osd_scrub_max_interval</code> value from the Ceph
       configuration file is used.
      </p></dd><dt id="id-1.4.5.3.10.6.5.13"><span class="term">deep_scrub_interval</span></dt><dd><p>
       The interval in seconds for the pool <span class="emphasis"><em>deep</em></span>
       scrubbing. The default <code class="literal">0</code> means that the
       <code class="option">osd_deep_scrub</code> value from the Ceph configuration file
       is used.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Replicated pool values </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.10.6.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.3.10.6.6.2"><span class="term">size</span></dt><dd><p>
       Sets the number of replicas for objects in the pool. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="18.5.6. Setting the number of object replicas">Section 18.5.6, “Setting the number of object replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.4.5.3.10.6.6.3"><span class="term">min_size</span></dt><dd><p>
       Sets the minimum number of replicas required for I/O. See
       <a class="xref" href="#ceph-pools-options-num-of-replicas" title="18.5.6. Setting the number of object replicas">Section 18.5.6, “Setting the number of object replicas”</a> for further
       details. Replicated pools only.
      </p></dd><dt id="id-1.4.5.3.10.6.6.4"><span class="term">nosizechange</span></dt><dd><p>
       Prevents the pool's size from being changed. When a pool is created, the
       default value is taken from the value of the
       <code class="option">osd_pool_default_flag_nosizechange</code> parameter which is
       <code class="literal">false</code> by default. Applies to replicated pools only
       because you cannot change size for EC pools.
      </p></dd><dt id="id-1.4.5.3.10.6.6.5"><span class="term">hit_set_type</span></dt><dd><p>
       Enables hit set tracking for cache pools. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. This option can have the
       following values: <code class="literal">bloom</code>,
       <code class="literal">explicit_hash</code>, <code class="literal">explicit_object</code>.
       Default is <code class="literal">bloom</code>, other values are for testing only.
      </p></dd><dt id="id-1.4.5.3.10.6.6.6"><span class="term">hit_set_count</span></dt><dd><p>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <code class="systemitem">ceph-osd</code> daemon.
       Default is <code class="literal">0</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.7"><span class="term">hit_set_period</span></dt><dd><p>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <code class="systemitem">ceph-osd</code> daemon. When a pool is created, the
       default value is taken from the value of the
       <code class="option">osd_tier_default_cache_hit_set_period</code> parameter, which
       is <code class="literal">1200</code> by default. Applies to replicated pools only
       because EC pools cannot be used as a cache tier.
      </p></dd><dt id="id-1.4.5.3.10.6.6.8"><span class="term">hit_set_fpp</span></dt><dd><p>
       The false positive probability for the bloom hit set type. See
       <a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom
       Filter</a> for additional information. Valid range is 0.0 - 1.0
       Default is <code class="literal">0.05</code>
      </p></dd><dt id="id-1.4.5.3.10.6.6.9"><span class="term">use_gmt_hitset</span></dt><dd><p>
       Force OSDs to use GMT (Greenwich Mean Time) time stamps when creating a
       hit set for cache tiering. This ensures that nodes in different time
       zones return the same result. Default is <code class="literal">1</code>. This
       value should not be changed.
      </p></dd><dt id="id-1.4.5.3.10.6.6.10"><span class="term">cache_target_dirty_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <code class="literal">0.4</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.11"><span class="term">cache_target_dirty_high_ratio</span></dt><dd><p>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <code class="literal">0.6</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.12"><span class="term">cache_target_full_ratio</span></dt><dd><p>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <code class="literal">0.8</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.13"><span class="term">target_max_bytes</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_bytes</code> threshold is triggered.
      </p></dd><dt id="id-1.4.5.3.10.6.6.14"><span class="term">target_max_objects</span></dt><dd><p>
       Ceph will begin flushing or evicting objects when the
       <code class="option">max_objects</code> threshold is triggered.
      </p></dd><dt id="id-1.4.5.3.10.6.6.15"><span class="term">hit_set_grade_decay_rate</span></dt><dd><p>
       Temperature decay rate between two successive
       <code class="literal">hit_set</code>s. Default is <code class="literal">20</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.16"><span class="term">hit_set_search_last_n</span></dt><dd><p>
       Count at most <code class="literal">N</code> appearances in
       <code class="literal">hit_set</code>s for temperature calculation. Default is
       <code class="literal">1</code>.
      </p></dd><dt id="id-1.4.5.3.10.6.6.17"><span class="term">cache_min_flush_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </p></dd><dt id="id-1.4.5.3.10.6.6.18"><span class="term">cache_min_evict_age</span></dt><dd><p>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </p></dd></dl></div><div class="variablelist" id="pool-values-ec" data-id-title="Erasure coded pool values"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Erasure coded pool values </span><a title="Permalink" class="permalink" href="#pool-values-ec">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.5.3.10.6.7.2"><span class="term">fast_read</span></dt><dd><p>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <span class="emphasis"><em>jerasure</em></span> and <span class="emphasis"><em>isa</em></span> erasure
       plug-ins, when the first <code class="literal">K</code> replies return, then the
       client’s request is served immediately using the data decoded from
       these replies. This approach causes more CPU load and less disk/network
       load. Currently, this flag is only supported for erasure coding pools.
       Default is <code class="literal">0</code>.
      </p></dd></dl></div></section><section class="sect2" id="ceph-pools-options-num-of-replicas" data-id-title="Setting the number of object replicas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.5.6 </span><span class="title-name">Setting the number of object replicas</span> <a title="Permalink" class="permalink" href="#ceph-pools-options-num-of-replicas">#</a></h3></div></div></div><p>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">poolname</em> size <em class="replaceable">num-replicas</em></pre></div><p>
    The <em class="replaceable">num-replicas</em> includes the object itself. For
    example if you want the object and two copies of the object for a total of
    three instances of the object, specify 3.
   </p><div id="id-1.4.5.3.10.7.5" data-id-title="Do not set less than 3 replicas" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Do not set less than 3 replicas</h6><p>
     If you set the <em class="replaceable">num-replicas</em> to 2, there will be
     only <span class="emphasis"><em>one</em></span> copy of your data. If you lose one object
     instance, you need to trust that the other copy has not been corrupted,
     for example since the last scrubbing during recovery (refer to
     <a class="xref" href="#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a> for details).
    </p><p>
     Setting a pool to one replica means that there is exactly
     <span class="emphasis"><em>one</em></span> instance of the data object in the pool. If the
     OSD fails, you lose the data. A possible usage for a pool with one replica
     is storing temporary data for a short time.
    </p></div><div id="id-1.4.5.3.10.7.6" data-id-title="Setting more than 3 replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Setting more than 3 replicas</h6><p>
     Setting 4 replicas for a pool increases the reliability by 25%.
    </p><p>
     In case of two data centers, you need to set at least 4 replicas for a
     pool to have two copies in each data center so that if one data center is
     lost, two copies still exist and you can still lose one disk without
     losing data.
    </p></div><div id="id-1.4.5.3.10.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     An object might accept I/Os in degraded mode with fewer than <code class="literal">pool
     size</code> replicas. To set a minimum number of required replicas for
     I/O, you should use the <code class="literal">min_size</code> setting. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set data min_size 2</pre></div><p>
     This ensures that no object in the data pool will receive I/O with fewer
     than <code class="literal">min_size</code> replicas.
    </p></div><div id="id-1.4.5.3.10.7.8" data-id-title="Get the number of object replicas" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Get the number of object replicas</h6><p>
     To get the number of object replicas, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd dump | grep 'replicated size'</pre></div><p>
     Ceph will list the pools, with the <code class="literal">replicated size</code>
     attribute highlighted. By default, Ceph creates two replicas of an
     object (a total of three copies, or a size of 3).
    </p></div></section></section><section class="sect1" id="pools-migration" data-id-title="Pool migration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.6 </span><span class="title-name">Pool migration</span> <a title="Permalink" class="permalink" href="#pools-migration">#</a></h2></div></div></div><p>
   When creating a pool (see <a class="xref" href="#ceph-pools-operate-add-pool" title="18.1. Creating a pool">Section 18.1, “Creating a pool”</a>) you
   need to specify its initial parameters, such as the pool type or the number
   of placement groups. If you later decide to change any of these
   parameters—for example when converting a replicated pool into an
   erasure coded one, or decreasing the number of placement groups—you
   need to migrate the pool data to another one whose parameters suit your
   deployment.
  </p><p>
   This section describes two migration methods—a <span class="emphasis"><em>cache
   tier</em></span> method for general pool data migration, and a method using
   <code class="command">rbd migrate</code> sub-commands to migrate RBD images to a new
   pool. Each method has its specifics and limitations.
  </p><section class="sect2" id="pool-migrate-limits" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.6.1 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#pool-migrate-limits">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      You can use the <span class="emphasis"><em>cache tier</em></span> method to migrate from a
      replicated pool to either an EC pool or another replicated pool.
      Migrating from an EC pool is not supported.
     </p></li><li class="listitem"><p>
      You cannot migrate RBD images and CephFS exports from a replicated pool
      to an EC pool. The reason is that EC pools do not support
      <code class="literal">omap</code>, while RBD and CephFS use
      <code class="literal">omap</code> to store its metadata. For example, the header
      object of the RBD will fail to be flushed. But you can migrate data to EC
      pool, leaving metadata in replicated pool.
     </p></li><li class="listitem"><p>
      The <code class="command">rbd migration</code> method allows migrating images with
      minimal client downtime. You only need to stop the client before the
      <code class="option">prepare</code> step and start it afterward. Note that only a
      <code class="systemitem">librbd</code> client that supports this feature (Ceph
      Nautilus or newer) will be able to open the image just after the
      <code class="option">prepare</code> step, while older
      <code class="systemitem">librbd</code> clients or the
      <code class="systemitem">krbd</code> clients will not be able to open the image
      until the <code class="option">commit</code> step is executed.
     </p></li></ul></div></section><section class="sect2" id="pool-migrate-cache-tier" data-id-title="Migrating using cache tier"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.6.2 </span><span class="title-name">Migrating using cache tier</span> <a title="Permalink" class="permalink" href="#pool-migrate-cache-tier">#</a></h3></div></div></div><p>
    The principle is simple—include the pool that you need to migrate
    into a cache tier in reverse order. The following example migrates a
    replicated pool named 'testpool' to an erasure coded pool:
   </p><div class="procedure" id="id-1.4.5.3.11.5.3" data-id-title="Migrating replicated to erasure coded pool"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 18.1: </span><span class="title-name">Migrating replicated to erasure coded pool </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a new erasure coded pool named 'newpool'. Refer to
      <a class="xref" href="#ceph-pools-operate-add-pool" title="18.1. Creating a pool">Section 18.1, “Creating a pool”</a> for a detailed explanation
      of pool creation parameters.
     </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create newpool erasure default</pre></div><p>
      Verify that the used client keyring provides at least the same
      capabilities for 'newpool' as it does for 'testpool'.
     </p><p>
      Now you have two pools: the original replicated 'testpool' filled with
      data, and the new empty erasure coded 'newpool':
     </p><div class="figure" id="id-1.4.5.3.11.5.3.2.5"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate1.png"><img src="images/pool_migrate1.png" width="60%" alt="Pools before migration" title="Pools before migration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 18.1: </span><span class="title-name">Pools before migration </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3.2.5">#</a></h6></div></div></li><li class="step"><p>
      Set up the cache tier and configure the replicated pool 'testpool' as a
      cache pool. The <code class="option">-force-nonempty</code> option allows adding a
      cache tier even if the pool already has data:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<code class="prompt user">cephuser@adm &gt; </code>ceph osd tier add newpool testpool --force-nonempty
<code class="prompt user">cephuser@adm &gt; </code>ceph osd tier cache-mode testpool proxy</pre></div><div class="figure" id="id-1.4.5.3.11.5.3.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate2.png"><img src="images/pool_migrate2.png" width="60%" alt="Cache tier setup" title="Cache tier setup"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 18.2: </span><span class="title-name">Cache tier setup </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3.3.3">#</a></h6></div></div></li><li class="step"><p>
      Force the cache pool to move all objects to the new pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados -p testpool cache-flush-evict-all</pre></div><div class="figure" id="id-1.4.5.3.11.5.3.4.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate3.png"><img src="images/pool_migrate3.png" width="60%" alt="Data flushing" title="Data flushing"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 18.3: </span><span class="title-name">Data flushing </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3.4.3">#</a></h6></div></div></li><li class="step"><p>
      Until all the data has been flushed to the new erasure coded pool, you
      need to specify an overlay so that objects are searched on the old pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tier set-overlay newpool testpool</pre></div><p>
      With the overlay, all operations are forwarded to the old replicated
      'testpool':
     </p><div class="figure" id="id-1.4.5.3.11.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate4.png"><img src="images/pool_migrate4.png" width="60%" alt="Setting overlay" title="Setting overlay"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 18.4: </span><span class="title-name">Setting overlay </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3.5.4">#</a></h6></div></div><p>
      Now you can switch all the clients to access objects on the new pool.
     </p></li><li class="step"><p>
      After all data is migrated to the erasure coded 'newpool', remove the
      overlay and the old cache pool 'testpool':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tier remove-overlay newpool
<code class="prompt user">cephuser@adm &gt; </code>ceph osd tier remove newpool testpool</pre></div><div class="figure" id="id-1.4.5.3.11.5.3.6.3"><div class="figure-contents"><div class="mediaobject"><a href="images/pool_migrate5.png"><img src="images/pool_migrate5.png" width="60%" alt="Migration complete" title="Migration complete"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 18.5: </span><span class="title-name">Migration complete </span><a title="Permalink" class="permalink" href="#id-1.4.5.3.11.5.3.6.3">#</a></h6></div></div></li><li class="step"><p>
      Run
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'</pre></div></li></ol></div></div></section><section class="sect2" id="migrate-rbd-image" data-id-title="Migrating RBD images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.6.3 </span><span class="title-name">Migrating RBD images</span> <a title="Permalink" class="permalink" href="#migrate-rbd-image">#</a></h3></div></div></div><p>
    The following is the recommended way to migrate RBD images from one
    replicated pool to another replicated pool.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop clients (such as a virtual machine) from accessing the RBD image.
     </p></li><li class="step"><p>
      Create a new image in the target pool, with the parent set to the source
      image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd migration prepare <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em> <em class="replaceable">TARGET_POOL</em>/<em class="replaceable">IMAGE</em></pre></div><div id="id-1.4.5.3.11.6.3.2.3" data-id-title="Migrate only data to an erasure coded pool" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Migrate only data to an erasure coded pool</h6><p>
       If you need to migrate only the image data to a new EC pool and leave
       the metadata in the original replicated pool, run the following command
       instead:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd migration prepare <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em> \
 --data-pool <em class="replaceable">TARGET_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></div></li><li class="step"><p>
      Let clients access the image in the target pool.
     </p></li><li class="step"><p>
      Migrate data to the target pool:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd migration execute <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li><li class="step"><p>
      Remove the old image:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd migration commit <em class="replaceable">SRC_POOL</em>/<em class="replaceable">IMAGE</em></pre></div></li></ol></div></div></section></section><section class="sect1" id="cha-ceph-snapshots-pool" data-id-title="Pool snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.7 </span><span class="title-name">Pool snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-pool">#</a></h2></div></div></div><p>
   Pool snapshots are snapshots of the state of the whole Ceph pool. With
   pool snapshots, you can retain the history of the pool's state. Creating
   pool snapshots consumes storage space proportional to the pool size. Always
   check the related storage for enough disk space before creating a snapshot
   of a pool.
  </p><section class="sect2" id="ceph-make-snapshot-pool" data-id-title="Making a snapshot of a pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.7.1 </span><span class="title-name">Making a snapshot of a pool</span> <a title="Permalink" class="permalink" href="#ceph-make-snapshot-pool">#</a></h3></div></div></div><p>
    To make a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool mksnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</pre></div></section><section class="sect2" id="ceph-listing-snapshots-pool" data-id-title="Listing snapshots of a pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.7.2 </span><span class="title-name">Listing snapshots of a pool</span> <a title="Permalink" class="permalink" href="#ceph-listing-snapshots-pool">#</a></h3></div></div></div><p>
    To list existing snapshots of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados lssnap -p <em class="replaceable">POOL_NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps</pre></div></section><section class="sect2" id="ceph-removing-snapshot-pool" data-id-title="Removing a snapshot of a pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.7.3 </span><span class="title-name">Removing a snapshot of a pool</span> <a title="Permalink" class="permalink" href="#ceph-removing-snapshot-pool">#</a></h3></div></div></div><p>
    To remove a snapshot of a pool, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rmsnap <em class="replaceable">POOL-NAME</em> <em class="replaceable">SNAP-NAME</em></pre></div></section></section><section class="sect1" id="sec-ceph-pool-compression" data-id-title="Data compression"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">18.8 </span><span class="title-name">Data compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression">#</a></h2></div></div></div><p>
   BlueStore (find more details in <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.4 “BlueStore”</span>)
   provides on-the-fly data compression to save disk space. The compression
   ratio depends on the data stored in the system. Note that
   compression/decompression requires additional CPU power.
  </p><p>
   You can configure data compression globally (see
   <a class="xref" href="#sec-ceph-pool-bluestore-compression-options" title="18.8.3. Global compression options">Section 18.8.3, “Global compression options”</a>) and then
   override specific compression settings for each individual pool.
  </p><p>
   You can enable or disable pool data compression, or change the compression
   algorithm and mode at any time, regardless of whether the pool contains data
   or not.
  </p><p>
   No compression will be applied to existing data after enabling the pool
   compression.
  </p><p>
   After disabling the compression of a pool, all its data will be
   decompressed.
  </p><section class="sect2" id="sec-ceph-pool-compression-enable" data-id-title="Enabling compression"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.8.1 </span><span class="title-name">Enabling compression</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-enable">#</a></h3></div></div></div><p>
    To enable data compression for a pool named
    <em class="replaceable">POOL_NAME</em>, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm <em class="replaceable">COMPRESSION_ALGORITHM</em>
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_mode <em class="replaceable">COMPRESSION_MODE</em></pre></div><div id="id-1.4.5.3.13.7.4" data-id-title="Disabling pool compression" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Disabling pool compression</h6><p>
     To disable data compression for a pool, use 'none' as the compression
     algorithm:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> osd pool set <em class="replaceable">POOL_NAME</em> compression_algorithm none</pre></div></div></section><section class="sect2" id="sec-ceph-pool-compression-options" data-id-title="Pool compression options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.8.2 </span><span class="title-name">Pool compression options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-compression-options">#</a></h3></div></div></div><p>
    A full list of compression settings:
   </p><div class="variablelist"><dl class="variablelist"><dt id="compr-algorithm"><span class="term">compression_algorithm</span></dt><dd><p>
       Possible values are <code class="literal">none</code>, <code class="literal">zstd</code>,
       <code class="literal">snappy</code>. Default is <code class="literal">snappy</code>.
      </p><p>
       Which compression algorithm to use depends on the specific use case.
       Several recommendations follow:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Use the default <code class="literal">snappy</code> as long as you do not have a
         good reason to change it.
        </p></li><li class="listitem"><p>
         <code class="literal">zstd</code> offers a good compression ratio, but causes
         high CPU overhead when compressing small amounts of data.
        </p></li><li class="listitem"><p>
         Run a benchmark of these algorithms on a sample of your actual data
         while keeping an eye on the CPU and memory usage of your cluster.
        </p></li></ul></div></dd><dt id="compr-mode"><span class="term">compression_mode</span></dt><dd><p>
       Possible values are <code class="literal">none</code>,
       <code class="literal">aggressive</code>, <code class="literal">passive</code>,
       <code class="literal">force</code>. Default is <code class="literal">none</code>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">none</code>: compress never
        </p></li><li class="listitem"><p>
         <code class="literal">passive</code>: compress if hinted
         <code class="literal">COMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">aggressive</code>: compress unless hinted
         <code class="literal">INCOMPRESSIBLE</code>
        </p></li><li class="listitem"><p>
         <code class="literal">force</code>: compress always
        </p></li></ul></div></dd><dt id="compr-ratio"><span class="term">compression_required_ratio</span></dt><dd><p>
       Value: Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Default is
       <code class="literal">0.875</code>, which means that if the compression does not
       reduce the occupied space by at least 12.5%, the object will not be
       compressed.
      </p><p>
       Objects above this ratio will not be stored compressed because of the
       low net gain.
      </p></dd><dt id="id-1.4.5.3.13.8.3.4"><span class="term">compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed.
      </p></dd><dt id="id-1.4.5.3.13.8.3.5"><span class="term">compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed.
      </p></dd></dl></div></section><section class="sect2" id="sec-ceph-pool-bluestore-compression-options" data-id-title="Global compression options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">18.8.3 </span><span class="title-name">Global compression options</span> <a title="Permalink" class="permalink" href="#sec-ceph-pool-bluestore-compression-options">#</a></h3></div></div></div><p>
    The following configuration options can be set in the Ceph configuration
    and apply to all OSDs and not only a single pool. The pool specific
    configuration listed in <a class="xref" href="#sec-ceph-pool-compression-options" title="18.8.2. Pool compression options">Section 18.8.2, “Pool compression options”</a>
    takes precedence.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.3.13.9.3.1"><span class="term">bluestore_compression_algorithm</span></dt><dd><p>
       See <a class="xref" href="#compr-algorithm">compression_algorithm</a>
      </p></dd><dt id="id-1.4.5.3.13.9.3.2"><span class="term">bluestore_compression_mode</span></dt><dd><p>
       See <a class="xref" href="#compr-mode">compression_mode</a>
      </p></dd><dt id="id-1.4.5.3.13.9.3.3"><span class="term">bluestore_compression_required_ratio</span></dt><dd><p>
       See <a class="xref" href="#compr-ratio">compression_required_ratio</a>
      </p></dd><dt id="id-1.4.5.3.13.9.3.4"><span class="term">bluestore_compression_min_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Minimum size of objects that are compressed. The setting is ignored by
       default in favor of
       <code class="option">bluestore_compression_min_blob_size_hdd</code> and
       <code class="option">bluestore_compression_min_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.4.5.3.13.9.3.5"><span class="term">bluestore_compression_max_blob_size</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">0</code>
      </p><p>
       Maximum size of objects that are compressed before they will be split
       into smaller chunks. The setting is ignored by default in favor of
       <code class="option">bluestore_compression_max_blob_size_hdd</code> and
       <code class="option">bluestore_compression_max_blob_size_ssd</code>. It takes
       precedence when set to a non-zero value.
      </p></dd><dt id="id-1.4.5.3.13.9.3.6"><span class="term">bluestore_compression_min_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">8K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on solid-state
       drive.
      </p></dd><dt id="id-1.4.5.3.13.9.3.7"><span class="term">bluestore_compression_max_blob_size_ssd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">64K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on solid-state
       drive before they will be split into smaller chunks.
      </p></dd><dt id="id-1.4.5.3.13.9.3.8"><span class="term">bluestore_compression_min_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">128K</code>
      </p><p>
       Minimum size of objects that are compressed and stored on hard disks.
      </p></dd><dt id="id-1.4.5.3.13.9.3.9"><span class="term">bluestore_compression_max_blob_size_hdd</span></dt><dd><p>
       Value: Unsigned Integer, size in bytes. Default: <code class="literal">512K</code>
      </p><p>
       Maximum size of objects that are compressed and stored on hard disks
       before they will be split into smaller chunks.
      </p></dd></dl></div></section></section></section><section class="chapter" id="cha-ceph-erasure" data-id-title="Erasure coded pools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">19 </span><span class="title-name">Erasure coded pools</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure">#</a></h1></div></div></div><p>
  Ceph provides an alternative to the normal replication of data in pools,
  called <span class="emphasis"><em>erasure</em></span> or <span class="emphasis"><em>erasure coded</em></span>
  pool. Erasure pools do not provide all functionality of
  <span class="emphasis"><em>replicated</em></span> pools (for example, they cannot store
  metadata for RBD pools), but require less raw storage. A default erasure pool
  capable of storing 1 TB of data requires 1.5 TB of raw storage, allowing a
  single disk failure. This compares favorably to a replicated pool, which
  needs 2 TB of raw storage for the same purpose.
 </p><p>
  For background information on Erasure Code, see
  <a class="link" href="https://en.wikipedia.org/wiki/Erasure_code" target="_blank">https://en.wikipedia.org/wiki/Erasure_code</a>.
 </p><p>
  For a list of pool values related to EC pools, refer to
  <a class="xref" href="#pool-values-ec" title="Erasure coded pool values">Erasure coded pool values</a>.
 </p><section class="sect1" id="ec-prerequisite" data-id-title="Prerequisite for erasure coded Pools"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.1 </span><span class="title-name">Prerequisite for erasure coded Pools</span> <a title="Permalink" class="permalink" href="#ec-prerequisite">#</a></h2></div></div></div><p>
   To make use of erasure coding, you need to:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Define an erasure rule in the CRUSH Map.
    </p></li><li class="listitem"><p>
     Define an erasure code profile that specifies the coding algorithm to be
     used.
    </p></li><li class="listitem"><p>
     Create a pool using the previously mentioned rule and profile.
    </p></li></ul></div><p>
   Keep in mind that changing the profile and the details in the profile will
   not be possible after the pool is created and has data.
  </p><p>
   Ensure that the CRUSH rules for <span class="emphasis"><em>erasure pools</em></span> use
   <code class="literal">indep</code> for <code class="literal">step</code>. For details see
   <a class="xref" href="#datamgm-rules-step-mode" title="17.3.2. firstn and indep">Section 17.3.2, “<code class="literal">firstn</code> and <code class="literal">indep</code>”</a>.
  </p></section><section class="sect1" id="cha-ceph-erasure-default-profile" data-id-title="Creating a sample erasure coded pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.2 </span><span class="title-name">Creating a sample erasure coded pool</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-default-profile">#</a></h2></div></div></div><p>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     The command <code class="command">ceph osd pool create</code> is used to create a
     pool with type <span class="emphasis"><em>erasure</em></span>. The <code class="literal">12</code>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</pre></div></li><li class="step"><p>
     The string <code class="literal">ABCDEFGHI</code> is written into an object called
     <code class="literal">NYAN</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -</pre></div></li><li class="step"><p>
     For testing purposes OSDs can now be disabled, for example by
     disconnecting them from the network.
    </p></li><li class="step"><p>
     To test whether the pool can handle the failure of devices, the content of
     the file can be accessed with the <code class="command">rados</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div></li></ol></div></div></section><section class="sect1" id="cha-ceph-erasure-erasure-profiles" data-id-title="Erasure code profiles"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.3 </span><span class="title-name">Erasure code profiles</span> <a title="Permalink" class="permalink" href="#cha-ceph-erasure-erasure-profiles">#</a></h2></div></div></div><p>
   When the <code class="command">ceph osd pool create</code> command is invoked to
   create an <span class="emphasis"><em>erasure pool</em></span>, the default profile is used,
   unless another profile is specified. Profiles define the redundancy of data.
   This is done by setting two parameters, arbitrarily named
   <code class="literal">k</code> and <code class="literal">m</code>. k and m define in how many
   <code class="literal">chunks</code> a piece of data is split and how many coding
   chunks are created. Redundant chunks are then stored on different OSDs.
  </p><p>
   Definitions required for erasure pool profiles:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.4.1"><span class="term">chunk</span></dt><dd><p>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </p></dd><dt id="id-1.4.5.4.8.4.2"><span class="term">k</span></dt><dd><p>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example, if <code class="literal">k = 2</code> a 10
      kB object will be divided into <code class="literal">k</code> objects of 5 kB each.
      The default <code class="literal">min_size</code> on erasure coded pools is
      <code class="literal">k + 1</code>. However, we recommend
      <code class="literal">min_size</code> to be <code class="literal">k + 2</code> or more to
      prevent loss of writes and data.
     </p></dd><dt id="id-1.4.5.4.8.4.3"><span class="term">m</span></dt><dd><p>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </p></dd><dt id="id-1.4.5.4.8.4.4"><span class="term">crush-failure-domain</span></dt><dd><p>
      defines to which devices the chunks are distributed. A bucket type needs
      to be set as value. For all bucket types, see
      <a class="xref" href="#datamgm-buckets" title="17.2. Buckets">Section 17.2, “Buckets”</a>. If the failure domain is
      <code class="literal">rack</code>, the chunks will be stored on different racks to
      increase the resilience in case of rack failures. Keep in mind that this
      requires k+m racks.
     </p></dd></dl></div><p>
   With the default erasure code profile used in
   <a class="xref" href="#cha-ceph-erasure-default-profile" title="19.2. Creating a sample erasure coded pool">Section 19.2, “Creating a sample erasure coded pool”</a>, you will not lose
   cluster data if a single OSD or host fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage is
   required for 1 TB of data (because of k=2, m=1). This is equivalent to a
   common RAID 5 configuration. For comparison, a replicated pool needs 2 TB of
   raw storage to store 1 TB of data.
  </p><p>
   The settings of the default profile can be displayed with:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</pre></div><p>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new one (see
   <a class="xref" href="#pools-migration" title="18.6. Pool migration">Section 18.6, “Pool migration”</a>).
  </p><p>
   The most important parameters of the profile are <code class="literal">k</code>,
   <code class="literal">m</code> and <code class="literal">crush-failure-domain</code> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66%, the following profile can be defined. Note that this is
   only valid with a CRUSH Map that has buckets of type 'rack':
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd erasure-code-profile set <em class="replaceable">myprofile</em> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</pre></div><p>
   The example <a class="xref" href="#cha-ceph-erasure-default-profile" title="19.2. Creating a sample erasure coded pool">Section 19.2, “Creating a sample erasure coded pool”</a> can be
   repeated with this new profile:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create ecpool 12 12 erasure <em class="replaceable">myprofile</em>
<code class="prompt user">cephuser@adm &gt; </code>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<code class="prompt user">cephuser@adm &gt; </code>rados --pool ecpool get NYAN -
ABCDEFGHI</pre></div><p>
   The NYAN object will be divided in three (<code class="literal">k=3</code>) and two
   additional chunks will be created (<code class="literal">m=2</code>). The value of
   <code class="literal">m</code> defines how many OSDs can be lost simultaneously
   without losing any data. The <code class="literal">crush-failure-domain=rack</code>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </p><div class="informalfigure"><div class="mediaobject"><a href="images/ceph_erasure_obj.png"><img src="images/ceph_erasure_obj.png" width="60%" alt="Image" title="Image"/></a></div></div><section class="sect2" id="ec-create" data-id-title="Creating a new erasure code profile"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.3.1 </span><span class="title-name">Creating a new erasure code profile</span> <a title="Permalink" class="permalink" href="#ec-create">#</a></h3></div></div></div><p>
    The following command creates a new erasure code profile:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd erasure-code-profile set <em class="replaceable">NAME</em> \
 directory=<em class="replaceable">DIRECTORY</em> \
 plugin=<em class="replaceable">PLUGIN</em> \
 stripe_unit=<em class="replaceable">STRIPE_UNIT</em> \
 <em class="replaceable">KEY</em>=<em class="replaceable">VALUE</em> ... \
 --force</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.4.8.15.4.1"><span class="term">DIRECTORY</span></dt><dd><p>
       Optional. Set the directory name from which the erasure code plugin is
       loaded. Default is <code class="filename">/usr/lib/ceph/erasure-code</code>.
      </p></dd><dt id="id-1.4.5.4.8.15.4.2"><span class="term">PLUGIN</span></dt><dd><p>
       Optional. Use the erasure code plugin to compute coding chunks and
       recover missing chunks. Available plugins are 'jerasure', 'isa', 'lrc',
       and 'shes'. Default is 'jerasure'.
      </p></dd><dt id="id-1.4.5.4.8.15.4.3"><span class="term">STRIPE_UNIT</span></dt><dd><p>
       Optional. The amount of data in a data chunk, per stripe. For example, a
       profile with 2 data chunks and stripe_unit=4K would put the range 0-4K
       in chunk 0, 4K-8K in chunk 1, then 8K-12K in chunk 0 again. This should
       be a multiple of 4K for best performance. The default value is taken
       from the monitor configuration option
       <code class="option">osd_pool_erasure_code_stripe_unit</code> when a pool is
       created. The 'stripe_width' of a pool using this profile will be the
       number of data chunks multiplied by this 'stripe_unit'.
      </p></dd><dt id="id-1.4.5.4.8.15.4.4"><span class="term">KEY=VALUE</span></dt><dd><p>
       Key/value pairs of options specific to the selected erasure code plugin.
      </p></dd><dt id="id-1.4.5.4.8.15.4.5"><span class="term">--force</span></dt><dd><p>
       Optional. Override an existing profile by the same name, and allow
       setting a non-4K-aligned stripe_unit.
      </p></dd></dl></div></section><section class="sect2" id="ec-rm" data-id-title="Removing an erasure code profile"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.3.2 </span><span class="title-name">Removing an erasure code profile</span> <a title="Permalink" class="permalink" href="#ec-rm">#</a></h3></div></div></div><p>
    The following command removes an erasure code profile as identified by its
    <em class="replaceable">NAME</em>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd erasure-code-profile rm <em class="replaceable">NAME</em></pre></div><div id="id-1.4.5.4.8.16.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     If the profile is referenced by a pool, the deletion will fail.
    </p></div></section><section class="sect2" id="ec-get" data-id-title="Displaying an erasure code profiles details"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.3.3 </span><span class="title-name">Displaying an erasure code profile's details</span> <a title="Permalink" class="permalink" href="#ec-get">#</a></h3></div></div></div><p>
    The following command displays details of an erasure code profile as
    identified by its <em class="replaceable">NAME</em>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd erasure-code-profile get <em class="replaceable">NAME</em></pre></div></section><section class="sect2" id="ec-ls" data-id-title="Listing erasure code profiles"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">19.3.4 </span><span class="title-name">Listing erasure code profiles</span> <a title="Permalink" class="permalink" href="#ec-ls">#</a></h3></div></div></div><p>
    The following command lists the names of all erasure code profiles:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd erasure-code-profile ls</pre></div></section></section><section class="sect1" id="ec-rbd" data-id-title="Marking erasure coded pools with RADOS Block Device"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">19.4 </span><span class="title-name">Marking erasure coded pools with RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ec-rbd">#</a></h2></div></div></div><p>
   To mark an EC pool as an RBD pool, tag it accordingly:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool application enable rbd <em class="replaceable">ec_pool_name</em></pre></div><p>
   RBD can store image <span class="emphasis"><em>data</em></span> in EC pools. However, the
   image header and metadata still need to be stored in a replicated pool.
   Assuming you have the pool named 'rbd' for this purpose:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd create rbd/<em class="replaceable">image_name</em> --size 1T --data-pool <em class="replaceable">ec_pool_name</em></pre></div><p>
   You can use the image normally like any other image, except that all of the
   data will be stored in the <em class="replaceable">ec_pool_name</em> pool
   instead of 'rbd' pool.
  </p></section></section><section class="chapter" id="ceph-rbd" data-id-title="RADOS Block Device"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">20 </span><span class="title-name">RADOS Block Device</span> <a title="Permalink" class="permalink" href="#ceph-rbd">#</a></h1></div></div></div><p>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like Ceph.
 </p><p>
  Ceph block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a Ceph cluster. Ceph block
  devices leverage RADOS capabilities such as snapshotting, replication, and
  consistency. Ceph's RADOS Block Devices (RBD) interact with OSDs using kernel modules or
  the <code class="systemitem">librbd</code> library.
 </p><div class="figure" id="id-1.4.5.5.5"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_rbd_schema.png"><img src="images/ceph_rbd_schema.png" width="70%" alt="RADOS protocol" title="RADOS protocol"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 20.1: </span><span class="title-name">RADOS protocol </span><a title="Permalink" class="permalink" href="#id-1.4.5.5.5">#</a></h6></div></div><p>
  Ceph's block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as QEMU, or
  cloud-based computing systems such as OpenStack that rely on <code class="systemitem">libvirt</code>. You
  can use the same cluster to operate the Object Gateway, CephFS, and RADOS Block Devices
  simultaneously.
 </p><section class="sect1" id="ceph-rbd-commands" data-id-title="Block device commands"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.1 </span><span class="title-name">Block device commands</span> <a title="Permalink" class="permalink" href="#ceph-rbd-commands">#</a></h2></div></div></div><p>
   The <code class="command">rbd</code> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </p><section class="sect2" id="ceph-rbd-cmds-create" data-id-title="Creating a block device image in a replicated pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.1 </span><span class="title-name">Creating a block device image in a replicated pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create">#</a></h3></div></div></div><p>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <a class="xref" href="#ceph-pools" title="Chapter 18. Manage storage pools">Chapter 18, <em>Manage storage pools</em></a>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd create --size <em class="replaceable">MEGABYTES</em> <em class="replaceable">POOL-NAME</em>/<em class="replaceable">IMAGE-NAME</em></pre></div><p>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd create --size 1024 mypool/myimage</pre></div><div id="id-1.4.5.5.7.3.6" data-id-title="Image size units" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Image size units</h6><p>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </p></div></section><section class="sect2" id="ceph-rbd-cmds-create-ec" data-id-title="Creating a block device image in an erasure coded pool"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.2 </span><span class="title-name">Creating a block device image in an erasure coded pool</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-create-ec">#</a></h3></div></div></div><p>
    It is possible to store data of a block device image directly in erasure
    coded (EC) pools. A RADOS Block Device image consists of <span class="emphasis"><em>data</em></span> and
    <span class="emphasis"><em>metadata</em></span> parts. You can store only the data part of a
    RADOS Block Device image in an EC pool. The pool needs to have the
    <code class="option">overwrite</code> flag set to <span class="emphasis"><em>true</em></span>, and that
    is only possible if all OSDs where the pool is stored use BlueStore.
   </p><p>
    You cannot store the image's metadata part in an EC pool. You can specify
    the replicated pool for storing the image's metadata with the
    <code class="option">--pool=</code> option of the <code class="command">rbd create</code>
    command or specify <code class="option">pool/</code> as a prefix to the image name.
   </p><p>
    Create an EC pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create <em class="replaceable">EC_POOL</em> 12 12 erasure
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">EC_POOL</em> allow_ec_overwrites true</pre></div><p>
    Specify the replicated pool for storing metadata:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd create <em class="replaceable">IMAGE_NAME</em> --size=1G --data-pool <em class="replaceable">EC_POOL</em> --pool=<em class="replaceable">POOL</em></pre></div><p>
    Or:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd create <em class="replaceable">POOL/IMAGE_NAME</em> --size=1G --data-pool EC_POOL</pre></div></section><section class="sect2" id="ceph-rbd-cmds-list" data-id-title="Listing block device images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.3 </span><span class="title-name">Listing block device images</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-list">#</a></h3></div></div></div><p>
    To list block devices in a pool named 'mypool', execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd ls mypool</pre></div></section><section class="sect2" id="ceph-rbd-cmds-info" data-id-title="Retrieving image information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.4 </span><span class="title-name">Retrieving image information</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-info">#</a></h3></div></div></div><p>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd info mypool/myimage</pre></div></section><section class="sect2" id="ceph-rbd-cmds-resize" data-id-title="Resizing a block device image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.5 </span><span class="title-name">Resizing a block device image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-resize">#</a></h3></div></div></div><p>
    RADOS Block Device images are thin provisioned—they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <code class="option">--size</code> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> # to increase
<code class="prompt user">cephuser@adm &gt; </code>rbd resize --size 2048 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --allow-shrink # to decrease</pre></div></section><section class="sect2" id="ceph-rbd-cmds-rm" data-id-title="Removing a block device image"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.1.6 </span><span class="title-name">Removing a block device image</span> <a title="Permalink" class="permalink" href="#ceph-rbd-cmds-rm">#</a></h3></div></div></div><p>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd rm mypool/myimage</pre></div></section></section><section class="sect1" id="storage-bp-integration-mount-rbd" data-id-title="Mounting and unmounting"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.2 </span><span class="title-name">Mounting and unmounting</span> <a title="Permalink" class="permalink" href="#storage-bp-integration-mount-rbd">#</a></h2></div></div></div><p>
   After you create a RADOS Block Device, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </p><p>
   The <code class="command">rbd</code> command defaults to accessing the cluster using
   the Ceph <code class="literal">admin</code> user account. This account has full
   administrative access to the cluster. This runs the risk of accidentally
   causing damage, similarly to logging in to a Linux workstation as <code class="systemitem">root</code>.
   Thus, it is preferable to create user accounts with fewer privileges and use
   these accounts for normal read/write RADOS Block Device access.
  </p><section class="sect2" id="ceph-rbd-creatuser" data-id-title="Creating a Ceph user account"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.2.1 </span><span class="title-name">Creating a Ceph user account</span> <a title="Permalink" class="permalink" href="#ceph-rbd-creatuser">#</a></h3></div></div></div><p>
    To create a new user account with Ceph Manager, Ceph Monitor, and Ceph OSD capabilities, use
    the <code class="command">ceph</code> command with the <code class="command">auth
    get-or-create</code> subcommand:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.<em class="replaceable">ID</em> mon 'profile rbd' osd 'profile <em class="replaceable">profile name</em> \
  [pool=<em class="replaceable">pool-name</em>] [, profile ...]' mgr 'profile rbd [pool=<em class="replaceable">pool-name</em>]'</pre></div><p>
    For example, to create a user called <em class="replaceable">qemu</em> with
    read-write access to the pool <em class="replaceable">vms</em> and read-only
    access to the pool <em class="replaceable">images</em>, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">ceph auth get-or-create client.<em class="replaceable">qemu</em> mon 'profile rbd' osd 'profile rbd pool=<em class="replaceable">vms</em>, profile rbd-read-only pool=<em class="replaceable">images</em>' \
  mgr 'profile rbd pool=<em class="replaceable">images</em>'</pre></div><p>
    The output from the <code class="command">ceph auth get-or-create</code> command will
    be the keyring for the specified user, which can be written to
    <code class="filename">/etc/ceph/ceph.client.<em class="replaceable">ID</em>.keyring</code>.
   </p><div id="id-1.4.5.5.8.4.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     When using the <code class="command">rbd</code> command, you can specify the user ID
     by providing the optional <code class="command">--id</code>
     <em class="replaceable">ID</em> argument.
    </p></div><p>
    For more details on managing Ceph user accounts, refer to
    <a class="xref" href="#cha-storage-cephx" title="Chapter 30. Authentication with cephx">Chapter 30, <em>Authentication with <code class="systemitem">cephx</code></em></a>.
   </p></section><section class="sect2" id="ceph-rbd-auth" data-id-title="User authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.2.2 </span><span class="title-name">User authentication</span> <a title="Permalink" class="permalink" href="#ceph-rbd-auth">#</a></h3></div></div></div><p>
    To specify a user name, use <code class="option">--id
    <em class="replaceable">user-name</em></code>. If you use
    <code class="systemitem">cephx</code> authentication, you also need to specify a
    secret. It may come from a keyring or a file containing the secret:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</pre></div><p>
    or
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</pre></div></section><section class="sect2" id="ceph-rbd-prep" data-id-title="Preparing a RADOS Block Device for use"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.2.3 </span><span class="title-name">Preparing a RADOS Block Device for use</span> <a title="Permalink" class="permalink" href="#ceph-rbd-prep">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make sure your Ceph cluster includes a pool with the disk image you
      want to map. Assume the pool is called <code class="literal">mypool</code> and the
      image is <code class="literal">myimage</code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd list mypool</pre></div></li><li class="step"><p>
      Map the image to a new block device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device map --pool mypool myimage</pre></div></li><li class="step"><p>
      List all mapped devices:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</pre></div><p>
      The device we want to work on is <code class="filename">/dev/rbd0</code>.
     </p><div id="id-1.4.5.5.8.6.2.3.4" data-id-title="RBD device path" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: RBD device path</h6><p>
       Instead of
       <code class="filename">/dev/rbd<em class="replaceable">DEVICE_NUMBER</em></code>,
       you can use
       <code class="filename">/dev/rbd/<em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></code>
       as a persistent device path. For example:
      </p><div class="verbatim-wrap"><pre class="screen">       /dev/rbd/mypool/myimage</pre></div></div></li><li class="step"><p>
      Make an XFS file system on the <code class="filename">/dev/rbd0</code> device:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div></li><li class="step"><p>
      Replacing <code class="filename">/mnt</code> with your mount point, mount the
      device and check it is correctly mounted:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount /dev/rbd0 /mnt
      <code class="prompt root"># </code>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</pre></div><p>
      Now you can move data to and from the device as if it was a local
      directory.
     </p><div id="id-1.4.5.5.8.6.2.5.4" data-id-title="Increasing the size of RBD device" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Increasing the size of RBD device</h6><p>
       If you find that the size of the RBD device is no longer enough, you can
       easily increase it.
      </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
         Increase the size of the RBD image, for example up to 10 GB.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
         Grow the file system to fill up the new size of the device:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></div></li><li class="step"><p>
      After you finish accessing the device, you can unmap and unmount it.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device unmap /dev/rbd0
<code class="prompt root"># </code>unmount /mnt</pre></div></li></ol></div></div><div id="id-1.4.5.5.8.6.3" data-id-title="Manual mounting and unmounting" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Manual mounting and unmounting</h6><p>
     A <code class="command">rbdmap</code> script and <code class="systemitem">systemd</code> unit is provided to make
     the process of mapping and mounting RBDs after boot, and unmounting them
     before shutdown, smoother. Refer to <a class="xref" href="#ceph-rbd-rbdmap" title="20.2.4. rbdmap Map RBD devices at boot time">Section 20.2.4, “<code class="command">rbdmap</code> Map RBD devices at boot time”</a>.
    </p></div></section><section class="sect2" id="ceph-rbd-rbdmap" data-id-title="rbdmap Map RBD devices at boot time"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.2.4 </span><span class="title-name"><code class="command">rbdmap</code> Map RBD devices at boot time</span> <a title="Permalink" class="permalink" href="#ceph-rbd-rbdmap">#</a></h3></div></div></div><p>
    <code class="command">rbdmap</code> is a shell script that automates <code class="command">rbd
    map</code> and <code class="command">rbd device unmap</code> operations on one or
    more RBD images. Although you can run the script manually at any time, the
    main advantage is automatic mapping and mounting of RBD images at boot time
    (and unmounting and unmapping at shutdown), as triggered by the Init
    system. A <code class="systemitem">systemd</code> unit file, <code class="filename">rbdmap.service</code> is
    included with the <code class="systemitem">ceph-common</code> package for this
    purpose.
   </p><p>
    The script takes a single argument, which can be either
    <code class="option">map</code> or <code class="option">unmap</code>. In either case, the script
    parses a configuration file. It defaults to
    <code class="filename">/etc/ceph/rbdmap</code>, but can be overridden via an
    environment variable <code class="literal">RBDMAPFILE</code>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </p><p>
    The configuration file has the following format:
   </p><div class="verbatim-wrap"><pre class="screen">image_specification rbd_options</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.8.7.6.1"><span class="term"><code class="option">image_specification</code></span></dt><dd><p>
       Path to an image within a pool. Specify as
       <em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>.
      </p></dd><dt id="id-1.4.5.5.8.7.6.2"><span class="term"><code class="option">rbd_options</code></span></dt><dd><p>
       An optional list of parameters to be passed to the underlying
       <code class="command">rbd device map</code> command. These parameters and their
       values should be specified as a comma-separated string, for example:
      </p><div class="verbatim-wrap"><pre class="screen">PARAM1=VAL1,PARAM2=VAL2,...</pre></div><p>
       The example makes the <code class="command">rbdmap</code> script run the following
       command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd device map <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> --PARAM1 VAL1 --PARAM2 VAL2</pre></div><p>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbdmap device map mypool/myimage id=<em class="replaceable">rbd_user</em>,keyring=/etc/ceph/ceph.client.rbd.keyring</pre></div></dd></dl></div><p>
    When run as <code class="command">rbdmap map</code>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using the <code class="command">rbd device map</code> command) and
    then mount the image.
   </p><p>
    When run as <code class="command">rbdmap unmap</code>, images listed in the
    configuration file will be unmounted and unmapped.
   </p><p>
    <code class="command">rbdmap unmap-all</code> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </p><p>
    If successful, the <code class="command">rbd device map</code> operation maps the
    image to a <code class="filename">/dev/rbdX</code> device, at which point a udev
    rule is triggered to create a friendly device name symbolic link
    <code class="filename">/dev/rbd/<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em></code>
    pointing to the real mapped device.
   </p><p>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <code class="filename">/etc/fstab</code>.
    When writing <code class="filename">/etc/fstab</code> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early—before the device in
    question even exists, as <code class="filename">rbdmap.service</code> is typically
    triggered quite late in the boot sequence.
   </p><p>
    For a complete list of <code class="command">rbd</code> options, see the
    <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>).
   </p><p>
    For examples of the <code class="command">rbdmap</code> usage, see the
    <code class="command">rbdmap</code> manual page (<code class="command">man 8 rbdmap</code>).
   </p></section><section class="sect2" id="increasing-size-rbd-device" data-id-title="Increasing the size of RBD devices"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.2.5 </span><span class="title-name">Increasing the size of RBD devices</span> <a title="Permalink" class="permalink" href="#increasing-size-rbd-device">#</a></h3></div></div></div><p>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Increase the size of the RBD image, for example up to 10GB.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</pre></div></li><li class="listitem"><p>
      Grow the file system to fill up the new size of the device.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</pre></div></li></ol></div></section></section><section class="sect1" id="cha-ceph-snapshots-rbd" data-id-title="Snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.3 </span><span class="title-name">Snapshots</span> <a title="Permalink" class="permalink" href="#cha-ceph-snapshots-rbd">#</a></h2></div></div></div><p>
   An RBD snapshot is a snapshot of a RADOS Block Device image. With snapshots, you retain a
   history of the image's state. Ceph also supports snapshot layering, which
   allows you to clone VM images quickly and easily. Ceph supports block
   device snapshots using the <code class="command">rbd</code> command and many
   higher-level interfaces, including QEMU, <code class="systemitem">libvirt</code>,
   OpenStack, and CloudStack.
  </p><div id="id-1.4.5.5.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </p></div><section class="sect2" id="rbd-enable-configure-cephx" data-id-title="Enabling and configuring cephx"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.3.1 </span><span class="title-name">Enabling and configuring <code class="systemitem">cephx</code></span> <a title="Permalink" class="permalink" href="#rbd-enable-configure-cephx">#</a></h3></div></div></div><p>
    When <code class="systemitem">cephx</code> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <a class="xref" href="#cha-storage-cephx" title="Chapter 30. Authentication with cephx">Chapter 30, <em>Authentication with <code class="systemitem">cephx</code></em></a> for more details. You may
    also add the <code class="systemitem">CEPH_ARGS</code> environment variable to
    avoid re-entry of the following parameters.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --id <em class="replaceable">user-ID</em> --keyring=/path/to/secret <em class="replaceable">commands</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd --name <em class="replaceable">username</em> --keyring=/path/to/secret <em class="replaceable">commands</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --id admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <em class="replaceable">commands</em></pre></div><div id="id-1.4.5.5.9.4.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Add the user and secret to the <code class="systemitem">CEPH_ARGS</code>
     environment variable so that you do not need to enter them each time.
    </p></div></section><section class="sect2" id="rbd-snapshot-basics" data-id-title="Snapshot basics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.3.2 </span><span class="title-name">Snapshot basics</span> <a title="Permalink" class="permalink" href="#rbd-snapshot-basics">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <code class="command">rbd</code> command on the command line.
   </p><section class="sect3" id="rbd-creating-snapshots" data-id-title="Creating snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.2.1 </span><span class="title-name">Creating snapshots</span> <a title="Permalink" class="permalink" href="#rbd-creating-snapshots">#</a></h4></div></div></div><p>
     To create a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     create</code> option, the pool name, and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap create --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap create <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool rbd snap create --snap snapshot1 image1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap create rbd/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-listing-snapshots" data-id-title="Listing snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.2.2 </span><span class="title-name">Listing snapshots</span> <a title="Permalink" class="permalink" href="#rbd-listing-snapshots">#</a></h4></div></div></div><p>
     To list snapshots of an image, specify the pool name and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap ls <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap ls <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool rbd snap ls image1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap ls rbd/image1</pre></div></section><section class="sect3" id="rbd-rollback-snapshots" data-id-title="Rolling back snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.2.3 </span><span class="title-name">Rolling back snapshots</span> <a title="Permalink" class="permalink" href="#rbd-rollback-snapshots">#</a></h4></div></div></div><p>
     To rollback to a snapshot with <code class="command">rbd</code>, specify the
     <code class="option">snap rollback</code> option, the pool name, the image name, and
     the snapshot name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rollback --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap rollback <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 snap rollback --snap snapshot1 image1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap rollback pool1/image1@snapshot1</pre></div><div id="id-1.4.5.5.9.5.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <span class="emphasis"><em>faster to
      clone</em></span> from a snapshot <span class="emphasis"><em>than to rollback</em></span> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </p></div></section><section class="sect3" id="rbd-deleting-snapshots" data-id-title="Deleting a snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.2.4 </span><span class="title-name">Deleting a snapshot</span> <a title="Permalink" class="permalink" href="#rbd-deleting-snapshots">#</a></h4></div></div></div><p>
     To delete a snapshot with <code class="command">rbd</code>, specify the <code class="option">snap
     rm</code> option, the pool name, the image name, and the user name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap rm --snap <em class="replaceable">snap-name</em> <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap rm <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snap-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 snap rm --snap snapshot1 image1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap rm pool1/image1@snapshot1</pre></div><div id="id-1.4.5.5.9.5.6.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </p></div></section><section class="sect3" id="rbd-purging-snapshots" data-id-title="Purging snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.2.5 </span><span class="title-name">Purging snapshots</span> <a title="Permalink" class="permalink" href="#rbd-purging-snapshots">#</a></h4></div></div></div><p>
     To delete all snapshots for an image with <code class="command">rbd</code>, specify
     the <code class="option">snap purge</code> option and the image name.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap purge <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap purge <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 snap purge image1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap purge pool1/image1</pre></div></section></section><section class="sect2" id="ceph-snapshoti-layering" data-id-title="Snapshot layering"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.3.3 </span><span class="title-name">Snapshot layering</span> <a title="Permalink" class="permalink" href="#ceph-snapshoti-layering">#</a></h3></div></div></div><p>
    Ceph supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables Ceph block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics—making it possible to create clones rapidly.
   </p><div id="id-1.4.5.5.9.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a Ceph block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </p></div><p>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </p><p>
    A COW clone of a snapshot behaves exactly like any other Ceph block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <span class="emphasis"><em>must</em></span> protect the snapshot before you clone it.
   </p><div id="id-1.4.5.5.9.6.6" data-id-title="--image-format 1 not supported" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">--image-format 1</code> not supported</h6><p>
     You cannot create snapshots of images created with the deprecated
     <code class="command">rbd create --image-format 1</code> option. Ceph only
     supports cloning of the default <span class="emphasis"><em>format 2</em></span> images.
    </p></div><section class="sect3" id="rbd-start-layering" data-id-title="Getting started with layering"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.1 </span><span class="title-name">Getting started with layering</span> <a title="Permalink" class="permalink" href="#rbd-start-layering">#</a></h4></div></div></div><p>
     Ceph block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </p><p>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Image Template</em></span>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, SUSE Linux Enterprise Server), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <code class="command">zypper ref &amp;&amp; zypper patch</code>
       followed by <code class="command">rbd snap create</code>). As the image matures,
       the user can clone any one of the snapshots.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Extended Template</em></span>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Template Pool</em></span>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Image Migration/Recovery</em></span>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </p></li></ul></div></section><section class="sect3" id="rbd-protecting-snapshot" data-id-title="Protecting a snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.2 </span><span class="title-name">Protecting a snapshot</span> <a title="Permalink" class="permalink" href="#rbd-protecting-snapshot">#</a></h4></div></div></div><p>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap protect \
 --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap protect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap protect pool1/image1@snapshot1</pre></div><div id="id-1.4.5.5.9.6.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You cannot delete a protected snapshot.
     </p></div></section><section class="sect3" id="rbd-cloning-snapshots" data-id-title="Cloning a snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.3 </span><span class="title-name">Cloning a snapshot</span> <a title="Permalink" class="permalink" href="#rbd-cloning-snapshots">#</a></h4></div></div></div><p>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd clone --pool <em class="replaceable">pool-name</em> --image <em class="replaceable">parent-image</em> \
 --snap <em class="replaceable">snap-name</em> --dest-pool <em class="replaceable">pool-name</em> \
 --dest <em class="replaceable">child-image</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd clone <em class="replaceable">pool-name</em>/<em class="replaceable">parent-image</em>@<em class="replaceable">snap-name</em> \
<em class="replaceable">pool-name</em>/<em class="replaceable">child-image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd clone pool1/image1@snapshot1 pool1/image2</pre></div><div id="id-1.4.5.5.9.6.9.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </p></div></section><section class="sect3" id="rbd-unprotecting-snapshots" data-id-title="Unprotecting a snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.4 </span><span class="title-name">Unprotecting a snapshot</span> <a title="Permalink" class="permalink" href="#rbd-unprotecting-snapshots">#</a></h4></div></div></div><p>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <span class="emphasis"><em>not</em></span> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> snap unprotect --image <em class="replaceable">image-name</em> \
 --snap <em class="replaceable">snapshot-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd snap unprotect <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<code class="prompt user">cephuser@adm &gt; </code>rbd snap unprotect pool1/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-list-children-snapshots" data-id-title="Listing children of a snapshot"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.5 </span><span class="title-name">Listing children of a snapshot</span> <a title="Permalink" class="permalink" href="#rbd-list-children-snapshots">#</a></h4></div></div></div><p>
     To list the children of a snapshot, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> children --image <em class="replaceable">image-name</em> --snap <em class="replaceable">snap-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd children <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 children --image image1 --snap snapshot1
<code class="prompt user">cephuser@adm &gt; </code>rbd children pool1/image1@snapshot1</pre></div></section><section class="sect3" id="rbd-flatten-cloned-image" data-id-title="Flattening a cloned image"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.3.3.6 </span><span class="title-name">Flattening a cloned image</span> <a title="Permalink" class="permalink" href="#rbd-flatten-cloned-image">#</a></h4></div></div></div><p>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool <em class="replaceable">pool-name</em> flatten --image <em class="replaceable">image-name</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd flatten <em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool pool1 flatten --image image1
<code class="prompt user">cephuser@adm &gt; </code>rbd flatten pool1/image1</pre></div><div id="id-1.4.5.5.9.6.12.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </p></div></section></section></section><section class="sect1" id="ceph-rbd-mirror" data-id-title="RBD image mirrors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.4 </span><span class="title-name">RBD image mirrors</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror">#</a></h2></div></div></div><p>
   RBD images can be asynchronously mirrored between two Ceph clusters. This
   capability is available in two modes:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.10.3.1"><span class="term">Journal-based</span></dt><dd><p>
      This mode uses the RBD journaling image feature to ensure point-in-time,
      crash-consistent replication between clusters. Every write to the RBD
      image is first recorded to the associated journal before modifying the
      actual image. The <code class="literal">remote</code> cluster will read from the
      journal and replay the updates to its local copy of the image. Since each
      write to the RBD image will result in two writes to the Ceph cluster,
      expect write latencies to nearly double when using the RBD journaling
      image feature.
     </p></dd><dt id="id-1.4.5.5.10.3.2"><span class="term">Snapshot-based</span></dt><dd><p>
      This mode uses periodically-scheduled or manually-created RBD image
      mirror-snapshots to replicate crash-consistent RBD images between
      clusters. The <code class="literal">remote</code> cluster will determine any data
      or metadata updates between two mirror-snapshots, and copy the deltas to
      its local copy of the image. With the help of the RBD fast-diff image
      feature, updated data blocks can be quickly computed without the need to
      scan the full RBD image. Since this mode is not point-in-time consistent,
      the full snapshot delta will need to be synchronized prior to use during
      a failover scenario. Any partially-applied snapshot deltas will be rolled
      back to the last fully synchronized snapshot prior to use.
     </p></dd></dl></div><p>
   Mirroring is configured on a per-pool basis within peer clusters. This can
   be configured on a specific subset of images within the pool, or configured
   to automatically mirror all images within a pool when using journal-based
   mirroring only. Mirroring is configured using the <code class="command">rbd</code>
   command. The <code class="systemitem">rbd-mirror</code> daemon is responsible for pulling image updates
   from the <code class="literal">remote</code>, peer cluster and applying them to the
   image within the <code class="literal">local</code> cluster.
  </p><p>
   Depending on the desired needs for replication, RBD mirroring can be
   configured for either one- or two-way replication:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.10.6.1"><span class="term">One-way Replication</span></dt><dd><p>
      When data is only mirrored from a primary cluster to a secondary cluster,
      the <code class="systemitem">rbd-mirror</code> daemon runs only on the secondary cluster.
     </p></dd><dt id="id-1.4.5.5.10.6.2"><span class="term">Two-way Replication</span></dt><dd><p>
      When data is mirrored from primary images on one cluster to non-primary
      images on another cluster (and vice-versa), the <code class="systemitem">rbd-mirror</code> daemon runs
      on both clusters.
     </p></dd></dl></div><div id="id-1.4.5.5.10.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    Each instance of the <code class="systemitem">rbd-mirror</code> daemon needs to be able to connect to both
    the <code class="literal">local</code> and <code class="literal">remote</code> Ceph clusters
    simultaneously. For example, all monitor and OSD hosts. Additionally, the
    network needs to have sufficient bandwidth between the two data centers to
    handle mirroring workload.
   </p></div><section class="sect2" id="ceph-rbd-mirror-poolconfig" data-id-title="Pool configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.1 </span><span class="title-name">Pool configuration</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror-poolconfig">#</a></h3></div></div></div><p>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <code class="command">rbd</code> command. Mirroring is configured on a per-pool basis
    within the Ceph clusters.
   </p><p>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named <code class="literal">local</code> and
    <code class="literal">remote</code>, are accessible from a single host for clarity.
   </p><p>
    See the <code class="command">rbd</code> manual page (<code class="command">man 8 rbd</code>)
    for additional details on how to connect to different Ceph clusters.
   </p><div id="id-1.4.5.5.10.8.5" data-id-title="Multiple clusters" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Multiple clusters</h6><p>
     The cluster name in the following examples corresponds to a Ceph
     configuration file of the same name
     <code class="filename">/etc/ceph/remote.conf</code> and Ceph keyring file of the
     same name <code class="filename">/etc/ceph/remote.client.admin.keyring</code>.
    </p></div><section class="sect3" id="rbd-enable-mirroring-pool" data-id-title="Enable mirroring on a pool"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.1 </span><span class="title-name">Enable mirroring on a pool</span> <a title="Permalink" class="permalink" href="#rbd-enable-mirroring-pool">#</a></h4></div></div></div><p>
     To enable mirroring on a pool, specify the <code class="command">mirror pool
     enable</code> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.10.8.6.3.1"><span class="term">pool</span></dt><dd><p>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </p></dd><dt id="id-1.4.5.5.10.8.6.3.2"><span class="term">image</span></dt><dd><p>
        Mirroring needs to be explicitly enabled on each image. See
        <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="20.4.2.1. Enabling image mirroring">Section 20.4.2.1, “Enabling image mirroring”</a> for more
        information.
       </p></dd></dl></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror pool enable <em class="replaceable">POOL_NAME</em> pool
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster remote mirror pool enable <em class="replaceable">POOL_NAME</em> pool</pre></div></section><section class="sect3" id="rbd-disable-mirroring-pool" data-id-title="Disable mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.2 </span><span class="title-name">Disable mirroring</span> <a title="Permalink" class="permalink" href="#rbd-disable-mirroring-pool">#</a></h4></div></div></div><p>
     To disable mirroring on a pool, specify the <code class="command">mirror pool
     disable</code> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror pool disable <em class="replaceable">POOL_NAME</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster remote mirror pool disable <em class="replaceable">POOL_NAME</em></pre></div></section><section class="sect3" id="ceph-rbd-mirror-bootstrap-peer" data-id-title="Bootstrapping peers"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.3 </span><span class="title-name">Bootstrapping peers</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror-bootstrap-peer">#</a></h4></div></div></div><p>
     In order for the <code class="systemitem">rbd-mirror</code> daemon to discover its peer cluster, the peer
     needs to be registered to the pool and a user account needs to be created.
     This process can be automated with <code class="command">rbd</code> and the
     <code class="command">mirror pool peer bootstrap create</code> and <code class="command">mirror
     pool peer bootstrap import</code> commands.
    </p><p>
     To manually create a new bootstrap token with <code class="command">rbd</code>,
     specify the <code class="command">mirror pool peer bootstrap create</code> command,
     a pool name, along with an optional friendly site name to describe the
     <code class="literal">local</code> cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@local &gt; </code>rbd mirror pool peer bootstrap create \
 [--site-name <em class="replaceable">LOCAL_SITE_NAME</em>] <em class="replaceable">POOL_NAME</em></pre></div><p>
     The output of <code class="command">mirror pool peer bootstrap create</code> will be
     a token that should be provided to the <code class="command">mirror pool peer bootstrap
     import</code> command. For example, on the <code class="literal">local</code>
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@local &gt; </code>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</pre></div><p>
     To manually import the bootstrap token created by another cluster with the
     <code class="command">rbd</code> command, use the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen">rbd mirror pool peer bootstrap import \
 [--site-name <em class="replaceable">LOCAL_SITE_NAME</em>] \
 [--direction <em class="replaceable">DIRECTION</em> \
 <em class="replaceable">POOL_NAME</em> <em class="replaceable">TOKEN_PATH</em></pre></div><p>
     Where:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.10.8.8.10.1"><span class="term"><em class="replaceable">LOCAL_SITE_NAME</em></span></dt><dd><p>
        An optional friendly site name to describe the <code class="literal">local</code>
        cluster.
       </p></dd><dt id="id-1.4.5.5.10.8.8.10.2"><span class="term"><em class="replaceable">DIRECTION</em></span></dt><dd><p>
        A mirroring direction. Defaults to <code class="literal">rx-tx</code> for
        bidirectional mirroring, but can also be set to
        <code class="literal">rx-only</code> for unidirectional mirroring.
       </p></dd><dt id="id-1.4.5.5.10.8.8.10.3"><span class="term"><em class="replaceable">POOL_NAME</em></span></dt><dd><p>
        Name of the pool.
       </p></dd><dt id="id-1.4.5.5.10.8.8.10.4"><span class="term"><em class="replaceable">TOKEN_PATH</em></span></dt><dd><p>
        A file path to the created token (or <code class="literal">-</code> to read it
        from the standard input).
       </p></dd></dl></div><p>
     For example, on the <code class="literal">remote</code> cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@remote &gt; </code>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</pre></div></section><section class="sect3" id="ceph-rbd-mirror-add-peer" data-id-title="Adding a cluster peer manually"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.4 </span><span class="title-name">Adding a cluster peer manually</span> <a title="Permalink" class="permalink" href="#ceph-rbd-mirror-add-peer">#</a></h4></div></div></div><p>
     Alternatively to bootstrapping peers as described in
     <a class="xref" href="#ceph-rbd-mirror-bootstrap-peer" title="20.4.1.3. Bootstrapping peers">Section 20.4.1.3, “Bootstrapping peers”</a>, you can specify
     peers manually. The remote <code class="systemitem">rbd-mirror</code> daemon will need access to the
     local cluster to perform mirroring. Create a new local Ceph user that
     the remote <code class="systemitem">rbd-mirror</code> daemon will use, for example
     <code class="literal">rbd-mirror-peer</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'</pre></div><p>
     Use the following syntax to add a mirroring peer Ceph cluster with the
     <code class="command">rbd</code> command:
    </p><div class="verbatim-wrap"><pre class="screen">rbd mirror pool peer add <em class="replaceable">POOL_NAME</em> <em class="replaceable">CLIENT_NAME</em>@<em class="replaceable">CLUSTER_NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a</pre></div><p>
     By default, the <code class="systemitem">rbd-mirror</code> daemon needs to have access to the Ceph
     configuration file located at
     <code class="filename">/etc/ceph/.<em class="replaceable">CLUSTER_NAME</em>.conf</code>.
     It provides IP addresses of the peer cluster’s MONs and a keyring for a
     client named <em class="replaceable">CLIENT_NAME</em> located in the default
     or custom keyring search paths, for example
     <code class="filename">/etc/ceph/<em class="replaceable">CLUSTER_NAME</em>.<em class="replaceable">CLIENT_NAME</em>.keyring</code>.
    </p><p>
     Alternatively, the peer cluster’s MON and/or client key can be securely
     stored within the local Ceph config-key store. To specify the peer
     cluster connection attributes when adding a mirroring peer, use the
     <code class="option">--remote-mon-host</code> and <code class="option">--remote-key-file</code>
     options. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <em class="replaceable">/PATH/TO/KEY_FILE</em>
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...</pre></div></section><section class="sect3" id="rbd-remove-cluster-peer" data-id-title="Remove cluster peer"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.5 </span><span class="title-name">Remove cluster peer</span> <a title="Permalink" class="permalink" href="#rbd-remove-cluster-peer">#</a></h4></div></div></div><p>
     To remove a mirroring peer cluster, specify the <code class="command">mirror pool peer
     remove</code> subcommand, the pool name, and the peer UUID (available
     from the <code class="command">rbd mirror pool info</code> command):
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 55672766-c02b-4729-8567-f13a66893445
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster remote mirror pool peer remove <em class="replaceable">POOL_NAME</em> \
 60c0e299-b38f-4234-91f6-eed0a367be08</pre></div></section><section class="sect3" id="rbd-data-pools" data-id-title="Data pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.1.6 </span><span class="title-name">Data pools</span> <a title="Permalink" class="permalink" href="#rbd-data-pools">#</a></h4></div></div></div><p>
     When creating images in the destination cluster, <code class="systemitem">rbd-mirror</code> selects a
     data pool as follows:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If the destination cluster has a default data pool configured (with the
       <code class="option">rbd_default_data_pool</code> configuration option), it will be
       used.
      </p></li><li class="listitem"><p>
       Otherwise, if the source image uses a separate data pool, and a pool
       with the same name exists on the destination cluster, that pool will be
       used.
      </p></li><li class="listitem"><p>
       If neither of the above is true, no data pool will be set.
      </p></li></ul></div></section></section><section class="sect2" id="rbd-mirror-imageconfig" data-id-title="RBD Image configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.2 </span><span class="title-name">RBD Image configuration</span> <a title="Permalink" class="permalink" href="#rbd-mirror-imageconfig">#</a></h3></div></div></div><p>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer Ceph cluster.
   </p><p>
    Mirrored RBD images are designated as either <span class="emphasis"><em>primary</em></span>
    or <span class="emphasis"><em>non-primary</em></span>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </p><p>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <a class="xref" href="#rbd-mirror-enable-image-mirroring" title="20.4.2.1. Enabling image mirroring">Section 20.4.2.1, “Enabling image mirroring”</a>) by the
    <code class="command">rbd</code> command).
   </p><section class="sect3" id="rbd-mirror-enable-image-mirroring" data-id-title="Enabling image mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.1 </span><span class="title-name">Enabling image mirroring</span> <a title="Permalink" class="permalink" href="#rbd-mirror-enable-image-mirroring">#</a></h4></div></div></div><p>
     If mirroring is configured in the <code class="literal">image</code> mode, then it
     is necessary to explicitly enable mirroring for each image within the
     pool. To enable mirroring for a specific image with
     <code class="command">rbd</code>, specify the <code class="command">mirror image enable</code>
     subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image enable \
 <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     The mirror image mode can either be <code class="literal">journal</code> or
     <code class="literal">snapshot</code>:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.10.9.5.5.1"><span class="term">journal (default)</span></dt><dd><p>
        When configured in <code class="literal">journal</code> mode, mirroring will use
        the RBD journaling image feature to replicate the image contents. If
        the RBD journaling image feature is not yet enabled on the image, it
        will be automatically enabled.
       </p></dd><dt id="id-1.4.5.5.10.9.5.5.2"><span class="term">snapshot</span></dt><dd><p>
        When configured in <code class="literal">snapshot</code> mode, mirroring will use
        RBD image mirror-snapshots to replicate the image contents. When
        enabled, an initial mirror-snapshot will automatically be created.
        Additional RBD image mirror-snapshots can be created by the
        <code class="command">rbd</code> command.
       </p></dd></dl></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image enable image-pool/image-2 journal</pre></div></section><section class="sect3" id="rbd-enable-image-jouranling" data-id-title="Enabling the image journaling feature"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.2 </span><span class="title-name">Enabling the image journaling feature</span> <a title="Permalink" class="permalink" href="#rbd-enable-image-jouranling">#</a></h4></div></div></div><p>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. When using the
     <code class="literal">image</code> mirroring mode, the journaling feature will be
     automatically enabled if mirroring is enabled on the image. When using the
     <code class="literal">pool</code> mirroring mode, before an image can be mirrored to
     a peer cluster, the RBD image journaling feature must be enabled. The
     feature can be enabled at image creation time by providing the
     <code class="option">--image-feature exclusive-lock,journaling</code> option to the
     <code class="command">rbd</code> command.
    </p><p>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <code class="command">feature enable</code> subcommand, the pool and image name, and
     the feature name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local feature enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> exclusive-lock
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local feature enable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em> journaling</pre></div><div id="id-1.4.5.5.10.9.6.5" data-id-title="Option dependency" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Option dependency</h6><p>
      The <code class="option">journaling</code> feature is dependent on the
      <code class="option">exclusive-lock</code> feature. If the
      <code class="option">exclusive-lock</code> feature is not already enabled, you need
      to enable it prior to enabling the <code class="option">journaling</code> feature.
     </p></div><div id="id-1.4.5.5.10.9.6.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      You can enable journaling on all new images by default by adding
      <code class="option">rbd default features =
      layering,exclusive-lock,object-map,deep-flatten,journaling</code> to
      your Ceph configuration file.
     </p></div></section><section class="sect3" id="rbd-create-image-mirror-snapshots" data-id-title="Creating image mirror-snapshots"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.3 </span><span class="title-name">Creating image mirror-snapshots</span> <a title="Permalink" class="permalink" href="#rbd-create-image-mirror-snapshots">#</a></h4></div></div></div><p>
     When using snapshot-based mirroring, mirror-snapshots will need to be
     created whenever it is desired to mirror the changed contents of the RBD
     image. To create a mirror-snapshot manually with <code class="command">rbd</code>,
     specify the <code class="command">mirror image snapshot</code> command along with
     the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror image snapshot <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image snapshot image-pool/image-1</pre></div><p>
     By default only three mirror-snapshots will be created per image. The most
     recent mirror-snapshot is automatically pruned if the limit is reached.
     The limit can be overridden via the
     <code class="option">rbd_mirroring_max_mirroring_snapshots</code> configuration
     option if required. Additionally, mirror-snapshots are automatically
     deleted when the image is removed or when mirroring is disabled.
    </p><p>
     Mirror-snapshots can also be automatically created on a periodic basis if
     mirror-snapshot schedules are defined. The mirror-snapshot can be
     scheduled globally, per-pool, or per-image levels. Multiple
     mirror-snapshot schedules can be defined at any level, but only the
     most-specific snapshot schedules that match an individual mirrored image
     will run.
    </p><p>
     To create a mirror-snapshot schedule with <code class="command">rbd</code>, specify
     the <code class="command">mirror snapshot schedule add</code> command along with an
     optional pool or image name, interval, and optional start time.
    </p><p>
     The interval can be specified in days, hours, or minutes using the
     suffixes <code class="option">d</code>, <code class="option">h</code>, or <code class="option">m</code>
     respectively. The optional start time can be specified using the ISO 8601
     time format. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h</pre></div><p>
     To remove a mirror-snapshot schedule with <code class="command">rbd</code>, specify
     the <code class="command">mirror snapshot schedule remove</code> command with
     options that match the corresponding add schedule command.
    </p><p>
     To list all snapshot schedules for a specific level (global, pool, or
     image) with <code class="command">rbd</code>, specify the <code class="command">mirror snapshot
     schedule ls</code> command along with an optional pool or image name.
     Additionally, the <code class="option">--recursive</code> option can be specified to
     list all schedules at the specified level and below. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h</pre></div><p>
     To find out when the next snapshots will be created for snapshot-based
     mirroring RBD images with <code class="command">rbd</code>, specify the
     <code class="command">mirror snapshot schedule status</code> command along with an
     optional pool or image name. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1</pre></div></section><section class="sect3" id="rbd-disenable-image-mirroring" data-id-title="Disabling image mirroring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.4 </span><span class="title-name">Disabling image mirroring</span> <a title="Permalink" class="permalink" href="#rbd-disenable-image-mirroring">#</a></h4></div></div></div><p>
     To disable mirroring for a specific image, specify the <code class="command">mirror
     image disable</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image disable <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section><section class="sect3" id="rbd-image-promotion-demotion" data-id-title="Promoting and demoting images"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.5 </span><span class="title-name">Promoting and demoting images</span> <a title="Permalink" class="permalink" href="#rbd-image-promotion-demotion">#</a></h4></div></div></div><p>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </p><div id="id-1.4.5.5.10.9.9.3" data-id-title="Forced promotion" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Forced promotion</h6><p>
      Promotion can be forced using the <code class="option">--force</code> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <code class="command">resync</code> subcommand is issued.
     </p></div><p>
     To demote a specific image to non-primary, specify the <code class="command">mirror
     image demote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror image demote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To demote all primary images within a pool to non-primary, specify the
     <code class="command">mirror pool demote</code> subcommand along with the pool name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror pool demote <em class="replaceable">POOL_NAME</em></pre></div><p>
     To promote a specific image to primary, specify the <code class="command">mirror image
     promote</code> subcommand along with the pool and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster remote mirror image promote <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
     To promote all non-primary images within a pool to primary, specify the
     <code class="command">mirror pool promote</code> subcommand along with the pool
     name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --cluster local mirror pool promote <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.4.5.5.10.9.9.12" data-id-title="Split I/O load" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Split I/O load</h6><p>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </p></div></section><section class="sect3" id="rbd-force-image-resync" data-id-title="Forcing image resync"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">20.4.2.6 </span><span class="title-name">Forcing image resync</span> <a title="Permalink" class="permalink" href="#rbd-force-image-resync">#</a></h4></div></div></div><p>
     If a split-brain event is detected by the <code class="systemitem">rbd-mirror</code> daemon, it will not
     attempt to mirror the affected image until corrected. To resume mirroring
     for an image, first demote the image determined to be out of date and then
     request a resync to the primary image. To request an image resync, specify
     the <code class="command">mirror image resync</code> subcommand along with the pool
     and image name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror image resync <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div></section></section><section class="sect2" id="rbd-mirror-status" data-id-title="Checking the mirror status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.4.3 </span><span class="title-name">Checking the mirror status</span> <a title="Permalink" class="permalink" href="#rbd-mirror-status">#</a></h3></div></div></div><p>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <code class="command">mirror image
    status</code> and <code class="command">mirror pool status</code> subcommands:
   </p><p>
    To request the mirror image status, specify the <code class="command">mirror image
    status</code> subcommand along with the pool and image name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror image status <em class="replaceable">POOL_NAME</em>/<em class="replaceable">IMAGE_NAME</em></pre></div><p>
    To request the mirror pool summary status, specify the <code class="command">mirror pool
    status</code> subcommand along with the pool name:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd mirror pool status <em class="replaceable">POOL_NAME</em></pre></div><div id="id-1.4.5.5.10.10.7" data-id-title="" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: </h6><p>
     Adding the <code class="option">--verbose</code> option to the <code class="command">mirror pool
     status</code> subcommand will additionally output status details for
     every mirroring image in the pool.
    </p></div></section></section><section class="sect1" id="rbd-cache-settings" data-id-title="Cache settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.5 </span><span class="title-name">Cache settings</span> <a title="Permalink" class="permalink" href="#rbd-cache-settings">#</a></h2></div></div></div><p>
   The user space implementation of the Ceph block device
   (<code class="systemitem">librbd</code>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <span class="emphasis"><em>Least
   Recently Used</em></span> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </p><p>
   Ceph supports write-back caching for RBD. To enable it, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set client rbd_cache true</pre></div><p>
   By default, <code class="systemitem">librbd</code> does not perform any caching.
   Writes and reads go directly to the storage cluster, and writes return only
   when the data is on disk on all replicas. With caching enabled, writes
   return immediately, unless there are more unflushed bytes than set in the
   <code class="option">rbd cache max dirty</code> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </p><p>
   Ceph supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, run
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set client rbd_cache_max_dirty 0</pre></div><p>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </p><p>
   The following parameters affect the behavior of RADOS Block Devices. To set them, use the
   <code class="literal">client</code> category:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set client <em class="replaceable">PARAMETER</em> <em class="replaceable">VALUE</em></pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.11.11.1"><span class="term"><code class="option">rbd cache</code></span></dt><dd><p>
      Enable caching for RADOS Block Device (RBD). Default is 'true'.
     </p></dd><dt id="id-1.4.5.5.11.11.2"><span class="term"><code class="option">rbd cache size</code></span></dt><dd><p>
      The RBD cache size in bytes. Default is 32 MB.
     </p></dd><dt id="id-1.4.5.5.11.11.3"><span class="term"><code class="option">rbd cache max dirty</code></span></dt><dd><p>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <code class="option">rbd cache max dirty</code> needs to be less than <code class="option">rbd
      cache size</code>. If set to 0, uses write-through caching. Default is
      24 MB.
     </p></dd><dt id="id-1.4.5.5.11.11.4"><span class="term"><code class="option">rbd cache target dirty</code></span></dt><dd><p>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </p></dd><dt id="id-1.4.5.5.11.11.5"><span class="term"><code class="option">rbd cache max dirty age</code></span></dt><dd><p>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </p></dd><dt id="id-1.4.5.5.11.11.6"><span class="term"><code class="option">rbd cache writethrough until flush</code></span></dt><dd><p>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <code class="systemitem">rbd</code>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-qos" data-id-title="QoS settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.6 </span><span class="title-name">QoS settings</span> <a title="Permalink" class="permalink" href="#rbd-qos">#</a></h2></div></div></div><p>
   Generally, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </p><div id="id-1.4.5.5.12.3" data-id-title="Not supported by iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Not supported by iSCSI</h6><p>
    The following QoS settings are used only by the user space RBD
    implementation <code class="systemitem">librbd</code> and
    <span class="emphasis"><em>not</em></span> used by the <code class="systemitem">kRBD</code>
    implementation. Because iSCSI uses <code class="systemitem">kRBD</code>, it does
    not use the QoS settings. However, for iSCSI you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.12.4.1"><span class="term"><code class="option">rbd qos iops limit</code></span></dt><dd><p>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.2"><span class="term"><code class="option">rbd qos bps limit</code></span></dt><dd><p>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.3"><span class="term"><code class="option">rbd qos read iops limit</code></span></dt><dd><p>
      The desired limit of read operations per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.4"><span class="term"><code class="option">rbd qos write iops limit</code></span></dt><dd><p>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </p></dd><dt id="id-1.4.5.5.12.4.5"><span class="term"><code class="option">rbd qos read bps limit</code></span></dt><dd><p>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.6"><span class="term"><code class="option">rbd qos write bps limit</code></span></dt><dd><p>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.7"><span class="term"><code class="option">rbd qos iops burst</code></span></dt><dd><p>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.8"><span class="term"><code class="option">rbd qos bps burst</code></span></dt><dd><p>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.9"><span class="term"><code class="option">rbd qos read iops burst</code></span></dt><dd><p>
      The desired burst limit of read operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.10"><span class="term"><code class="option">rbd qos write iops burst</code></span></dt><dd><p>
      The desired burst limit of write operations. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.11"><span class="term"><code class="option">rbd qos read bps burst</code></span></dt><dd><p>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.12"><span class="term"><code class="option">rbd qos write bps burst</code></span></dt><dd><p>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </p></dd><dt id="id-1.4.5.5.12.4.13"><span class="term"><code class="option">rbd qos schedule tick min</code></span></dt><dd><p>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </p></dd></dl></div></section><section class="sect1" id="rbd-readahead-settings" data-id-title="Read-ahead settings"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.7 </span><span class="title-name">Read-ahead settings</span> <a title="Permalink" class="permalink" href="#rbd-readahead-settings">#</a></h2></div></div></div><p>
   RADOS Block Device supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </p><div id="id-1.4.5.5.13.3" data-id-title="Not supported by iSCSI" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Not supported by iSCSI</h6><p>
    The following read-ahead settings are used only by the user space RBD
    implementation <code class="systemitem">librbd</code> and
    <span class="emphasis"><em>not</em></span> used by the <code class="systemitem">kRBD</code>
    implementation. Because iSCSI uses <code class="systemitem">kRBD</code>, it does
    not use the read-ahead settings. However, for iSCSI you can configure
    read-ahead on the kernel block device layer using standard kernel
    facilities.
   </p></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.13.4.1"><span class="term"><code class="option">rbd readahead trigger requests</code></span></dt><dd><p>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </p></dd><dt id="id-1.4.5.5.13.4.2"><span class="term"><code class="option">rbd readahead max bytes</code></span></dt><dd><p>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </p></dd><dt id="id-1.4.5.5.13.4.3"><span class="term"><code class="option">rbd readahead disable after bytes</code></span></dt><dd><p>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </p></dd></dl></div></section><section class="sect1" id="rbd-features" data-id-title="Advanced features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.8 </span><span class="title-name">Advanced features</span> <a title="Permalink" class="permalink" href="#rbd-features">#</a></h2></div></div></div><p>
   RADOS Block Device supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the Ceph configuration file by using the
   <code class="option">rbd_default_features</code> option.
  </p><p>
   You can specify the values of the <code class="option">rbd_default_features</code>
   option in two ways:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     As a sum of features' internal values. Each feature has its own internal
     value—for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = 17</pre></div></li><li class="listitem"><p>
     As a comma-separated list of features. The previous example will look as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rbd_default_features = layering,fast-diff</pre></div></li></ul></div><div id="id-1.4.5.5.14.5" data-id-title="Features not supported by iSCSI" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Features not supported by iSCSI</h6><p>
    RBD images with the following features will not be supported by iSCSI:
    <code class="option">deep-flatten</code>, <code class="option">object-map</code>,
    <code class="option">journaling</code>, <code class="option">fast-diff</code>,
    <code class="option">striping</code>
   </p></div><p>
   A list of advanced RBD features follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.5.14.7.1"><span class="term"><code class="option">layering</code></span></dt><dd><p>
      Layering enables you to use cloning.
     </p><p>
      Internal value is 1, default is 'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.2"><span class="term"><code class="option">striping</code></span></dt><dd><p>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy RADOS Block Devices.
     </p><p>
      Internal value is 2, default is 'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.3"><span class="term"><code class="option">exclusive-lock</code></span></dt><dd><p>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.4"><span class="term"><code class="option">object-map</code></span></dt><dd><p>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </p><p>
      Internal value is 8, default is 'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.5"><span class="term"><code class="option">fast-diff</code></span></dt><dd><p>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </p><p>
      Internal value is 16, default is 'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.6"><span class="term"><code class="option">deep-flatten</code></span></dt><dd><p>
      Deep-flatten makes the <code class="command">rbd flatten</code> (see
      <a class="xref" href="#rbd-flatten-cloned-image" title="20.3.3.6. Flattening a cloned image">Section 20.3.3.6, “Flattening a cloned image”</a>) work on all the snapshots of
      an image, in addition to the image itself. Without it, snapshots of an
      image will still rely on the parent, therefore you will not be able to
      delete the parent image until the snapshots are deleted. Deep-flatten
      makes a parent independent of its clones, even if they have snapshots.
     </p><p>
      Internal value is 32, default is 'yes'.
     </p></dd><dt id="id-1.4.5.5.14.7.7"><span class="term"><code class="option">journaling</code></span></dt><dd><p>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <a class="xref" href="#ceph-rbd-mirror" title="20.4. RBD image mirrors">Section 20.4, “RBD image mirrors”</a>) uses the journal to replicate a crash
      consistent image to a <code class="literal">remote</code> cluster.
     </p><p>
      Internal value is 64, default is 'no'.
     </p></dd></dl></div></section><section class="sect1" id="rbd-old-clients-map" data-id-title="Mapping RBD using old kernel clients"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.9 </span><span class="title-name">Mapping RBD using old kernel clients</span> <a title="Permalink" class="permalink" href="#rbd-old-clients-map">#</a></h2></div></div></div><p>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with SUSE Enterprise Storage 7.1 forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </p><div class="verbatim-wrap"><pre class="screen">2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000</pre></div><div id="id-1.4.5.5.15.4" data-id-title="Changing CRUSH Map bucket types causes massive rebalancing" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Changing CRUSH Map bucket types causes massive rebalancing</h6><p>
    If you intend to switch the CRUSH Map bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Disable any RBD image features that are not supported. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd feature disable pool1/image1 object-map
<code class="prompt user">cephuser@adm &gt; </code>rbd feature disable pool1/image1 exclusive-lock</pre></div></li><li class="step"><p>
     Change the CRUSH Map bucket types from 'straw2' to 'straw':
    </p><ol type="a" class="substeps"><li class="step"><p>
       Save the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap -o crushmap.original</pre></div></li><li class="step"><p>
       Decompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -d crushmap.original -o crushmap.txt</pre></div></li><li class="step"><p>
       Edit the CRUSH Map and replace 'straw2' with 'straw'.
      </p></li><li class="step"><p>
       Recompile the CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -c crushmap.txt -o crushmap.new</pre></div></li><li class="step"><p>
       Set the new CRUSH Map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd setcrushmap -i crushmap.new</pre></div></li></ol></li></ol></div></div></section><section class="sect1" id="rbd-kubernetes" data-id-title="Enabling block devices and Kubernetes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">20.10 </span><span class="title-name">Enabling block devices and Kubernetes</span> <a title="Permalink" class="permalink" href="#rbd-kubernetes">#</a></h2></div></div></div><p>
   You can use Ceph RBD with Kubernetes v1.13 and higher through the
   <code class="literal">ceph-csi</code> driver. This driver dynamically provisions RBD
   images to back Kubernetes volumes, and maps these RBD images as block devices
   (optionally mounting a file system contained within the image) on worker
   nodes running pods that reference an RBD-backed volume.
  </p><p>
   To use Ceph block devices with Kubernetes, you must install and configure
   <code class="literal">ceph-csi</code> within your Kubernetes environment.
  </p><div id="id-1.4.5.5.16.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    <code class="literal">ceph-csi</code> uses the RBD kernel modules by default which
    may not support all Ceph CRUSH tunables or RBD image features.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     By default, Ceph block devices use the RBD pool. Create a pool for
     Kubernetes volume storage. Ensure your Ceph cluster is running, then create
     the pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create kubernetes</pre></div></li><li class="step"><p>
     Use the RBD tool to initialize the pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd pool init kubernetes</pre></div></li><li class="step"><p>
     Create a new user for Kubernetes and <code class="literal">ceph-csi</code>. Execute the
     following and record the generated key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</pre></div></li><li class="step"><p>
     <code class="literal">ceph-csi</code> requires a ConfigMap object stored in Kubernetes
     to define the Ceph monitor addresses for the Ceph cluster. Collect
     both the Ceph cluster unique fsid and the monitor addresses:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</pre></div></li><li class="step"><p>
     Generate a <code class="filename">csi-config-map.yaml</code> file similar to the
     example below, substituting the FSID for <code class="literal">clusterID</code>, and
     the monitor addresses for <code class="literal">monitors</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</pre></div></li><li class="step"><p>
     When generated, store the new ConfigMap object in Kubernetes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f csi-config-map.yaml</pre></div></li><li class="step"><p>
     <code class="literal">ceph-csi</code> requires the cephx credentials for
     communicating with the Ceph cluster. Generate a
     <code class="filename">csi-rbd-secret.yaml</code> file similar to the example
     below, using the newly-created Kubernetes user ID and cephx key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</pre></div></li><li class="step"><p>
     When generated, store the new secret object in Kubernetes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f csi-rbd-secret.yaml</pre></div></li><li class="step"><p>
     Create the required ServiceAccount and RBAC ClusterRole/ClusterRoleBinding
     Kubernetes objects. These objects do not necessarily need to be customized for
     your Kubernetes environment, and therefore can be used directly from the
     <code class="literal">ceph-csi</code> deployment YAML files:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</pre></div></li><li class="step"><p>
     Create the <code class="literal">ceph-csi</code> provisioner and node plugins:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f csi-rbdplugin-provisioner.yaml
<code class="prompt user">kubectl@adm &gt; </code>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f csi-rbdplugin.yaml</pre></div><div id="id-1.4.5.5.16.5.10.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      By default, the provisioner and node plugin YAML files will pull the
      development release of the <code class="literal">ceph-csi</code> container. The
      YAML files should be updated to use a release version.
     </p></div></li></ol></div></div><section class="sect2" id="using-rbd-kubernetes" data-id-title="Using Ceph block devices in Kubernetes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">20.10.1 </span><span class="title-name">Using Ceph block devices in Kubernetes</span> <a title="Permalink" class="permalink" href="#using-rbd-kubernetes">#</a></h3></div></div></div><p>
    The Kubernetes StorageClass defines a class of storage. Multiple StorageClass
    objects can be created to map to different quality-of-service levels and
    features. For example, NVMe versus HDD-based pools.
   </p><p>
    To create a <code class="literal">ceph-csi</code> StorageClass that maps to the
    Kubernetes pool created above, the following YAML file can be used, after
    ensuring that the <code class="literal">clusterID</code> property matches your Ceph
    cluster's FSID:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f csi-rbd-sc.yaml</pre></div><p>
    A <code class="literal">PersistentVolumeClaim</code> is a request for abstract
    storage resources by a user. The <code class="literal">PersistentVolumeClaim</code>
    would then be associated to a pod resource to provision a
    <code class="literal">PersistentVolume</code>, which would be backed by a Ceph
    block image. An optional <code class="option">volumeMode</code> can be included to
    select between a mounted file system (default) or raw block-device-based
    volume.
   </p><p>
    Using <code class="literal">ceph-csi</code>, specifying <code class="option">Filesystem</code>
    for <code class="option">volumeMode</code> can support both
    <code class="literal">ReadWriteOnce</code> and <code class="literal">ReadOnlyMany
    accessMode</code> claims, and specifying <code class="option">Block</code> for
    <code class="option">volumeMode</code> can support <code class="literal">ReadWriteOnce</code>,
    <code class="literal">ReadWriteMany</code>, and <code class="literal">ReadOnlyMany
    accessMode</code> claims.
   </p><p>
    For example, to create a block-based
    <code class="literal">PersistentVolumeClaim</code> that uses the
    <code class="literal">ceph-csi-based StorageClass</code> created above, the following
    YAML file can be used to request raw block storage from the
    <code class="literal">csi-rbd-sc StorageClass</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f raw-block-pvc.yaml</pre></div><p>
    The following demonstrates and example of binding the above
    <code class="literal">PersistentVolumeClaim</code> to a pod resource as a raw block
    device:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f raw-block-pod.yaml</pre></div><p>
    To create a file-system-based <code class="literal">PersistentVolumeClaim</code> that
    uses the <code class="literal">ceph-csi-based StorageClass</code> created above, the
    following YAML file can be used to request a mounted file system (backed by
    an RBD image) from the <code class="literal">csi-rbd-sc StorageClass</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f pvc.yaml</pre></div><p>
    The following demonstrates an example of binding the above
    <code class="literal">PersistentVolumeClaim</code> to a pod resource as a mounted
    file system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<code class="prompt user">kubectl@adm &gt; </code>kubectl apply -f pod.yaml</pre></div></section></section></section></div><div class="part" id="part-accessing-data" data-id-title="Accessing Cluster Data"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part IV </span><span class="title-name">Accessing Cluster Data </span><a title="Permalink" class="permalink" href="#part-accessing-data">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-gw"><span class="title-number">21 </span><span class="title-name">Ceph Object Gateway</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-iscsi"><span class="title-number">22 </span><span class="title-name">Ceph iSCSI gateway</span></a></span></li><dd class="toc-abstract"><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.5 “Deploying iSCSI Gateways”</span>.
 </p></dd><li><span class="chapter"><a href="#cha-ceph-cephfs"><span class="title-number">23 </span><span class="title-name">Clustered file system</span></a></span></li><dd class="toc-abstract"><p>This chapter describes administration tasks that are normally performed after the cluster is set up and CephFS exported. If you need more information on setting up CephFS, refer to Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.3 “Deploying Meta…</p></dd><li><span class="chapter"><a href="#cha-ses-cifs"><span class="title-number">24 </span><span class="title-name">Export Ceph data via Samba</span></a></span></li><dd class="toc-abstract"><p>This chapter describes how to export data stored in a Ceph cluster via a Samba/CIFS share so that you can easily access them from Windows* client machines. It also includes information that will help you configure a Ceph Samba gateway to join Active Directory in the Windows* domain to authenticate a…</p></dd><li><span class="chapter"><a href="#cha-ceph-nfsganesha"><span class="title-number">25 </span><span class="title-name">NFS Ganesha</span></a></span></li><dd class="toc-abstract"><p>NFS Ganesha is an NFS server that runs in a user address space instead of as part of the operating system kernel. With NFS Ganesha, you can plug in your own storage mechanism—such as Ceph—and access it from any NFS client. For installation instructions, see Book “Deployment Guide”, Chapter 8 “Deploy…</p></dd></ul></div><section class="chapter" id="cha-ceph-gw" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">21 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-gw">#</a></h1></div></div></div><p>
  This chapter introduces details about administration tasks related to Object Gateway,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </p><section class="sect1" id="sec-ceph-rgw-limits" data-id-title="Object Gateway restrictions and naming limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.1 </span><span class="title-name">Object Gateway restrictions and naming limitations</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-limits">#</a></h2></div></div></div><p>
   Following is a list of important Object Gateway limits:
  </p><section class="sect2" id="ogw-limits-bucket" data-id-title="Bucket limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.1.1 </span><span class="title-name">Bucket limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-bucket">#</a></h3></div></div></div><p>
    When approaching Object Gateway via the S3 API, bucket names are limited to
    DNS-compliant names with a dash character '-' allowed. When approaching
    Object Gateway via the Swift API, you may use any combination of UTF-8 supported
    characters except for a slash character '/'. The maximum length of a bucket
    name is 255 characters. Bucket names must be unique.
   </p><div id="id-1.4.6.2.4.3.3" data-id-title="Use DNS-compliant bucket names" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Use DNS-compliant bucket names</h6><p>
     Although you may use any UTF-8 based bucket name via the Swift API, it
     is recommended to name buckets with regard to the S3 naming limitations to
     avoid problems accessing the same bucket via the S3 API.
    </p></div></section><section class="sect2" id="ogw-limits-object" data-id-title="Stored object limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.1.2 </span><span class="title-name">Stored object limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-object">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.4.4.2.1"><span class="term">Maximum number of objects per user</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.4.6.2.4.4.2.2"><span class="term">Maximum number of objects per bucket</span></dt><dd><p>
       No restriction by default (limited by ~ 2^63).
      </p></dd><dt id="id-1.4.6.2.4.4.2.3"><span class="term">Maximum size of an object to upload/store</span></dt><dd><p>
       Single uploads are restricted to 5 GB. Use multipart for larger object
       sizes. The maximum number of multipart chunks is 10000.
      </p></dd></dl></div></section><section class="sect2" id="ogw-limits-http" data-id-title="HTTP header limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.1.3 </span><span class="title-name">HTTP header limitations</span> <a title="Permalink" class="permalink" href="#ogw-limits-http">#</a></h3></div></div></div><p>
    HTTP header and request limitation depend on the Web front-end used. The
    default Beast restricts the size of the HTTP header to 16 kB.
   </p></section></section><section class="sect1" id="ogw-deploy" data-id-title="Deploying the Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.2 </span><span class="title-name">Deploying the Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-deploy">#</a></h2></div></div></div><p>
   The Ceph Object Gateway deployment follows the same procedure as the deployment of other
   Ceph services—by means of cephadm. For more details, refer to
   <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.2 “Service and placement specification”</span>, specifically to
   <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.4 “Deploying Object Gateways”</span>.
  </p></section><section class="sect1" id="ceph-rgw-operating" data-id-title="Operating the Object Gateway service"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.3 </span><span class="title-name">Operating the Object Gateway service</span> <a title="Permalink" class="permalink" href="#ceph-rgw-operating">#</a></h2></div></div></div><p>
   You can operate the Object Gateways same as other Ceph services by first
   identifying the service name with the <code class="command">ceph orch ps</code>
   command, and running the following command for operating services, for
   example:
  </p><div class="verbatim-wrap"><pre class="screen">ceph orch daemon restart <em class="replaceable">OGW_SERVICE_NAME</em></pre></div><p>
   Refer to <a class="xref" href="#cha-ceph-operating" title="Chapter 14. Operation of Ceph services">Chapter 14, <em>Operation of Ceph services</em></a> for complete information about
   operating Ceph services.
  </p></section><section class="sect1" id="ogw-config-parameters" data-id-title="Configuration options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.4 </span><span class="title-name">Configuration options</span> <a title="Permalink" class="permalink" href="#ogw-config-parameters">#</a></h2></div></div></div><p>
   Refer to <a class="xref" href="#config-ogw" title="28.5. Ceph Object Gateway">Section 28.5, “Ceph Object Gateway”</a> for a list of Object Gateway configuration
   options.
  </p></section><section class="sect1" id="ceph-rgw-access" data-id-title="Managing Object Gateway access"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.5 </span><span class="title-name">Managing Object Gateway access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-access">#</a></h2></div></div></div><p>
   You can communicate with Object Gateway using either S3- or Swift-compatible
   interface. S3 interface is compatible with a large subset of the Amazon S3
   RESTful API. Swift interface is compatible with a large subset of the
   OpenStack Swift API.
  </p><p>
   Both interfaces require you to create a specific user, and install the
   relevant client software to communicate with the gateway using the user's
   secret key.
  </p><section class="sect2" id="accessing-ragos-gateway" data-id-title="Accessing Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.5.1 </span><span class="title-name">Accessing Object Gateway</span> <a title="Permalink" class="permalink" href="#accessing-ragos-gateway">#</a></h3></div></div></div><section class="sect3" id="ogw-s3-interface-access" data-id-title="S3 interface access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.1.1 </span><span class="title-name">S3 interface access</span> <a title="Permalink" class="permalink" href="#ogw-s3-interface-access">#</a></h4></div></div></div><p>
     To access the S3 interface, you need a REST client.
     <code class="command">S3cmd</code> is a command line S3 client. You can find it in
     the
     <a class="link" href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd" target="_blank">OpenSUSE
     Build Service</a>. The repository contains versions for both SUSE Linux Enterprise and
     openSUSE based distributions.
    </p><p>
     If you want to test your access to the S3 interface, you can also write a
     small a Python script. The script will connect to Object Gateway, create a new
     bucket, and list all buckets. The values for
     <code class="option">aws_access_key_id</code> and
     <code class="option">aws_secret_access_key</code> are taken from the values of
     <code class="option">access_key</code> and <code class="option">secret_key</code> returned by
     the <code class="command">radosgw_admin</code> command from
     <a class="xref" href="#adding-s3-swift-users" title="21.5.2.1. Adding S3 and Swift users">Section 21.5.2.1, “Adding S3 and Swift users”</a>.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Install the <code class="systemitem">python-boto</code> package:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in python-boto</pre></div></li><li class="step"><p>
       Create a new Python script called <code class="filename">s3test.py</code> with
       the following content:
       
      </p><div class="verbatim-wrap"><pre class="screen">import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '<em class="replaceable">HOSTNAME</em>',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
  print "<em class="replaceable">NAME</em>\t<em class="replaceable">CREATED</em>".format(
  name = bucket.name,
  created = bucket.creation_date,
  )</pre></div><p>
       Replace <code class="literal"><em class="replaceable">HOSTNAME</em></code> with the
       host name of the host where you configured the Object Gateway service, for
       example <code class="literal">gateway_host</code>.
      </p></li><li class="step"><p>
       Run the script:
      </p><div class="verbatim-wrap"><pre class="screen">python s3test.py</pre></div><p>
       The script outputs something like the following:
      </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket 2015-07-22T15:37:42.000Z</pre></div></li></ol></div></div></section><section class="sect3" id="swift-interface-access" data-id-title="Swift interface access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.1.2 </span><span class="title-name">Swift interface access</span> <a title="Permalink" class="permalink" href="#swift-interface-access">#</a></h4></div></div></div><p>
     To access Object Gateway via Swift interface, you need the <code class="command">swift</code>
     command line client. Its manual page <code class="command">man 1 swift</code> tells
     you more about its command line options.
    </p><p>
     The package is included in the 'Public Cloud' module for SUSE Linux Enterprise 12 from SP3
     and SUSE Linux Enterprise 15. Before installing the package, you need to activate the
     module and refresh the software repository:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>SUSEConnect -p sle-module-public-cloud/12/<em class="replaceable">SYSTEM-ARCH</em>
sudo zypper refresh</pre></div><p>
     Or
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>SUSEConnect -p sle-module-public-cloud/15/<em class="replaceable">SYSTEM-ARCH</em>
<code class="prompt root"># </code>zypper refresh</pre></div><p>
     To install the <code class="command">swift</code> command, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in python-swiftclient</pre></div><p>
     The swift access uses the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>swift -A http://<em class="replaceable">IP_ADDRESS</em>/auth/1.0 \
-U example_user:swift -K '<em class="replaceable">SWIFT_SECRET_KEY</em>' list</pre></div><p>
     Replace <em class="replaceable">IP_ADDRESS</em> with the IP address of the
     gateway server, and <em class="replaceable">SWIFT_SECRET_KEY</em> with its
     value from the output of the <code class="command">radosgw-admin key create</code>
     command executed for the <code class="systemitem">swift</code> user in
     <a class="xref" href="#adding-s3-swift-users" title="21.5.2.1. Adding S3 and Swift users">Section 21.5.2.1, “Adding S3 and Swift users”</a>.
    </p><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</pre></div><p>
     The output is:
    </p><div class="verbatim-wrap"><pre class="screen">my-new-bucket</pre></div></section></section><section class="sect2" id="s3-swift-accounts-managment" data-id-title="Manage S3 and Swift accounts"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.5.2 </span><span class="title-name">Manage S3 and Swift accounts</span> <a title="Permalink" class="permalink" href="#s3-swift-accounts-managment">#</a></h3></div></div></div><section class="sect3" id="adding-s3-swift-users" data-id-title="Adding S3 and Swift users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.2.1 </span><span class="title-name">Adding S3 and Swift users</span> <a title="Permalink" class="permalink" href="#adding-s3-swift-users">#</a></h4></div></div></div><p>
     You need to create a user, access key and secret to enable end users to
     interact with the gateway. There are two types of users: a
     <span class="emphasis"><em>user</em></span> and <span class="emphasis"><em>subuser</em></span>. While
     <span class="emphasis"><em>users</em></span> are used when interacting with the S3
     interface, <span class="emphasis"><em>subusers</em></span> are users of the Swift
     interface. Each subuser is associated to a user.
    </p><p>
     To create a Swift user, follow the steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       To create a Swift user—which is a <span class="emphasis"><em>subuser</em></span>
       in our terminology—you need to create the associated
       <span class="emphasis"><em>user</em></span> first.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USERNAME</em> \
 --display-name="<em class="replaceable">DISPLAY-NAME</em>" --email=<em class="replaceable">EMAIL</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div></li><li class="step"><p>
       To create a subuser (Swift interface) for the user, you must specify
       the user ID (--uid=<em class="replaceable">USERNAME</em>), a subuser ID,
       and the access level for the subuser.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin subuser create --uid=<em class="replaceable">UID</em> \
 --subuser=<em class="replaceable">UID</em> \
 --access=[ <em class="replaceable">read | write | readwrite | full</em> ]</pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</pre></div></li><li class="step"><p>
       Generate a secret key for the user.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</pre></div></li><li class="step"><p>
       Both commands will output JSON-formatted data showing the user state.
       Notice the following lines, and remember the
       <code class="literal">secret_key</code> value:
      </p><div class="verbatim-wrap"><pre class="screen">"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</pre></div></li></ol></div></div><p>
     When accessing Object Gateway through the S3 interface you need to create an S3
     user by running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create --uid=<em class="replaceable">USERNAME</em> \
 --display-name="<em class="replaceable">DISPLAY-NAME</em>" --email=<em class="replaceable">EMAIL</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</pre></div><p>
     The command also creates the user's access and secret key. Check its
     output for <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     keywords and their values:
    </p><div class="verbatim-wrap"><pre class="screen">[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</pre></div></section><section class="sect3" id="removing-s3-swift-users" data-id-title="Removing S3 and Swift users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.2.2 </span><span class="title-name">Removing S3 and Swift users</span> <a title="Permalink" class="permalink" href="#removing-s3-swift-users">#</a></h4></div></div></div><p>
     The procedure for deleting users is similar for S3 and Swift users. But
     in case of Swift users you may need to delete the user including its
     subusers.
    </p><p>
     To remove a S3 or Swift user (including all its subusers), specify
     <code class="option">user rm</code> and the user ID in the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user rm --uid=example_user</pre></div><p>
     To remove a subuser, specify <code class="option">subuser rm</code> and the subuser
     ID.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin subuser rm --uid=example_user:swift</pre></div><p>
     You can make use of the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.8.5.3.8.1"><span class="term">--purge-data</span></dt><dd><p>
        Purges all data associated to the user ID.
       </p></dd><dt id="id-1.4.6.2.8.5.3.8.2"><span class="term">--purge-keys</span></dt><dd><p>
        Purges all keys associated to the user ID.
       </p></dd></dl></div><div id="id-1.4.6.2.8.5.3.9" data-id-title="Removing a subuser" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Removing a subuser</h6><p>
      When you remove a subuser, you are removing access to the Swift
      interface. The user will remain in the system.
     </p></div></section><section class="sect3" id="changing-s3-swift-users-password" data-id-title="Changing S3 and Swift user access and secret keys"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.2.3 </span><span class="title-name">Changing S3 and Swift user access and secret keys</span> <a title="Permalink" class="permalink" href="#changing-s3-swift-users-password">#</a></h4></div></div></div><p>
     The <code class="literal">access_key</code> and <code class="literal">secret_key</code>
     parameters identify the Object Gateway user when accessing the gateway. Changing
     the existing user keys is the same as creating new ones, as the old keys
     get overwritten.
    </p><p>
     For S3 users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin key create --uid=<em class="replaceable">EXAMPLE_USER</em> --key-type=s3 --gen-access-key --gen-secret</pre></div><p>
     For Swift users, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin key create --subuser=<em class="replaceable">EXAMPLE_USER</em>:swift --key-type=swift --gen-secret</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.8.5.4.7.1"><span class="term"><code class="option">--key-type=<em class="replaceable">TYPE</em></code></span></dt><dd><p>
        Specifies the type of key. Either <code class="literal">swift</code> or
        <code class="literal">s3</code>.
       </p></dd><dt id="id-1.4.6.2.8.5.4.7.2"><span class="term"><code class="option">--gen-access-key</code></span></dt><dd><p>
        Generates a random access key (for S3 user by default).
       </p></dd><dt id="id-1.4.6.2.8.5.4.7.3"><span class="term"><code class="option">--gen-secret</code></span></dt><dd><p>
        Generates a random secret key.
       </p></dd><dt id="id-1.4.6.2.8.5.4.7.4"><span class="term"><code class="option">--secret=<em class="replaceable">KEY</em></code></span></dt><dd><p>
        Specifies a secret key, for example manually generated.
       </p></dd></dl></div></section><section class="sect3" id="user-quota-managment" data-id-title="Enabling user quota management"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.5.2.4 </span><span class="title-name">Enabling user quota management</span> <a title="Permalink" class="permalink" href="#user-quota-managment">#</a></h4></div></div></div><p>
     The Ceph Object Gateway enables you to set quotas on users and buckets owned by users.
     Quotas include the maximum number of objects in a bucket and the maximum
     storage size in megabytes.
    </p><p>
     Before you enable a user quota, you first need to set its parameters:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin quota set --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em> \
 --max-objects=1024 --max-size=1024</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.8.5.5.5.1"><span class="term"><code class="option">--max-objects</code></span></dt><dd><p>
        Specifies the maximum number of objects. A negative value disables the
        check.
       </p></dd><dt id="id-1.4.6.2.8.5.5.5.2"><span class="term"><code class="option">--max-size</code></span></dt><dd><p>
        Specifies the maximum number of bytes. A negative value disables the
        check.
       </p></dd><dt id="id-1.4.6.2.8.5.5.5.3"><span class="term"><code class="option">--quota-scope</code></span></dt><dd><p>
        Sets the scope for the quota. The options are <code class="literal">bucket</code>
        and <code class="literal">user</code>. Bucket quotas apply to buckets a user
        owns. User quotas apply to a user.
       </p></dd></dl></div><p>
     Once you set a user quota, you may enable it:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin quota enable --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To disable a quota:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin quota disable --quota-scope=user --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To list quota settings:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user info --uid=<em class="replaceable">EXAMPLE_USER</em></pre></div><p>
     To update quota statistics:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user stats --uid=<em class="replaceable">EXAMPLE_USER</em> --sync-stats</pre></div></section></section></section><section class="sect1" id="ogw-http-frontends" data-id-title="HTTP front-ends"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.6 </span><span class="title-name">HTTP front-ends</span> <a title="Permalink" class="permalink" href="#ogw-http-frontends">#</a></h2></div></div></div><p>
   The Ceph Object Gateway supports two embedded HTTP front-ends: <span class="emphasis"><em>Beast</em></span>
   and <span class="emphasis"><em>Civetweb</em></span>.
  </p><p>
   The Beast front-end uses the Boost.Beast library for HTTP parsing and the
   Boost.Asio library for asynchronous network I/O.
  </p><p>
   The Civetweb front-end uses the Civetweb HTTP library, which is a fork of
   Mongoose.
  </p><p>
   You can configure them with the <code class="option">rgw_frontends</code> option. Refer
   to <a class="xref" href="#config-ogw" title="28.5. Ceph Object Gateway">Section 28.5, “Ceph Object Gateway”</a> for a list of configuration options.
  </p></section><section class="sect1" id="ceph-rgw-https" data-id-title="Enable HTTPS/SSL for Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.7 </span><span class="title-name">Enable HTTPS/SSL for Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-https">#</a></h2></div></div></div><p>
   To enable the Object Gateway to communicate securely using SSL, you need to either
   have a CA-issued certificate or create a self-signed one.
  </p><section class="sect2" id="ogw-selfcert" data-id-title="Creating a self-signed certificate"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.7.1 </span><span class="title-name">Creating a self-signed certificate</span> <a title="Permalink" class="permalink" href="#ogw-selfcert">#</a></h3></div></div></div><div id="id-1.4.6.2.10.3.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Skip this section if you already have a valid certificate signed by CA.
    </p></div><p>
    The following procedure describes how to generate a self-signed SSL
    certificate on the Salt Master.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      If you need your Object Gateway to be known by additional subject identities, add
      them to the <code class="option">subjectAltName</code> option in the
      <code class="literal">[v3_req]</code> section of the
      <code class="filename">/etc/ssl/openssl.cnf</code> file:
     </p><div class="verbatim-wrap"><pre class="screen">[...]
[ v3_req ]
subjectAltName = DNS:server1.example.com DNS:server2.example.com
[...]</pre></div><div id="id-1.4.6.2.10.3.4.1.3" data-id-title="IP addresses in subjectAltName" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: IP addresses in <code class="option">subjectAltName</code></h6><p>
       To use IP addresses instead of domain names in the
       <code class="option">subjectAltName</code> option, replace the example line with
       the following:
      </p><div class="verbatim-wrap"><pre class="screen">subjectAltName = IP:10.0.0.10 IP:10.0.0.11</pre></div></div></li><li class="step"><p>
      Create the key and the certificate using <code class="command">openssl</code>.
      Enter all data you need to include in your certificate. We recommend
      entering the FQDN as the common name. Before signing the certificate,
      verify that 'X509v3 Subject Alternative Name:' is included in requested
      extensions, and that the resulting certificate has "X509v3 Subject
      Alternative Name:" set.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>openssl req -x509 -nodes -days 1095 \
 -newkey rsa:4096 -keyout rgw.key
 -out rgw.pem</pre></div></li><li class="step"><p>
      Append the key to the certificate file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cat rgw.key &gt;&gt; rgw.pem</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-sssl-config" data-id-title="Configuring Object Gateway with SSL"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.7.2 </span><span class="title-name">Configuring Object Gateway with SSL</span> <a title="Permalink" class="permalink" href="#ogw-sssl-config">#</a></h3></div></div></div><p>
    To configure Object Gateway to use SSL certificates, use the
    <code class="option">rgw_frontends</code> option. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set <em class="replaceable">WHO</em> rgw_frontends \
 beast ssl_port=443 ssl_certificate=config://<em class="replaceable">CERT</em> ssl_key=config://<em class="replaceable">KEY</em></pre></div><p>
    If you do not specify the <em class="replaceable">CERT</em> and
    <em class="replaceable">KEY</em> configuration keys, then the Object Gateway service
    will look for the SSL certificate and key under the following configuration
    keys:
   </p><div class="verbatim-wrap"><pre class="screen">rgw/cert/<em class="replaceable">RGW_REALM</em>/<em class="replaceable">RGW_ZONE</em>.key
rgw/cert/<em class="replaceable">RGW_REALM</em>/<em class="replaceable">RGW_ZONE</em>.crt</pre></div><p>
    If you want to override the default SSL key and certificate location,
    import them to the configuration database by using the following command:
   </p><div class="verbatim-wrap"><pre class="screen">ceph config-key set <em class="replaceable">CUSTOM_CONFIG_KEY</em> -i <em class="replaceable">PATH_TO_CERT_FILE</em></pre></div><p>
    Then use your custom configuration keys using the
    <code class="literal">config://</code> directive.
   </p></section></section><section class="sect1" id="ceph-rgw-sync" data-id-title="Synchronization modules"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.8 </span><span class="title-name">Synchronization modules</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync">#</a></h2></div></div></div><p>
   Object Gateway is deployed as a multi-site service while you can mirror data and
   metadata between the zones. <span class="emphasis"><em>Synchronization modules</em></span> are
   built atop of the multisite framework that allows for forwarding data and
   metadata to a different external tier. A synchronization module allows for a
   set of actions to be performed whenever a change in data occurs (for
   example, metadata operations such as bucket or user creation). As the Object Gateway
   multisite changes are eventually consistent at remote sites, changes are
   propagated asynchronously. This covers use cases such as backing up the
   object storage to an external cloud cluster, a custom backup solution using
   tape drives, or indexing metadata in ElasticSearch.
  </p><section class="sect2" id="ogw-sync-general-config" data-id-title="Configuring synchronization modules"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.8.1 </span><span class="title-name">Configuring synchronization modules</span> <a title="Permalink" class="permalink" href="#ogw-sync-general-config">#</a></h3></div></div></div><p>
    All synchronization modules are configured in a similar way. You need to
    create a new zone (refer to <a class="xref" href="#ceph-rgw-fed" title="21.13. Multisite Object Gateways">Section 21.13, “Multisite Object Gateways”</a> for more
    details) and set its <code class="option">--tier_type</code> option, for example
    <code class="option">--tier-type=cloud</code> for the cloud synchronization module:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --endpoints=http://endpoint1.example.com,http://endpoint2.example.com, [...] \
 --tier-type=cloud</pre></div><p>
    You can configure the specific tier by using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=<em class="replaceable">KEY1</em>=<em class="replaceable">VALUE1</em>,<em class="replaceable">KEY2</em>=<em class="replaceable">VALUE2</em></pre></div><p>
    The <em class="replaceable">KEY</em> in the configuration specifies the
    configuration variable that you want to update, and the
    <em class="replaceable">VALUE</em> specifies its new value. Nested values can
    be accessed using period. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=connection.access_key=<em class="replaceable">KEY</em>,connection.secret=<em class="replaceable">SECRET</em></pre></div><p>
    You can access array entries by appending square brackets '[]' with the
    referenced entry. You can add a new array entry by using square brackets
    '[]'. Index value of -1 references the last entry in the array. It is not
    possible to create a new entry and reference it again in the same command.
    For example, a command to create a new profile for buckets starting with
    <em class="replaceable">PREFIX</em> follows:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=profiles[].source_bucket=<em class="replaceable">PREFIX</em>'*'
<code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> \
 --tier-config=profiles[-1].connection_id=<em class="replaceable">CONNECTION_ID</em>,profiles[-1].acls_id=<em class="replaceable">ACLS_ID</em></pre></div><div id="id-1.4.6.2.11.3.10" data-id-title="Adding and removing configuration entries" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Adding and removing configuration entries</h6><p>
     You can add a new tier configuration entry by using the
     <code class="option">--tier-config-add=<em class="replaceable">KEY</em>=<em class="replaceable">VALUE</em></code>
     parameter.
    </p><p>
     You can remove an existing entry by using
     <code class="option">--tier-config-rm=<em class="replaceable">KEY</em></code>.
    </p></div></section><section class="sect2" id="ceph-rgw-sync-zones" data-id-title="Synchronizing zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.8.2 </span><span class="title-name">Synchronizing zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones">#</a></h3></div></div></div><p>
    A synchronization module configuration is local to a zone. The
    synchronization module determines whether the zone exports data or can only
    consume data that was modified in another zone. As of Luminous the
    supported synchronization plug-ins are <code class="literal">ElasticSearch</code>,
    <code class="literal">rgw</code>, which is the default synchronization plug-in that
    synchronizes data between the zones and <code class="literal">log</code> which is a
    trivial synchronization plug-in that logs the metadata operation that
    happens in the remote zones. The following sections are written with the
    example of a zone using <code class="literal">ElasticSearch</code> synchronization
    module. The process would be similar for configuring any other
    synchronization plug-in.
   </p><div id="id-1.4.6.2.11.4.3" data-id-title="Default synchronization plug-in" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Default synchronization plug-in</h6><p>
     <code class="literal">rgw</code> is the default synchronization plug-in and there is
     no need to explicitly configure this.
    </p></div><section class="sect3" id="ceph-rgw-sync-zones-req" data-id-title="Requirements and assumptions"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.2.1 </span><span class="title-name">Requirements and assumptions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-req">#</a></h4></div></div></div><p>
     Let us assume a simple multisite configuration as described in
     <a class="xref" href="#ceph-rgw-fed" title="21.13. Multisite Object Gateways">Section 21.13, “Multisite Object Gateways”</a> consists of 2 zones:
     <code class="literal">us-east</code> and <code class="literal">us-west</code>. Now we add a
     third zone <code class="literal">us-east-es</code> which is a zone that only
     processes metadata from the other sites. This zone can be in the same or a
     different Ceph cluster than <code class="literal">us-east</code>. This zone would
     only consume metadata from other zones and Object Gateways in this zone will not
     serve any end user requests directly.
    </p></section><section class="sect3" id="ceph-rgw-sync-zones-configure" data-id-title="Configuring zones"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.2.2 </span><span class="title-name">Configuring zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-zones-configure">#</a></h4></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Create the third zone similar to the ones described in
       <a class="xref" href="#ceph-rgw-fed" title="21.13. Multisite Object Gateways">Section 21.13, “Multisite Object Gateways”</a>, for example
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">radosgw-admin</code> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key=<em class="replaceable">SYSTEM-KEY</em> --secret=<em class="replaceable">SECRET</em> --endpoints=http://rgw-es:80</pre></div></li><li class="step"><p>
       A synchronization module can be configured for this zone via the
       following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --tier-type=<em class="replaceable">TIER-TYPE</em> \
--tier-config={set of key=value pairs}</pre></div></li><li class="step"><p>
       For example in the <code class="literal">ElasticSearch</code> synchronization
       module
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">radosgw-admin</code> zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1</pre></div><p>
       For the various supported tier-config options refer to
       <a class="xref" href="#ceph-rgw-sync-elastic" title="21.8.3. ElasticSearch synchronization module">Section 21.8.3, “ElasticSearch synchronization module”</a>.
      </p></li><li class="step"><p>
       Finally update the period
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">radosgw-admin</code> period update --commit</pre></div></li><li class="step"><p>
       Now start the Object Gateway in the zone
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch start rgw.<em class="replaceable">REALM-NAME</em>.<em class="replaceable">ZONE-NAME</em></pre></div></li></ol></div></div></section></section><section class="sect2" id="ceph-rgw-sync-elastic" data-id-title="ElasticSearch synchronization module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.8.3 </span><span class="title-name">ElasticSearch synchronization module</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic">#</a></h3></div></div></div><p>
    This synchronization module writes the metadata from other zones to
    ElasticSearch. As of Luminous this is JSON of data fields we currently
    store in ElasticSearch.
   </p><div class="verbatim-wrap"><pre class="screen">{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}</pre></div><section class="sect3" id="ceph-rgw-sync-elastic-config" data-id-title="ElasticSearch tier type configuration parameters"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.3.1 </span><span class="title-name">ElasticSearch tier type configuration parameters</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-config">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.5.4.2.1"><span class="term">endpoint</span></dt><dd><p>
        Specifies the ElasticSearch server endpoint to access.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.2"><span class="term">num_shards</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of shards that ElasticSearch
        will be configured with on data synchronization initialization. Note
        that this cannot be changed after initialization. Any change here
        requires rebuild of the ElasticSearch index and reinitialization of the
        data synchronization process.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.3"><span class="term">num_replicas</span></dt><dd><p>
        <span class="emphasis"><em>(integer)</em></span> The number of replicas that
        ElasticSearch will be configured with on data synchronization
        initialization.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.4"><span class="term">explicit_custom_meta</span></dt><dd><p>
        <span class="emphasis"><em>(true | false)</em></span> Specifies whether all user custom
        metadata will be indexed, or whether user will need to configure (at
        the bucket level) what customer metadata entries should be indexed.
        This is false by default
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.5"><span class="term">index_buckets_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty, all
        buckets will be indexed. Otherwise, only buckets specified here will be
        indexed. It is possible to provide bucket prefixes (for example
        'foo*'), or bucket suffixes (for example '*bar').
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.6"><span class="term">approved_owners_list</span></dt><dd><p>
        <span class="emphasis"><em>(comma separated list of strings)</em></span> If empty,
        buckets of all owners will be indexed (subject to other restrictions),
        otherwise, only buckets owned by specified owners will be indexed.
        Suffixes and prefixes can also be provided.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.7"><span class="term">override_index_path</span></dt><dd><p>
        <span class="emphasis"><em>(string)</em></span> if not empty, this string will be used as
        the ElasticSearch index path. Otherwise the index path will be
        determined and generated on synchronization initialization.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.8"><span class="term">username</span></dt><dd><p>
        Specifies a user name for ElasticSearch if authentication is required.
       </p></dd><dt id="id-1.4.6.2.11.5.4.2.9"><span class="term">password</span></dt><dd><p>
        Specifies a password for ElasticSearch if authentication is required.
       </p></dd></dl></div></section><section class="sect3" id="ceph-rgw-sync-elastic-query" data-id-title="Metadata queries"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.3.2 </span><span class="title-name">Metadata queries</span> <a title="Permalink" class="permalink" href="#ceph-rgw-sync-elastic-query">#</a></h4></div></div></div><p>
     Since the ElasticSearch cluster now stores object metadata, it is
     important that the ElasticSearch endpoint is not exposed to the public and
     only accessible to the cluster administrators. For exposing metadata
     queries to the end user itself this poses a problem since we'd want the
     user to only query their metadata and not of any other users, this would
     require the ElasticSearch cluster to authenticate users in a way similar
     to RGW does which poses a problem.
    </p><p>
     As of Luminous RGW in the metadata master zone can now service end user
     requests. This allows for not exposing the ElasticSearch endpoint in
     public and also solves the authentication and authorization problem since
     RGW itself can authenticate the end user requests. For this purpose RGW
     introduces a new query in the bucket APIs that can service ElasticSearch
     requests. All these requests must be sent to the metadata master zone.
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.5.5.4.1"><span class="term">Get an ElasticSearch Query</span></dt><dd><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?query=<em class="replaceable">QUERY-EXPR</em></pre></div><p>
        request params:
       </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          max-keys: max number of entries to return
         </p></li><li class="listitem"><p>
          marker: pagination marker
         </p></li></ul></div><div class="verbatim-wrap"><pre class="screen">expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]</pre></div><p>
        op is one of the following: &lt;, &lt;=, ==, &gt;=, &gt;
       </p><p>
        For example:
       </p><div class="verbatim-wrap"><pre class="screen">GET /?query=name==foo</pre></div><p>
        Will return all the indexed keys that user has read permission to, and
        are named 'foo'. The output will be a list of keys in XML that is
        similar to the S3 list buckets response.
       </p></dd><dt id="id-1.4.6.2.11.5.5.4.2"><span class="term">Configure custom metadata fields</span></dt><dd><p>
        Define which custom metadata entries should be indexed (under the
        specified bucket), and what are the types of these keys. If explicit
        custom metadata indexing is configured, this is needed so that rgw will
        index the specified custom metadata values. Otherwise it is needed in
        cases where the indexed metadata keys are of a type other than string.
       </p><div class="verbatim-wrap"><pre class="screen">POST /<em class="replaceable">BUCKET</em>?mdsearch
x-amz-meta-search: &lt;key [; type]&gt; [, ...]</pre></div><p>
        Multiple metadata fields must be comma separated, a type can be forced
        for a field with a `;`. The currently allowed types are
        string(default), integer and date, for example, if you want to index a
        custom object metadata x-amz-meta-year as int, x-amz-meta-date as type
        date and x-amz-meta-title as string, you would do
       </p><div class="verbatim-wrap"><pre class="screen">POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string</pre></div></dd><dt id="id-1.4.6.2.11.5.5.4.3"><span class="term">Delete custom metadata configuration</span></dt><dd><p>
        Delete custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">DELETE /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd><dt id="id-1.4.6.2.11.5.5.4.4"><span class="term">Get custom metadata configuration</span></dt><dd><p>
        Retrieve custom metadata bucket configuration.
       </p><div class="verbatim-wrap"><pre class="screen">GET /<em class="replaceable">BUCKET</em>?mdsearch</pre></div></dd></dl></div></section></section><section class="sect2" id="ogw-cloud-sync" data-id-title="Cloud synchronization module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.8.4 </span><span class="title-name">Cloud synchronization module</span> <a title="Permalink" class="permalink" href="#ogw-cloud-sync">#</a></h3></div></div></div><p>
    This section introduces a module that synchronizes the zone data to a
    remote cloud service. The synchronization is only unidirectional—the
    date is not synchronized back from the remote zone. The main goal of this
    module is to enable synchronizing data to multiple cloud service providers.
    Currently it supports cloud providers that are compatible with AWS (S3).
   </p><p>
    To synchronize data to a remote cloud service, you need to configure user
    credentials. Because many cloud services introduce limits on the number of
    buckets that each user can create, you can configure the mapping of source
    objects and buckets, different targets to different buckets and bucket
    prefixes. Note that source access lists (ACLs) will not be preserved. It is
    possible to map permissions of specific source users to specific
    destination users.
   </p><p>
    Because of API limitations, there is no way to preserve original object
    modification time and HTTP entity tag (ETag). The cloud synchronization
    module stores these as metadata attributes on the destination objects.
   </p><section class="sect3" id="cloud-sync-module" data-id-title="Configuring the cloud synchronization module"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.4.1 </span><span class="title-name">Configuring the cloud synchronization module</span> <a title="Permalink" class="permalink" href="#cloud-sync-module">#</a></h4></div></div></div><p>
     Following are examples of a trivial and non-trivial configuration for the
     cloud synchronization module. Note that the trivial configuration can
     collide with the non-trivial one.
    </p><div class="example" id="id-1.4.6.2.11.6.5.3" data-id-title="Trivial configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 21.1: </span><span class="title-name">Trivial configuration </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.11.6.5.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "connection": {
    "access_key": <em class="replaceable">ACCESS</em>,
    "secret": <em class="replaceable">SECRET</em>,
    "endpoint": <em class="replaceable">ENDPOINT</em>,
    "host_style": <em class="replaceable">path | virtual</em>,
  },
  "acls": [ { "type": <em class="replaceable">id | email | uri</em>,
    "source_id": <em class="replaceable">SOURCE_ID</em>,
    "dest_id": <em class="replaceable">DEST_ID</em> } ... ],
  "target_path": <em class="replaceable">TARGET_PATH</em>,
}</pre></div></div></div><div class="example" id="id-1.4.6.2.11.6.5.4" data-id-title="Non-trivial configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 21.2: </span><span class="title-name">Non-trivial configuration </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.11.6.5.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">{
  "default": {
    "connection": {
      "access_key": <em class="replaceable">ACCESS</em>,
      "secret": <em class="replaceable">SECRET</em>,
      "endpoint": <em class="replaceable">ENDPOINT</em>,
      "host_style" <em class="replaceable">path | virtual</em>,
    },
    "acls": [
    {
      "type": <em class="replaceable">id | email | uri</em>,   #  optional, default is id
      "source_id": <em class="replaceable">ID</em>,
      "dest_id": <em class="replaceable">ID</em>
    } ... ]
    "target_path": <em class="replaceable">PATH</em> # optional
  },
  "connections": [
  {
    "connection_id": <em class="replaceable">ID</em>,
    "access_key": <em class="replaceable">ACCESS</em>,
    "secret": <em class="replaceable">SECRET</em>,
    "endpoint": <em class="replaceable">ENDPOINT</em>,
    "host_style": <em class="replaceable">path | virtual</em>,  # optional
  } ... ],
  "acl_profiles": [
  {
    "acls_id": <em class="replaceable">ID</em>, # acl mappings
    "acls": [ {
      "type": <em class="replaceable">id | email | uri</em>,
      "source_id": <em class="replaceable">ID</em>,
      "dest_id": <em class="replaceable">ID</em>
    } ... ]
  }
  ],
  "profiles": [
  {
   "source_bucket": <em class="replaceable">SOURCE</em>,
   "connection_id": <em class="replaceable">CONNECTION_ID</em>,
   "acls_id": <em class="replaceable">MAPPINGS_ID</em>,
   "target_path": <em class="replaceable">DEST</em>,          # optional
  } ... ],
}</pre></div></div></div><p>
     Explanation of used configuration terms follows:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.6.5.6.1"><span class="term">connection</span></dt><dd><p>
        Represents a connection to the remote cloud service. Contains
        'connection_id', 'access_key', 'secret', 'endpoint', and 'host_style'.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.2"><span class="term">access_key</span></dt><dd><p>
        The remote cloud access key that will be used for the specific
        connection.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.3"><span class="term">secret</span></dt><dd><p>
        The secret key for the remote cloud service.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.4"><span class="term">endpoint</span></dt><dd><p>
        URL of remote cloud service endpoint.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.5"><span class="term">host_style</span></dt><dd><p>
        Type of host style ('path' or 'virtual') to be used when accessing
        remote cloud endpoint. Default is 'path'.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.6"><span class="term">acls</span></dt><dd><p>
        Array of access list mappings.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.7"><span class="term">acl_mapping</span></dt><dd><p>
        Each 'acl_mapping' structure contains 'type', 'source_id', and
        'dest_id'. These will define the ACL mutation for each object. An ACL
        mutation allows converting source user ID to a destination ID.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.8"><span class="term">type</span></dt><dd><p>
        ACL type: 'id' defines user ID, 'email' defines user by e-mail, and
        'uri' defines user by uri (group).
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.9"><span class="term">source_id</span></dt><dd><p>
        ID of user in the source zone.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.10"><span class="term">dest_id</span></dt><dd><p>
        ID of user in the destination.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.11"><span class="term">target_path</span></dt><dd><p>
        A string that defines how the target path is created. The target path
        specifies a prefix to which the source object name is appended. The
        target path configurable can include any of the following variables:
       </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.6.5.6.11.2.2.1"><span class="term">SID</span></dt><dd><p>
           A unique string that represents the synchronization instance ID.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.2"><span class="term">ZONEGROUP</span></dt><dd><p>
           Zonegroup name.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.3"><span class="term">ZONEGROUP_ID</span></dt><dd><p>
           Zonegroup ID.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.4"><span class="term">ZONE</span></dt><dd><p>
           Zone name.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.5"><span class="term">ZONE_ID</span></dt><dd><p>
           Zone ID.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.6"><span class="term">BUCKET</span></dt><dd><p>
           Source bucket name.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.11.2.2.7"><span class="term">OWNER</span></dt><dd><p>
           Source bucket owner ID.
          </p></dd></dl></div><p>
        For example: target_path =
        rgwx-<em class="replaceable">ZONE</em>-<em class="replaceable">SID</em>/<em class="replaceable">OWNER</em>/<em class="replaceable">BUCKET</em>
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.12"><span class="term">acl_profiles</span></dt><dd><p>
        An array of access list profiles.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.13"><span class="term">acl_profile</span></dt><dd><p>
        Each profile contains 'acls_id' that represents the profile, and an
        'acls' array that holds a list of 'acl_mappings'.
       </p></dd><dt id="id-1.4.6.2.11.6.5.6.14"><span class="term">profiles</span></dt><dd><p>
        A list of profiles. Each profile contains the following:
       </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.6.5.6.14.2.2.1"><span class="term">source_bucket</span></dt><dd><p>
           Either a bucket name, or a bucket prefix (if ends with *) that
           defines the source bucket(s) for this profile.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.14.2.2.2"><span class="term">target_path</span></dt><dd><p>
           See above for the explanation.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.14.2.2.3"><span class="term">connection_id</span></dt><dd><p>
           ID of the connection that will be used for this profile.
          </p></dd><dt id="id-1.4.6.2.11.6.5.6.14.2.2.4"><span class="term">acls_id</span></dt><dd><p>
           ID of ACL's profile that will be used for this profile.
          </p></dd></dl></div></dd></dl></div></section><section class="sect3" id="s3-specific-configurables" data-id-title="S3 specific configurables"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.4.2 </span><span class="title-name">S3 specific configurables</span> <a title="Permalink" class="permalink" href="#s3-specific-configurables">#</a></h4></div></div></div><p>
     The cloud synchronization module will only work with back-ends that are
     compatible with AWS S3. There are a few configurables that can be used to
     tweak its behavior when accessing S3 cloud services:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "multipart_sync_threshold": <em class="replaceable">OBJECT_SIZE</em>,
  "multipart_min_part_size": <em class="replaceable">PART_SIZE</em>
}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.11.6.6.4.1"><span class="term">multipart_sync_threshold</span></dt><dd><p>
        Objects whose size is equal to or larger than this value will be
        synchronized with the cloud service using multipart upload.
       </p></dd><dt id="id-1.4.6.2.11.6.6.4.2"><span class="term">multipart_min_part_size</span></dt><dd><p>
        Minimum parts size to use when synchronizing objects using multipart
        upload.
       </p></dd></dl></div></section></section><section class="sect2" id="archive-sync-module" data-id-title="Archive synchronization module"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.8.5 </span><span class="title-name">Archive synchronization module</span> <a title="Permalink" class="permalink" href="#archive-sync-module">#</a></h3></div></div></div><p>
    The <span class="emphasis"><em>archive sync module</em></span> uses the versioning feature of
    S3 objects in Object Gateway. You can configure an <span class="emphasis"><em>archive zone</em></span>
    that captures the different versions of S3 objects as they occur over time
    in other zones. The history of versions that the archive zone keeps can
    only be eliminated via gateways associated with the archive zone.
   </p><p>
    With such an architecture, several non-versioned zones can mirror their
    data and metadata via their zone gateways providing high availability to
    the end users, while the archive zone captures all the data updates to
    consolidate them as versions of S3 objects.
   </p><p>
    By including the archive zone in a multi-zone configuration, you gain the
    flexibility of an S3 object history in one zone while saving the space that
    the replicas of the versioned S3 objects would consume in the remaining
    zones.
   </p><section class="sect3" id="archive-sync-module-configuration" data-id-title="Configuring the archive synchronization module"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.8.5.1 </span><span class="title-name">Configuring the archive synchronization module</span> <a title="Permalink" class="permalink" href="#archive-sync-module-configuration">#</a></h4></div></div></div><div id="id-1.4.6.2.11.7.5.2" data-id-title="More information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More information</h6><p>
      Refer to <a class="xref" href="#ceph-rgw-fed" title="21.13. Multisite Object Gateways">Section 21.13, “Multisite Object Gateways”</a> for details on configuring
      multisite gateways.
     </p><p>
      Refer to <a class="xref" href="#ceph-rgw-sync" title="21.8. Synchronization modules">Section 21.8, “Synchronization modules”</a> for details on configuring
      synchronization modules.
     </p></div><p>
     To use the archive sync module, you need to create a new zone whose tier
     type is set to <code class="literal">archive</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE_GROUP_NAME</em> \
 --rgw-zone=<em class="replaceable">OGW_ZONE_NAME</em> \
 --endpoints=<em class="replaceable">http://OGW_ENDPOINT1_URL[,http://OGW_ENDPOINT2_URL,...]</em>
 --tier-type=archive</pre></div></section></section></section><section class="sect1" id="ceph-rgw-ldap" data-id-title="LDAP authentication"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.9 </span><span class="title-name">LDAP authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap">#</a></h2></div></div></div><p>
   Apart from the default local user authentication, Object Gateway can use LDAP server
   services to authenticate users as well.
  </p><section class="sect2" id="ceph-rgw-ldap-how-works" data-id-title="Authentication mechanism"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.9.1 </span><span class="title-name">Authentication mechanism</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-how-works">#</a></h3></div></div></div><p>
    The Object Gateway extracts the user's LDAP credentials from a token. A search
    filter is constructed from the user name. The Object Gateway uses the configured
    service account to search the directory for a matching entry. If an entry
    is found, the Object Gateway attempts to bind to the found distinguished name with
    the password from the token. If the credentials are valid, the bind will
    succeed, and the Object Gateway grants access.
   </p><p>
    You can limit the allowed users by setting the base for the search to a
    specific organizational unit or by specifying a custom search filter, for
    example requiring specific group membership, custom object classes, or
    attributes.
   </p></section><section class="sect2" id="ceph-rgw-ldap-reqs" data-id-title="Requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.9.2 </span><span class="title-name">Requirements</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-reqs">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>LDAP or Active Directory</em></span>: A running LDAP instance
      accessible by the Object Gateway.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Service account</em></span>: LDAP credentials to be used by the
      Object Gateway with search permissions.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>User account</em></span>: At least one user account in the LDAP
      directory.
     </p></li></ul></div><div id="id-1.4.6.2.12.4.3" data-id-title="Do not overlap LDAP and local users" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Do not overlap LDAP and local users</h6><p>
     You should not use the same user names for local users and for users being
     authenticated by using LDAP. The Object Gateway cannot distinguish them and it
     treats them as the same user.
    </p></div><div id="id-1.4.6.2.12.4.4" data-id-title="Sanity checks" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sanity checks</h6><p>
     Use the <code class="command">ldapsearch</code> utility to verify the service
     account or the LDAP connection. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</pre></div><p>
     Make sure to use the same LDAP parameters as in the Ceph configuration
     file to eliminate possible problems.
    </p></div></section><section class="sect2" id="ceph-rgw-ldap-config" data-id-title="Configuring Object Gateway to use LDAP authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.9.3 </span><span class="title-name">Configuring Object Gateway to use LDAP authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-config">#</a></h3></div></div></div><p>
    The following parameters are related to the LDAP authentication:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.12.5.3.1"><span class="term"><code class="option">rgw_s3_auth_use_ldap</code></span></dt><dd><p>
       Set this option to <code class="literal">true</code> to enable S3 authentication
       with LDAP.
      </p></dd><dt id="id-1.4.6.2.12.5.3.2"><span class="term"><code class="option">rgw_ldap_uri</code></span></dt><dd><p>
       Specifies the LDAP server to use. Make sure to use the
       <code class="literal">ldaps://<em class="replaceable">FQDN</em>:<em class="replaceable">PORT</em></code>
       parameter to avoid transmitting the plain text credentials openly.
      </p></dd><dt id="id-1.4.6.2.12.5.3.3"><span class="term"><code class="option">rgw_ldap_binddn</code></span></dt><dd><p>
       The Distinguished Name (DN) of the service account used by the Object Gateway.
      </p></dd><dt id="id-1.4.6.2.12.5.3.4"><span class="term"><code class="option">rgw_ldap_secret</code></span></dt><dd><p>
       The password for the service account.
      </p></dd><dt id="id-1.4.6.2.12.5.3.5"><span class="term">rgw_ldap_searchdn</span></dt><dd><p>
       Specifies the base in the directory information tree for searching
       users. This might be your users organizational unit or some more
       specific Organizational Unit (OU).
      </p></dd><dt id="id-1.4.6.2.12.5.3.6"><span class="term"><code class="option">rgw_ldap_dnattr</code></span></dt><dd><p>
       The attribute being used in the constructed search filter to match a
       user name. Depending on your Directory Information Tree (DIT) this would
       probably be <code class="literal">uid</code> or <code class="literal">cn</code>.
      </p></dd><dt id="id-1.4.6.2.12.5.3.7"><span class="term"><code class="option">rgw_search_filter</code></span></dt><dd><p>
       If not specified, the Object Gateway automatically constructs the search filter
       with the <code class="option">rgw_ldap_dnattr</code> setting. Use this parameter to
       narrow the list of allowed users in very flexible ways. Consult
       <a class="xref" href="#ceph-rgw-ldap-filter" title="21.9.4. Using a custom search filter to limit user access">Section 21.9.4, “Using a custom search filter to limit user access”</a> for details.
      </p></dd></dl></div></section><section class="sect2" id="ceph-rgw-ldap-filter" data-id-title="Using a custom search filter to limit user access"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.9.4 </span><span class="title-name">Using a custom search filter to limit user access</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-filter">#</a></h3></div></div></div><p>
    There are two ways you can use the <code class="option">rgw_search_filter</code>
    parameter.
   </p><section class="sect3" id="partial-filter-search-filter" data-id-title="Partial filter to further limit the constructed search filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.9.4.1 </span><span class="title-name">Partial filter to further limit the constructed search filter</span> <a title="Permalink" class="permalink" href="#partial-filter-search-filter">#</a></h4></div></div></div><p>
     An example of a partial filter:
    </p><div class="verbatim-wrap"><pre class="screen">"objectclass=inetorgperson"</pre></div><p>
     The Object Gateway will generate the search filter as usual with the user name from
     the token and the value of <code class="option">rgw_ldap_dnattr</code>. The
     constructed filter is then combined with the partial filter from the
     <code class="option">rgw_search_filter</code> attribute. Depending on the user name
     and the settings the final search filter may become:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=hari)(objectclass=inetorgperson))"</pre></div><p>
     In that case, user 'hari' will only be granted access if he is found in
     the LDAP directory, has an object class of 'inetorgperson', and did
     specify a valid password.
    </p></section><section class="sect3" id="complete-filter" data-id-title="Complete filter"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.9.4.2 </span><span class="title-name">Complete filter</span> <a title="Permalink" class="permalink" href="#complete-filter">#</a></h4></div></div></div><p>
     A complete filter must contain a <code class="option">USERNAME</code> token which
     will be substituted with the user name during the authentication attempt.
     The <code class="option">rgw_ldap_dnattr</code> parameter is not used anymore in this
     case. For example, to limit valid users to a specific group, use the
     following filter:
    </p><div class="verbatim-wrap"><pre class="screen">"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</pre></div><div id="id-1.4.6.2.12.6.4.4" data-id-title="memberOf attribute" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="literal">memberOf</code> attribute</h6><p>
      Using the <code class="literal">memberOf</code> attribute in LDAP searches requires
      server side support from you specific LDAP server implementation.
     </p></div></section></section><section class="sect2" id="ceph-rgw-ldap-token" data-id-title="Generating an access token for LDAP authentication"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.9.5 </span><span class="title-name">Generating an access token for LDAP authentication</span> <a title="Permalink" class="permalink" href="#ceph-rgw-ldap-token">#</a></h3></div></div></div><p>
    The <code class="command">radosgw-token</code> utility generates the access token
    based on the LDAP user name and password. It outputs a base-64 encoded
    string which is the actual access token. Use your favorite S3 client (refer
    to <a class="xref" href="#accessing-ragos-gateway" title="21.5.1. Accessing Object Gateway">Section 21.5.1, “Accessing Object Gateway”</a>) and specify the token as the
    access key and use an empty secret key.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>export RGW_ACCESS_KEY_ID="<em class="replaceable">USERNAME</em>"
<code class="prompt user">&gt; </code>export RGW_SECRET_ACCESS_KEY="<em class="replaceable">PASSWORD</em>"
<code class="prompt user">cephuser@adm &gt; </code>radosgw-token --encode --ttype=ldap</pre></div><div id="id-1.4.6.2.12.7.4" data-id-title="Clear text credentials" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Clear text credentials</h6><p>
     The access token is a base-64 encoded JSON structure and contains the LDAP
     credentials as a clear text.
    </p></div><div id="id-1.4.6.2.12.7.5" data-id-title="Active Directory" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Active Directory</h6><p>
     For Active Directory, use the <code class="option">--ttype=ad</code> parameter.
    </p></div></section></section><section class="sect1" id="ogw-bucket-sharding" data-id-title="Bucket index sharding"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.10 </span><span class="title-name">Bucket index sharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding">#</a></h2></div></div></div><p>
   The Object Gateway stores bucket index data in an index pool, which defaults to
   <code class="literal">.rgw.buckets.index</code>. If you put too many (hundreds of
   thousands) objects into a single bucket and the quota for maximum number of
   objects per bucket (<code class="option">rgw bucket default quota max objects</code>)
   is not set, the performance of the index pool may degrade. <span class="emphasis"><em>Bucket
   index sharding</em></span> prevents such performance decreases and allows a
   high number of objects per bucket.
  </p><section class="sect2" id="ogw-bucket-reshard" data-id-title="Bucket index resharding"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.10.1 </span><span class="title-name">Bucket index resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-reshard">#</a></h3></div></div></div><p>
    If a bucket has grown large and its initial configuration is not sufficient
    anymore, the bucket's index pool needs to be resharded. You can either use
    automatic online bucket index resharding (refer to
    <a class="xref" href="#ogw-bucket-sharding-dyn" title="21.10.1.1. Dynamic resharding">Section 21.10.1.1, “Dynamic resharding”</a>), or reshard the bucket index
    offline manually (refer to <a class="xref" href="#ogw-bucket-sharding-re" title="21.10.1.2. Resharding manually">Section 21.10.1.2, “Resharding manually”</a>).
   </p><section class="sect3" id="ogw-bucket-sharding-dyn" data-id-title="Dynamic resharding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.10.1.1 </span><span class="title-name">Dynamic resharding</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-dyn">#</a></h4></div></div></div><p>
     From SUSE Enterprise Storage 5, we support online bucket resharding. This detects if
     the number of objects per bucket reaches a certain threshold, and
     automatically increases the number of shards used by the bucket index.
     This process reduces the number of entries in each bucket index shard.
    </p><p>
     The detection process runs:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       When new objects are added to the bucket.
      </p></li><li class="listitem"><p>
       In a background process that periodically scans all the buckets. This is
       needed in order to deal with existing buckets that are not being
       updated.
      </p></li></ul></div><p>
     A bucket that requires resharding is added to the
     <code class="option">reshard_log</code> queue and will be scheduled to be resharded
     later. The reshard threads run in the background and execute the scheduled
     resharding, one at a time.
    </p><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Configuring dynamic resharding </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.13.3.3.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.6.2.13.3.3.6.2"><span class="term"><code class="option">rgw_dynamic_resharding</code></span></dt><dd><p>
        Enables or disables dynamic bucket index resharding. Possible values
        are 'true' or 'false'. Defaults to 'true'.
       </p></dd><dt id="id-1.4.6.2.13.3.3.6.3"><span class="term"><code class="option">rgw_reshard_num_logs</code></span></dt><dd><p>
        Number of shards for the resharding log. Defaults to 16.
       </p></dd><dt id="id-1.4.6.2.13.3.3.6.4"><span class="term"><code class="option">rgw_reshard_bucket_lock_duration</code></span></dt><dd><p>
        Duration of lock on the bucket object during resharding. Defaults to
        120 seconds.
       </p></dd><dt id="id-1.4.6.2.13.3.3.6.5"><span class="term"><code class="option">rgw_max_objs_per_shard</code></span></dt><dd><p>
        Maximum number of objects per bucket index shard. Defaults to 100000
        objects.
       </p></dd><dt id="id-1.4.6.2.13.3.3.6.6"><span class="term"><code class="option">rgw_reshard_thread_interval</code></span></dt><dd><p>
        Maximum time between rounds of reshard thread processing. Defaults to
        600 seconds.
       </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Commands to administer the resharding process </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.13.3.3.7">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.6.2.13.3.3.7.2"><span class="term">Add a bucket to the resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin reshard add \
 --bucket <em class="replaceable">BUCKET_NAME</em> \
 --num-shards <em class="replaceable">NEW_NUMBER_OF_SHARDS</em></pre></div></dd><dt id="id-1.4.6.2.13.3.3.7.3"><span class="term">List resharding queue:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin reshard list</pre></div></dd><dt id="id-1.4.6.2.13.3.3.7.4"><span class="term">Process/schedule a bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin reshard process</pre></div></dd><dt id="id-1.4.6.2.13.3.3.7.5"><span class="term">Display the bucket resharding status:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin reshard status --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd><dt id="id-1.4.6.2.13.3.3.7.6"><span class="term">Cancel pending bucket resharding:</span></dt><dd><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin reshard cancel --bucket <em class="replaceable">BUCKET_NAME</em></pre></div></dd></dl></div></section><section class="sect3" id="ogw-bucket-sharding-re" data-id-title="Resharding manually"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.10.1.2 </span><span class="title-name">Resharding manually</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-re">#</a></h4></div></div></div><p>
     Dynamic resharding as mentioned in
     <a class="xref" href="#ogw-bucket-sharding-dyn" title="21.10.1.1. Dynamic resharding">Section 21.10.1.1, “Dynamic resharding”</a> is supported only for simple
     Object Gateway configurations. For multisite configurations, use manual resharding
     as described in this section.
    </p><p>
     To reshard the bucket index manually offline, use the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket reshard</pre></div><p>
     The <code class="command">bucket reshard</code> command performs the following:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Creates a new set of bucket index objects for the specified object.
      </p></li><li class="listitem"><p>
       Spreads all entries of these index objects.
      </p></li><li class="listitem"><p>
       Creates a new bucket instance.
      </p></li><li class="listitem"><p>
       Links the new bucket instance with the bucket so that all new index
       operations go through the new bucket indexes.
      </p></li><li class="listitem"><p>
       Prints the old and the new bucket ID to the standard output.
      </p></li></ul></div><div id="id-1.4.6.2.13.3.4.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      When choosing a number of shards, note the following: aim for no more
      than 100000 entries per shard. Bucket index shards that are prime numbers
      tend to work better in evenly distributing bucket index entries across
      the shards. For example, 503 bucket index shards is better than 500 since
      the former is prime.
     </p></div><div id="id-1.4.6.2.13.3.4.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       Multi-site configurations do not support resharding a bucket index.
     </p><p>
       For multi-site configurations, resharding a bucket index requires
       resynchronizing all data from the master zone to all slave zones.
       Depending on the bucket size, this can take a considerable amount of time
       and resources.
     </p></div><div class="procedure" id="id-1.4.6.2.13.3.4.9" data-id-title="Resharding the bucket index"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 21.1: </span><span class="title-name">Resharding the bucket index </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.13.3.4.9">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Make sure that all operations to the bucket are stopped.
      </p></li><li class="step"><p>
       Back up the original bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bi list \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 &gt; <em class="replaceable">BUCKET_NAME</em>.list.backup</pre></div></li><li class="step"><p>
       Reshard the bucket index:
      </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket reshard \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 --num-shards=<em class="replaceable">NEW_SHARDS_NUMBER</em></pre></div><div id="id-1.4.6.2.13.3.4.9.4.3" data-id-title="Old bucket ID" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Old bucket ID</h6><p>
        As part of its output, this command also prints the new and the old
        bucket ID.
       </p></div></li></ol></div></div></section></section><section class="sect2" id="ogw-bucket-sharding-new" data-id-title="Bucket index sharding for new buckets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.10.2 </span><span class="title-name">Bucket index sharding for new buckets</span> <a title="Permalink" class="permalink" href="#ogw-bucket-sharding-new">#</a></h3></div></div></div><p>
    There are two options that affect bucket index sharding:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Use the <code class="option">rgw_override_bucket_index_max_shards</code> option for
      simple configurations.
     </p></li><li class="listitem"><p>
      Use the <code class="option">bucket_index_max_shards</code> option for multisite
      configurations.
     </p></li></ul></div><p>
    Setting the options to <code class="literal">0</code> disables bucket index sharding.
    A value greater than <code class="literal">0</code> enables bucket index sharding and
    sets the maximum number of shards.
   </p><p>
    The following formula helps you calculate the recommended number of shards:
   </p><div class="verbatim-wrap"><pre class="screen">number_of_objects_expected_in_a_bucket / 100000</pre></div><p>
    Be aware that the maximum number of shards is 7877.
   </p><section class="sect3" id="multisite-config-bucket" data-id-title="Multisite configurations"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.10.2.1 </span><span class="title-name">Multisite configurations</span> <a title="Permalink" class="permalink" href="#multisite-config-bucket">#</a></h4></div></div></div><p>
     Multisite configurations can have a different index pool to manage
     failover. To configure a consistent shard count for zones in one zone
     group, set the <code class="option">bucket_index_max_shards</code> option in the zone
     group's configuration:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Export the zonegroup configuration to the
       <code class="filename">zonegroup.json</code> file:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup get &gt; zonegroup.json</pre></div></li><li class="step"><p>
       Edit the <code class="filename">zonegroup.json</code> file and set the
       <code class="option">bucket_index_max_shards</code> option for each named zone.
      </p></li><li class="step"><p>
       Reset the zonegroup:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup set &lt; zonegroup.json</pre></div></li><li class="step"><p>
       Update the period. See
       <a class="xref" href="#ceph-rgw-fed-masterzone-updateperiod" title="21.13.3.6. Update the period">Section 21.13.3.6, “Update the period”</a>.
      </p></li></ol></div></div></section></section></section><section class="sect1" id="ogw-keystone" data-id-title="OpenStack Keystone integration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.11 </span><span class="title-name">OpenStack Keystone integration</span> <a title="Permalink" class="permalink" href="#ogw-keystone">#</a></h2></div></div></div><p>
   OpenStack Keystone is an identity service for the OpenStack product. You can
   integrate the Object Gateway with Keystone to set up a gateway that accepts a
   Keystone authentication token. A user authorized by Keystone to access
   the gateway will be verified on the Ceph Object Gateway side and automatically created if
   needed. The Object Gateway queries Keystone periodically for a list of revoked
   tokens.
  </p><section class="sect2" id="ogw-keystone-ostack" data-id-title="Configuring OpenStack"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.11.1 </span><span class="title-name">Configuring OpenStack</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ostack">#</a></h3></div></div></div><p>
    Before configuring the Ceph Object Gateway, you need to configure the OpenStack Keystone to
    enable the Swift service and point it to the Ceph Object Gateway:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      <span class="emphasis"><em>Set the Swift service.</em></span> To use OpenStack to validate
      Swift users, first create the Swift service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>openstack service create \
 --name=swift \
 --description="Swift Service" \
 object-store</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Set the endpoints.</em></span> After you create the Swift
      service, point to the Ceph Object Gateway. Replace
      <em class="replaceable">REGION_NAME</em> with the name of the gateway’s
      zonegroup name or region name.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>openstack endpoint create --region <em class="replaceable">REGION_NAME</em> \
 --publicurl   "http://radosgw.example.com:8080/swift/v1" \
 --adminurl    "http://radosgw.example.com:8080/swift/v1" \
 --internalurl "http://radosgw.example.com:8080/swift/v1" \
 swift</pre></div></li><li class="step"><p>
      <span class="emphasis"><em>Verify the settings.</em></span> After you create the Swift
      service and set the endpoints, show the endpoints to verify that all the
      settings are correct.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>openstack endpoint show object-store</pre></div></li></ol></div></div></section><section class="sect2" id="ogw-keystone-ogw" data-id-title="Configuring the Ceph Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.11.2 </span><span class="title-name">Configuring the Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#ogw-keystone-ogw">#</a></h3></div></div></div><section class="sect3" id="id-1.4.6.2.14.4.2" data-id-title="Configure SSL certificates"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.11.2.1 </span><span class="title-name">Configure SSL certificates</span> <a title="Permalink" class="permalink" href="#id-1.4.6.2.14.4.2">#</a></h4></div></div></div><p>
     The Ceph Object Gateway queries Keystone periodically for a list of revoked tokens.
     These requests are encoded and signed. Keystone may be also configured
     to provide self-signed tokens, which are also encoded and signed. You need
     to configure the gateway so that it can decode and verify these signed
     messages. Therefore, the OpenSSL certificates that Keystone uses to
     create the requests need to be converted to the 'nss db' format:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /var/ceph/nss
<code class="prompt root"># </code>openssl x509 -in /etc/keystone/ssl/certs/ca.pem \
 -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
<code class="systemitem">root</code>openssl x509 -in /etc/keystone/ssl/certs/signing_cert.pem \
 -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"</pre></div><p>
     To allow Ceph Object Gateway to interact with OpenStack Keystone, OpenStack Keystone can use a
     self-signed SSL certificate. Either install Keystone’s SSL certificate
     on the node running the Ceph Object Gateway, or alternatively set the value of the
     option <code class="option">rgw keystone verify ssl</code> to 'false'. Setting
     <code class="option">rgw keystone verify ssl</code> to 'false' means that the gateway
     will not attempt to verify the certificate.
    </p></section><section class="sect3" id="config-ogw-options" data-id-title="Configure the Object Gateways options"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.11.2.2 </span><span class="title-name">Configure the Object Gateway's options</span> <a title="Permalink" class="permalink" href="#config-ogw-options">#</a></h4></div></div></div><p>
     You can configure Keystone integration using the following options:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.14.4.3.3.1"><span class="term"><code class="option">rgw keystone api version</code></span></dt><dd><p>
        Version of the Keystone API. Valid options are 2 or 3. Defaults to 2.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.2"><span class="term"><code class="option">rgw keystone url</code></span></dt><dd><p>
        The URL and port number of the administrative RESTful API on the
        Keystone server. Follows the pattern
        <em class="replaceable">SERVER_URL:PORT_NUMBER</em>.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.3"><span class="term"><code class="option">rgw keystone admin token</code></span></dt><dd><p>
        The token or shared secret that is configured internally in Keystone
        for administrative requests.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.4"><span class="term"><code class="option">rgw keystone accepted roles</code></span></dt><dd><p>
        The roles required to serve requests. Defaults to 'Member, admin'.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.5"><span class="term"><code class="option">rgw keystone accepted admin roles</code></span></dt><dd><p>
        The list of roles allowing a user to gain administrative privileges.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.6"><span class="term"><code class="option">rgw keystone token cache size</code></span></dt><dd><p>
        The maximum number of entries in the Keystone token cache.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.7"><span class="term"><code class="option">rgw keystone revocation interval</code></span></dt><dd><p>
        The number of seconds before checking revoked tokens. Defaults to 15 *
        60.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.8"><span class="term"><code class="option">rgw keystone implicit tenants</code></span></dt><dd><p>
        Create new users in their own tenants of the same name. Defaults to
        'false'.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.9"><span class="term"><code class="option">rgw s3 auth use keystone</code></span></dt><dd><p>
        If set to 'true', the Ceph Object Gateway will authenticate users using Keystone.
        Defaults to 'false'.
       </p></dd><dt id="id-1.4.6.2.14.4.3.3.10"><span class="term"><code class="option">nss db path</code></span></dt><dd><p>
        The path to the NSS database.
       </p></dd></dl></div><p>
     It is also possible to configure the Keystone service tenant, user, and
     password for Keystone (for version 2.0 of the OpenStack Identity API),
     similar to the way OpenStack services tend to be configured. This way you
     can avoid setting the shared secret <code class="option">rgw keystone admin
     token</code> in the configuration file, which should be disabled in
     production environments. The service tenant credentials should have admin
     privileges. For more details refer to the
     <a class="link" href="https://docs.openstack.org/keystone/latest/#setting-up-projects-users-and-roles" target="_blank">official
     OpenStack Keystone documentation</a>. The related configuration options
     follow:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.14.4.3.5.1"><span class="term"><code class="option">rgw keystone admin user</code></span></dt><dd><p>
        The Keystone administrator user name.
       </p></dd><dt id="id-1.4.6.2.14.4.3.5.2"><span class="term"><code class="option">rgw keystone admin password</code></span></dt><dd><p>
        The keystone administrator user password.
       </p></dd><dt id="id-1.4.6.2.14.4.3.5.3"><span class="term"><code class="option">rgw keystone admin tenant</code></span></dt><dd><p>
        The Keystone version 2.0 administrator user tenant.
       </p></dd></dl></div><p>
     A Ceph Object Gateway user is mapped to a Keystone tenant. A Keystone user has
     different roles assigned to it, possibly on more than one tenant. When the
     Ceph Object Gateway gets the ticket, it looks at the tenant and the user roles that are
     assigned to that ticket, and accepts or rejects the request according to
     the setting of the <code class="option">rgw keystone accepted roles</code> option.
    </p><div id="id-1.4.6.2.14.4.3.7" data-id-title="Mapping to OpenStack tenants" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mapping to OpenStack tenants</h6><p>
      Although Swift tenants are mapped to the Object Gateway user by default, they
      can be also mapped to OpenStack tenants via the <code class="option">rgw keystone
      implicit tenants</code> option. This will make containers use the
      tenant namespace instead of the S3 like global namespace that the Object Gateway
      defaults to. We recommend deciding on the mapping method at the planning
      stage to avoid confusion. The reason for this is that toggling the option
      later affects only newer requests which get mapped under a tenant, while
      older buckets created before still continue to be in a global namespace.
     </p></div><p>
     For version 3 of the OpenStack Identity API, you should replace the
     <code class="option">rgw keystone admin tenant</code> option with:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.14.4.3.9.1"><span class="term"><code class="option">rgw keystone admin domain</code></span></dt><dd><p>
        The Keystone administrator user domain.
       </p></dd><dt id="id-1.4.6.2.14.4.3.9.2"><span class="term"><code class="option">rgw keystone admin project</code></span></dt><dd><p>
        The Keystone administrator user project.
       </p></dd></dl></div></section></section></section><section class="sect1" id="ogw-storage-classes" data-id-title="Pool placement and storage classes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.12 </span><span class="title-name">Pool placement and storage classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes">#</a></h2></div></div></div><section class="sect2" id="ogw-storage-classes-placement-targets" data-id-title="Displaying placement targets"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.12.1 </span><span class="title-name">Displaying placement targets</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-placement-targets">#</a></h3></div></div></div><p>
    Placement targets control which pools are associated with a particular
    bucket. A bucket’s placement target is selected on creation, and cannot be
    modified. You can display its <code class="literal">placement_rule</code> by running
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket stats</pre></div><p>
    The zonegroup configuration contains a list of placement targets with an
    initial target named 'default-placement'. The zone configuration then maps
    each zonegroup placement target name onto its local storage. This zone
    placement information includes the 'index_pool' name for the bucket index,
    the 'data_extra_pool' name for metadata about incomplete multipart uploads,
    and a 'data_pool' name for each storage class.
   </p></section><section class="sect2" id="ogw-storage-classes-itself" data-id-title="Storage classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.12.2 </span><span class="title-name">Storage classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-itself">#</a></h3></div></div></div><p>
    Storage classes help customizing the placement of object data. S3 Bucket
    Lifecycle rules can automate the transition of objects between storage
    classes.
   </p><p>
    Storage classes are defined in terms of placement targets. Each zonegroup
    placement target lists its available storage classes with an initial class
    named 'STANDARD'. The zone configuration is responsible for providing a
    'data_pool' pool name for each of the zonegroup’s storage classes.
   </p></section><section class="sect2" id="ogw-storage-classes-zone-config" data-id-title="Configuring zonegroups and zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.12.3 </span><span class="title-name">Configuring zonegroups and zones</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-zone-config">#</a></h3></div></div></div><p>
    Use the <code class="command">radosgw-admin</code> command on the zonegroups and zones
    to configure their placement. You can query the zonegroup placement
    configuration using the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup get
{
    "id": "ab01123f-e0df-4f29-9d71-b44888d67cd5",
    "name": "default",
    "api_name": "default",
    ...
    "placement_targets": [
        {
            "name": "default-placement",
            "tags": [],
            "storage_classes": [
                "STANDARD"
            ]
        }
    ],
    "default_placement": "default-placement",
    ...
}</pre></div><p>
    To query the zone placement configuration, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone get
{
    "id": "557cdcee-3aae-4e9e-85c7-2f86f5eddb1f",
    "name": "default",
    "domain_root": "default.rgw.meta:root",
    ...
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "default.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "default.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "default.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    ...
}</pre></div><div id="id-1.4.6.2.15.4.6" data-id-title="No previous multisite configuration" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: No previous multisite configuration</h6><p>
     If you have not done any previous multisite configuration, a 'default'
     zone and zonegroup are created for you, and changes to the zone/zonegroup
     will not take effect until you restart the Ceph Object Gateways. If you have created a
     realm for multisite, the zone/zonegroup changes will take effect after you
     commit the changes with the <code class="command">radosgw-admin period update
     --commit</code> command.
    </p></div><section class="sect3" id="adding-placement-target" data-id-title="Adding a placement target"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.3.1 </span><span class="title-name">Adding a placement target</span> <a title="Permalink" class="permalink" href="#adding-placement-target">#</a></h4></div></div></div><p>
     To create a new placement target named 'temporary', start by adding it to
     the zonegroup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id temporary</pre></div><p>
     Then provide the zone placement info for that target:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id temporary \
      --data-pool default.rgw.temporary.data \
      --index-pool default.rgw.temporary.index \
      --data-extra-pool default.rgw.temporary.non-ec</pre></div></section><section class="sect3" id="adding-storage-class" data-id-title="Adding a storage class"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.3.2 </span><span class="title-name">Adding a storage class</span> <a title="Permalink" class="permalink" href="#adding-storage-class">#</a></h4></div></div></div><p>
     To add a new storage class named 'COLD' to the 'default-placement' target,
     start by adding it to the zonegroup:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id default-placement \
      --storage-class COLD</pre></div><p>
     Then provide the zone placement info for that storage class:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id default-placement \
      --storage-class COLD \
      --data-pool default.rgw.cold.data \
      --compression lz4</pre></div></section></section><section class="sect2" id="ogw-storage-classes-customizing-placement" data-id-title="Placement customization"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.12.4 </span><span class="title-name">Placement customization</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-customizing-placement">#</a></h3></div></div></div><section class="sect3" id="edit-default-zgroup-placement" data-id-title="Editing default zonegroup placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.4.1 </span><span class="title-name">Editing default zonegroup placement</span> <a title="Permalink" class="permalink" href="#edit-default-zgroup-placement">#</a></h4></div></div></div><p>
     By default, new buckets will use the zonegroup’s
     <code class="literal">default_placement</code> target. You can change this zonegroup
     setting with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup placement default \
      --rgw-zonegroup default \
      --placement-id new-placement</pre></div></section><section class="sect3" id="edit-default-user-placement" data-id-title="Editing default user placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.4.2 </span><span class="title-name">Editing default user placement</span> <a title="Permalink" class="permalink" href="#edit-default-user-placement">#</a></h4></div></div></div><p>
     A Ceph Object Gateway user can override the zonegroup’s default placement target by
     setting a non-empty <code class="literal">default_placement</code> field in the user
     info. Similarly, the <code class="literal">default_storage_class</code> can override
     the <code class="option">STANDARD</code> storage class applied to objects by default.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user info --uid testid
{
    ...
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    ...
}</pre></div><p>
     If a zonegroup’s placement target contains any tags, users will be unable
     to create buckets with that placement target unless their user info
     contains at least one matching tag in its 'placement_tags' field. This can
     be useful to restrict access to certain types of storage.
    </p><p>
     The <code class="command">radosgw-admin</code> command cannot modify these fields
     directly, therefore you need to edit the JSON format manually:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin metadata get user:<em class="replaceable">USER-ID</em> &gt; user.json
<code class="prompt user">&gt; </code>vi user.json     # edit the file as required
<code class="prompt user">cephuser@adm &gt; </code>radosgw-admin metadata put user:<em class="replaceable">USER-ID</em> &lt; user.json</pre></div></section><section class="sect3" id="s3-bucket-default-placement" data-id-title="Editing the S3 default bucket placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.4.3 </span><span class="title-name">Editing the S3 default bucket placement</span> <a title="Permalink" class="permalink" href="#s3-bucket-default-placement">#</a></h4></div></div></div><p>
     When creating a bucket with the S3 protocol, a placement target can be
     provided as part of the <code class="option">LocationConstraint</code> to override
     the default placement targets from the user and zonegroup.
    </p><p>
     Normally, the <code class="option">LocationConstraint</code> needs to match the
     zonegroup’s <code class="option">api_name</code>:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;LocationConstraint&gt;default&lt;/LocationConstraint&gt;</pre></div><p>
     You can add a custom placement target to the <code class="option">api_name</code>
     following a colon:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;LocationConstraint&gt;default:new-placement&lt;/LocationConstraint&gt;</pre></div></section><section class="sect3" id="swift-default-bucket-placement" data-id-title="Editing the Swift bucket placement"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.12.4.4 </span><span class="title-name">Editing the Swift bucket placement</span> <a title="Permalink" class="permalink" href="#swift-default-bucket-placement">#</a></h4></div></div></div><p>
     When creating a bucket with the Swift protocol, you can provide a
     placement target in the HTTP header's <code class="literal">X-Storage-Policy</code>:
    </p><div class="verbatim-wrap"><pre class="screen">X-Storage-Policy: <em class="replaceable">NEW-PLACEMENT</em></pre></div></section></section><section class="sect2" id="ogw-storage-classes-usage" data-id-title="Using storage classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.12.5 </span><span class="title-name">Using storage classes</span> <a title="Permalink" class="permalink" href="#ogw-storage-classes-usage">#</a></h3></div></div></div><p>
    All placement targets have a <code class="option">STANDARD</code> storage class which
    is applied to new objects by default. You can override this default with
    its <code class="literal">default_storage_class</code>.
   </p><p>
    To create an object in a non-default storage class, provide that storage
    class name in an HTTP header with the request. The S3 protocol uses the
    <code class="literal">X-Amz-Storage-Class</code> header, while the Swift protocol
    uses the <code class="literal">X-Object-Storage-Class</code> header.
   </p><p>
    You can use <span class="emphasis"><em>S3 Object Lifecycle Management</em></span> to move
    object data between storage classes using <code class="option">Transition</code>
    actions.
   </p></section></section><section class="sect1" id="ceph-rgw-fed" data-id-title="Multisite Object Gateways"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">21.13 </span><span class="title-name">Multisite Object Gateways</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed">#</a></h2></div></div></div><p>
   Ceph supports several multi-site configuration options for the Ceph Object Gateway:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.16.3.1"><span class="term">Multi-zone</span></dt><dd><p>
      A configuration consisting of one zonegroup and multiple zones, each zone
      with one or more <code class="systemitem">ceph-radosgw</code>
      instances. Each zone is backed by its own Ceph Storage Cluster.
      Multiple zones in a zone group provide disaster recovery for the zonegroup
      should one of the zones experience a significant failure. Each zone is
      active and may receive write operations. In addition to disaster
      recovery, multiple active zones may also serve as a foundation for
      content delivery networks.
     </p></dd><dt id="id-1.4.6.2.16.3.2"><span class="term">Multi-zone-group</span></dt><dd><p>
      Ceph Object Gateway supports multiple zonegroups, each zonegroup with one or more zones.
      Objects stored to zones in one zonegroup within the same realm as another
      zonegroup share a global object namespace, ensuring unique object IDs
      across zonegroups and zones.
     </p><div id="id-1.4.6.2.16.3.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       It is important to note that zonegroups <span class="emphasis"><em>only</em></span> sync
       metadata amongst themselves. Data and metadata are replicated between
       the zones within the zonegroup. No data or metadata is shared across a
       realm.
      </p></div></dd><dt id="id-1.4.6.2.16.3.3"><span class="term">Multiple realms</span></dt><dd><p>
      Ceph Object Gateway supports the notion of realms; a globally unique namespace.
      Multiple realms are supported which may encompass single or multiple
      zonegroups.
     </p></dd></dl></div><p>
   You can configure each Object Gateway to work in an active-active zone configuration,
   allowing for writes to non-master zones. The multi-site configuration is
   stored within a container called a realm. The realm stores zonegroups, zones,
   and a time period with multiple epochs for tracking changes to the
   configuration. The <code class="systemitem">rgw</code> daemons
   handle the synchronization, eliminating the need for a separate
   synchronization agent. This approach to synchronization allows the Ceph Object Gateway to
   operate with an active-active configuration instead of active-passive.
  </p><section class="sect2" id="ceph-rgw-multi-req-assump" data-id-title="Requirements and assumptions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.1 </span><span class="title-name">Requirements and assumptions</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multi-req-assump">#</a></h3></div></div></div><p>
    A multi-site configuration requires at least two Ceph storage clusters,
    and at least two Ceph Object Gateway instances, one for each Ceph storage cluster. The
    following configuration assumes at least two Ceph storage clusters are in
    geographically separate locations. However, the configuration can work on
    the same site. For example, named <code class="literal">rgw1</code> and
    <code class="literal">rgw2</code>.
   </p><p>
    A multi-site configuration requires a master zonegroup and a master zone. A
    master zone is the source of truth with regard to all metadata operations
    in a multisite cluster. Additionally, each zonegroup requires a master zone.
    zonegroups may have one or more secondary or non-master zones. In this
    guide, the <code class="literal">rgw1</code> host serves as the master zone of the
    master zonegroup and the <code class="literal">rgw2</code> host serves as the
    secondary zone of the master zonegroup.
   </p></section><section class="sect2" id="ceph-rgw-multi-limitations" data-id-title="Limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.2 </span><span class="title-name">Limitations</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multi-limitations">#</a></h3></div></div></div><p>
      Multi-site configurations do not support resharding a bucket index.
    </p><p>
    As a workaround, the bucket can be purged from the slave zones, resharded on
    the master zone, and then resynchronized. Depending on the contents of the
    bucket, this can be a time- and resource-intensive operation.
    </p></section><section class="sect2" id="ceph-rgw-config-master-zone" data-id-title="Configuring a master zone"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.3 </span><span class="title-name">Configuring a master zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-config-master-zone">#</a></h3></div></div></div><p>
    All gateways in a multi-site configuration retrieve their configuration
    from a <code class="systemitem">ceph-radosgw</code> daemon on a
    host within the master zonegroup and master zone. To configure your gateways
    in a multi-site configuration, select a
    <code class="systemitem">ceph-radosgw</code> instance to configure
    the master zonegroup and master zone.
   </p><section class="sect3" id="ceph-rgw-fed-realm" data-id-title="Creating a realm"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.1 </span><span class="title-name">Creating a realm</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-realm">#</a></h4></div></div></div><p>
     A realm represents a globally unique namespace consisting of one or more
     zonegroups containing one or more zones. Zones contain buckets, which in
     turn contain objects. A realm enables the Ceph Object Gateway to support multiple
     namespaces and their configuration on the same hardware. A realm contains
     the notion of periods. Each period represents the state of the zonegroup
     and zone configuration in time. Each time you make a change to a zonegroup
     or zone, update the period and commit it. By default, the Ceph Object Gateway does not
     create a realm for backward compatibility. As a best practice, we
     recommend creating realms for new clusters.
    </p><p>
     Create a new realm called <code class="literal">gold</code> for the multi-site
     configuration by opening a command line interface on a host identified to
     serve in the master zonegroup and zone. Then, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm create --rgw-realm=gold --default</pre></div><p>
     If the cluster has a single realm, specify the <code class="option">--default</code>
     flag. If <code class="option">--default</code> is specified,
     <code class="command">radosgw-admin</code> uses this realm by default. If
     <code class="option">--default</code> is not specified, adding zone-groups and zones
     requires specifying either the <code class="option">--rgw-realm</code> flag or the
     <code class="option">--realm-id</code> flag to identify the realm when adding
     zonegroups and zones.
    </p><p>
     After creating the realm, <code class="command">radosgw-admin</code> returns the
     realm configuration:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</pre></div><div id="id-1.4.6.2.16.7.3.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Ceph generates a unique ID for the realm, which allows the renaming of
      a realm if the need arises.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-createmasterzonegrp" data-id-title="Creating a master zonegroup"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.2 </span><span class="title-name">Creating a master zonegroup</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-createmasterzonegrp">#</a></h4></div></div></div><p>
     A realm must have at least one zonegroup to serve as the master zonegroup
     for the realm. Create a new master zonegroup for the multi-site
     configuration by opening a command line interface on a host identified to
     serve in the master zonegroup and zone. Create a master zonegroup called
     <code class="literal">us</code> by executing the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default</pre></div><p>
     If the realm only has a single zonegroup, specify the
     <code class="option">--default</code> flag. If <code class="option">--default</code> is
     specified, <code class="command">radosgw-admin</code> uses this zonegroup by default
     when adding new zones. If <code class="option">--default</code> is not specified,
     adding zones requires either the <code class="option">--rgw-zonegroup</code> flag or
     the <code class="option">--zonegroup-id</code> flag to identify the zonegroup when
     adding or modifying zones.
    </p><p>
     After creating the master zonegroup, <code class="command">radosgw-admin</code>
     returns the zonegroup configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">{
 "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
 "name": "us",
 "api_name": "us",
 "is_master": "true",
 "endpoints": [
     "http:\/\/rgw1:80"
 ],
 "hostnames": [],
 "hostnames_s3website": [],
 "master_zone": "",
 "zones": [],
 "placement_targets": [],
 "default_placement": "",
 "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone" data-id-title="Creating a master zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.3 </span><span class="title-name">Creating a master zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone">#</a></h4></div></div></div><div id="id-1.4.6.2.16.7.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Zones need to be created on a Ceph Object Gateway node that will be within the zone.
     </p></div><p>
     Create a new master zone for the multi-site configuration by opening a
     command line interface on a host identified to serve in the master
     zonegroup and zone. Execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">SYSTEM_SECRET_KEY</em></pre></div><div id="id-1.4.6.2.16.7.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The <code class="option">--access-key</code> and <code class="option">--secret</code> options
      are not specified in the above example. These settings are added to the
      zone when the user is created in the next section.
     </p></div><p>
     After creating the master zone, <code class="command">radosgw-admin</code> returns
     the zone configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">  {
      "id": "56dfabbb-2f4e-4223-925e-de3c72de3866",
      "name": "us-east-1",
      "domain_root": "us-east-1.rgw.meta:root",
      "control_pool": "us-east-1.rgw.control",
      "gc_pool": "us-east-1.rgw.log:gc",
      "lc_pool": "us-east-1.rgw.log:lc",
      "log_pool": "us-east-1.rgw.log",
      "intent_log_pool": "us-east-1.rgw.log:intent",
      "usage_log_pool": "us-east-1.rgw.log:usage",
      "reshard_pool": "us-east-1.rgw.log:reshard",
      "user_keys_pool": "us-east-1.rgw.meta:users.keys",
      "user_email_pool": "us-east-1.rgw.meta:users.email",
      "user_swift_pool": "us-east-1.rgw.meta:users.swift",
      "user_uid_pool": "us-east-1.rgw.meta:users.uid",
      "otp_pool": "us-east-1.rgw.otp",
      "system_key": {
          "access_key": "1555b35654ad1656d804",
          "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
      },
      "placement_pools": [
          {
              "key": "us-east-1-placement",
              "val": {
                  "index_pool": "us-east-1.rgw.buckets.index",
                  "storage_classes": {
                      "STANDARD": {
                          "data_pool": "us-east-1.rgw.buckets.data"
                      }
                  },
                  "data_extra_pool": "us-east-1.rgw.buckets.non-ec",
                  "index_type": 0
              }
          }
      ],
      "metadata_heap": "",
      "realm_id": ""
  }</pre></div></section><section class="sect3" id="ceph-rgw-fed-deldefzonegrp" data-id-title="Deleting the default zone and group"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.4 </span><span class="title-name">Deleting the default zone and group</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-deldefzonegrp">#</a></h4></div></div></div><div id="id-1.4.6.2.16.7.6.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following steps assume a multi-site configuration using newly
      installed systems that are not storing data yet. <span class="bold"><strong>Do
      not delete</strong></span> the default zone and its pools if you are already
      using it to store data, or the data will be deleted and unrecoverable.
     </p></div><p>
     The default installation of Object Gateway creates the default zonegroup called
     <code class="literal">default</code>. Delete the default zone if it exists. Make
     sure to remove it from the default zonegroup first.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup delete --rgw-zonegroup=default</pre></div><p>
     Delete the default pools in your Ceph storage cluster if they exist:
    </p><div id="id-1.4.6.2.16.7.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following step assumes a multi-site configuration using newly
      installed systems that are not currently storing data.
      <span class="bold"><strong>Do not delete</strong></span> the default zonegroup if
      you are already using it to store data.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it</pre></div><div id="id-1.4.6.2.16.7.6.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      If you delete the default zonegroup, you are also deleting the system
      user. If your admin user keys are not propagated, the Object Gateway management
      functionality of the Ceph Dashboard will fail. Follow on to the next section
      to re-create your system user if you go ahead with this step.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-createuser" data-id-title="Creating system users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.5 </span><span class="title-name">Creating system users</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-createuser">#</a></h4></div></div></div><p>
     The <code class="systemitem">ceph-radosgw</code> daemons must
     authenticate before pulling realm and period information. In the master
     zone, create a system user to simplify authentication between daemons:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> \
--secret=<em class="replaceable">SYSTEM_SECRET_KEY</em> --system</pre></div><p>
     Make a note of the <code class="option">access_key</code> and
     <code class="option">secret_key</code> as the secondary zones require them to
     authenticate with the master zone.
    </p><p>
     Add the system user to the master zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zone=us-east-1 \
--access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET</em></pre></div><p>
     Update the period to make the changes take effect:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-updateperiod" data-id-title="Update the period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.6 </span><span class="title-name">Update the period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-updateperiod">#</a></h4></div></div></div><p>
     After updating the master zone configuration, update the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div><p>
     After updating the period, <code class="command">radosgw-admin</code> returns the
     period configuration. For example:
    </p><div class="verbatim-wrap"><pre class="screen">{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</pre></div><div id="id-1.4.6.2.16.7.8.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Updating the period changes the epoch and ensures that other zones
      receive the updated configuration.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-masterzone-startrgw" data-id-title="Start the gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.3.7 </span><span class="title-name">Start the gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-masterzone-startrgw">#</a></h4></div></div></div><p>
     On the Object Gateway host, start and enable the Ceph Object Gateway service. To identify the
     unique FSID of the cluster, run <code class="command">ceph fsid</code>. To identify
     the Object Gateway daemon name, run <code class="command">ceph orch ps --hostname
     <em class="replaceable">HOSTNAME</em></code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@ogw &gt; </code>systemctl start ceph-<em class="replaceable">FSID</em>@<em class="replaceable">DAEMON_NAME</em>
<code class="prompt user">cephuser@ogw &gt; </code>systemctl enable ceph-<em class="replaceable">FSID</em>@<em class="replaceable">DAEMON_NAME</em></pre></div></section></section><section class="sect2" id="ceph-rgw-config-secondaryzone" data-id-title="Configure secondary zones"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.4 </span><span class="title-name">Configure secondary zones</span> <a title="Permalink" class="permalink" href="#ceph-rgw-config-secondaryzone">#</a></h3></div></div></div><p>
    Zones within a zonegroup replicate all data to ensure that each zone has the
    same data. When creating the secondary zone, execute all of the following
    operations on a host identified to serve the secondary zone.
   </p><div id="id-1.4.6.2.16.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     To add a third zone, follow the same procedures as for adding the
     secondary zone. Use different zone name.
    </p></div><div id="id-1.4.6.2.16.8.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     You must execute metadata operations, such as user creation, on a host
     within the master zone. The master zone and the secondary zone can receive
     bucket operations, but the secondary zone redirects bucket operations to
     the master zone. If the master zone is down, bucket operations will fail.
    </p></div><section class="sect3" id="ceph-rgw-pull-realm" data-id-title="Pulling the realm"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.1 </span><span class="title-name">Pulling the realm</span> <a title="Permalink" class="permalink" href="#ceph-rgw-pull-realm">#</a></h4></div></div></div><p>
     Using the URL path, access key, and secret of the master zone in the
     master zonegroup, pull the realm configuration to the host. To pull a
     non-default realm, specify the realm using the
     <code class="option">--rgw-realm</code> or <code class="option">--realm-id</code> configuration
     options.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm pull --url=<em class="replaceable">url-to-master-zone-gateway</em> --access-key=<em class="replaceable">access-key</em> --secret=<em class="replaceable">secret</em></pre></div><div id="id-1.4.6.2.16.8.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Pulling the realm also retrieves the remote's current period
      configuration, and makes it the current period on this host as well.
     </p></div><p>
     If this realm is the default realm or the only realm, make the realm the
     default realm.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm default --rgw-realm=<em class="replaceable">REALM-NAME</em></pre></div></section><section class="sect3" id="cceph-rgw-create-secondaryzone" data-id-title="Creating a secondary zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.2 </span><span class="title-name">Creating a secondary zone</span> <a title="Permalink" class="permalink" href="#cceph-rgw-create-secondaryzone">#</a></h4></div></div></div><p>
     Create a secondary zone for the multi-site configuration by opening a
     command line interface on a host identified to serve the secondary zone.
     Specify the zonegroup ID, the new zone name and an endpoint for the zone.
     <span class="emphasis"><em>Do not</em></span> use the <code class="option">--master</code> flag. All
     zones run in an active-active configuration by default. If the secondary
     zone should not accept write operations, specify the
     <code class="option">--read-only</code> flag to create an active-passive
     configuration between the master zone and the secondary zone.
     Additionally, provide the <code class="option">access_key</code> and
     <code class="option">secret_key</code> of the generated system user stored in the
     master zone of the master zonegroup. Execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=<em class="replaceable">ZONE-GROUP-NAME</em>\
 --rgw-zone=<em class="replaceable">ZONE-NAME</em> --endpoints=<em class="replaceable">URL</em> \
 --access-key=<em class="replaceable">SYSTEM-KEY</em> --secret=<em class="replaceable">SECRET</em>\
 --endpoints=http://<em class="replaceable">FQDN</em>:80 \
 [--read-only]</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<em class="replaceable">SYSTEM_ACCESS_KEY</em> --secret=<em class="replaceable">SYSTEM_SECRET_KEY</em>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</pre></div><div id="id-1.4.6.2.16.8.6.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      The following steps assume a multi-site configuration using
      newly-installed systems that are not yet storing data.
      <span class="bold"><strong>Do not delete</strong></span> the default zone
      and its pools if you are already using it to store data, or the data will
      be lost and unrecoverable.
     </p></div><p>
     Delete the default zone if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone delete --rgw-zone=default</pre></div><p>
     Delete the default pools in your Ceph storage cluster if needed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool rm default.rgw.users.uid default.rgw.users.uid --yes-i-really-really-mean-it</pre></div></section><section class="sect3" id="ceph-rgw-secondzone-update-config" data-id-title="Updating the Ceph configuration file"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.3 </span><span class="title-name">Updating the Ceph configuration file</span> <a title="Permalink" class="permalink" href="#ceph-rgw-secondzone-update-config">#</a></h4></div></div></div><p>
     Update the Ceph configuration file on the secondary zone hosts by adding
     the <code class="literal">rgw_zone</code> configuration option and the name of the
     secondary zone to the instance entry.
    </p><p>
     To do so, execute the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set <em class="replaceable">SERVICE_NAME</em> rgw_zone us-west</pre></div></section><section class="sect3" id="ceph-rgw-fed-secondzone-updateperiod" data-id-title="Updating the period"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.4 </span><span class="title-name">Updating the period</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-updateperiod">#</a></h4></div></div></div><p>
     After updating the master zone configuration, update the period:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</pre></div><div id="id-1.4.6.2.16.8.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Updating the period changes the epoch and ensures that other zones
      receive the updated configuration.
     </p></div></section><section class="sect3" id="ceph-rgw-fed-secondzone-startrgw" data-id-title="Starting the Object Gateway"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.5 </span><span class="title-name">Starting the Object Gateway</span> <a title="Permalink" class="permalink" href="#ceph-rgw-fed-secondzone-startrgw">#</a></h4></div></div></div><p>
     On the Object Gateway host, start and enable the Ceph Object Gateway service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch start rgw.us-east-2</pre></div></section><section class="sect3" id="ceph-rgw-check-sync-status" data-id-title="Checking the synchronization status"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.6 </span><span class="title-name">Checking the synchronization status</span> <a title="Permalink" class="permalink" href="#ceph-rgw-check-sync-status">#</a></h4></div></div></div><p>
     When the secondary zone is up and running, check the synchronization
     status. Synchronization copies users and buckets created in the master
     zone to the secondary zone.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin sync status</pre></div><p>
     The output provides the status of synchronization operations. For example:
    </p><div class="verbatim-wrap"><pre class="screen">realm f3239bc5-e1a8-4206-a81d-e1576480804d (gold)
    zonegroup c50dbb7e-d9ce-47cc-a8bb-97d9b399d388 (us)
         zone 4c453b70-4a16-4ce8-8185-1893b05d346e (us-west)
metadata sync syncing
              full sync: 0/64 shards
              metadata is caught up with master
              incremental sync: 64/64 shards
    data sync source: 1ee9da3e-114d-4ae3-a8a4-056e8a17f532 (us-east)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source</pre></div><div id="id-1.4.6.2.16.8.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Secondary zones accept bucket operations; however, secondary zones
      redirect bucket operations to the master zone and then synchronize with
      the master zone to receive the result of the bucket operations. If the
      master zone is down, bucket operations executed on the secondary zone
      will fail, but object operations should succeed.
     </p></div></section><section class="sect3" id="ceph-rgw-object-verification" data-id-title="Verification of an Object"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.4.7 </span><span class="title-name">Verification of an Object</span> <a title="Permalink" class="permalink" href="#ceph-rgw-object-verification">#</a></h4></div></div></div><p>
     By default, objects are not verified again after the synchronization of an
     object was successful. To enable verification, set the
     <code class="option">rgw_sync_obj_etag_verify</code> option to <code class="option">true</code>.
     After enabling, the optional objects will be synchronized. An additional
     MD5 checksum will verify that it is computed on the source and the
     destination. This is to ensure the integrity of the objects fetched from a
     remote server over HTTP including multisite sync. This option can decrease
     the performance of RGWs as more computation is needed.
    </p></section></section><section class="sect2" id="ceph-rgw-maintenance" data-id-title="General Object Gateway maintenance"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.5 </span><span class="title-name">General Object Gateway maintenance</span> <a title="Permalink" class="permalink" href="#ceph-rgw-maintenance">#</a></h3></div></div></div><section class="sect3" id="ceph-rgw-check-sync" data-id-title="Checking the synchronization status"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.5.1 </span><span class="title-name">Checking the synchronization status</span> <a title="Permalink" class="permalink" href="#ceph-rgw-check-sync">#</a></h4></div></div></div><p>
     Information about the replication status of a zone can be queried with:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin sync status
        realm b3bc1c37-9c44-4b89-a03b-04c269bea5da (gold)
    zonegroup f54f9b22-b4b6-4a0e-9211-fa6ac1693f49 (us)
         zone adce11c9-b8ed-4a90-8bc5-3fc029ff0816 (us-west)
        metadata sync syncing
              full sync: 0/64 shards
              incremental sync: 64/64 shards
              metadata is behind on 1 shards
              oldest incremental change not applied: 2017-03-22 10:20:00.0.881361s
data sync source: 341c2d81-4574-4d08-ab0f-5a2a7b168028 (us-east)
                  syncing
                  full sync: 0/128 shards
                  incremental sync: 128/128 shards
                  data is caught up with source
          source: 3b5d1a3f-3f27-4e4a-8f34-6072d4bb1275 (us-3)
                  syncing
                  full sync: 0/128 shards
                  incremental sync: 128/128 shards
                  data is caught up with source</pre></div><p>
     The output can differ depending on the sync status. The shards are
     described as two different types during sync:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.2.16.9.2.5.1"><span class="term">Behind shards</span></dt><dd><p>
        Behind shards are shards that need a full data synchronization and shards needing
        an incremental data synchronization because they are not up-to-date.
       </p></dd><dt id="id-1.4.6.2.16.9.2.5.2"><span class="term">Recovery shards</span></dt><dd><p>
        Recovery shards are shards that encountered an error during synchronization and
        marked for retry. The error mostly occurs on minor issues like
        acquiring a lock on a bucket. This will typically resolve itself.
       </p></dd></dl></div></section><section class="sect3" id="ceph-rgw-multisite-logs" data-id-title="Check the logs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.5.2 </span><span class="title-name">Check the logs</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multisite-logs">#</a></h4></div></div></div><p>
     For multi-site only, you can check out the metadata log
     (<code class="option">mdlog</code>), the bucket index log (<code class="option">bilog</code>)
     and the data log (<code class="option">datalog</code>). You can list them and also
     trim them. This is not needed in most cases as
     <code class="option">rgw_sync_log_trim_interval</code> option is set to 20 minutes as
     default. If it is not manually set to 0, you will not need to trim it at
     any time as it could cause side effects otherwise.
    </p></section><section class="sect3" id="ceph-rgw-metadata-master" data-id-title="Changing the metadata master zone"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.5.3 </span><span class="title-name">Changing the metadata master zone</span> <a title="Permalink" class="permalink" href="#ceph-rgw-metadata-master">#</a></h4></div></div></div><div id="id-1.4.6.2.16.9.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Be careful when changing which zone is the metadata master. If a zone has
      not finished synchronizing metadata from the current master zone, it is unable
      to serve any remaining entries when promoted to master and those changes
      will be lost. For this reason, we recommend waiting for a zone's
      <code class="command">radosgw-admin</code> synchronization status to catch up on
      metadata synchronization before promoting it to master. Similarly, if changes to
      metadata are being processed by the current master zone while another
      zone is being promoted to master, those changes are likely to be lost. To
      avoid this, we recommend shutting down any Object Gateway instances on the
      previous master zone. After promoting another zone, its new period can be
      fetched with <code class="command">radosgw-admin</code> period pull and the
      gateway(s) can be restarted.
     </p></div><p>
     To promote a zone (for example, zone <code class="literal">us-west</code> in
     zonegroup <code class="literal">us</code>) to metadata master, run the following
     commands on that zone:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@ogw &gt; </code>radosgw-admin zone modify --rgw-zone=us-west --master
<code class="prompt user">cephuser@ogw &gt; </code>radosgw-admin zonegroup modify --rgw-zonegroup=us --master
<code class="prompt user">cephuser@ogw &gt; </code>radosgw-admin period update --commit</pre></div><p>
     This generates a new period, and the Object Gateway instance(s) in zone
     <code class="literal">us-west</code> sends this period to other zones.
    </p></section><section class="sect3" id="ceph-rgw-multisite-bucket-reshard" data-id-title="Resharding a bucket index"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">21.13.5.4 </span><span class="title-name">Resharding a bucket index</span> <a title="Permalink" class="permalink" href="#ceph-rgw-multisite-bucket-reshard">#</a></h4></div></div></div><div id="id-1.4.6.2.16.9.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       Resharding a bucket index in a multi-site setup requires a full
       resynchronization of the bucket content. Depending on the size and number
       of objects in the bucket, this is a time- and resource-intensive operation.
       </p></div><div class="procedure" id="id-1.4.6.2.16.9.5.3" data-id-title="Resharding the bucket index"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 21.2: </span><span class="title-name">Resharding the bucket index </span><a title="Permalink" class="permalink" href="#id-1.4.6.2.16.9.5.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Make sure that all operations to the bucket are stopped.
         </p></li><li class="step"><p>
          Back up the original bucket index:
         </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bi list \
 --bucket=<em class="replaceable">BUCKET_NAME</em> \
 &gt; <em class="replaceable">BUCKET_NAME</em>.list.backup</pre></div></li><li class="step"><p>
        Disable bucket synchronization for the affected bucket:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket sync disable --bucket=<em class="replaceable">BUCKET_NAME</em></pre></div></li><li class="step"><p>
         Wait for the synchronization to finish on all zones. Check on master and
         slave zones with the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin sync status</pre></div></li><li class="step"><p>
         Stop the Object Gateway instances. First on all slave zones, then on the master
         zone, too.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@ogw &gt; </code>systemctl stop ceph-radosgw@rgw.<em class="replaceable">NODE</em>.service</pre></div></li><li class="step"><p>
       Reshard the bucket index on the master zone:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket reshard \
  --bucket=<em class="replaceable">BUCKET_NAME</em> \
  --num-shards=<em class="replaceable">NEW_SHARDS_NUMBER</em></pre></div><div id="id-1.4.6.2.16.9.5.3.7.3" data-id-title="Old bucket ID" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Old bucket ID</h6><p>
        As part of its output, this command also prints the new and the old
        bucket ID.
       </p></div></li><li class="step"><p>
        Purge the bucket on all slave zones:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket rm \
  --purge-objects \
  --bucket=<em class="replaceable">BUCKET_NAME</em> \
  --yes-i-really-mean-it</pre></div></li><li class="step"><p>
        Restart the Object Gateway on the master zone first, then on the slave zones as well.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@ogw &gt; </code>systemctl restart ceph-radosgw.target</pre></div></li><li class="step"><p>
        On the master zone, re-enable bucket synchronization.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin bucket sync enable --bucket=<em class="replaceable">BUCKET_NAME</em></pre></div></li></ol></div></div></section></section><section class="sect2" id="ceph-rgw-failover-dr" data-id-title="Performing failover and disaster recovery"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">21.13.6 </span><span class="title-name">Performing failover and disaster recovery</span> <a title="Permalink" class="permalink" href="#ceph-rgw-failover-dr">#</a></h3></div></div></div><p>
    If the master zone should fail, failover to the secondary zone for disaster
    recovery.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make the secondary zone the master and default zone. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default</pre></div><p>
      By default, Ceph Object Gateway runs in an active-active configuration. If the cluster
      was configured to run in an active-passive configuration, the secondary
      zone is a read-only zone. Remove the <code class="option">--read-only</code> status
      to allow the zone to receive write operations. For example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default \
                                                   --read-only=false</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart rgw</pre></div></li></ol></div></div><p>
    If the former master zone recovers, revert the operation.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      From the recovered zone, pull the latest realm configuration from the
      current master zone.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm pull --url=<em class="replaceable">URL-TO-MASTER-ZONE-GATEWAY</em> \
                           --access-key=<em class="replaceable">ACCESS-KEY</em> --secret=<em class="replaceable">SECRET</em></pre></div></li><li class="step"><p>
      Make the recovered zone the master and default zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --master --default</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway in the recovered zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart rgw@rgw</pre></div></li><li class="step"><p>
      If the secondary zone needs to be a read-only configuration, update the
      secondary zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify --rgw-zone=<em class="replaceable">ZONE-NAME</em> --read-only</pre></div></li><li class="step"><p>
      Update the period to make the changes take effect:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></li><li class="step"><p>
      Restart the Ceph Object Gateway in the secondary zone:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart@rgw</pre></div></li></ol></div></div></section></section></section><section class="chapter" id="cha-ceph-iscsi" data-id-title="Ceph iSCSI gateway"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">22 </span><span class="title-name">Ceph iSCSI gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-iscsi">#</a></h1></div></div></div><p>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.5 “Deploying iSCSI Gateways”</span>.
 </p><section class="sect1" id="ceph-iscsi-connect" data-id-title="ceph-iscsi managed targets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.1 </span><span class="title-name"><code class="systemitem">ceph-iscsi</code> managed targets</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect">#</a></h2></div></div></div><p>
   This chapter describes how to connect to <code class="systemitem">ceph-iscsi</code> managed targets from
   clients running Linux, Microsoft Windows, or VMware.
  </p><section class="sect2" id="ceph-iscsi-connect-linux" data-id-title="Connecting to open-iscsi"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.1.1 </span><span class="title-name">Connecting to <code class="systemitem">open-iscsi</code></span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux">#</a></h3></div></div></div><p>
    Connecting to <code class="systemitem">ceph-iscsi</code> backed iSCSI targets with
    <code class="systemitem">open-iscsi</code> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </p><p>
    Both steps require that the <code class="systemitem">open-iscsi</code> daemon is
    running. The way you start the <code class="systemitem">open-iscsi</code> daemon
    is dependent on your Linux distribution:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      On SUSE Linux Enterprise Server (SLES); and Red Hat Enterprise Linux (RHEL) hosts, run <code class="command">systemctl start
      iscsid</code> (or <code class="command">service iscsid start</code> if
      <code class="command">systemctl</code> is not available).
     </p></li><li class="listitem"><p>
      On Debian and Ubuntu hosts, run <code class="command">systemctl start
      open-iscsi</code> (or <code class="command">service open-iscsi start</code>).
     </p></li></ul></div><p>
    If your initiator host runs SUSE Linux Enterprise Server, refer to
    <a class="link" href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-iscsi.html#sec-iscsi-initiator" target="_blank">https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-iscsi.html#sec-iscsi-initiator</a>
    for details on how to connect to an iSCSI target.
   </p><p>
    For any other Linux distribution supporting
    <code class="systemitem">open-iscsi</code>, proceed to discover targets on your
    <code class="systemitem">ceph-iscsi</code> gateway (this example uses iscsi1.example.com as the portal
    address; for multipath access repeat these steps with iscsi2.example.com):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</pre></div><p>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    Repeat this process for other portal IP addresses or hosts.
   </p><p>
    If your system has the <code class="systemitem">lsscsi</code> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </p><div class="verbatim-wrap"><pre class="screen">lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</pre></div><p>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <code class="systemitem">multipath</code> utility:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</pre></div><p>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</pre></div><p>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </p><p>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol, portal: 192.168.124.104,3260] successful.</pre></div><p>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </p><section class="sect3" id="ceph-iscsi-connect-linux-multipath" data-id-title="Configuring multipath"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">22.1.1.1 </span><span class="title-name">Configuring multipath</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-linux-multipath">#</a></h4></div></div></div><p>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <code class="systemitem">ceph-iscsi</code> configuration. Select a strategy prior to
     using block storage. After editing the
     <code class="filename">/etc/multipath.conf</code>, restart
     <code class="systemitem">multipathd</code> with
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart multipathd</pre></div><p>
     For an active-passive configuration with friendly names, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. After connecting to your
     targets successfully, run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</pre></div><p>
     Note the status of each link. For an active-active configuration, add
    </p><div class="verbatim-wrap"><pre class="screen">defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</pre></div><p>
     to your <code class="filename">/etc/multipath.conf</code>. Restart
     <code class="systemitem">multipathd</code> and run
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</pre></div></section></section><section class="sect2" id="ceph-iscsi-connect-win" data-id-title="Connecting Microsoft Windows (Microsoft iSCSI initiator)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.1.2 </span><span class="title-name">Connecting Microsoft Windows (Microsoft iSCSI initiator)</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-win">#</a></h3></div></div></div><p>
    To connect to a SUSE Enterprise Storage iSCSI target from a Windows 2012 server,
    follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Open Windows Server Manager. From the Dashboard, select
      <span class="guimenu">Tools</span> › <span class="guimenu">iSCSI
      Initiator</span>. The <span class="guimenu">iSCSI Initiator
      Properties</span> dialog appears. Select the
      <span class="guimenu">Discovery</span> tab:
     </p><div class="figure" id="id-1.4.6.3.4.4.3.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-initiator-props.png"><img src="images/iscsi-initiator-props.png" width="70%" alt="iSCSI initiator properties" title="iSCSI initiator properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.1: </span><span class="title-name">iSCSI initiator properties </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.1.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">Discover Target Portal</span> dialog, enter the
      target's host name or IP address in the <span class="guimenu">Target</span> field
      and click <span class="guimenu">OK</span>:
     </p><div class="figure" id="id-1.4.6.3.4.4.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip.png"><img src="images/iscsi-target-ip.png" width="70%" alt="Discover target portal" title="Discover target portal"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.2: </span><span class="title-name">Discover target portal </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <span class="guimenu">Target Portals</span> list:
     </p><div class="figure" id="id-1.4.6.3.4.4.3.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-ip-list.png"><img src="images/iscsi-target-ip-list.png" width="70%" alt="Target portals" title="Target portals"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.3: </span><span class="title-name">Target portals </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.3.2">#</a></h6></div></div></li><li class="step"><p>
      Next, switch to the <span class="guimenu">Targets</span> tab and review your
      discovered target(s).
     </p><div class="figure" id="id-1.4.6.3.4.4.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-targets.png"><img src="images/iscsi-targets.png" width="70%" alt="Targets" title="Targets"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.4: </span><span class="title-name">Targets </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.4.2">#</a></h6></div></div></li><li class="step"><p>
      Click <span class="guimenu">Connect</span> in the <span class="guimenu">Targets</span> tab.
      The <span class="guimenu">Connect To Target</span> dialog appears. Select the
      <span class="guimenu">Enable Multi-path</span> check box to enable multipath I/O
      (MPIO), then click <span class="guimenu">OK</span>:
     </p></li><li class="step"><p>
      When the <span class="guimenu">Connect to Target</span> dialog closes, select
      <span class="guimenu">Properties</span> to review the target's properties:
     </p><div class="figure" id="id-1.4.6.3.4.4.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-target-properties.png"><img src="images/iscsi-target-properties.png" width="70%" alt="iSCSI target properties" title="iSCSI target properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.5: </span><span class="title-name">iSCSI target properties </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.6.2">#</a></h6></div></div></li><li class="step"><p>
      Select <span class="guimenu">Devices</span>, and click <span class="guimenu">MPIO</span> to
      review the multipath I/O configuration:
     </p><div class="figure" id="id-1.4.6.3.4.4.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-device-details.png"><img src="images/iscsi-device-details.png" width="70%" alt="Device details" title="Device details"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.6: </span><span class="title-name">Device details </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.3.7.2">#</a></h6></div></div><p>
      The default <span class="guimenu">Load Balance policy</span> is <span class="guimenu">Round
      Robin With Subset</span>. If you prefer a pure failover configuration,
      change it to <span class="guimenu">Fail Over Only</span>.
     </p></li></ol></div></div><p>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <span class="guimenu">OK</span> to close the <span class="guimenu">iSCSI
    Initiator Properties</span> dialog, and proceed with the<span class="guimenu"> File
    and Storage Services</span> role from the <span class="guimenu">Server
    Manager</span> dashboard.
   </p><p>
    Observe the newly connected volume. It identifies as <span class="emphasis"><em>SUSE RBD
    SCSI Multi-Path Drive</em></span> on the iSCSI bus, and is initially marked
    with an <span class="emphasis"><em>Offline</em></span> status and a partition table type of
    <span class="emphasis"><em>Unknown</em></span>. If the new volume does not appear
    immediately, select <span class="guimenu">Rescan Storage</span> from the
    <span class="guimenu">Tasks</span> drop-down box to rescan the iSCSI bus.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Right-click on the iSCSI volume and select <span class="guimenu">New Volume</span>
      from the context menu. The <span class="guimenu">New Volume Wizard</span> appears.
      Click <span class="guimenu">Next</span>, highlight the newly connected iSCSI volume
      and click <span class="guimenu">Next</span> to begin.
     </p><div class="figure" id="id-1.4.6.3.4.4.6.1.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-wizard.png"><img src="images/iscsi-volume-wizard.png" width="70%" alt="New volume wizard" title="New volume wizard"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.7: </span><span class="title-name">New volume wizard </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.6.1.2">#</a></h6></div></div></li><li class="step"><p>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </p><div class="figure" id="id-1.4.6.3.4.4.6.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-win-prompt1.png"><img src="images/iscsi-win-prompt1.png" width="70%" alt="Offline disk prompt" title="Offline disk prompt"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.8: </span><span class="title-name">Offline disk prompt </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.6.2.2">#</a></h6></div></div></li><li class="step"><p>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or directory name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <span class="guimenu">Create</span> to finish creating the volume:
     </p><div class="figure" id="id-1.4.6.3.4.4.6.3.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-volume-confirm.png"><img src="images/iscsi-volume-confirm.png" width="70%" alt="Confirm volume selections" title="Confirm volume selections"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.9: </span><span class="title-name">Confirm volume selections </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.4.6.3.2">#</a></h6></div></div><p>
      When the process finishes, review the results, then
      <span class="guimenu">Close</span> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-iscsi-connect-vmware" data-id-title="Connecting VMware"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">22.1.3 </span><span class="title-name">Connecting VMware</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-connect-vmware">#</a></h3></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      To connect to <code class="systemitem">ceph-iscsi</code> managed iSCSI volumes you need a configured iSCSI
      software adapter. If no such adapter is available in your vSphere
      configuration, create one by selecting
      <span class="guimenu">Configuration</span> › <span class="guimenu">Storage
      Adapters</span> › <span class="guimenu">Add</span> › <span class="guimenu">iSCSI Software
      initiator</span>.
     </p></li><li class="step"><p>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <span class="guimenu">Properties</span> from the context
      menu:
     </p><div class="figure" id="id-1.4.6.3.4.5.3.2.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi_vmware_adapter_props.png"><img src="images/iscsi_vmware_adapter_props.png" width="70%" alt="iSCSI initiator properties" title="iSCSI initiator properties"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.10: </span><span class="title-name">iSCSI initiator properties </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.2.2">#</a></h6></div></div></li><li class="step"><p>
      In the <span class="guimenu">iSCSI Software Initiator</span> dialog, click the
      <span class="guimenu">Configure</span> button. Then go to the <span class="guimenu">Dynamic
      Discovery</span> tab and select <span class="guimenu">Add</span>.
     </p></li><li class="step"><p>
      Enter the IP address or host name of your <code class="systemitem">ceph-iscsi</code> iSCSI gateway. If you
      run multiple iSCSI gateways in a failover configuration, repeat this step
      for as many gateways as you operate.
     </p><div class="figure" id="id-1.4.6.3.4.5.3.4.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-target.png"><img src="images/iscsi-vmware-add-target.png" width="70%" alt="Add target server" title="Add target server"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.11: </span><span class="title-name">Add target server </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.4.2">#</a></h6></div></div><p>
      When you have entered all iSCSI gateways, click <span class="guimenu">OK</span> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </p></li><li class="step"><p>
      When the rescan completes, the new iSCSI device appears below the
      <span class="guimenu">Storage Adapters</span> list in the
      <span class="guimenu">Details</span> pane. For multipath devices, you can now
      right-click on the adapter and select <span class="guimenu">Manage Paths</span>
      from the context menu:
     </p><div class="figure" id="id-1.4.6.3.4.5.3.5.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-multipath.png"><img src="images/iscsi-vmware-multipath.png" width="70%" alt="Manage multipath devices" title="Manage multipath devices"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.12: </span><span class="title-name">Manage multipath devices </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.5.2">#</a></h6></div></div><p>
      You should now see all paths with a green light under
      <span class="guimenu">Status</span>. One of your paths should be marked
      <span class="guimenu">Active (I/O)</span> and all others simply
      <span class="guimenu">Active</span>:
     </p><div class="figure" id="id-1.4.6.3.4.5.3.5.4"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-paths.png"><img src="images/iscsi-vmware-paths.png" width="70%" alt="Paths listing for multipath" title="Paths listing for multipath"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.13: </span><span class="title-name">Paths listing for multipath </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.5.4">#</a></h6></div></div></li><li class="step"><p>
      You can now switch from <span class="guimenu">Storage Adapters</span> to the item
      labeled <span class="guimenu">Storage</span>. Select <span class="guimenu">Add
      Storage...</span> in the top-right corner of the pane to bring up the
      <span class="guimenu">Add Storage</span> dialog. Then, select
      <span class="guimenu">Disk/LUN</span> and click <span class="guimenu">Next</span>. The newly
      added iSCSI device appears in the <span class="guimenu">Select Disk/LUN</span>
      list. Select it, then click <span class="guimenu">Next</span> to proceed:
     </p><div class="figure" id="id-1.4.6.3.4.5.3.6.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-add-storage-dialog.png"><img src="images/iscsi-vmware-add-storage-dialog.png" width="70%" alt="Add storage dialog" title="Add storage dialog"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.14: </span><span class="title-name">Add storage dialog </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.6.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Next</span> to accept the default disk layout.
     </p></li><li class="step"><p>
      In the <span class="guimenu">Properties</span> pane, assign a name to the new
      datastore, and click <span class="guimenu">Next</span>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <span class="guimenu">Custom Space Setting</span> for a smaller datastore:
     </p><div class="figure" id="id-1.4.6.3.4.5.3.7.2"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-custom-datastore.png"><img src="images/iscsi-vmware-custom-datastore.png" width="70%" alt="Custom space setting" title="Custom space setting"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.15: </span><span class="title-name">Custom space setting </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.7.2">#</a></h6></div></div><p>
      Click <span class="guimenu">Finish</span> to complete the datastore creation.
     </p><p>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the <code class="systemitem">ceph-iscsi</code> backed iSCSI
      volume like any other vSphere datastore.
     </p><div class="figure" id="id-1.4.6.3.4.5.3.7.5"><div class="figure-contents"><div class="mediaobject"><a href="images/iscsi-vmware-overview.png"><img src="images/iscsi-vmware-overview.png" width="70%" alt="iSCSI datastore overview" title="iSCSI datastore overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 22.16: </span><span class="title-name">iSCSI datastore overview </span><a title="Permalink" class="permalink" href="#id-1.4.6.3.4.5.3.7.5">#</a></h6></div></div></li></ol></div></div></section></section><section class="sect1" id="ceph-iscsi-conclude" data-id-title="Conclusion"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">22.2 </span><span class="title-name">Conclusion</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-conclude">#</a></h2></div></div></div><p>
   <code class="systemitem">ceph-iscsi</code> is a key component of SUSE Enterprise Storage 7.1 that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using <code class="systemitem">ceph-iscsi</code> on one or
   more iSCSI gateway hosts, Ceph RBD images become available as Logical
   Units (LUs) associated with iSCSI targets, which can be accessed in an
   optionally load-balanced, highly available fashion.
  </p><p>
   Since all of <code class="systemitem">ceph-iscsi</code> configuration is stored in the Ceph RADOS object
   store, <code class="systemitem">ceph-iscsi</code> gateway hosts are inherently without persistent state and
   thus can be replaced, augmented, or reduced at will. As a result,
   SUSE Enterprise Storage 7.1 enables SUSE customers to run a truly
   distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open source
   platform.
  </p></section></section><section class="chapter" id="cha-ceph-cephfs" data-id-title="Clustered file system"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">23 </span><span class="title-name">Clustered file system</span> <a title="Permalink" class="permalink" href="#cha-ceph-cephfs">#</a></h1></div></div></div><p>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and CephFS exported. If you need more information on
  setting up CephFS, refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.3 “Deploying Metadata Servers”</span>.
 </p><section class="sect1" id="ceph-cephfs-cephfs-mount" data-id-title="Mounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.1 </span><span class="title-name">Mounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-mount">#</a></h2></div></div></div><p>
   When the file system is created and the MDS is active, you are ready to
   mount the file system from a client host.
  </p><section class="sect2" id="cephfs-client-preparation" data-id-title="Preparing the client"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.1 </span><span class="title-name">Preparing the client</span> <a title="Permalink" class="permalink" href="#cephfs-client-preparation">#</a></h3></div></div></div><p>
    If the client host is running SUSE Linux Enterprise 12 SP2 or later, the system is ready to
    mount CephFS 'out of the box'.
   </p><p>
    If the client host is running SUSE Linux Enterprise 12 SP1, you need to apply all the
    latest patches before mounting CephFS.
   </p><p>
    In any case, everything needed to mount CephFS is included in SUSE Linux Enterprise. The
    SUSE Enterprise Storage 7.1 product is not needed.
   </p><p>
    To support the full <code class="command">mount</code> syntax, the
    <span class="package">ceph-common</span> package (which is shipped with SUSE Linux Enterprise) should
    be installed before trying to mount CephFS.
   </p><div id="id-1.4.6.4.4.3.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Without the <span class="package">ceph-common</span> package (and thus without the
     <code class="command">mount.ceph</code> helper), the monitors' IPs will need to be
     used instead of their names. This is because the kernel client will be
     unable to perform name resolution.
    </p><p>
     The basic mount syntax is:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph <em class="replaceable">MON1_IP</em>[:<em class="replaceable">PORT</em>],<em class="replaceable">MON2_IP</em>[:<em class="replaceable">PORT</em>],...:<em class="replaceable">CEPHFS_MOUNT_TARGET</em> \
<em class="replaceable">MOUNT_POINT</em> -o name=<em class="replaceable">CEPHX_USER_NAME</em>,secret=<em class="replaceable">SECRET_STRING</em></pre></div></div></section><section class="sect2" id="Creating-Secret-File" data-id-title="Creating a secret file"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.2 </span><span class="title-name">Creating a secret file</span> <a title="Permalink" class="permalink" href="#Creating-Secret-File">#</a></h3></div></div></div><p>
    The Ceph cluster runs with authentication turned on by default. You
    should create a file that stores your secret key (not the keyring itself).
    To obtain the secret key for a particular user and then create the file, do
    the following:
   </p><div class="procedure" id="id-1.4.6.4.4.4.3" data-id-title="Creating a secret key"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 23.1: </span><span class="title-name">Creating a secret key </span><a title="Permalink" class="permalink" href="#id-1.4.6.4.4.4.3">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      View the key for the particular user in a keyring file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat /etc/ceph/ceph.client.admin.keyring</pre></div></li><li class="step"><p>
      Copy the key of the user who will be using the mounted Ceph FS file
      system. Usually, the key looks similar to the following:
     </p><div class="verbatim-wrap"><pre class="screen">AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</pre></div></li><li class="step"><p>
      Create a file with the user name as a file name part, for example
      <code class="filename">/etc/ceph/admin.secret</code> for the user
      <span class="emphasis"><em>admin</em></span>.
     </p></li><li class="step"><p>
      Paste the key value to the file created in the previous step.
     </p></li><li class="step"><p>
      Set proper access rights to the file. The user should be the only one who
      can read the file—others may not have any access rights.
     </p></li></ol></div></div></section><section class="sect2" id="ceph-cephfs-krnldrv" data-id-title="Mounting CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.1.3 </span><span class="title-name">Mounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-krnldrv">#</a></h3></div></div></div><p>
    You can mount CephFS with the <code class="command">mount</code> command. You need
    to specify the monitor host name or IP address. Because the
    <code class="systemitem">cephx</code> authentication is enabled by default in
    SUSE Enterprise Storage, you need to specify a user name and their related secret as
    well:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><p>
    As the previous command remains in the shell history, a more secure
    approach is to read the secret from a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    Note that the secret file should only contain the actual keyring secret. In
    our example, the file will then contain only the following line:
   </p><div class="verbatim-wrap"><pre class="screen">AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</pre></div><div id="id-1.4.6.4.4.5.8" data-id-title="Specify multiple monitors" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Specify multiple monitors</h6><p>
     It is a good idea to specify multiple monitors separated by commas on the
     <code class="command">mount</code> command line in case one monitor happens to be
     down at the time of mount. Each monitor address takes the form
     <code class="literal">host[:port]</code>. If the port is not specified, it defaults
     to 6789.
    </p></div><p>
    Create the mount point on the local host:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir /mnt/cephfs</pre></div><p>
    Mount the CephFS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    A subdirectory <code class="filename">subdir</code> may be specified if a subset of
    the file system is to be mounted:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><p>
    You can specify more than one monitor host in the <code class="command">mount</code>
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</pre></div><div id="id-1.4.6.4.4.5.17" data-id-title="Read access to the root directory" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Read access to the root directory</h6><p>
     If clients with path restriction are used, the MDS capabilities need to
     include read access to the root directory. For example, a keyring may look
     as follows:
    </p><div class="verbatim-wrap"><pre class="screen">client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</pre></div><p>
     The <code class="literal">allow r path=/</code> part means that path-restricted
     clients are able to see the root volume, but cannot write to it. This may
     be an issue for use cases where complete isolation is a requirement.
    </p></div></section></section><section class="sect1" id="ceph-cephfs-cephfs-unmount" data-id-title="Unmounting CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.2 </span><span class="title-name">Unmounting CephFS</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-unmount">#</a></h2></div></div></div><p>
   To unmount the CephFS, use the <code class="command">umount</code> command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>umount /mnt/cephfs</pre></div></section><section class="sect1" id="ceph-cephfs-cephfs-fstab" data-id-title="Mounting CephFS in /etc/fstab"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.3 </span><span class="title-name">Mounting CephFS in <code class="filename">/etc/fstab</code></span> <a title="Permalink" class="permalink" href="#ceph-cephfs-cephfs-fstab">#</a></h2></div></div></div><p>
   To mount CephFS automatically upon client start-up, insert the
   corresponding line in its file systems table
   <code class="filename">/etc/fstab</code>:
  </p><div class="verbatim-wrap"><pre class="screen">mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</pre></div></section><section class="sect1" id="ceph-cephfs-activeactive" data-id-title="Multiple active MDS daemons (active-active MDS)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.4 </span><span class="title-name">Multiple active MDS daemons (active-active MDS)</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-activeactive">#</a></h2></div></div></div><p>
   CephFS is configured for a single active MDS daemon by default. To scale
   metadata performance for large-scale systems, you can enable multiple active
   MDS daemons, which will share the metadata workload with one another.
  </p><section class="sect2" id="using-active-active-mds" data-id-title="Using active-active MDS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.1 </span><span class="title-name">Using active-active MDS</span> <a title="Permalink" class="permalink" href="#using-active-active-mds">#</a></h3></div></div></div><p>
    Consider using multiple active MDS daemons when your metadata performance
    is bottlenecked on the default single MDS.
   </p><p>
    Adding more daemons does not increase performance on all workload types.
    For example, a single application running on a single client will not
    benefit from an increased number of MDS daemons unless the application is
    doing a lot of metadata operations in parallel.
   </p><p>
    Workloads that typically benefit from a larger number of active MDS daemons
    are those with many clients, perhaps working on many separate directories.
   </p></section><section class="sect2" id="cephfs-activeactive-increase" data-id-title="Increasing the MDS active cluster size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.2 </span><span class="title-name">Increasing the MDS active cluster size</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-increase">#</a></h3></div></div></div><p>
    Each CephFS file system has a <code class="option">max_mds</code> setting, which
    controls how many ranks will be created. The actual number of ranks in the
    file system will only be increased if a spare daemon is available to take
    on the new rank. For example, if there is only one MDS daemon running and
    <code class="option">max_mds</code> is set to two, no second rank will be created.
   </p><p>
    In the following example, we set the <code class="option">max_mds</code> option to 2
    to create a new rank apart from the default one. To see the changes, run
    <code class="command">ceph status</code> before and after you set
    <code class="option">max_mds</code>, and watch the line containing
    <code class="literal">fsmap</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 2
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</pre></div><p>
    The newly created rank (1) passes through the 'creating' state and then
    enter its 'active' state.
   </p><div id="id-1.4.6.4.7.4.6" data-id-title="Standby daemons" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Standby daemons</h6><p>
     Even with multiple active MDS daemons, a highly available system still
     requires standby daemons to take over if any of the servers running an
     active daemon fail.
    </p><p>
     Consequently, the practical maximum of <code class="option">max_mds</code> for highly
     available systems is one less than the total number of MDS servers in your
     system. To remain available in the event of multiple server failures,
     increase the number of standby daemons in the system to match the number
     of server failures you need to survive.
    </p></div></section><section class="sect2" id="cephfs-activeactive-decrease" data-id-title="Decreasing the number of ranks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.3 </span><span class="title-name">Decreasing the number of ranks</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-decrease">#</a></h3></div></div></div><p>
    All ranks—including the ranks to be removed—must first be
    active. This means that you need to have at least <code class="option">max_mds</code>
    MDS daemons available.
   </p><p>
    First, set <code class="option">max_mds</code> to a lower number. For example, go back
    to having a single active MDS:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> fs set cephfs max_mds 1
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</pre></div></section><section class="sect2" id="cephfs-activeactive-pinning" data-id-title="Manually pinning directory trees to a rank"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.4.4 </span><span class="title-name">Manually pinning directory trees to a rank</span> <a title="Permalink" class="permalink" href="#cephfs-activeactive-pinning">#</a></h3></div></div></div><p>
    In multiple active metadata server configurations, a balancer runs, which
    works to spread metadata load evenly across the cluster. This usually works
    well enough for most users, but sometimes it is desirable to override the
    dynamic balancer with explicit mappings of metadata to particular ranks.
    This can allow the administrator or users to evenly spread application load
    or limit impact of users' metadata requests on the entire cluster.
   </p><p>
    The mechanism provided for this purpose is called an 'export pin'. It is an
    extended attribute of directories. The name of this extended attribute is
    <code class="literal">ceph.dir.pin</code>. Users can set this attribute using
    standard commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>setfattr -n ceph.dir.pin -v 2 <em class="replaceable">/path/to/dir</em></pre></div><p>
    The value (<code class="option">-v</code>) of the extended attribute is the rank to
    assign the directory sub-tree to. A default value of -1 indicates that the
    directory is not pinned.
   </p><p>
    A directory export pin is inherited from its closest parent with a set
    export pin. Therefore, setting the export pin on a directory affects all of
    its children. However, the parent's pin can be overridden by setting the
    child directory export pin. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</pre></div></section></section><section class="sect1" id="ceph-cephfs-failover" data-id-title="Managing failover"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.5 </span><span class="title-name">Managing failover</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover">#</a></h2></div></div></div><p>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <code class="option">mds_beacon_grace</code> seconds (default 15 seconds) before
   marking the daemon as <span class="emphasis"><em>laggy</em></span>. You can configure one or
   more 'standby' daemons that will take over during the MDS daemon failover.
  </p><section class="sect2" id="ceph-cephfs-failover-standby" data-id-title="Configuring standby replay"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.5.1 </span><span class="title-name">Configuring standby replay</span> <a title="Permalink" class="permalink" href="#ceph-cephfs-failover-standby">#</a></h3></div></div></div><p>
    Each CephFS file system may be configured to add standby-replay daemons.
    These standby daemons follow the active MDS's metadata journal to reduce
    failover time in the event that the active MDS becomes unavailable. Each
    active MDS may have only one standby-replay daemon following it.
   </p><p>
    Configure standby-replay on a file system with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph fs set <em class="replaceable">FS-NAME</em> allow_standby_replay <em class="replaceable">BOOL</em></pre></div><p>
    When set the monitors will assign available standby daemons to follow the
    active MDSs in that file system.
   </p><p>
    When an MDS has entered the standby-replay state, it will only be used as a
    standby for the rank that it is following. If another rank fails, this
    standby-replay daemon will not be used as a replacement, even if no other
    standbys are available. For this reason, it is advised that if
    standby-replay is used then every active MDS should have a standby-replay
    daemon.
   </p></section></section><section class="sect1" id="cephfs-quotas" data-id-title="Setting CephFS quotas"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.6 </span><span class="title-name">Setting CephFS quotas</span> <a title="Permalink" class="permalink" href="#cephfs-quotas">#</a></h2></div></div></div><p>
   You can set quotas on any subdirectory of the Ceph file system. The quota
   restricts either the number of <span class="bold"><strong>bytes</strong></span> or
   <span class="bold"><strong>files</strong></span> stored beneath the specified point in
   the directory hierarchy.
  </p><section class="sect2" id="cephfs-quotas-limitation" data-id-title="CephFS quota limitations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.6.1 </span><span class="title-name">CephFS quota limitations</span> <a title="Permalink" class="permalink" href="#cephfs-quotas-limitation">#</a></h3></div></div></div><p>
    Using quotas with CephFS has the following limitations:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.4.9.3.3.1"><span class="term">Quotas are cooperative and non-competing.</span></dt><dd><p>
       Ceph quotas rely on the client that is mounting the file system to
       stop writing to it when a limit is reached. The server part cannot
       prevent a malicious client from writing as much data as it needs. Do not
       use quotas to prevent filling the file system in environments where the
       clients are fully untrusted.
      </p></dd><dt id="id-1.4.6.4.9.3.3.2"><span class="term">Quotas are imprecise.</span></dt><dd><p>
       Processes that are writing to the file system will be stopped shortly
       after the quota limit is reached. They will inevitably be allowed to
       write some amount of data over the configured limit. Client writers will
       be stopped within tenths of seconds after crossing the configured limit.
      </p></dd><dt id="id-1.4.6.4.9.3.3.3"><span class="term">Quotas are implemented in the kernel client from version 4.17.</span></dt><dd><p>
       Quotas are supported by the user space client (libcephfs, ceph-fuse).
       Linux kernel clients 4.17 and higher support CephFS quotas on
       SUSE Enterprise Storage 7.1 clusters. Kernel clients (even recent
       versions) will fail to handle quotas on older clusters, even if they are
       able to set the quotas extended attributes. SLE12-SP3 (and later)
       kernels already include the required backports to handle quotas.
      </p></dd><dt id="id-1.4.6.4.9.3.3.4"><span class="term">Configure quotas carefully when used with path-based mount restrictions.</span></dt><dd><p>
       The client needs to have access to the directory inode on which quotas
       are configured in order to enforce them. If the client has restricted
       access to a specific path (for example <code class="filename">/home/user</code>)
       based on the MDS capability, and a quota is configured on an ancestor
       directory they do not have access to (<code class="filename">/home</code>), the
       client will not enforce it. When using path-based access restrictions,
       be sure to configure the quota on the directory that the client can
       access (for example <code class="filename">/home/user</code> or
       <code class="filename">/home/user/quota_dir</code>).
      </p></dd></dl></div></section><section class="sect2" id="cephfs-quotas-config" data-id-title="Configuring CephFS quotas"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.6.2 </span><span class="title-name">Configuring CephFS quotas</span> <a title="Permalink" class="permalink" href="#cephfs-quotas-config">#</a></h3></div></div></div><p>
    You can configure CephFS quotas by using virtual extended attributes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.6.4.9.4.3.1"><span class="term"><code class="option">ceph.quota.max_files</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>file</em></span> limit.
      </p></dd><dt id="id-1.4.6.4.9.4.3.2"><span class="term"><code class="option">ceph.quota.max_bytes</code></span></dt><dd><p>
       Configures a <span class="emphasis"><em>byte</em></span> limit.
      </p></dd></dl></div><p>
    If the attributes appear on a directory inode, a quota is configured there.
    If they are not present then no quota is set on that directory (although
    one may still be configured on a parent directory).
   </p><p>
    To set a 100 MB quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 100000000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To set a 10,000 files quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mds &gt; </code>setfattr -n ceph.quota.max_files -v 10000 <em class="replaceable">/SOME/DIRECTORY</em></pre></div><p>
    To view quota setting, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mds &gt; </code>getfattr -n ceph.quota.max_bytes <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mds &gt; </code>getfattr -n ceph.quota.max_files <em class="replaceable">/SOME/DIRECTORY</em></pre></div><div id="id-1.4.6.4.9.4.12" data-id-title="Quota not set" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Quota not set</h6><p>
     If the value of the extended attribute is '0', the quota is not set.
    </p></div><p>
    To remove a quota, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mds &gt; </code>setfattr -n ceph.quota.max_bytes -v 0 <em class="replaceable">/SOME/DIRECTORY</em>
<code class="prompt user">cephuser@mds &gt; </code>setfattr -n ceph.quota.max_files -v 0 <em class="replaceable">/SOME/DIRECTORY</em></pre></div></section></section><section class="sect1" id="cephfs-snapshots" data-id-title="Managing CephFS snapshots"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">23.7 </span><span class="title-name">Managing CephFS snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots">#</a></h2></div></div></div><p>
   CephFS snapshots create a read-only view of the file system at the point
   in time they are taken. You can create a snapshot in any directory. The
   snapshot will cover all data in the file system under the specified
   directory. After creating a snapshot, the buffered data is flushed out
   asynchronously from various clients. As a result, creating a snapshot is
   very fast.
  </p><div id="id-1.4.6.4.10.3" data-id-title="Multiple file systems" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Multiple file systems</h6><p>
    If you have multiple CephFS file systems sharing a single pool (via name
    spaces), their snapshots will collide, and deleting one snapshot will
    result in missing file data for other snapshots sharing the same pool.
   </p></div><section class="sect2" id="cephfs-snapshots-create" data-id-title="Creating snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.7.1 </span><span class="title-name">Creating snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots-create">#</a></h3></div></div></div><p>
    The CephFS snapshot feature is enabled by default on new file systems. To
    enable it on existing file systems, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph fs set <em class="replaceable">CEPHFS_NAME</em> allow_new_snaps true</pre></div><p>
    After you enable snapshots, all directories in the CephFS will have a
    special <code class="filename">.snap</code> subdirectory.
   </p><div id="id-1.4.6.4.10.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This is a <span class="emphasis"><em>virtual</em></span> subdirectory. It does not appear in
     the directory listing of the parent directory, but the name
     <code class="filename">.snap</code> cannot be used as a file or directory name. To
     access the <code class="filename">.snap</code> directory one needs to explicitly
     access it, for example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ls -la /<em class="replaceable">CEPHFS_MOUNT</em>/.snap/</pre></div></div><div id="id-1.4.6.4.10.4.6" data-id-title="Kernel clients limitation" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Kernel clients limitation</h6><p>
     CephFS kernel clients have a limitation: they cannot handle more than
     400 snapshots in a file system. The number of snapshots should always be
     kept below this limit, regardless of which client you are using. If using
     older CephFS clients, such as SLE12-SP3, keep in mind that going above
     400 snapshots is harmful to operations as the client will crash.
    </p></div><div id="id-1.4.6.4.10.4.7" data-id-title="Custom snapshot subdirectory name" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Custom snapshot subdirectory name</h6><p>
     You may configure a different name for the snapshots subdirectory by
     setting the <code class="option">client snapdir</code> setting.
    </p></div><p>
    To create a snapshot, create a subdirectory under the
    <code class="filename">.snap</code> directory with a custom name. For example, to
    create a snapshot of the directory
    <code class="filename">/<em class="replaceable">CEPHFS_MOUNT</em>/2/3/</code>, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>mkdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section><section class="sect2" id="cephfs-snapshots-delete" data-id-title="Deleting snapshots"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">23.7.2 </span><span class="title-name">Deleting snapshots</span> <a title="Permalink" class="permalink" href="#cephfs-snapshots-delete">#</a></h3></div></div></div><p>
    To delete a snapshot, remove its subdirectory inside the
    <code class="filename">.snap</code> directory:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>rmdir /<em class="replaceable">CEPHFS_MOUNT</em>/2/3/.snap/<em class="replaceable">CUSTOM_SNAPSHOT_NAME</em></pre></div></section></section></section><section class="chapter" id="cha-ses-cifs" data-id-title="Export Ceph data via Samba"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">24 </span><span class="title-name">Export Ceph data via Samba</span> <a title="Permalink" class="permalink" href="#cha-ses-cifs">#</a></h1></div></div></div><p>
  This chapter describes how to export data stored in a Ceph cluster via a
  Samba/CIFS share so that you can easily access them from Windows* client
  machines. It also includes information that will help you configure a Ceph
  Samba gateway to join Active Directory in the Windows* domain to authenticate and
  authorize users.
 </p><div id="id-1.4.6.5.4" data-id-title="Samba gateway performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Samba gateway performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing CephFS
   via a Samba Gateway may significantly reduce application performance when
   compared to native Ceph clients.
  </p></div><section class="sect1" id="cephfs-samba" data-id-title="Export CephFS via Samba share"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.1 </span><span class="title-name">Export CephFS via Samba share</span> <a title="Permalink" class="permalink" href="#cephfs-samba">#</a></h2></div></div></div><div id="id-1.4.6.5.5.2" data-id-title="Cross protocol access" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Cross protocol access</h6><p>
    Native CephFS and NFS clients are not restricted by file locks obtained
    via Samba, and vice versa. Applications that rely on cross protocol file
    locking may experience data corruption if CephFS backed Samba share paths
    are accessed via other means.
   </p></div><section class="sect2" id="cephfs-samba-packages" data-id-title="Configuring and exporting Samba packages"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.1.1 </span><span class="title-name">Configuring and exporting Samba packages</span> <a title="Permalink" class="permalink" href="#cephfs-samba-packages">#</a></h3></div></div></div><p>
    To configure and export a Samba share, the following packages need to be
    installed: <span class="package">samba-ceph</span> and
    <span class="package">samba-winbind</span>. If these packages are not installed,
    install them:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>zypper install samba-ceph samba-winbind</pre></div></section><section class="sect2" id="sec-ses-cifs-example" data-id-title="Single gateway example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.1.2 </span><span class="title-name">Single gateway example</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-example">#</a></h3></div></div></div><p>
    In preparation for exporting a Samba share, choose an appropriate node to
    act as a Samba Gateway. The node needs to have access to the Ceph client network,
    as well as sufficient CPU, memory, and networking resources.
   </p><p>
    Failover functionality can be provided with CTDB and the SUSE Linux Enterprise High Availability Extension.
    Refer to <a class="xref" href="#sec-ses-cifs-ha" title="24.1.3. Configuring high availability">Section 24.1.3, “Configuring high availability”</a> for more information on HA
    setup.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Make sure that a working CephFS already exists in your cluster.
     </p></li><li class="step"><p>
      Create a Samba Gateway specific keyring on the Ceph admin node and copy it to
      both Samba Gateway nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephuser@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <em class="replaceable">SAMBA_NODE</em>:/etc/ceph/</pre></div><p>
      Replace <em class="replaceable">SAMBA_NODE</em> with the name of the Samba
      gateway node.
     </p></li><li class="step"><p>
      The following steps are executed on the Samba Gateway node. Install Samba
      together with the Ceph integration package:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>sudo zypper in samba samba-ceph</pre></div></li><li class="step"><p>
      Replace the default contents of the
      <code class="filename">/etc/samba/smb.conf</code> file with the following:
     </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<em class="replaceable">SHARE_NAME</em>]
  path = <em class="replaceable">CEPHFS_MOUNT</em>
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><p>
      The <em class="replaceable">CEPHFS_MOUNT</em> path above must be mounted
      prior to starting Samba with a kernel CephFS share configuration. See
      <a class="xref" href="#ceph-cephfs-cephfs-fstab" title="23.3. Mounting CephFS in /etc/fstab">Section 23.3, “Mounting CephFS in <code class="filename">/etc/fstab</code>”</a>.
     </p><p>
      The above share configuration uses the Linux kernel CephFS client,
      which is recommended for performance reasons. As an alternative, the
      Samba <code class="systemitem">vfs_ceph</code> module can also be used to
      communicate with the Ceph cluster. The instructions are shown below for
      legacy purposes and are not recommended for new Samba deployments:
     </p><div class="verbatim-wrap"><pre class="screen">[<em class="replaceable">SHARE_NAME</em>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><div id="id-1.4.6.5.5.4.4.4.6" data-id-title="Oplocks and share modes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Oplocks and share modes</h6><p>
       <code class="option">oplocks</code> (also known as SMB2+ leases) allow for improved
       performance through aggressive client caching, but are currently unsafe
       when Samba is deployed together with other CephFS clients, such as
       kernel <code class="literal">mount.ceph</code>, FUSE, or NFS Ganesha.
      </p><p>
       If all CephFS file system path access is exclusively handled by
       Samba, then the <code class="option">oplocks</code> parameter can be safely
       enabled.
      </p><p>
       Currently <code class="option">kernel share modes</code> needs to be disabled in a
       share running with the CephFS vfs module for file serving to work
       properly.
      </p></div><div id="id-1.4.6.5.5.4.4.4.7" data-id-title="Permitting access" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Permitting access</h6><p>
       Samba maps SMB users and groups to local accounts. Local users can be
       assigned a password for Samba share access via:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>smbpasswd -a <em class="replaceable">USERNAME</em></pre></div><p>
       For successful I/O, the share path's access control list (ACL) needs to
       permit access to the user connected via Samba. You can modify the ACL
       by temporarily mounting via the CephFS kernel client and using the
       <code class="command">chmod</code>, <code class="command">chown</code>, or
       <code class="command">setfacl</code> utilities against the share path. For
       example, to permit access for all users, run:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>chmod 777 <em class="replaceable">MOUNTED_SHARE_PATH</em></pre></div></div></li></ol></div></div><section class="sect3" id="samba-service-restart" data-id-title="Starting Samba services"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.1.2.1 </span><span class="title-name">Starting Samba services</span> <a title="Permalink" class="permalink" href="#samba-service-restart">#</a></h4></div></div></div><p>
     Start or restart stand-alone Samba services using the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart smb.service
<code class="prompt root"># </code>systemctl restart nmb.service
<code class="prompt root"># </code>systemctl restart winbind.service</pre></div><p>
     To ensure that Samba services start on boot, enable them via:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl enable smb.service
<code class="prompt root"># </code>systemctl enable nmb.service
<code class="prompt root"># </code>systemctl enable winbind.service</pre></div><div id="id-1.4.6.5.5.4.5.6" data-id-title="Optional nmb and winbind services" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Optional <code class="systemitem">nmb</code> and <code class="systemitem">winbind</code> services</h6><p>
      If you do not require network share browsing, you do not need to enable
      and start the <code class="systemitem">nmb</code> service.
     </p><p>
      The <code class="systemitem">winbind</code> service is only
      needed when configured as an Active Directory domain member. See
      <a class="xref" href="#cephfs-ad" title="24.2. Joining Samba Gateway and Active Directory">Section 24.2, “Joining Samba Gateway and Active Directory”</a>.
     </p></div></section></section><section class="sect2" id="sec-ses-cifs-ha" data-id-title="Configuring high availability"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.1.3 </span><span class="title-name">Configuring high availability</span> <a title="Permalink" class="permalink" href="#sec-ses-cifs-ha">#</a></h3></div></div></div><div id="id-1.4.6.5.5.5.2" data-id-title="Transparent failover not supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Transparent failover not supported</h6><p>
     Although a multi-node Samba + CTDB deployment is more highly available
     compared to the single node (see <a class="xref" href="#cha-ses-cifs" title="Chapter 24. Export Ceph data via Samba">Chapter 24, <em>Export Ceph data via Samba</em></a>),
     client-side transparent failover is not supported. Applications will
     likely experience a short outage on Samba Gateway node failure.
    </p></div><p>
    This section provides an example of how to set up a two-node high
    availability configuration of Samba servers. The setup requires the SUSE Linux Enterprise
    High Availability Extension. The two nodes are called <code class="systemitem">earth</code>
    (<code class="systemitem">192.168.1.1</code>) and <code class="systemitem">mars</code>
    (<code class="systemitem">192.168.1.2</code>).
   </p><p>
    For details about SUSE Linux Enterprise High Availability Extension, see
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP1/" target="_blank">https://documentation.suse.com/sle-ha/15-SP1/</a>.
   </p><p>
    Additionally, two floating virtual IP addresses allow clients to connect to
    the service no matter which physical node it is running on.
    <code class="systemitem">192.168.1.10</code> is used for cluster
    administration with Hawk2 and
    <code class="systemitem">192.168.2.1</code> is used exclusively
    for the CIFS exports. This makes it easier to apply security restrictions
    later.
   </p><p>
    The following procedure describes the example installation. More details
    can be found at
    <a class="link" href="https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/art-sleha-install-quick.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/art-sleha-install-quick.html</a>.
   </p><div class="procedure" id="proc-sec-ses-cifs-ha"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a Samba Gateway specific keyring on the Admin Node and copy it to both nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph</code> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<code class="prompt user">cephuser@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">earth</code>:/etc/ceph/
<code class="prompt user">cephuser@adm &gt; </code><code class="command">scp</code> ceph.client.samba.gw.keyring <code class="systemitem">mars</code>:/etc/ceph/</pre></div></li><li class="step"><p>
      SLE-HA setup requires a fencing device to avoid a <span class="emphasis"><em>split
      brain</em></span> situation when active cluster nodes become
      unsynchronized. For this purpose, you can use a Ceph RBD image with
      Stonith Block Device (SBD). Refer to
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/cha-ha-storage-protect.html#sec-ha-storage-protect-fencing-setup" target="_blank">https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/cha-ha-storage-protect.html#sec-ha-storage-protect-fencing-setup</a>
      for more details.
     </p><p>
      If it does not yet exist, create an RBD pool called
      <code class="literal">rbd</code> (see
      <a class="xref" href="#ceph-pools-operate-add-pool" title="18.1. Creating a pool">Section 18.1, “Creating a pool”</a>) and associate it
      with <code class="literal">rbd</code> (see
      <a class="xref" href="#ceph-pools-associate" title="18.5.1. Associating pools with an application">Section 18.5.1, “Associating pools with an application”</a>). Then create a related
      RBD image called <code class="literal">sbd01</code>:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create rbd
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool application enable rbd rbd
<code class="prompt user">cephuser@adm &gt; </code>rbd -p rbd create sbd01 --size 64M --image-shared</pre></div></li><li class="step"><p>
      Prepare <code class="systemitem">earth</code> and <code class="systemitem">mars</code> to host the Samba service:
     </p><ol type="a" class="substeps"><li class="step"><p>
        Make sure the following packages are installed before you proceed:
        <span class="package">ctdb</span>, <span class="package">tdb-tools</span>, and
        <span class="package">samba</span>.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">zypper</code> in ctdb tdb-tools samba samba-ceph</pre></div></li><li class="step"><p>
        Make sure the Samba and CTDB services are stopped and disabled:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl disable ctdb
<code class="prompt root"># </code>systemctl disable smb
<code class="prompt root"># </code>systemctl disable nmb
<code class="prompt root"># </code>systemctl disable winbind
<code class="prompt root"># </code>systemctl stop ctdb
<code class="prompt root"># </code>systemctl stop smb
<code class="prompt root"># </code>systemctl stop nmb
<code class="prompt root"># </code>systemctl stop winbind</pre></div></li><li class="step"><p>
        Open port <code class="literal">4379</code> of your firewall on all nodes. This
        is needed for CTDB to communicate with other cluster nodes.
       </p></li></ol></li><li class="step"><p>
      On <code class="systemitem">earth</code>, create the configuration files for Samba. They will later
      automatically synchronize to <code class="systemitem">mars</code>.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Insert a list of private IP addresses of Samba Gateway nodes in the
        <code class="filename">/etc/ctdb/nodes</code> file. Find more details in the
        ctdb manual page (<code class="command">man 7 ctdb</code>).
       </p><div class="verbatim-wrap"><pre class="screen">192.168.1.1
192.168.1.2</pre></div></li><li class="step"><p>
        Configure Samba. Add the following lines in the
        <code class="literal">[global]</code> section of
        <code class="filename">/etc/samba/smb.conf</code>. Use the host name of your
        choice in place of <em class="replaceable">CTDB-SERVER</em> (all nodes in
        the cluster will appear as one big node with this name). Add a share
        definition as well, consider <em class="replaceable">SHARE_NAME</em> as
        an example:
       </p><div class="verbatim-wrap"><pre class="screen">[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no</pre></div><p>
        Note that the <code class="filename">/etc/ctdb/nodes</code> and
        <code class="filename">/etc/samba/smb.conf</code> files need to match on all
        Samba Gateway nodes.
       </p></li></ol></li><li class="step"><p>
      Install and bootstrap the SUSE Linux Enterprise High Availability cluster.
     </p><ol type="a" class="substeps"><li class="step"><p>
        Register the SUSE Linux Enterprise High Availability Extension on <code class="systemitem">earth</code> and <code class="systemitem">mars</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">SUSEConnect</code> -r <em class="replaceable">ACTIVATION_CODE</em> -e <em class="replaceable">E_MAIL</em></pre></div></li><li class="step"><p>
        Install <span class="package">ha-cluster-bootstrap</span> on both nodes:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">zypper</code> in ha-cluster-bootstrap</pre></div></li><li class="step"><p>
        Map the RBD image <code class="literal">sbd01</code> on both Samba Gateways via
        <code class="systemitem">rbdmap.service</code>.
       </p><p>
        Edit <code class="filename">/etc/ceph/rbdmap</code> and add an entry for the SBD
        image:
       </p><div class="verbatim-wrap"><pre class="screen">rbd/sbd01 id=samba.gw,keyring=/etc/ceph/ceph.client.samba.gw.keyring</pre></div><p>
        Enable and start
        <code class="systemitem">rbdmap.service</code>:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
<code class="prompt user">root@mars # </code>systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service</pre></div><p>
        The <code class="filename">/dev/rbd/rbd/sbd01</code> device should be available
        on both Samba Gateways.
       </p></li><li class="step"><p>
        Initialize the cluster on <code class="systemitem">earth</code> and let <code class="systemitem">mars</code> join it.
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">ha-cluster-init</code></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@mars # </code><code class="command">ha-cluster-join</code> -c earth</pre></div><div id="id-1.4.6.5.5.5.7.5.2.4.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
         During the process of initialization and joining the cluster, you will
         be interactively asked whether to use SBD. Confirm with
         <code class="option">y</code> and then specify
         <code class="filename">/dev/rbd/rbd/sbd01</code> as a path to the storage
         device.
        </p></div></li></ol></li><li class="step"><p>
      Check the status of the cluster. You should see two nodes added in the
      cluster:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</pre></div></li><li class="step"><p>
      Execute the following commands on <code class="systemitem">earth</code> to configure the CTDB
      resource:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> configure
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> winbind systemd:winbind \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
<code class="prompt user">crm(live)configure# </code><code class="command">group</code> g-ctdb ctdb winbind nmb smb
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ctdb g-ctdb meta interleave="true"
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><div id="id-1.4.6.5.5.5.7.7.3" data-id-title="Optional nmb and winbind primitives" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Optional <code class="systemitem">nmb</code> and <code class="systemitem">winbind</code> primitives</h6><p>
       If you do not require network share browsing, you do not need to add the
       <code class="systemitem">nmb</code> primitive.
      </p><p>
       The <code class="systemitem">winbind</code> primitive is only
       needed when configured as an Active Directory domain member. See
       <a class="xref" href="#cephfs-ad" title="24.2. Joining Samba Gateway and Active Directory">Section 24.2, “Joining Samba Gateway and Active Directory”</a>.
      </p></div><p>
      The binary
      <code class="command">/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</code> in the
      configuration option <code class="literal">ctdb_recovery_lock</code> has the
      parameters <em class="replaceable">CLUSTER_NAME</em>,
      <em class="replaceable">CEPHX_USER</em>,
      <em class="replaceable">RADOS_POOL</em>, and
      <em class="replaceable">RADOS_OBJECT</em>, in this order.
     </p><p>
      An extra lock-timeout parameter can be appended to override the default
      value used (10 seconds). A higher value will increase the CTDB recovery
      master failover time, whereas a lower value may result in the recovery
      master being incorrectly detected as down, triggering flapping failovers.
     </p></li><li class="step"><p>
      Add a clustered IP address:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">crm(live)configure# </code><code class="command">primitive</code> ip ocf:heartbeat:IPaddr2
    params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<code class="prompt user">crm(live)configure# </code><code class="command">clone</code> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<code class="prompt user">crm(live)configure# </code><code class="command">colocation</code> col-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">order</code> o-with-ctdb 0: cl-ip cl-ctdb
<code class="prompt user">crm(live)configure# </code><code class="command">commit</code></pre></div><p>
      If <code class="literal">unique_clone_address</code> is set to
      <code class="literal">true</code>, the IPaddr2 resource agent adds a clone ID to
      the specified address, leading to three different IP addresses. These are
      usually not needed, but help with load balancing. For further information
      about this topic, see
      <a class="link" href="https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/cha-ha-lb.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP3/html/SLE-HA-all/cha-ha-lb.html</a>.
     </p></li><li class="step"><p>
      Check the result:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@earth # </code><code class="command">crm</code> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</pre></div></li><li class="step"><p>
      Test from a client machine. On a Linux client, run the following command
      to see if you can copy files from and to the system:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">smbclient</code> <code class="option">//192.168.2.1/myshare</code></pre></div></li></ol></div></div><section class="sect3" id="samba-ha-service-restart" data-id-title="Restarting HA Samba resources"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.1.3.1 </span><span class="title-name">Restarting HA Samba resources</span> <a title="Permalink" class="permalink" href="#samba-ha-service-restart">#</a></h4></div></div></div><p>
     Following any Samba or CTDB configuration changes, HA resources may need
     to be restarted for the changes to take effect. This can be done by via:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">crm</code> resource restart cl-ctdb</pre></div></section></section></section><section class="sect1" id="cephfs-ad" data-id-title="Joining Samba Gateway and Active Directory"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">24.2 </span><span class="title-name">Joining Samba Gateway and Active Directory</span> <a title="Permalink" class="permalink" href="#cephfs-ad">#</a></h2></div></div></div><p>
   You can configure the Ceph Samba gateway to become a member of Samba
   domain with Active Directory (AD) support. As a Samba domain member, you can use
   domain users and groups in local access lists (ACLs) on files and
   directories from the exported CephFS.
  </p><section class="sect2" id="cephfs-ad-preparation" data-id-title="Preparing Samba installation"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.1 </span><span class="title-name">Preparing Samba installation</span> <a title="Permalink" class="permalink" href="#cephfs-ad-preparation">#</a></h3></div></div></div><p>
    This section introduces preparatory steps that you need to take care of
    before configuring the Samba itself. Starting with a clean environment
    helps you prevent confusion and verifies that no files from the previous
    Samba installation are mixed with the new domain member installation.
   </p><div id="id-1.4.6.5.6.3.3" data-id-title="Synchronizing clocks" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Synchronizing clocks</h6><p>
     All Samba Gateway nodes' clocks need to be synchronized with the Active Directory Domain
     controller. Clock skew may result in authentication failures.
    </p></div><p>
    Verify that no Samba or name caching processes are running:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>ps ax | egrep "samba|smbd|nmbd|winbindd|nscd"</pre></div><p>
    If the output lists any <code class="literal">samba</code>, <code class="literal">smbd</code>,
    <code class="literal">nmbd</code>, <code class="literal">winbindd</code>, or
    <code class="literal">nscd</code> processes, stop them.
   </p><p>
    If you have previously run a Samba installation on this host, remove the
    <code class="filename">/etc/samba/smb.conf</code> file. Also remove all Samba
    database files, such as <code class="filename">*.tdb</code> and
    <code class="filename">*.ldb</code> files. To list directories containing Samba
    databases, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>smbd -b | egrep "LOCKDIR|STATEDIR|CACHEDIR|PRIVATE_DIR"</pre></div></section><section class="sect2" id="cephfs-ad-dns" data-id-title="Verifying DNS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.2 </span><span class="title-name">Verifying DNS</span> <a title="Permalink" class="permalink" href="#cephfs-ad-dns">#</a></h3></div></div></div><p>
    Active Directory (AD) uses DNS to locate other domain controllers (DCs) and services,
    such as Kerberos. Therefore AD domain members and servers need to be able
    to resolve the AD DNS zones.
   </p><p>
    Verify that DNS is correctly configured and that both forward and reverse
    lookup resolve correctly, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>nslookup DC1.domain.example.com
Server:         10.99.0.1
Address:        10.99.0.1#53

Name:   DC1.domain.example.com
Address: 10.99.0.1</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>10.99.0.1
Server:        10.99.0.1
Address:	10.99.0.1#53

1.0.99.10.in-addr.arpa	name = DC1.domain.example.com.</pre></div></section><section class="sect2" id="cephfs-ad-srv" data-id-title="Resolving SRV records"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.3 </span><span class="title-name">Resolving SRV records</span> <a title="Permalink" class="permalink" href="#cephfs-ad-srv">#</a></h3></div></div></div><p>
    AD uses SRV records to locate services, such as Kerberos and LDAP. To
    verify that SRV records are resolved correctly, use the
    <code class="command">nslookup</code> interactive shell, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>nslookup
Default Server:  10.99.0.1
Address:  10.99.0.1

&gt; set type=SRV
&gt; _ldap._tcp.domain.example.com.
Server:  UnKnown
Address:  10.99.0.1

_ldap._tcp.domain.example.com   SRV service location:
          priority       = 0
          weight         = 100
          port           = 389
          svr hostname   = dc1.domain.example.com
domain.example.com      nameserver = dc1.domain.example.com
dc1.domain.example.com  internet address = 10.99.0.1</pre></div></section><section class="sect2" id="cephfs-ad-kerberos" data-id-title="Configuring Kerberos"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.4 </span><span class="title-name">Configuring Kerberos</span> <a title="Permalink" class="permalink" href="#cephfs-ad-kerberos">#</a></h3></div></div></div><p>
    Samba supports Heimdal and MIT Kerberos back-ends. To configure Kerberos
    on the domain member, set the following in your
    <code class="filename">/etc/krb5.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">[libdefaults]
	default_realm = DOMAIN.EXAMPLE.COM
	dns_lookup_realm = false
	dns_lookup_kdc = true</pre></div><p>
    The previous example configures Kerberos for the DOMAIN.EXAMPLE.COM realm.
    We do not recommend to set any further parameters in the
    <code class="filename">/etc/krb5.conf</code> file. If your
    <code class="filename">/etc/krb5.conf</code> contains an <code class="literal">include</code>
    line it will not work—you <span class="bold"><strong>must</strong></span>
    remove this line.
   </p></section><section class="sect2" id="cephfs-ad-local-resolution" data-id-title="Resolving localhost name"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.5 </span><span class="title-name">Resolving localhost name</span> <a title="Permalink" class="permalink" href="#cephfs-ad-local-resolution">#</a></h3></div></div></div><p>
    When you join a host to the domain, Samba tries to register the host name
    in the AD DNS zone. For this, the <code class="command">net</code> utility needs to
    be able to resolve the host name using DNS or using a correct entry in the
    <code class="filename">/etc/hosts</code> file.
   </p><p>
    To verify that your host name resolves correctly, use the <code class="command">getent
    hosts</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>getent hosts example-host
10.99.0.5      example-host.domain.example.com    example-host</pre></div><p>
    The host name and FQDN must not resolve to the 127.0.0.1 IP address or any
    IP address other than the one used on the LAN interface of the domain
    member. If no output is displayed or the host is resolved to the wrong IP
    address and you are not using DHCP, set the correct entry in the
    <code class="filename">/etc/hosts</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">127.0.0.1      localhost
10.99.0.5      example-host.samdom.example.com    example-host</pre></div><div id="id-1.4.6.5.6.7.7" data-id-title="DHCP and /etc/hosts" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: DHCP and <code class="filename">/etc/hosts</code></h6><p>
     If you are using DHCP, check that <code class="filename">/etc/hosts</code> only
     contains the '127.0.0.1' line. If you continue to have problems, contact
     the administrator of your DHCP server.
    </p><p>
     If you need to add aliases to the machine host name, add them to the end
     of the line that starts with the machine's IP address, not to the
     '127.0.0.1' line.
    </p></div></section><section class="sect2" id="cephfs-ad-smb-conf" data-id-title="Configuring Samba"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.6 </span><span class="title-name">Configuring Samba</span> <a title="Permalink" class="permalink" href="#cephfs-ad-smb-conf">#</a></h3></div></div></div><p>
    This section introduces information about specific configuration options
    that you need to include in the Samba configuration.
   </p><p>
    Active Directory domain membership is primarily configured by setting <code class="literal">security
    = ADS</code> alongside appropriate Kerberos realm and ID mapping
    parameters in the <code class="literal">[global]</code> section of
    <code class="filename">/etc/samba/smb.conf</code>.
   </p><div class="verbatim-wrap"><pre class="screen">[global]
  security = ADS
  workgroup = DOMAIN
  realm = DOMAIN.EXAMPLE.COM
  ...</pre></div><section class="sect3" id="smb-backend-id-mapping-winbindd" data-id-title="Choosing the back-end for ID mapping in winbindd"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.6.1 </span><span class="title-name">Choosing the back-end for ID mapping in <code class="systemitem">winbindd</code></span> <a title="Permalink" class="permalink" href="#smb-backend-id-mapping-winbindd">#</a></h4></div></div></div><p>
     If you need your users to have different login shells and/or Unix home
     directory paths, or you want them to have the same ID everywhere, you will
     need to use the winbind 'ad' back-end and add RFC2307 attributes to AD.
    </p><div id="id-1.4.6.5.6.8.5.3" data-id-title="RFC2307 Attributes and ID Numbers" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: RFC2307 Attributes and ID Numbers</h6><p>
      The RFC2307 attributes are not added automatically when users or groups
      are created.
     </p><p>
      The ID numbers found on a DC (numbers in the 3000000 range) are
      <span class="emphasis"><em>not</em></span> RFC2307 attributes and will not be used on Unix
      Domain Members. If you need to have the same ID numbers everywhere, add
      <code class="literal">uidNumber</code> and <code class="literal">gidNumber</code> attributes
      to AD and use the winbind 'ad' back-end on Unix Domain Members. If you do
      decide to add <code class="literal">uidNumber</code> and
      <code class="literal">gidNumber</code> attributes to AD, do not use numbers in the
      3000000 range.
     </p></div><p>
     If your users will only use the Samba AD DC for authentication and will
     not store data on it or log in to it, you can use the winbind 'rid'
     back-end. This calculates the user and group IDs from the Windows* RID. If
     you use the same <code class="literal">[global]</code> section of the
     <code class="filename">smb.conf</code> on every Unix domain member, you will get
     the same IDs. If you use the 'rid' back-end, you do not need to add
     anything to AD and RFC2307 attributes will be ignored. When using the
     'rid' back-end, set the <code class="option">template shell</code> and
     <code class="option">template homedir</code> parameters in
     <code class="filename">smb.conf</code>. These settings are global and everyone gets
     the same login shell and Unix home directory path (unlike the RFC2307
     attributes where you can set individual Unix home directory paths and
     shells).
    </p><p>
     There is another way of setting up Samba—when you require your
     users and groups to have the same ID everywhere, but only need your users
     to have the same login shell and use the same Unix home directory path.
     You can do this by using the winbind 'ad' back-end and using the template
     lines in <code class="filename">smb.conf</code>. This way you only need to add
     <code class="literal">uidNumber</code> and <code class="literal">gidNumber</code> attributes
     to AD.
    </p><div id="id-1.4.6.5.6.8.5.6" data-id-title="More Information about Back-ends for ID Mapping" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information about Back-ends for ID Mapping</h6><p>
      Find more detailed information about available ID mapping back-ends in
      the related manual pages: <code class="command">man 8 idmap_ad</code>, <code class="command">man
      8 idmap_rid</code>, and <code class="command">man 8 idmap_autorid</code>.
     </p></div></section><section class="sect3" id="smb-setting-user-group-id-ranges" data-id-title="Setting user and group ID ranges"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.6.2 </span><span class="title-name">Setting user and group ID ranges</span> <a title="Permalink" class="permalink" href="#smb-setting-user-group-id-ranges">#</a></h4></div></div></div><p>
     After you decide which winbind back-end to use, you need to specify the
     ranges to use with the <code class="option">idmap config</code> option in
     <code class="filename">smb.conf</code>. By default, there are multiple blocks of
     user and group IDs reserved on a Unix domain member:
    </p><div class="table" id="id-1.4.6.5.6.8.6.3" data-id-title="Default Users and Group ID Blocks"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 24.1: </span><span class="title-name">Default Users and Group ID Blocks </span><a title="Permalink" class="permalink" href="#id-1.4.6.5.6.8.6.3">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">IDs</th><th style="border-bottom: 1px solid ; ">Range</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">0-999</td><td style="border-bottom: 1px solid ; ">Local system users and groups.</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">Starting at 1000</td><td style="border-bottom: 1px solid ; ">Local Unix users and groups.</td></tr><tr><td style="border-right: 1px solid ; ">Starting at 10000</td><td>DOMAIN users and groups.</td></tr></tbody></table></div></div><p>
     As you can see from the above ranges, you should not set either the '*' or
     'DOMAIN' ranges to start at 999 or less, as they would interfere with the
     local system users and groups. You also should leave a space for any local
     Unix users and groups, so starting the <code class="option">idmap config</code>
     ranges at 3000 seems to be a good compromise.
    </p><p>
     You need to decide how large your 'DOMAIN' is likely to grow and if you
     plan to have any trusted domains. Then you can set the <code class="option">idmap
     config</code> ranges as follows:
    </p><div class="table" id="id-1.4.6.5.6.8.6.6" data-id-title="ID Ranges"><div class="table-title-wrap"><h6 class="table-title"><span class="title-number">Table 24.2: </span><span class="title-name">ID Ranges </span><a title="Permalink" class="permalink" href="#id-1.4.6.5.6.8.6.6">#</a></h6></div><div class="table-contents"><table style="width: 50%; border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col/><col/></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; ">Domain</th><th style="border-bottom: 1px solid ; ">Range</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">*</td><td style="border-bottom: 1px solid ; ">3000-7999</td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">DOMAIN</td><td style="border-bottom: 1px solid ; ">10000-999999</td></tr><tr><td style="border-right: 1px solid ; ">TRUSTED</td><td>1000000-9999999</td></tr></tbody></table></div></div></section><section class="sect3" id="smb-mapping-domain-admin-account-local" data-id-title="Mapping the domain administrator account to the local root user"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.6.3 </span><span class="title-name">Mapping the domain administrator account to the local <code class="systemitem">root</code> user</span> <a title="Permalink" class="permalink" href="#smb-mapping-domain-admin-account-local">#</a></h4></div></div></div><p>
     Samba enables you to map domain accounts to a local account. Use this
     feature to execute file operations on the domain member's file system as a
     different user than the account that requested the operation on the
     client.
    </p><div id="id-1.4.6.5.6.8.7.3" data-id-title="Mapping the Domain Administrator (Optional)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mapping the Domain Administrator (Optional)</h6><p>
      Mapping the domain administrator to the local <code class="systemitem">root</code> account is
      optional. Only configure the mapping if the domain administrator needs to
      be able to execute file operations on the domain member using <code class="systemitem">root</code>
      permissions. Be aware that mapping Administrator to the <code class="systemitem">root</code>
      account does not allow you to log in to Unix domain members as
      'Administrator'.
     </p></div><p>
     To map the domain administrator to the local <code class="systemitem">root</code> account, follow
     these steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Add the following parameter to the <code class="literal">[global]</code> section
       of your <code class="filename">smb.conf</code> file:
      </p><div class="verbatim-wrap"><pre class="screen">username map = /etc/samba/user.map</pre></div></li><li class="step"><p>
       Create the <code class="filename">/etc/samba/user.map</code> file with the
       following content:
      </p><div class="verbatim-wrap"><pre class="screen">!root = <em class="replaceable">DOMAIN</em>\Administrator</pre></div></li></ol></div></div><div id="id-1.4.6.5.6.8.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      When using the 'ad' ID mapping back-end, do not set the
      <code class="option">uidNumber</code> attribute for the domain administrator
      account. If the account has the attribute set, the value overrides the
      local UID '0' of the <code class="systemitem">root</code> user, and therefore the mapping fails.
     </p></div><p>
     For more details, see the <code class="option">username map</code> parameter in the
     <code class="filename">smb.conf</code> manual page (<code class="command">man 5
     smb.conf</code>).
    </p></section></section><section class="sect2" id="cephfs-ad-joining" data-id-title="Joining the Active Directory domain"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.7 </span><span class="title-name">Joining the Active Directory domain</span> <a title="Permalink" class="permalink" href="#cephfs-ad-joining">#</a></h3></div></div></div><p>
    To join the host to an Active Directory, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>net ads join -U administrator
Enter administrator's password: <em class="replaceable">PASSWORD</em>
Using short domain name -- <em class="replaceable">DOMAIN</em>
Joined <em class="replaceable">EXAMPLE-HOST</em> to dns domain <em class="replaceable">'DOMAIN</em>.example.com'</pre></div></section><section class="sect2" id="cephfs-ad-nss" data-id-title="Configuring the name service switch"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.8 </span><span class="title-name">Configuring the name service switch</span> <a title="Permalink" class="permalink" href="#cephfs-ad-nss">#</a></h3></div></div></div><p>
    To make domain users and groups available to the local system, you need to
    enable the name service switch (NSS) library. Append the
    <code class="option">winbind</code> entry to the following databases in the
    <code class="filename">/etc/nsswitch.conf</code> file:
   </p><div class="verbatim-wrap"><pre class="screen">passwd: files winbind
group:  files winbind</pre></div><div id="id-1.4.6.5.6.10.4" data-id-title="Points to Consider" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Points to Consider</h6><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Keep the <code class="option">files</code> entry as the first source for both
       databases. This enables NSS to look up domain users and groups from the
       <code class="filename">/etc/passwd</code> and <code class="filename">/etc/group</code>
       files before querying the
       <code class="systemitem">winbind</code> service.
      </p></li><li class="listitem"><p>
       Do not add the <code class="option">winbind</code> entry to the NSS
       <code class="literal">shadow</code> database. This can cause the
       <code class="command">wbinfo</code> utility to fail.
      </p></li><li class="listitem"><p>
       Do not use the same user names in the local
       <code class="filename">/etc/passwd</code> file as in the domain.
      </p></li></ul></div></div></section><section class="sect2" id="cephfs-ad-services" data-id-title="Starting the services"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.9 </span><span class="title-name">Starting the services</span> <a title="Permalink" class="permalink" href="#cephfs-ad-services">#</a></h3></div></div></div><p>
    Following configuration changes, restart Samba services as per
    <a class="xref" href="#samba-service-restart" title="24.1.2.1. Starting Samba services">Section 24.1.2.1, “Starting Samba services”</a> or
    <a class="xref" href="#samba-ha-service-restart" title="24.1.3.1. Restarting HA Samba resources">Section 24.1.3.1, “Restarting HA Samba resources”</a>.
   </p></section><section class="sect2" id="cephfs-ad-testing" data-id-title="Test the winbindd connectivity"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">24.2.10 </span><span class="title-name">Test the <code class="systemitem">winbindd</code> connectivity</span> <a title="Permalink" class="permalink" href="#cephfs-ad-testing">#</a></h3></div></div></div><section class="sect3" id="cephfs-ad-send-ping" data-id-title="Sending a winbindd ping"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.10.1 </span><span class="title-name">Sending a <code class="systemitem">winbindd</code> ping</span> <a title="Permalink" class="permalink" href="#cephfs-ad-send-ping">#</a></h4></div></div></div><p>
     To verify if the <code class="systemitem">winbindd</code> service
     is able to connect to AD Domain Controllers (DC) or a primary domain
     controller (PDC), enter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>wbinfo --ping-dc
checking the NETLOGON for domain[<em class="replaceable">DOMAIN</em>] dc connection to "DC.DOMAIN.EXAMPLE.COM" succeeded</pre></div><p>
     If the previous command fails, verify that the
     <code class="systemitem">winbindd</code> service is running and
     that the <code class="filename">smb.conf</code> file is set up correctly.
    </p></section><section class="sect3" id="smb-domain-users-groups-cephfs" data-id-title="Looking up domain users and groups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.10.2 </span><span class="title-name">Looking up domain users and groups</span> <a title="Permalink" class="permalink" href="#smb-domain-users-groups-cephfs">#</a></h4></div></div></div><p>
     The <code class="systemitem">libnss_winbind</code> library enables you to look up
     domain users and groups. For example, to look up the domain user
     'DOMAIN\demo01':
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>getent passwd DOMAIN\\demo01
DOMAIN\demo01:*:10000:10000:demo01:/home/demo01:/bin/bash</pre></div><p>
     To look up the domain group 'Domain Users':
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>getent group "DOMAIN\\Domain Users"
DOMAIN\domain users:x:10000:</pre></div></section><section class="sect3" id="smb-assign-file-perms-domain-users-groups" data-id-title="Assigning file permissions to domain users and groups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">24.2.10.3 </span><span class="title-name">Assigning file permissions to domain users and groups</span> <a title="Permalink" class="permalink" href="#smb-assign-file-perms-domain-users-groups">#</a></h4></div></div></div><p>
     The name service switch (NSS) library enables you to use domain user
     accounts and groups in commands. For example to set the owner of a file to
     the 'demo01' domain user and the group to the 'Domain Users' domain group,
     enter:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@smb &gt; </code>chown "DOMAIN\\demo01:DOMAIN\\domain users" file.txt</pre></div></section></section></section></section><section class="chapter" id="cha-ceph-nfsganesha" data-id-title="NFS Ganesha"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">25 </span><span class="title-name">NFS Ganesha</span> <a title="Permalink" class="permalink" href="#cha-ceph-nfsganesha">#</a></h1></div></div></div><p>
  NFS Ganesha is an NFS server that runs in a user address space instead of as
  part of the operating system kernel. With NFS Ganesha, you can plug in your own
  storage mechanism—such as Ceph—and access it from any NFS
  client. For installation instructions, see
  <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.3.6 “Deploying NFS Ganesha”</span>.
 </p><div id="id-1.4.6.6.4" data-id-title="NFS Ganesha performance" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha performance</h6><p>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing Ceph via
   an NFS Gateway may significantly reduce application performance when
   compared to native CephFS.
  </p></div><p>
  Each NFS Ganesha service consists of a configuration hierarchy that contains:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    A bootstrap <code class="filename">ganesha.conf</code>
   </p></li><li class="listitem"><p>
    A per-service RADOS common configuration object
   </p></li><li class="listitem"><p>
    A per export RADOS configuration object
   </p></li></ul></div><p>
  The bootstrap configuration is the minimal configuration to start the
  <code class="systemitem">nfs-ganesha</code> daemon within a
  container. Each bootstrap configuration will contain a
  <code class="literal">%url</code> directive that includes any additional configuration
  from the RADOS common configuration object. The common configuration object
  can include additional <code class="literal">%url</code> directives for each of the NFS
  exports defined in the export RADOS configuration objects.
 </p><div class="figure" id="id-1.4.6.6.8"><div class="figure-contents"><div class="mediaobject"><a href="images/nfs_ganesha_structure.png"><img src="images/nfs_ganesha_structure.png" width="75%" alt="NFS Ganesha structure" title="NFS Ganesha structure"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 25.1: </span><span class="title-name">NFS Ganesha structure </span><a title="Permalink" class="permalink" href="#id-1.4.6.6.8">#</a></h6></div></div><section class="sect1" id="ceph-nfsganesha-nfservice" data-id-title="Creating an NFS service"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.1 </span><span class="title-name">Creating an NFS service</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-nfservice">#</a></h2></div></div></div><p>
   The recommended way to specify the deployment of Ceph services is to
   create a YAML-formatted file with the specification of the services that you
   intend to deploy. You can create a separate specification file for each type
   of service, or you specify multiple (or all) services types in one file.
  </p><p>
   Depending on what you have chosen to do, you will need to update or create a
   relevant YAML-formatted file to create a NFS Ganesha service. For more
   information on creating the file, see
   <span class="intraxref">Book “Deployment Guide”, Chapter 8 “Deploying the remaining core services using cephadm”, Section 8.2 “Service and placement specification”</span>.
  </p><p>
   One you have updated or created the file, execute the following to create a
   <code class="literal">nfs-ganesha</code> service:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">FILE_NAME</em></pre></div></section><section class="sect1" id="ceph-nfsganesha-services" data-id-title="Starting or Restarting NFS Ganesha"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.2 </span><span class="title-name">Starting or Restarting NFS Ganesha</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-services">#</a></h2></div></div></div><div id="id-1.4.6.6.10.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    Starting the NFS Ganesha service does not automatically export a CephFS
    file system. To export a CephFS file system, create an export
    configuration file. Refer to
    <a class="xref" href="#ceph-nfsganesha-create-export" title="25.4. Creating an NFS export">Section 25.4, “Creating an NFS export”</a> for more details.
   </p></div><p>
   To start the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch start nfs.<em class="replaceable">SERVICE_ID</em></pre></div><p>
   To restart the NFS Ganesha service, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart nfs.<em class="replaceable">SERVICE_ID</em></pre></div><p>
   If you only want to restart a single NFS Ganesha daemon, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch daemon restart nfs.<em class="replaceable">SERVICE_ID</em></pre></div><p>
   When NFS Ganesha is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in the
   grace period.
  </p></section><section class="sect1" id="ceph-nfsganesha-list-objects" data-id-title="Listing objects in the NFS recovery pool"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.3 </span><span class="title-name">Listing objects in the NFS recovery pool</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-list-objects">#</a></h2></div></div></div><p>
   Execute the following to list the objects in the NFS recovery pool:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados --pool <em class="replaceable">POOL_NAME</em> --namespace <em class="replaceable">NAMESPACE_NAME</em> ls</pre></div></section><section class="sect1" id="ceph-nfsganesha-create-export" data-id-title="Creating an NFS export"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.4 </span><span class="title-name">Creating an NFS export</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-create-export">#</a></h2></div></div></div><p>
   You can create an NFS export either in the Ceph Dashboard, or manually on the
   command line. To create the export by using the Ceph Dashboard, refer to
   <a class="xref" href="#dash-webui-nfs" title="Chapter 7. Manage NFS Ganesha">Chapter 7, <em>Manage NFS Ganesha</em></a>, more specifically to
   <a class="xref" href="#dash-webui-nfs-create" title="7.1. Creating NFS exports">Section 7.1, “Creating NFS exports”</a>.
  </p><p>
   To create an NFS export manually, create a configuration file for the
   export. For example, a file <code class="filename">/tmp/export-1</code> with the
   following content:
  </p><div class="verbatim-wrap"><pre class="screen">EXPORT {
    export_id = 1;
    path = "/";
    pseudo = "/";
    access_type = "RW";
    squash = "no_root_squash";
    protocols = 3, 4;
    transports = "TCP", "UDP";
    FSAL {
        name = "CEPH";
        user_id = "admin";
        filesystem = "a";
        secret_access_key = "<em class="replaceable">SECRET_ACCESS_KEY</em>";
    }
}</pre></div><p>
   After you have created and saved the configuration file for the new export,
   run the following command to create the export:
  </p><div class="verbatim-wrap"><pre class="screen">rados --pool <em class="replaceable">POOL_NAME</em> --namespace <em class="replaceable">NAMESPACE_NAME</em> put <em class="replaceable">EXPORT_NAME</em> <em class="replaceable">EXPORT_CONFIG_FILE</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados --pool example_pool --namespace example_namespace put export-1 /tmp/export-1</pre></div><div id="id-1.4.6.6.12.9" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The FSAL block should be modified to include the desired <code class="systemitem">cephx</code> user ID
    and secret access key.
   </p></div></section><section class="sect1" id="ceph-nfsganesha-verify" data-id-title="Verifying the NFS export"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.5 </span><span class="title-name">Verifying the NFS export</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-verify">#</a></h2></div></div></div><p>
   NFS v4 will build a list of exports at the root of a pseudo file system. You
   can verify that the NFS shares are exported by mounting
   <code class="filename">/</code> of the NFS Ganesha server node:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -t nfs <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em>
<code class="prompt root"># </code><code class="command">ls</code> <em class="replaceable">/path/to/local/mountpoint</em> cephfs</pre></div><div id="id-1.4.6.6.13.4" data-id-title="NFS Ganesha is v4 only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: NFS Ganesha is v4 only</h6><p>
    By default, cephadm will configure an NFS v4 server. NFS v4 does not
    interact with <code class="literal">rpcbind</code> nor the <code class="literal">mountd</code>
    daemon. NFS client tools such as <code class="command">showmount</code> will not show
    any configured exports.
   </p></div></section><section class="sect1" id="ceph-nfsganesha-mount" data-id-title="Mounting the NFS export"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.6 </span><span class="title-name">Mounting the NFS export</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-mount">#</a></h2></div></div></div><p>
   To mount the exported NFS share on a client host, run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">mount</code> -t nfs <em class="replaceable">nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</em></pre></div></section><section class="sect1" id="ceph-nfsganesha-customrole" data-id-title="Multiple NFS Ganesha clusters"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">25.7 </span><span class="title-name">Multiple NFS Ganesha clusters</span> <a title="Permalink" class="permalink" href="#ceph-nfsganesha-customrole">#</a></h2></div></div></div><p>
   Multiple NFS Ganesha clusters can be defined. This allows for:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Separated NFS Ganesha clusters for accessing CephFS.
    </p></li></ul></div></section></section></div><div class="part" id="part-integration-virt" data-id-title="Integration with Virtualization Tools"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part V </span><span class="title-name">Integration with Virtualization Tools </span><a title="Permalink" class="permalink" href="#part-integration-virt">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-libvirt"><span class="title-number">26 </span><span class="title-name"><code class="systemitem">libvirt</code> and Ceph</span></a></span></li><dd class="toc-abstract"><p>The libvirt library creates a virtual machine abstraction layer between hypervisor interfaces and the software applications that use them. With libvirt, developers and system administrators can focus on a common management framework, common API, and common shell interface (virsh) to many different h…</p></dd><li><span class="chapter"><a href="#cha-ceph-kvm"><span class="title-number">27 </span><span class="title-name">Ceph as a back-end for QEMU KVM instance</span></a></span></li><dd class="toc-abstract"><p>The most frequent Ceph use case involves providing block device images to virtual machines. For example, a user may create a 'golden' image with an OS and any relevant software in an ideal configuration. Then, the user takes a snapshot of the image. Finally, the user clones the snapshot (usually man…</p></dd></ul></div><section class="chapter" id="cha-ceph-libvirt" data-id-title="libvirt and Ceph"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">26 </span><span class="title-name"><code class="systemitem">libvirt</code> and Ceph</span> <a title="Permalink" class="permalink" href="#cha-ceph-libvirt">#</a></h1></div></div></div><p>
  The <code class="systemitem">libvirt</code> library creates a virtual machine abstraction layer between
  hypervisor interfaces and the software applications that use them. With
  <code class="systemitem">libvirt</code>, developers and system administrators can focus on a common
  management framework, common API, and common shell interface
  (<code class="command">virsh</code>) to many different hypervisors, including
  QEMU/KVM, Xen, LXC, or VirtualBox.
 </p><p>
  Ceph block devices support QEMU/KVM. You can use Ceph block devices
  with software that interfaces with <code class="systemitem">libvirt</code>. The cloud solution uses
  <code class="systemitem">libvirt</code> to interact with QEMU/KVM, and QEMU/KVM interacts with Ceph
  block devices via <code class="systemitem">librbd</code>.
 </p><p>
  To create VMs that use Ceph block devices, use the procedures in the
  following sections. In the examples, we have used
  <code class="literal">libvirt-pool</code> for the pool name,
  <code class="literal">client.libvirt</code> for the user name, and
  <code class="literal">new-libvirt-image</code> for the image name. You may use any
  value you like, but ensure you replace those values when executing commands
  in the subsequent procedures.
 </p><section class="sect1" id="ceph-libvirt-cfg-ceph" data-id-title="Configuring Ceph with libvirt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.1 </span><span class="title-name">Configuring Ceph with <code class="systemitem">libvirt</code></span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-ceph">#</a></h2></div></div></div><p>
   To configure Ceph for use with <code class="systemitem">libvirt</code>, perform the following steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Create a pool. The following example uses the pool name
     <code class="literal">libvirt-pool</code> with 128 placement groups.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create libvirt-pool 128 128</pre></div><p>
     Verify that the pool exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd lspools</pre></div></li><li class="step"><p>
     Create a Ceph User. The following example uses the Ceph user name
     <code class="literal">client.libvirt</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.libvirt mon 'profile rbd' osd \
 'profile rbd pool=libvirt-pool'</pre></div><p>
     Verify the name exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><div id="id-1.4.7.2.6.3.2.5" data-id-title="User name or ID" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: User name or ID</h6><p>
      <code class="systemitem">libvirt</code> will access Ceph using the ID <code class="literal">libvirt</code>, not
      the Ceph name <code class="literal">client.libvirt</code>. See
      <a class="xref" href="#cephx-user" title="30.2.1.1. User">Section 30.2.1.1, “User”</a> for a detailed explanation of the difference
      between ID and name.
     </p></div></li><li class="step"><p>
     Use QEMU to create an image in your RBD pool. The following example uses
     the image name <code class="literal">new-libvirt-image</code> and references
     <code class="literal">libvirt-pool</code>.
    </p><div id="id-1.4.7.2.6.3.3.2" data-id-title="Keyring file location" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Keyring file location</h6><p>
      The <code class="systemitem">libvirt</code> user key is stored in a keyring file placed in the
      <code class="filename">/etc/ceph</code> directory. The keyring file needs to have
      an appropriate name that includes the name of the Ceph cluster it
      belongs to. For the default cluster name 'ceph', the keyring file name is
      <code class="filename">/etc/ceph/ceph.client.libvirt.keyring</code>.
     </p><p>
      If the keyring does not exist, create it with:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get client.libvirt &gt; /etc/ceph/ceph.client.libvirt.keyring</pre></div></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu-img create -f raw rbd:libvirt-pool/new-libvirt-image:id=libvirt 2G</pre></div><p>
     Verify the image exists.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd -p libvirt-pool ls</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-virt-manager" data-id-title="Preparing the VM manager"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.2 </span><span class="title-name">Preparing the VM manager</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-virt-manager">#</a></h2></div></div></div><p>
   You may use <code class="systemitem">libvirt</code> without a VM manager, but you may find it simpler to
   create your first domain with <code class="command">virt-manager</code>.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install a virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in virt-manager</pre></div></li><li class="step"><p>
     Prepare/download an OS image of the system you want to run virtualized.
    </p></li><li class="step"><p>
     Launch the virtual machine manager.
    </p><div class="verbatim-wrap"><pre class="screen">virt-manager</pre></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-create-vm" data-id-title="Creating a VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.3 </span><span class="title-name">Creating a VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-create-vm">#</a></h2></div></div></div><p>
   To create a VM with <code class="command">virt-manager</code>, perform the following
   steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Choose the connection from the list, right-click it, and select
     <span class="guimenu">New</span>.
    </p></li><li class="step"><p>
     <span class="guimenu">Import existing disk image</span> by providing the path to the
     existing storage. Specify OS type, memory settings, and
     <span class="guimenu">Name</span> the virtual machine, for example
     <code class="literal">libvirt-virtual-machine</code>.
    </p></li><li class="step"><p>
     Finish the configuration and start the VM.
    </p></li><li class="step"><p>
     Verify that the newly created domain exists with <code class="command">sudo virsh
     list</code>. If needed, specify the connection string, such as
    </p><div class="verbatim-wrap"><pre class="screen"><code class="command">virsh -c qemu+ssh://root@vm_host_hostname/system list</code>
Id    Name                           State
-----------------------------------------------
[...]
 9     libvirt-virtual-machine       running</pre></div></li><li class="step"><p>
     Log in to the VM and stop it before configuring it for use with Ceph.
    </p></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-cfg-vm" data-id-title="Configuring the VM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.4 </span><span class="title-name">Configuring the VM</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-cfg-vm">#</a></h2></div></div></div><p>
   In this chapter, we focus on configuring VMs for integration with Ceph
   using <code class="command">virsh</code>. <code class="command">virsh</code> commands often
   require root privileges (<code class="command">sudo</code>) and will not return
   appropriate results or notify you that root privileges are required. For a
   reference of <code class="command">virsh</code> commands, refer to <code class="command">man 1
   virsh</code> (requires the package <span class="package">libvirt-client</span> to
   be installed).
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Open the configuration file with <code class="command">virsh edit</code>
     <em class="replaceable">vm-domain-name</em>.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh edit libvirt-virtual-machine</pre></div></li><li class="step"><p>
     Under &lt;devices&gt; there should be a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-<em class="replaceable">SYSTEM-ARCH</em>&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;source file='/path/to/image/recent-linux.img'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;address type='drive' controller='0' bus='0' unit='0'/&gt;
    &lt;/disk&gt;</pre></div><p>
     Replace <code class="filename">/path/to/image/recent-linux.img</code> with the path
     to the OS image.
    </p><div id="id-1.4.7.2.9.3.2.4" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Use <code class="command">sudo virsh edit</code> instead of a text editor. If you
      edit the configuration file under <code class="filename">/etc/libvirt/qemu</code>
      with a text editor, <code class="systemitem">libvirt</code> may not recognize the change. If there is a
      discrepancy between the contents of the XML file under
      <code class="filename">/etc/libvirt/qemu</code> and the result of <code class="command">sudo
      virsh dumpxml</code> <em class="replaceable">vm-domain-name</em>, then
      your VM may not work properly.
     </p></div></li><li class="step"><p>
     Add the Ceph RBD image you previously created as a &lt;disk&gt; entry.
    </p><div class="verbatim-wrap"><pre class="screen">&lt;disk type='network' device='disk'&gt;
        &lt;source protocol='rbd' name='libvirt-pool/new-libvirt-image'&gt;
                &lt;host name='<em class="replaceable">monitor-host</em>' port='6789'/&gt;
        &lt;/source&gt;
        &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</pre></div><p>
     Replace <em class="replaceable">monitor-host</em> with the name of your
     host, and replace the pool and/or image name as necessary. You may add
     multiple &lt;host&gt; entries for your Ceph monitors. The
     <code class="literal">dev</code> attribute is the logical device name that will
     appear under the <code class="filename">/dev</code> directory of your VM. The
     optional bus attribute indicates the type of disk device to emulate. The
     valid settings are driver specific (for example ide, scsi, virtio, xen,
     usb or sata).
    </p></li><li class="step"><p>
     Save the file.
    </p></li><li class="step"><p>
     If your Ceph cluster has authentication enabled (it does by default),
     you must generate a secret. Open an editor of your choice and create a
     file called <code class="filename">secret.xml</code> with the following content:
    </p><div class="verbatim-wrap"><pre class="screen">&lt;secret ephemeral='no' private='no'&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.libvirt secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;</pre></div></li><li class="step"><p>
     Define the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;</pre></div></li><li class="step"><p>
     Get the <code class="literal">client.libvirt</code> key and save the key string to a
     file.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</pre></div></li><li class="step"><p>
     Set the UUID of the secret.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh secret-set-value --secret <em class="replaceable">uuid of secret</em> \
--base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</pre></div><p>
     You must also set the secret manually by adding the following
     <code class="literal">&lt;auth&gt;</code> entry to the
     <code class="literal">&lt;disk&gt;</code> element you entered earlier (replacing the
     uuid value with the result from the command line example above).
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh edit libvirt-virtual-machine</pre></div><p>
     Then, add <code class="literal">&lt;auth&gt;&lt;/auth&gt;</code> element to the
     domain configuration file:
    </p><div class="verbatim-wrap"><pre class="screen">...
&lt;/source&gt;
&lt;auth username='libvirt'&gt;
        &lt;secret type='ceph' uuid='9ec59067-fdbc-a6c0-03ff-df165c0587b8'/&gt;
&lt;/auth&gt;
&lt;target ...</pre></div><div id="id-1.4.7.2.9.3.8.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The exemplary ID is <code class="literal">libvirt</code>, not the Ceph name
      <code class="literal">client.libvirt</code> as generated at step 2 of
      <a class="xref" href="#ceph-libvirt-cfg-ceph" title="26.1. Configuring Ceph with libvirt">Section 26.1, “Configuring Ceph with <code class="systemitem">libvirt</code>”</a>. Ensure you use the ID component
      of the Ceph name you generated. If for some reason you need to regenerate
      the secret, you will need to execute <code class="command">sudo virsh
      secret-undefine</code> <em class="replaceable">uuid</em> before
      executing <code class="command">sudo virsh secret-set-value</code> again.
     </p></div></li></ol></div></div></section><section class="sect1" id="ceph-libvirt-summary" data-id-title="Summary"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">26.5 </span><span class="title-name">Summary</span> <a title="Permalink" class="permalink" href="#ceph-libvirt-summary">#</a></h2></div></div></div><p>
   Once you have configured the VM for use with Ceph, you can start the VM.
   To verify that the VM and Ceph are communicating, you may perform the
   following procedures.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Check to see if Ceph is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health</pre></div></li><li class="step"><p>
     Check to see if the VM is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh list</pre></div></li><li class="step"><p>
     Check to see if the VM is communicating with Ceph. Replace
     <em class="replaceable">vm-domain-name</em> with the name of your VM domain:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh qemu-monitor-command --hmp <em class="replaceable">vm-domain-name</em> 'info block'</pre></div></li><li class="step"><p>
     Check to see if the device from <code class="literal">&amp;target dev='hdb'
     bus='ide'/&gt;</code> appears under <code class="filename">/dev</code> or under
     <code class="filename">/proc/partitions</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>ls /dev
<code class="prompt user">&gt; </code>cat /proc/partitions</pre></div></li></ol></div></div></section></section><section class="chapter" id="cha-ceph-kvm" data-id-title="Ceph as a back-end for QEMU KVM instance"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">27 </span><span class="title-name">Ceph as a back-end for QEMU KVM instance</span> <a title="Permalink" class="permalink" href="#cha-ceph-kvm">#</a></h1></div></div></div><p>
  The most frequent Ceph use case involves providing block device images to
  virtual machines. For example, a user may create a 'golden' image with an OS
  and any relevant software in an ideal configuration. Then, the user takes a
  snapshot of the image. Finally, the user clones the snapshot (usually many
  times, see <a class="xref" href="#cha-ceph-snapshots-rbd" title="20.3. Snapshots">Section 20.3, “Snapshots”</a> for details). The ability
  to make copy-on-write clones of a snapshot means that Ceph can provision
  block device images to virtual machines quickly, because the client does not
  need to download an entire image each time it spins up a new virtual machine.
 </p><p>
  Ceph block devices can integrate with the QEMU virtual machines. For more
  information on QEMU KVM, see
  <a class="link" href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/part-virt-qemu.html" target="_blank">https://documentation.suse.com/sles/15-SP3/html/SLES-all/part-virt-qemu.html</a>.
 </p><section class="sect1" id="ceph-kvm-install" data-id-title="Installing qemu-block-rbd"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.1 </span><span class="title-name">Installing <code class="systemitem">qemu-block-rbd</code></span> <a title="Permalink" class="permalink" href="#ceph-kvm-install">#</a></h2></div></div></div><p>
   In order to use Ceph block devices, QEMU needs to have the appropriate
   driver installed. Check whether the <code class="systemitem">qemu-block-rbd</code>
   package is installed, and install it if needed:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper install qemu-block-rbd</pre></div></section><section class="sect1" id="ceph-kvm-usage" data-id-title="Using QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.2 </span><span class="title-name">Using QEMU</span> <a title="Permalink" class="permalink" href="#ceph-kvm-usage">#</a></h2></div></div></div><p>
   The QEMU command line expects you to specify the pool name and image name.
   You may also specify a snapshot name.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em>@<em class="replaceable">snapshot-name</em><em class="replaceable">:option1=value1</em><em class="replaceable">:option2=value2...</em></pre></div><p>
   For example, specifying the <em class="replaceable">id</em> and
   <em class="replaceable">conf</em> options might look like the following:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img <em class="replaceable">command</em> <em class="replaceable">options</em> \
rbd:<em class="replaceable">pool_name</em>/<em class="replaceable">image_name</em>:<code class="option">id=glance:conf=/etc/ceph/ceph.conf</code></pre></div></section><section class="sect1" id="creating-images-qemu" data-id-title="Creating images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.3 </span><span class="title-name">Creating images with QEMU</span> <a title="Permalink" class="permalink" href="#creating-images-qemu">#</a></h2></div></div></div><p>
   You can create a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   create. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img create -f raw rbd:pool1/image1 10G
Formatting 'rbd:pool1/image1', fmt=raw size=10737418240 nocow=off cluster_size=0</pre></div><div id="id-1.4.7.3.7.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The <code class="literal">raw</code> data format is really the only sensible format
    option to use with RBD. Technically, you could use other QEMU-supported
    formats such as <code class="literal">qcow2</code>, but doing so would add additional
    overhead, and would also render the volume unsafe for virtual machine live
    migration when caching is enabled.
   </p></div></section><section class="sect1" id="resizing-images-qemu" data-id-title="Resizing images with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.4 </span><span class="title-name">Resizing images with QEMU</span> <a title="Permalink" class="permalink" href="#resizing-images-qemu">#</a></h2></div></div></div><p>
   You can resize a block device image from QEMU. You must specify
   <code class="literal">rbd</code>, the pool name, and the name of the image you want to
   resize. You must also specify the size of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em> <em class="replaceable">size</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img resize rbd:pool1/image1 9G
Image resized.</pre></div></section><section class="sect1" id="retrieving-image-info-qemu" data-id-title="Retrieving image info with QEMU"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.5 </span><span class="title-name">Retrieving image info with QEMU</span> <a title="Permalink" class="permalink" href="#retrieving-image-info-qemu">#</a></h2></div></div></div><p>
   You can retrieve block device image information from QEMU. You must
   specify <code class="literal">rbd</code>, the pool name, and the name of the image.
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:<em class="replaceable">pool-name</em>/<em class="replaceable">image-name</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img info rbd:pool1/image1
image: rbd:pool1/image1
file format: raw
virtual size: 9.0G (9663676416 bytes)
disk size: unavailable
cluster_size: 4194304</pre></div></section><section class="sect1" id="running-qemu-rbd" data-id-title="Running QEMU with RBD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.6 </span><span class="title-name">Running QEMU with RBD</span> <a title="Permalink" class="permalink" href="#running-qemu-rbd">#</a></h2></div></div></div><p>
   QEMU can access an image as a virtual block device directly via
   <code class="systemitem">librbd</code>. This avoids an additional context switch,
   and can take advantage of RBD caching.
  </p><p>
   You can use <code class="command">qemu-img</code> to convert existing virtual machine
   images to Ceph block device images. For example, if you have a qcow2
   image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen">qemu-img convert -f qcow2 -O raw sles12.qcow2 rbd:pool1/sles12</pre></div><p>
   To run a virtual machine booting from that image, you could run:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12</pre></div><p>
   RBD caching can significantly improve performance. QEMU’s cache options
   control <code class="systemitem">librbd</code> caching:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu -m 1024 -drive format=rbd,file=rbd:pool1/sles12,cache=writeback</pre></div><p>
   For more information on RBD caching, refer to
   <a class="xref" href="#rbd-cache-settings" title="20.5. Cache settings">Section 20.5, “Cache settings”</a>.
  </p></section><section class="sect1" id="enabling-dicard-trim" data-id-title="Enabling discard and TRIM"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.7 </span><span class="title-name">Enabling discard and TRIM</span> <a title="Permalink" class="permalink" href="#enabling-dicard-trim">#</a></h2></div></div></div><p>
   Ceph block devices support the discard operation. This means that a guest
   can send TRIM requests to let a Ceph block device reclaim unused space.
   This can be enabled in the guest by mounting <code class="systemitem">XFS</code>
   with the discard option.
  </p><p>
   For this to be available to the guest, it must be explicitly enabled for the
   block device. To do this, you must specify a
   <code class="option">discard_granularity</code> associated with the drive:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>qemu -m 1024 -drive format=raw,file=rbd:pool1/sles12,id=drive1,if=none \
-device driver=ide-hd,drive=drive1,discard_granularity=512</pre></div><div id="id-1.4.7.3.11.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The above example uses the IDE driver. The virtio driver does not support
    discard.
   </p></div><p>
   If using <code class="systemitem">libvirt</code>, edit your libvirt domain’s
   configuration file using <code class="command">virsh edit</code> to include the
   <code class="literal">xmlns:qemu</code> value. Then, add a <code class="literal">qemu:commandline
   block</code> as a child of that domain. The following example shows how
   to set two devices with <code class="literal">qemu id=</code> to different
   <code class="literal">discard_granularity</code> values.
  </p><div class="verbatim-wrap"><pre class="screen">&lt;domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
 &lt;qemu:commandline&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-0.discard_granularity=4096'/&gt;
  &lt;qemu:arg value='-set'/&gt;
  &lt;qemu:arg value='block.scsi0-0-1.discard_granularity=65536'/&gt;
 &lt;/qemu:commandline&gt;
&lt;/domain&gt;</pre></div></section><section class="sect1" id="qemu-cache-options" data-id-title="Setting QEMU cache options"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">27.8 </span><span class="title-name">Setting QEMU cache options</span> <a title="Permalink" class="permalink" href="#qemu-cache-options">#</a></h2></div></div></div><p>
   QEMU’s cache options correspond to the following Ceph RBD Cache
   settings.
  </p><p>
   Writeback:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true</pre></div><p>
   Writethrough:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = true
rbd_cache_max_dirty = 0</pre></div><p>
   None:
  </p><div class="verbatim-wrap"><pre class="screen">rbd_cache = false</pre></div><p>
   QEMU’s cache settings override Ceph’s default settings (settings
   that are not explicitly set in the Ceph configuration file). If you
   explicitly set RBD Cache settings in your Ceph configuration file (refer
   to <a class="xref" href="#rbd-cache-settings" title="20.5. Cache settings">Section 20.5, “Cache settings”</a>), your Ceph settings override the
   QEMU cache settings. If you set cache settings on the QEMU command line,
   the QEMU command line settings override the Ceph configuration file
   settings.
  </p></section></section></div><div class="part" id="part-cluster-configuration" data-id-title="Configuring a Cluster"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part VI </span><span class="title-name">Configuring a Cluster </span><a title="Permalink" class="permalink" href="#part-cluster-configuration">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-configuration"><span class="title-number">28 </span><span class="title-name">Ceph cluster configuration</span></a></span></li><dd class="toc-abstract"><p>
  This chapter describes how to configure the Ceph cluster by means of
  configuration options.
 </p></dd><li><span class="chapter"><a href="#cha-mgr-modules"><span class="title-number">29 </span><span class="title-name">Ceph Manager modules</span></a></span></li><dd class="toc-abstract"><p>The architecture of the Ceph Manager (refer to Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.2.3 “Ceph nodes and daemons” for a brief introduction) allows extending its functionality via modules, such as 'dashboard' (see Part I, “Ceph Dashboard”), 'prometheus' (see Chapter 16, Monitor…</p></dd><li><span class="chapter"><a href="#cha-storage-cephx"><span class="title-number">30 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></span></li><dd class="toc-abstract"><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-configuration" data-id-title="Ceph cluster configuration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">28 </span><span class="title-name">Ceph cluster configuration</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration">#</a></h1></div></div></div><p>
  This chapter describes how to configure the Ceph cluster by means of
  configuration options.
 </p><section class="sect1" id="cha-ceph-configuration-ceph-conf" data-id-title="Configure the ceph.conf file"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.1 </span><span class="title-name">Configure the <code class="filename">ceph.conf</code> file</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-ceph-conf">#</a></h2></div></div></div><p>
   cephadm uses a basic <code class="filename">ceph.conf</code> file that only
   contains a minimal set of options for connecting to MONs, authenticating,
   and fetching configuration information. In most cases, this is limited to
   the <code class="option">mon_host</code> option (although this can be avoided through
   the use of DNS SRV records).
  </p><div id="id-1.4.8.2.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The <code class="filename">ceph.conf</code> file no longer serves as a central place
    for storing cluster configuration, in favor of the configuration database
    (see <a class="xref" href="#cha-ceph-configuration-db" title="28.2. Configuration database">Section 28.2, “Configuration database”</a>).
   </p><p>
    If you still need to change cluster configuration via the
    <code class="filename">ceph.conf</code> file—for example, because you use a
    client that does not support reading options form the configuration
    database—you need to run the following command, and take care of
    maintaining and distributing the <code class="filename">ceph.conf</code> file across
    the whole cluster:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/cephadm/manage_etc_ceph_ceph_conf false</pre></div></div><section class="sect2" id="cha-ceph-configuration-ceph-conf-containers" data-id-title="Accessing ceph.conf inside container images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.1.1 </span><span class="title-name">Accessing <code class="filename">ceph.conf</code> inside container images</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-ceph-conf-containers">#</a></h3></div></div></div><p>
    Although Ceph daemons run inside containers, you can still access their
    <code class="filename">ceph.conf</code> configuration file. It is
    <span class="emphasis"><em>bind-mounted</em></span> as the following file on the host system:
   </p><div class="verbatim-wrap"><pre class="screen">/var/lib/ceph/<em class="replaceable">CLUSTER_FSID</em>/<em class="replaceable">DAEMON_NAME</em>/config</pre></div><p>
    Replace <em class="replaceable">CLUSTER_FSID</em> with the unique FSID of the
    running cluster as returned by the <code class="command">ceph fsid</code> command,
    and <em class="replaceable">DAEMON_NAME</em> with the name of the specific
    daemon as listed by the <code class="command">ceph orch ps</code> command. For
    example:
   </p><div class="verbatim-wrap"><pre class="screen">/var/lib/ceph/b4b30c6e-9681-11ea-ac39-525400d7702d/osd.2/config</pre></div><p>
    To modify the configuration of a daemon, edit its
    <code class="filename">config</code> file and restart it:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart ceph-<em class="replaceable">CLUSTER_FSID</em>-<em class="replaceable">DAEMON_NAME</em></pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart ceph-b4b30c6e-9681-11ea-ac39-525400d7702d-osd.2</pre></div><div id="id-1.4.8.2.4.4.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     All custom settings will be lost after cephadm redeploys the daemon.
    </p></div></section></section><section class="sect1" id="cha-ceph-configuration-db" data-id-title="Configuration database"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.2 </span><span class="title-name">Configuration database</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db">#</a></h2></div></div></div><p>
   Ceph Monitors manage a central database of configuration options that affect the
   behavior of the whole cluster.
  </p><section class="sect2" id="cha-ceph-configuration-db-sections" data-id-title="Configuring sections and masks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.2.1 </span><span class="title-name">Configuring sections and masks</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db-sections">#</a></h3></div></div></div><p>
    Configuration options stored by the MON can live in a
    <span class="emphasis"><em>global</em></span> section, <span class="emphasis"><em>daemon type</em></span>
    section, or a <span class="emphasis"><em>specific daemon</em></span> section. In addition,
    options may also have a <span class="emphasis"><em>mask</em></span> associated with them to
    further restrict to which daemons or clients the option applies. Masks have
    two forms:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <em class="replaceable">TYPE</em>:<em class="replaceable">LOCATION</em> where
      <em class="replaceable">TYPE</em> is a CRUSH property such as
      <code class="literal">rack</code> or <code class="literal">host</code>, while
      <em class="replaceable">LOCATION</em> is a value for that property.
     </p><p>
      For example, <code class="literal">host:example_host</code> will limit the option
      only to daemons or clients running on a particular host.
     </p></li><li class="listitem"><p>
      <em class="replaceable">CLASS</em>:<em class="replaceable">DEVICE_CLASS</em>
      where <em class="replaceable">DEVICE_CLASS</em> is the name of a CRUSH
      device class such as <code class="literal">hdd</code> or <code class="literal">ssd</code>.
      For example, <code class="literal">class:ssd</code> will limit the option only to
      OSDs backed by SSDs. This mask has no effect for non-OSD daemons or
      clients.
     </p></li></ul></div></section><section class="sect2" id="cha-ceph-configuration-db-commands" data-id-title="Setting and reading configuration options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.2.2 </span><span class="title-name">Setting and reading configuration options</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db-commands">#</a></h3></div></div></div><p>
    Use the following commands to set or read cluster configuration options.
    The <em class="replaceable">WHO</em> parameter may be a section name, a mask,
    or a combination of both separated by a slash (/) character. For example,
    <code class="literal">osd/rack:foo</code> represents all OSD daemons in the rack
    called <code class="literal">foo</code>.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.5.4.3.1"><span class="term"><code class="command">ceph config dump</code></span></dt><dd><p>
       Dumps the entire configuration database for a whole cluster.
      </p></dd><dt id="id-1.4.8.2.5.4.3.2"><span class="term"><code class="command">ceph config get <em class="replaceable">WHO</em></code></span></dt><dd><p>
       Dumps the configuration for a specific daemon or client (for example,
       <code class="literal">mds.a</code>), as stored in the configuration database.
      </p></dd><dt id="id-1.4.8.2.5.4.3.3"><span class="term"><code class="command">ceph config set <em class="replaceable">WHO</em> <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></code></span></dt><dd><p>
       Sets the configuration option to the specified value in the
       configuration database.
      </p></dd><dt id="id-1.4.8.2.5.4.3.4"><span class="term"><code class="command">ceph config show <em class="replaceable">WHO</em></code></span></dt><dd><p>
       Shows the reported running configuration for a running daemon. These
       settings may differ from those stored by the monitors if there are also
       local configuration files in use, or options have been overridden on the
       command line or at runtime. The source of the option values is reported
       as part of the output.
      </p></dd><dt id="id-1.4.8.2.5.4.3.5"><span class="term"><code class="command">ceph config assimilate-conf -i <em class="replaceable">INPUT_FILE</em> -o <em class="replaceable">OUTPUT_FILE</em></code></span></dt><dd><p>
       Imports a configuration file specified as
       <em class="replaceable">INPUT_FILE</em> and stores any valid options into
       the configuration database. Any settings that are unrecognized, invalid,
       or cannot be controlled by the monitor will be returned in an
       abbreviated file stored as <em class="replaceable">OUTPUT_FILE</em>. This
       command is useful for transitioning from legacy configuration files to
       centralized monitor-based configuration.
      </p></dd></dl></div></section><section class="sect2" id="cha-ceph-configuration-db-runtime" data-id-title="Configuring daemons at runtime"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.2.3 </span><span class="title-name">Configuring daemons at runtime</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db-runtime">#</a></h3></div></div></div><p>
    In most cases, Ceph allows you to make changes to the configuration of a
    daemon at runtime. This is useful, for example, when you need to increase
    or decrease the amount of logging output, or when performing runtime
    cluster optimization.
   </p><p>
    You can update the values of configuration options with the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set <em class="replaceable">DAEMON</em> <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div><p>
    For example, to adjust the debugging log level on a specific OSD, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set osd.123 debug_ms 20</pre></div><div id="id-1.4.8.2.5.5.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the same option is also customized in a local configuration file, the
     monitor setting will be ignored because it has lower priority than the
     configuration file.
    </p></div><section class="sect3" id="cha-ceph-configuration-db-runtime-override" data-id-title="Overriding values"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.2.3.1 </span><span class="title-name">Overriding values</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db-runtime-override">#</a></h4></div></div></div><p>
     You can temporarily modify an option value using the
     <code class="command">tell</code> or <code class="command">daemon</code> subcommands. Such
     modification only affect the running process and is discarded after the
     daemon or process restarts.
    </p><p>
     There are two ways to override values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Use the <code class="command">tell</code> subcommand to send a message to a
       specific daemon from any cluster node:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell <em class="replaceable">DAEMON</em> config set <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell osd.123 config set debug_osd 20</pre></div><div id="id-1.4.8.2.5.5.8.4.1.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
        The <code class="command">tell</code> subcommand accepts wild cards as daemon
        identifiers. For example, to adjust the debug level on all OSD daemons,
        run:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph tell osd.* config set debug_osd 20</pre></div></div></li><li class="listitem"><p>
       Use the <code class="command">daemon</code> subcommand to connect to a specific
       daemon process via a socket in <code class="filename">/var/run/ceph</code> from
       the node where the process is running:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.<em class="replaceable">ID</em> -- ceph daemon <em class="replaceable">DAEMON</em> config set <em class="replaceable">OPTION</em> <em class="replaceable">VALUE</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.4 -- ceph daemon osd.4 config set debug_osd 20</pre></div></li></ul></div><div id="id-1.4.8.2.5.5.8.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      When viewing runtime settings with the <code class="command">ceph config
      show</code> command (see
      <a class="xref" href="#cha-ceph-configuration-db-runtime-view" title="28.2.3.2. Viewing runtime settings">Section 28.2.3.2, “Viewing runtime settings”</a>), temporarily
      overridden values will be shown with a source
      <code class="literal">override</code>.
     </p></div></section><section class="sect3" id="cha-ceph-configuration-db-runtime-view" data-id-title="Viewing runtime settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.2.3.2 </span><span class="title-name">Viewing runtime settings</span> <a title="Permalink" class="permalink" href="#cha-ceph-configuration-db-runtime-view">#</a></h4></div></div></div><p>
     To view all options set for a daemon:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config show-with-defaults osd.0</pre></div><p>
     To view all non-default options set for a daemon:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config show osd.0</pre></div><p>
     To inspect a specific option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config show osd.0 debug_osd</pre></div><p>
     You can also connect to a running daemon from the node where its process
     is running, and observe its configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.0 -- ceph daemon osd.0 config show</pre></div><p>
     To view only non-default settings:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.0 -- ceph daemon osd.0 config diff</pre></div><p>
     To inspect a specific option:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm enter --name osd.0 -- ceph daemon osd.0 config get debug_osd</pre></div></section></section></section><section class="sect1" id="cha-ceph-config-key-store" data-id-title="config-key store"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.3 </span><span class="title-name"><code class="systemitem">config-key</code> store</span> <a title="Permalink" class="permalink" href="#cha-ceph-config-key-store">#</a></h2></div></div></div><p>
   <code class="systemitem">config-key</code> is a general-purpose service offered by the Ceph Monitors. It
   simplifies managing configuration keys by storing key-value pairs
   persistently. <code class="systemitem">config-key</code> is mainly used by Ceph tools and daemons.
  </p><div id="id-1.4.8.2.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    After you add a new key or modify an existing one, restart the affected
    service for the changes to take effect. Find more details about operating
    Ceph services in <a class="xref" href="#cha-ceph-operating" title="Chapter 14. Operation of Ceph services">Chapter 14, <em>Operation of Ceph services</em></a>.
   </p></div><p>
   Use the <code class="command">config-key</code> command to operate the <code class="systemitem">config-key</code>
   store. The <code class="command">config-key</code> command uses the following
   subcommands:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.6.5.1"><span class="term"><code class="command">ceph config-key rm <em class="replaceable">KEY</em></code></span></dt><dd><p>
      Deletes the specified key.
     </p></dd><dt id="id-1.4.8.2.6.5.2"><span class="term"><code class="command">ceph config-key exists <em class="replaceable">KEY</em></code></span></dt><dd><p>
      Checks for the existence of the specified key.
     </p></dd><dt id="id-1.4.8.2.6.5.3"><span class="term"><code class="command">ceph config-key get <em class="replaceable">KEY</em></code></span></dt><dd><p>
      Retrieves the value of the specified key.
     </p></dd><dt id="id-1.4.8.2.6.5.4"><span class="term"><code class="command">ceph config-key ls</code></span></dt><dd><p>
      Lists all keys.
     </p></dd><dt id="id-1.4.8.2.6.5.5"><span class="term"><code class="command">ceph config-key dump</code></span></dt><dd><p>
      Dumps all keys and their values.
     </p></dd><dt id="id-1.4.8.2.6.5.6"><span class="term"><code class="command">ceph config-key set <em class="replaceable">KEY</em> <em class="replaceable">VALUE</em></code></span></dt><dd><p>
      Stores the specified key with the given value.
     </p></dd></dl></div><section class="sect2" id="cha-ceph-config-key-store-iscsi" data-id-title="iSCSI Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.3.1 </span><span class="title-name">iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-config-key-store-iscsi">#</a></h3></div></div></div><p>
    The iSCSI Gateway uses the <code class="systemitem">config-key</code> store to save or read its configuration
    options. All iSCSI Gateway related keys are prefixed with the
    <code class="literal">iscsi</code> string, for example:
   </p><div class="verbatim-wrap"><pre class="screen">iscsi/trusted_ip_list
iscsi/api_port
iscsi/api_user
iscsi/api_password
iscsi/api_secure</pre></div><p>
    If you need, for example, two sets of configuration options, extend the
    prefix with another descriptive keyword, for example
    <code class="literal">datacenterA</code> and <code class="literal">datacenterB</code>:
   </p><div class="verbatim-wrap"><pre class="screen">iscsi/datacenterA/trusted_ip_list
iscsi/datacenterA/api_port
[...]
iscsi/datacenterB/trusted_ip_list
iscsi/datacenterB/api_port
[...]</pre></div></section></section><section class="sect1" id="config-osd-and-bluestore" data-id-title="Ceph OSD and BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.4 </span><span class="title-name">Ceph OSD and BlueStore</span> <a title="Permalink" class="permalink" href="#config-osd-and-bluestore">#</a></h2></div></div></div><section class="sect2" id="config-auto-cache-sizing" data-id-title="Configuring automatic cache sizing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.4.1 </span><span class="title-name">Configuring automatic cache sizing</span> <a title="Permalink" class="permalink" href="#config-auto-cache-sizing">#</a></h3></div></div></div><p>
    BlueStore can be configured to automatically resize its caches when
    <code class="option">tc_malloc</code> is configured as the memory allocator and the
    <code class="option">bluestore_cache_autotune</code> setting is enabled. This option
    is currently enabled by default. BlueStore will attempt to keep OSD heap
    memory usage under a designated target size via the
    <code class="option">osd_memory_target</code> configuration option. This is a best
    effort algorithm and caches will not shrink smaller than the amount
    specified by <code class="option">osd_memory_cache_min</code>. Cache ratios will be
    chosen based on a hierarchy of priorities. If priority information is not
    available, the <code class="option">bluestore_cache_meta_ratio</code> and
    <code class="option">bluestore_cache_kv_ratio</code> options are used as fallbacks.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.7.2.3.1"><span class="term">bluestore_cache_autotune</span></dt><dd><p>
       Automatically tunes the ratios assigned to different BlueStore caches
       while respecting minimum values. Default is <code class="option">True</code>.
      </p></dd><dt id="id-1.4.8.2.7.2.3.2"><span class="term">osd_memory_target</span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, try to keep this
       many bytes mapped in memory.
      </p><div id="id-1.4.8.2.7.2.3.2.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        This may not exactly match the RSS memory usage of the process. While
        the total amount of heap memory mapped by the process should generally
        stay close to this target, there is no guarantee that the kernel will
        actually reclaim memory that has been unmapped.
       </p></div></dd><dt id="id-1.4.8.2.7.2.3.3"><span class="term">osd_memory_cache_min</span></dt><dd><p>
       When <code class="option">tc_malloc</code> and
       <code class="option">bluestore_cache_autotune</code> are enabled, set the minimum
       amount of memory used for caches.
      </p><div id="id-1.4.8.2.7.2.3.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        Setting this value too low can result in significant cache thrashing.
       </p></div></dd></dl></div></section></section><section class="sect1" id="config-ogw" data-id-title="Ceph Object Gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">28.5 </span><span class="title-name">Ceph Object Gateway</span> <a title="Permalink" class="permalink" href="#config-ogw">#</a></h2></div></div></div><p>
   You can influence the Object Gateway behavior by a number of options. If an option is
   not specified, its default value is used. A complete list of the Object Gateway
   options follows:
  </p><section class="sect2" id="config-ogw-general-settings" data-id-title="General Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.1 </span><span class="title-name">General Settings</span> <a title="Permalink" class="permalink" href="#config-ogw-general-settings">#</a></h3></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.8.3.2.1"><span class="term">rgw_frontends</span></dt><dd><p>
       Configures the HTTP front-end(s). Specify multiple front-ends in a
       comma-delimited list. Each front-end configuration may include a list of
       options separated by spaces, where each option is in the form
       “key=value” or “key”. Default is <code class="literal">beast
       port=7480</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.2"><span class="term">rgw_data</span></dt><dd><p>
       Sets the location of the data files for the Object Gateway. Default is
       <code class="filename">/var/lib/ceph/radosgw/<em class="replaceable">CLUSTER_ID</em></code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.3"><span class="term">rgw_enable_apis</span></dt><dd><p>
       Enables the specified APIs. Default is 's3, swift, swift_auth, admin All
       APIs'.
      </p></dd><dt id="id-1.4.8.2.8.3.2.4"><span class="term">rgw_cache_enabled</span></dt><dd><p>
       Enables or disables the Object Gateway cache. Default is <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.5"><span class="term">rgw_cache_lru_size</span></dt><dd><p>
       The number of entries in the Object Gateway cache. Default is 10000.
      </p></dd><dt id="id-1.4.8.2.8.3.2.6"><span class="term">rgw_socket_path</span></dt><dd><p>
       The socket path for the domain socket.
       <code class="option">FastCgiExternalServer</code> uses this socket. If you do not
       specify a socket path, the Object Gateway will not run as an external server. The
       path you specify here needs to be the same as the path specified in the
       <code class="filename">rgw.conf</code> file.
      </p></dd><dt id="id-1.4.8.2.8.3.2.7"><span class="term">rgw_fcgi_socket_backlog</span></dt><dd><p>
       The socket backlog for fcgi. Default is 1024.
      </p></dd><dt id="id-1.4.8.2.8.3.2.8"><span class="term">rgw_host</span></dt><dd><p>
       The host for the Object Gateway instance. It can be an IP address or a host name.
       Default is <code class="literal">0.0.0.0</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.9"><span class="term">rgw_port</span></dt><dd><p>
       The port number where the instance listens for requests. If not
       specified, the Object Gateway runs external FastCGI.
      </p></dd><dt id="id-1.4.8.2.8.3.2.10"><span class="term">rgw_dns_name</span></dt><dd><p>
       The DNS name of the served domain.
      </p></dd><dt id="id-1.4.8.2.8.3.2.11"><span class="term">rgw_script_uri</span></dt><dd><p>
       The alternative value for the SCRIPT_URI if not set in the request.
      </p></dd><dt id="id-1.4.8.2.8.3.2.12"><span class="term">rgw_request_uri</span></dt><dd><p>
       The alternative value for the REQUEST_URI if not set in the request.
      </p></dd><dt id="id-1.4.8.2.8.3.2.13"><span class="term">rgw_print_continue</span></dt><dd><p>
       Enable 100-continue if it is operational. Default is
       <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.14"><span class="term">rgw_remote_addr_param</span></dt><dd><p>
       The remote address parameter. For example, the HTTP field containing the
       remote address, or the X-Forwarded-For address if a reverse proxy is
       operational. Default is <code class="literal">REMOTE_ADDR</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.15"><span class="term">rgw_op_thread_timeout</span></dt><dd><p>
       The timeout in seconds for open threads. Default is 600.
      </p></dd><dt id="id-1.4.8.2.8.3.2.16"><span class="term">rgw_op_thread_suicide_timeout</span></dt><dd><p>
       The time timeout in seconds before the Object Gateway process dies. Disabled if
       set to 0 (default).
      </p></dd><dt id="id-1.4.8.2.8.3.2.17"><span class="term">rgw_thread_pool_size</span></dt><dd><p>
       Number of threads for the Beast server. Increase to a higher value if
       you need to serve more requests. Defaults to 100 threads.
      </p></dd><dt id="id-1.4.8.2.8.3.2.18"><span class="term">rgw_num_rados_handles</span></dt><dd><p>
       The number of RADOS cluster handles for Object Gateway. Each Object Gateway worker thread
       now gets to pick a RADOS handle for its lifetime. This option may be
       deprecated and removed in future releases. Default is 1.
      </p></dd><dt id="id-1.4.8.2.8.3.2.19"><span class="term">rgw_num_control_oids</span></dt><dd><p>
       The number of notification objects used for cache synchronization
       between different Object Gateway instances. Default is 8.
      </p></dd><dt id="id-1.4.8.2.8.3.2.20"><span class="term">rgw_init_timeout</span></dt><dd><p>
       The number of seconds before the Object Gateway gives up on initialization.
       Default is 30.
      </p></dd><dt id="id-1.4.8.2.8.3.2.21"><span class="term">rgw_mime_types_file</span></dt><dd><p>
       The path and location of the MIME types. Used for Swift auto-detection
       of object types. Default is <code class="filename">/etc/mime.types</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.22"><span class="term">rgw_gc_max_objs</span></dt><dd><p>
       The maximum number of objects that may be handled by garbage collection
       in one garbage collection processing cycle. Default is 32.
      </p></dd><dt id="id-1.4.8.2.8.3.2.23"><span class="term">rgw_gc_obj_min_wait</span></dt><dd><p>
       The minimum wait time before the object may be removed and handled by
       garbage collection processing. Default is 2 * 3600.
      </p></dd><dt id="id-1.4.8.2.8.3.2.24"><span class="term">rgw_gc_processor_max_time</span></dt><dd><p>
       The maximum time between the beginning of two consecutive garbage
       collection processing cycles. Default is 3600.
      </p></dd><dt id="id-1.4.8.2.8.3.2.25"><span class="term">rgw_gc_processor_period</span></dt><dd><p>
       The cycle time for garbage collection processing. Default is 3600.
      </p></dd><dt id="id-1.4.8.2.8.3.2.26"><span class="term">rgw_s3_success_create_obj_status</span></dt><dd><p>
       The alternate success status response for <code class="literal">create-obj</code>.
       Default is 0.
      </p></dd><dt id="id-1.4.8.2.8.3.2.27"><span class="term">rgw_resolve_cname</span></dt><dd><p>
       Whether the Object Gateway should use DNS CNAME record of the request host name
       field (if host name is not equal to the Object Gateway DNS name). Default is
       <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.28"><span class="term">rgw_obj_stripe_size</span></dt><dd><p>
       The size of an object stripe for Object Gateway objects. Default is <code class="literal">4
       &lt;&lt; 20</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.29"><span class="term">rgw_extended_http_attrs</span></dt><dd><p>
       Add a new set of attributes that can be set on an entity (for example, a
       user, a bucket, or an object). These extra attributes can be set through
       HTTP header fields when putting the entity or modifying it using the
       POST method. If set, these attributes will return as HTTP fields when
       requesting GET/HEAD on the entity. Default is <code class="literal">content_foo,
       content_bar, x-foo-bar</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.30"><span class="term">rgw_exit_timeout_secs</span></dt><dd><p>
       Number of seconds to wait for a process before exiting unconditionally.
       Default is 120.
      </p></dd><dt id="id-1.4.8.2.8.3.2.31"><span class="term">rgw_get_obj_window_size</span></dt><dd><p>
       The window size in bytes for a single object request. Default is
       <code class="literal">16 &lt;&lt; 20</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.32"><span class="term">rgw_get_obj_max_req_size</span></dt><dd><p>
       The maximum request size of a single GET operation sent to the Ceph
       Storage Cluster. Default is <code class="literal">4 &lt;&lt; 20</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.33"><span class="term">rgw_relaxed_s3_bucket_names</span></dt><dd><p>
       Enables relaxed S3 bucket name rules for US region buckets. Default is
       <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.34"><span class="term">rgw_list_buckets_max_chunk</span></dt><dd><p>
       The maximum number of buckets to retrieve in a single operation when
       listing user buckets. Default is 1000.
      </p></dd><dt id="id-1.4.8.2.8.3.2.35"><span class="term">rgw_override_bucket_index_max_shards</span></dt><dd><p>
       Represents the number of shards for the bucket index object. Setting 0
       (default) indicates there is no sharding. It is not recommended to set a
       value too large (for example 1000) as it increases the cost for bucket
       listing. This variable should be set in the client or global sections so
       that it is automatically applied to <code class="command">radosgw-admin</code>
       commands.
      </p></dd><dt id="id-1.4.8.2.8.3.2.36"><span class="term">rgw_curl_wait_timeout_ms</span></dt><dd><p>
       The timeout in milliseconds for certain <code class="command">curl</code> calls.
       Default is 1000.
      </p></dd><dt id="id-1.4.8.2.8.3.2.37"><span class="term">rgw_copy_obj_progress</span></dt><dd><p>
       Enables output of object progress during long copy operations. Default
       is <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.38"><span class="term">rgw_copy_obj_progress_every_bytes</span></dt><dd><p>
       The minimum bytes between copy progress output. Default is 1024 * 1024.
      </p></dd><dt id="id-1.4.8.2.8.3.2.39"><span class="term">rgw_admin_entry</span></dt><dd><p>
       The entry point for an admin request URL. Default is
       <code class="literal">admin</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.40"><span class="term">rgw_content_length_compat</span></dt><dd><p>
       Enable compatibility handling of FCGI requests with both CONTENT_LENGTH
       AND HTTP_CONTENT_LENGTH set. Default is <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.41"><span class="term">rgw_bucket_quota_ttl</span></dt><dd><p>
       The amount of time in seconds for which cached quota information is
       trusted. After this timeout, the quota information will be re-fetched
       from the cluster. Default is 600.
      </p></dd><dt id="id-1.4.8.2.8.3.2.42"><span class="term">rgw_user_quota_bucket_sync_interval</span></dt><dd><p>
       The amount of time in seconds for which the bucket quota information is
       accumulated before synchronizing to the cluster. During this time, other
       Object Gateway instances will not see the changes in the bucket quota stats
       related to operations on this instance. Default is 180.
      </p></dd><dt id="id-1.4.8.2.8.3.2.43"><span class="term">rgw_user_quota_sync_interval</span></dt><dd><p>
       The amount of time in seconds for which user quota information is
       accumulated before synchronizing to the cluster. During this time, other
       Object Gateway instances will not see the changes in the user quota stats related
       to operations on this instance. Default is 180.
      </p></dd><dt id="id-1.4.8.2.8.3.2.44"><span class="term">rgw_bucket_default_quota_max_objects</span></dt><dd><p>
       Default maximum number of objects per bucket. It is set on new users if
       no other quota is specified, and has no effect on existing users. This
       variable should be set in the client or global sections so that it is
       automatically applied to <code class="command">radosgw-admin</code> commands.
       Default is -1.
      </p></dd><dt id="id-1.4.8.2.8.3.2.45"><span class="term">rgw_bucket_default_quota_max_size</span></dt><dd><p>
       Default maximum capacity per bucket in bytes. It is set on new users if
       no other quota is specified, and has no effect on existing users.
       Default is -1.
      </p></dd><dt id="id-1.4.8.2.8.3.2.46"><span class="term">rgw_user_default_quota_max_objects</span></dt><dd><p>
       Default maximum number of objects for a user. This includes all objects
       in all buckets owned by the user. It is set on new users if no other
       quota is specified, and has no effect on existing users. Default is -1.
      </p></dd><dt id="id-1.4.8.2.8.3.2.47"><span class="term">rgw_user_default_quota_max_size</span></dt><dd><p>
       The value for user maximum size quota in bytes set on new users if no
       other quota is specified. It has no effect on existing users. Default is
       -1.
      </p></dd><dt id="id-1.4.8.2.8.3.2.48"><span class="term">rgw_verify_ssl</span></dt><dd><p>
       Verify SSL certificates while making requests. Default is
       <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.2.49"><span class="term">rgw_max_chunk_size</span></dt><dd><p>
       Maximum size of a chunk of data that will be read in a single operation.
       Increasing the value to 4 MB (4194304) will provide better performance
       when processing large objects. Default is 128 kB (131072).
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Multisite Settings </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.3.3">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.8.2.8.3.3.2"><span class="term">rgw_zone</span></dt><dd><p>
       The name of the zone for the gateway instance. If no zone is set, a
       cluster-wide default can be configured with the <code class="command">radosgw-admin
       zone default</code> command.
      </p></dd><dt id="id-1.4.8.2.8.3.3.3"><span class="term">rgw_zonegroup</span></dt><dd><p>
       The name of the zonegroup for the gateway instance. If no zonegroup is
       set, a cluster-wide default can be configured with the
       <code class="command">radosgw-admin zonegroup default</code> command.
      </p></dd><dt id="id-1.4.8.2.8.3.3.4"><span class="term">rgw_realm</span></dt><dd><p>
       The name of the realm for the gateway instance. If no realm is set, a
       cluster-wide default can be configured with the<code class="command">radosgw-admin
       realm default</code> command.
      </p></dd><dt id="id-1.4.8.2.8.3.3.5"><span class="term">rgw_run_sync_thread</span></dt><dd><p>
       If there are other zones in the realm to synchronize from, spawn threads
       to handle the synchronization of data and metadata. Default is
       <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.3.6"><span class="term">rgw_data_log_window</span></dt><dd><p>
       The data log entries window in seconds. Default is 30.
      </p></dd><dt id="id-1.4.8.2.8.3.3.7"><span class="term">rgw_data_log_changes_size</span></dt><dd><p>
       The number of in-memory entries to hold for the data changes log.
       Default is 1000.
      </p></dd><dt id="id-1.4.8.2.8.3.3.8"><span class="term">rgw_data_log_obj_prefix</span></dt><dd><p>
       The object name prefix for the data log. Default is 'data_log'.
      </p></dd><dt id="id-1.4.8.2.8.3.3.9"><span class="term">rgw_data_log_num_shards</span></dt><dd><p>
       The number of shards (objects) on which to keep the data changes log.
       Default is 128.
      </p></dd><dt id="id-1.4.8.2.8.3.3.10"><span class="term">rgw_md_log_max_shards</span></dt><dd><p>
       The maximum number of shards for the metadata log. Default is 64.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Swift Settings </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.3.4">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.8.2.8.3.4.2"><span class="term">rgw_enforce_swift_acls</span></dt><dd><p>
       Enforces the Swift Access Control List (ACL) settings. Default is
       <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.4.3"><span class="term">rgw_swift_token_expiration</span></dt><dd><p>
       The time in seconds for expiring a Swift token. Default is 24 * 3600.
      </p></dd><dt id="id-1.4.8.2.8.3.4.4"><span class="term">rgw_swift_url</span></dt><dd><p>
       The URL for the Ceph Object Gateway Swift API.
      </p></dd><dt id="id-1.4.8.2.8.3.4.5"><span class="term">rgw_swift_url_prefix</span></dt><dd><p>
       The URL prefix for the Swift StorageURL that goes in front of the '/v1'
       part. This allows to run several Gateway instances on the same host. For
       compatibility, setting this configuration variable to empty causes the
       default '/swift' to be used. Use explicit prefix '/' to start StorageURL
       at the root.
      </p><div id="id-1.4.8.2.8.3.4.5.2.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
        Setting this option to '/' will not work if S3 API is enabled. Keep in
        mind that disabling S3 will make it impossible to deploy the Object Gateway in
        the multisite configuration.
       </p></div></dd><dt id="id-1.4.8.2.8.3.4.6"><span class="term">rgw_swift_auth_url</span></dt><dd><p>
       Default URL for verifying v1 authentication tokens when the internal
       Swift authentication is not used.
      </p></dd><dt id="id-1.4.8.2.8.3.4.7"><span class="term">rgw_swift_auth_entry</span></dt><dd><p>
       The entry point for a Swift authentication URL. Default is
       <code class="literal">auth</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.4.8"><span class="term">rgw_swift_versioning_enabled</span></dt><dd><p>
       Enables the Object Versioning of OpenStack Object Storage API. This
       allows clients to put the <code class="literal">X-Versions-Location</code>
       attribute on containers that should be versioned. The attribute
       specifies the name of container storing archived versions. It must be
       owned by the same user as the versioned container for reasons of access
       control verification—ACLs are <span class="emphasis"><em>not</em></span> taken into
       consideration. Those containers cannot be versioned by the S3 object
       versioning mechanism. Default is <code class="literal">false</code>.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Logging Settings </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.3.5">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.8.2.8.3.5.2"><span class="term">rgw_log_nonexistent_bucket</span></dt><dd><p>
       Enables the Object Gateway to log a request for a non-existent bucket. Default is
       <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.5.3"><span class="term">rgw_log_object_name</span></dt><dd><p>
       The logging format for an object name. See the manual page <code class="command">man
       1 date</code> for details about format specifiers. Default is
       <code class="literal">%Y-%m-%d-%H-%i-%n</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.5.4"><span class="term">rgw_log_object_name_utc</span></dt><dd><p>
       Whether a logged object name includes a UTC time. If set to
       <code class="literal">false</code> (default), it uses the local time.
      </p></dd><dt id="id-1.4.8.2.8.3.5.5"><span class="term">rgw_usage_max_shards</span></dt><dd><p>
       The maximum number of shards for usage logging. Default is 32.
      </p></dd><dt id="id-1.4.8.2.8.3.5.6"><span class="term">rgw_usage_max_user_shards</span></dt><dd><p>
       The maximum number of shards used for a single user’s usage logging.
       Default is 1.
      </p></dd><dt id="id-1.4.8.2.8.3.5.7"><span class="term">rgw_enable_ops_log</span></dt><dd><p>
       Enable logging for each successful Object Gateway operation. Default is
       <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.5.8"><span class="term">rgw_enable_usage_log</span></dt><dd><p>
       Enable the usage log. Default is <code class="literal">false</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.5.9"><span class="term">rgw_ops_log_rados</span></dt><dd><p>
       Whether the operations log should be written to the Ceph Storage Cluster
       back-end. Default is <code class="literal">true</code>.
      </p></dd><dt id="id-1.4.8.2.8.3.5.10"><span class="term">rgw_ops_log_socket_path</span></dt><dd><p>
       The Unix domain socket for writing operations logs.
      </p></dd><dt id="id-1.4.8.2.8.3.5.11"><span class="term">rgw_ops_log_data_backlog</span></dt><dd><p>
       The maximum data backlog data size for operations logs written to a Unix
       domain socket. Default is 5 &lt;&lt; 20.
      </p></dd><dt id="id-1.4.8.2.8.3.5.12"><span class="term">rgw_usage_log_flush_threshold</span></dt><dd><p>
       The number of dirty merged entries in the usage log before flushing
       synchronously. Default is 1024.
      </p></dd><dt id="id-1.4.8.2.8.3.5.13"><span class="term">rgw_usage_log_tick_interval</span></dt><dd><p>
       Flush pending usage log data every 'n' seconds. Default is 30.
      </p></dd><dt id="id-1.4.8.2.8.3.5.14"><span class="term">rgw_log_http_headers</span></dt><dd><p>
       Comma-delimited list of HTTP headers to include in log entries. Header
       names are case-insensitive, and use the full header name with words
       separated by underscores. For example, 'http_x_forwarded_for',
       'http_x_special_k'.
      </p></dd><dt id="id-1.4.8.2.8.3.5.15"><span class="term">rgw_intent_log_object_name</span></dt><dd><p>
       The logging format for the intent log object name. See the manual page
       <code class="command">man 1 date</code> for details about format specifiers.
       Default is '%Y-%m-%d-%i-%n'.
      </p></dd><dt id="id-1.4.8.2.8.3.5.16"><span class="term">rgw_intent_log_object_name_utc</span></dt><dd><p>
       Whether the intent log object name includes a UTC time. If set to
       <code class="literal">false</code> (default), it uses the local time.
      </p></dd></dl></div><div class="variablelist"><div class="variablelist-title-wrap"><h6 class="variablelist-title"><span class="title-name">Keystone Settings </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.3.6">#</a></h6></div><dl class="variablelist"><dt id="id-1.4.8.2.8.3.6.2"><span class="term">rgw_keystone_url</span></dt><dd><p>
       The URL for the Keystone server.
      </p></dd><dt id="id-1.4.8.2.8.3.6.3"><span class="term">rgw_keystone_api_version</span></dt><dd><p>
       The version (2 or 3) of OpenStack Identity API that should be used for
       communication with the Keystone server. Default is 2.
      </p></dd><dt id="id-1.4.8.2.8.3.6.4"><span class="term">rgw_keystone_admin_domain</span></dt><dd><p>
       The name of the OpenStack domain with the administrator privilege when
       using OpenStack Identity API v3.
      </p></dd><dt id="id-1.4.8.2.8.3.6.5"><span class="term">rgw_keystone_admin_project</span></dt><dd><p>
       The name of the OpenStack project with the administrator privilege when
       using OpenStack Identity API v3. If not set, the value of the
       <code class="command">rgw keystone admin tenant</code> will be used instead.
      </p></dd><dt id="id-1.4.8.2.8.3.6.6"><span class="term">rgw_keystone_admin_token</span></dt><dd><p>
       The Keystone administrator token (shared secret). In the Object Gateway,
       authentication with the administrator token has priority over
       authentication with the administrator credentials (options <code class="option">rgw
       keystone admin user</code>, <code class="option">rgw keystone admin
       password</code>, <code class="option">rgw keystone admin tenant</code>,
       <code class="option">rgw keystone admin project</code>, and <code class="option">rgw keystone
       admin domain</code>). The administrator token feature is considered as
       deprecated.
      </p></dd><dt id="id-1.4.8.2.8.3.6.7"><span class="term">rgw_keystone_admin_tenant</span></dt><dd><p>
       The name of the OpenStack tenant with the administrator privilege
       (Service Tenant) when using OpenStack Identity API v2.
      </p></dd><dt id="id-1.4.8.2.8.3.6.8"><span class="term">rgw_keystone_admin_user</span></dt><dd><p>
       The name of the OpenStack user with the administrator privilege for
       Keystone authentication (Service User) when using OpenStack Identity API
       v2.
      </p></dd><dt id="id-1.4.8.2.8.3.6.9"><span class="term">rgw_keystone_admin_password</span></dt><dd><p>
       The password for the OpenStack administrator user when using OpenStack
       Identity API v2.
      </p></dd><dt id="id-1.4.8.2.8.3.6.10"><span class="term">rgw_keystone_accepted_roles</span></dt><dd><p>
       The roles required to serve requests. Default is 'Member, admin'.
      </p></dd><dt id="id-1.4.8.2.8.3.6.11"><span class="term">rgw_keystone_token_cache_size</span></dt><dd><p>
       The maximum number of entries in each Keystone token cache. Default is
       10000.
      </p></dd><dt id="id-1.4.8.2.8.3.6.12"><span class="term">rgw_keystone_revocation_interval</span></dt><dd><p>
       The number of seconds between token revocation checks. Default is 15 *
       60.
      </p></dd><dt id="id-1.4.8.2.8.3.6.13"><span class="term">rgw_keystone_verify_ssl</span></dt><dd><p>
       Verify SSL certificates while making token requests to Keystone. Default
       is <code class="literal">true</code>.
      </p></dd></dl></div><section class="sect3" id="sec-ceph-rgw-configuration-notes" data-id-title="Additional notes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.1.1 </span><span class="title-name">Additional notes</span> <a title="Permalink" class="permalink" href="#sec-ceph-rgw-configuration-notes">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.8.3.7.2.1"><span class="term">rgw_dns_name</span></dt><dd><p>
        Allows clients to use <code class="literal">vhost</code>-style buckets.
       </p><p>
        <code class="literal">vhost</code>-style access refers to the use of
        <em class="replaceable">bucketname</em>.<em class="replaceable">s3-endpoint</em>/<em class="replaceable">object-path</em>.
        This is in comparison to <code class="literal">path</code>-style access:
        <em class="replaceable">s3-endpoint</em>/<em class="replaceable">bucket</em>/<em class="replaceable">object</em>
       </p><p>
        If the <code class="literal">rgw dns name</code> is set, verify that the S3
        client is configured to direct requests to the endpoint specified by
        <code class="literal">rgw dns name</code>.
       </p></dd></dl></div></section></section><section class="sect2" id="config-ogw-http-frontends" data-id-title="Configuring HTTP front-ends"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">28.5.2 </span><span class="title-name">Configuring HTTP front-ends</span> <a title="Permalink" class="permalink" href="#config-ogw-http-frontends">#</a></h3></div></div></div><section class="sect3" id="config-http-beast" data-id-title="Beast"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.2.1 </span><span class="title-name">Beast</span> <a title="Permalink" class="permalink" href="#config-http-beast">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.8.4.2.2.1"><span class="term">port, ssl_port</span></dt><dd><p>
        IPv4 &amp; IPv6 listening port numbers. You can specify multiple port
        numbers:
       </p><div class="verbatim-wrap"><pre class="screen">port=80 port=8000 ssl_port=8080</pre></div><p>
        Default is 80.
       </p></dd><dt id="id-1.4.8.2.8.4.2.2.2"><span class="term">endpoint, ssl_endpoint</span></dt><dd><p>
        The listening addresses in the form 'address[:port]', where the address
        is an IPv4 address string in dotted decimal form, or an IPv6 address in
        hexadecimal notation surrounded by square brackets. Specifying an IPv6
        endpoint would listen to IPv6 only. The optional port number defaults
        to 80 for <code class="option">endpoint</code> and 443 for
        <code class="option">ssl_endpoint</code>. You can specify multiple addresses:
       </p><div class="verbatim-wrap"><pre class="screen">endpoint=[::1] endpoint=192.168.0.100:8000 ssl_endpoint=192.168.0.100:8080</pre></div></dd><dt id="id-1.4.8.2.8.4.2.2.3"><span class="term">ssl_private_key</span></dt><dd><p>
        Optional path to the private key file used for SSL-enabled endpoints.
        If not specified, the <code class="option">ssl_certificate</code> file is used as
        a private key.
       </p></dd><dt id="id-1.4.8.2.8.4.2.2.4"><span class="term">tcp_nodelay</span></dt><dd><p>
        If specified, the socket option will disable Nagle's algorithm on the
        connection. It means that packets will be sent as soon as possible
        instead of waiting for a full buffer or timeout to occur.
       </p><p>
        '1' disables Nagle's algorithm for all sockets.
       </p><p>
        '0' keeps Nagle's algorithm enabled (default).
       </p></dd></dl></div><div class="example" id="id-1.4.8.2.8.4.2.3" data-id-title="Example Beast Configuration"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 28.1: </span><span class="title-name">Example Beast Configuration </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.4.2.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set rgw.myrealm.myzone.ses-min1.kwwazo \
 rgw_frontends beast port=8000 ssl_port=443 \
 ssl_certificate=/etc/ssl/ssl.crt \
 error_log_file=/var/log/radosgw/beast.error.log</pre></div><div class="verbatim-wrap"><pre class="screen"/></div></div></div></section><section class="sect3" id="config-http-civetweb" data-id-title="CivetWeb"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.2.2 </span><span class="title-name">CivetWeb</span> <a title="Permalink" class="permalink" href="#config-http-civetweb">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.8.4.3.2.1"><span class="term">port</span></dt><dd><p>
        The listening port number. For SSL-enabled ports, add an 's' suffix
        (for example, '443s'). To bind a specific IPv4 or IPv6 address, use the
        form 'address:port'. You can specify multiple endpoints either by
        joining them with '+' or by providing multiple options:
       </p><div class="verbatim-wrap"><pre class="screen">port=127.0.0.1:8000+443s
port=8000 port=443s</pre></div><p>
        Default is 7480.
       </p></dd><dt id="id-1.4.8.2.8.4.3.2.2"><span class="term">num_threads</span></dt><dd><p>
        The number of threads spawned by Civetweb to handle incoming HTTP
        connections. This effectively limits the number of concurrent
        connections that the front-end can service.
       </p><p>
        Default is the value specified by the
        <code class="option">rgw_thread_pool_size</code> option.
       </p></dd><dt id="id-1.4.8.2.8.4.3.2.3"><span class="term">request_timeout_ms</span></dt><dd><p>
        The amount of time in milliseconds that Civetweb will wait for more
        incoming data before giving up.
       </p><p>
        Default is 30000 milliseconds.
       </p></dd><dt id="id-1.4.8.2.8.4.3.2.4"><span class="term">access_log_file</span></dt><dd><p>
        Path to the access log file. You can specify either a full path, or a
        path relative to the current working directory. If not specified
        (default), then accesses are not logged.
       </p></dd><dt id="id-1.4.8.2.8.4.3.2.5"><span class="term">error_log_file</span></dt><dd><p>
        Path to the error log file. You can specify either a full path, or a
        path relative to the current working directory. If not specified
        (default), then errors are not logged.
       </p></dd></dl></div><div class="example" id="id-1.4.8.2.8.4.3.3" data-id-title="Example Civetweb Configuration in /etc/ceph/ceph.conf"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 28.2: </span><span class="title-name">Example Civetweb Configuration in <code class="filename">/etc/ceph/ceph.conf</code> </span><a title="Permalink" class="permalink" href="#id-1.4.8.2.8.4.3.3">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set rgw.myrealm.myzone.ses-min2.ingabw \
 rgw_frontends civetweb port=8000+443s request_timeout_ms=30000 \
 error_log_file=/var/log/radosgw/civetweb.error.log</pre></div></div></div></section><section class="sect3" id="config-http-common-options" data-id-title="Common Options"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">28.5.2.3 </span><span class="title-name">Common Options</span> <a title="Permalink" class="permalink" href="#config-http-common-options">#</a></h4></div></div></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.2.8.4.4.2.1"><span class="term">ssl_certificate</span></dt><dd><p>
        Path to the SSL certificate file used for SSL-enabled endpoints.
       </p></dd><dt id="id-1.4.8.2.8.4.4.2.2"><span class="term">prefix</span></dt><dd><p>
        A prefix string that is inserted into the URI of all requests. For
        example, a Swift-only front-end could supply a URI prefix of
        <code class="literal">/swift</code>.
       </p></dd></dl></div></section></section></section></section><section class="chapter" id="cha-mgr-modules" data-id-title="Ceph Manager modules"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">29 </span><span class="title-name">Ceph Manager modules</span> <a title="Permalink" class="permalink" href="#cha-mgr-modules">#</a></h1></div></div></div><p>
  The architecture of the Ceph Manager (refer to
  <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.2.3 “Ceph nodes and daemons”</span> for a brief introduction) allows
  extending its functionality via <span class="emphasis"><em>modules</em></span>, such as
  'dashboard' (see <a class="xref" href="#part-dashboard" title="Part I. Ceph Dashboard">Part I, “Ceph Dashboard”</a>), 'prometheus' (see
  <a class="xref" href="#monitoring-alerting" title="Chapter 16. Monitoring and alerting">Chapter 16, <em>Monitoring and alerting</em></a>), or 'balancer'.
 </p><p>
  To list all available modules, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module ls
{
        "enabled_modules": [
                "restful",
                "status"
        ],
        "disabled_modules": [
                "dashboard"
        ]
}</pre></div><p>
  To enable or disable a specific module, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable <em class="replaceable">MODULE-NAME</em></pre></div><p>
  For example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module disable dashboard</pre></div><p>
  To list the services that the enabled modules provide, run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr services
{
        "dashboard": "http://myserver.com:7789/",
        "restful": "https://myserver.com:8789/"
}</pre></div><section class="sect1" id="mgr-modules-balancer" data-id-title="Balancer"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">29.1 </span><span class="title-name">Balancer</span> <a title="Permalink" class="permalink" href="#mgr-modules-balancer">#</a></h2></div></div></div><p>
   The balancer module optimizes the placement group (PG) distribution across
   OSDs for a more balanced deployment. Although the module is activated by
   default, it is inactive. It supports the following two modes:
   <code class="literal">crush-compat</code> and <code class="literal">upmap</code>.
  </p><div id="id-1.4.8.3.12.3" data-id-title="Current Balancer Status and Configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Current Balancer Status and Configuration</h6><p>
    To view the current balancer status and configuration information, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer status</pre></div></div><section class="sect2" id="mgr-balancer-crush-compat" data-id-title="The crush-compat mode"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.1.1 </span><span class="title-name">The 'crush-compat' mode</span> <a title="Permalink" class="permalink" href="#mgr-balancer-crush-compat">#</a></h3></div></div></div><p>
    In 'crush-compat' mode, the balancer adjusts the OSDs' reweight-sets to
    achieve improved distribution of the data. It moves PGs between OSDs,
    temporarily causing a <code class="literal">HEALTH_WARN</code> cluster state
    resulting from misplaced PGs.
   </p><div id="id-1.4.8.3.12.4.3" data-id-title="Mode Activation" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Mode Activation</h6><p>
     Although 'crush-compat' is the default mode, we recommend activating it
     explicitly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer mode crush-compat</pre></div></div></section><section class="sect2" id="mgr-balancer-planning-executing" data-id-title="Planning and executing of data balancing"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">29.1.2 </span><span class="title-name">Planning and executing of data balancing</span> <a title="Permalink" class="permalink" href="#mgr-balancer-planning-executing">#</a></h3></div></div></div><p>
    Using the balancer module, you can create a plan for data balancing. You
    can then execute the plan manually, or let the balancer balance PGs
    continuously.
   </p><p>
    The decision whether to run the balancer in manual or automatic mode
    depends on several factors, such as the current data imbalance, cluster
    size, PG count, or I/O activity. We recommend creating an initial plan and
    executing it at a time of low I/O load in the cluster. The reason for this
    is that the initial imbalance will probably be considerable and it is a
    good practice to keep the impact on clients low. After an initial manual
    run, consider activating the automatic mode and monitor the rebalance
    traffic under normal I/O load. The improvements in PG distribution need to
    be weighed against the rebalance traffic caused by the balancer.
   </p><div id="id-1.4.8.3.12.5.4" data-id-title="Movable Fraction of Placement Groups (PGs)" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Movable Fraction of Placement Groups (PGs)</h6><p>
     During the process of balancing, the balancer module throttles PG
     movements so that only a configurable fraction of PGs is moved. The
     default is 5% and you can adjust the fraction, to 9% for example, by
     running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr target_max_misplaced_ratio .09</pre></div></div><p>
    To create and execute a balancing plan, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Check the current cluster score:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer eval</pre></div></li><li class="step"><p>
      Create a plan. For example, 'great_plan':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer optimize great_plan</pre></div></li><li class="step"><p>
      See what changes the 'great_plan' will entail:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer show great_plan</pre></div></li><li class="step"><p>
      Check the potential cluster score if you decide to apply the
      'great_plan':
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer eval great_plan</pre></div></li><li class="step"><p>
      Execute the 'great_plan' for one time only:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer execute great_plan</pre></div></li><li class="step"><p>
      Observe the cluster balancing with the <code class="command">ceph -s</code>
      command. If you are satisfied with the result, activate automatic
      balancing:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer on</pre></div><p>
      If you later decide to deactivate automatic balancing, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer off</pre></div></li></ol></div></div><div id="id-1.4.8.3.12.5.7" data-id-title="Automatic Balancing without Initial Plan" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Automatic Balancing without Initial Plan</h6><p>
     You can activate automatic balancing without executing an initial plan. In
     such case, expect a potentially long running rebalancing of placement
     groups.
    </p></div></section></section><section class="sect1" id="mgr-modules-telemetry" data-id-title="Enabling the telemetry module"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">29.2 </span><span class="title-name">Enabling the telemetry module</span> <a title="Permalink" class="permalink" href="#mgr-modules-telemetry">#</a></h2></div></div></div><p>
   The telemetry plugin sends the Ceph project anonymous data about the
   cluster in which the plugin is running.
  </p><p>
   This (opt-in) component contains counters and statistics on how the cluster
   has been deployed, the version of Ceph, the distribution of the hosts and
   other parameters which help the project to gain a better understanding of
   the way Ceph is used. It does not contain any sensitive data like pool
   names, object names, object contents, or host names.
  </p><p>
   The purpose of the telemetry module is to provide an automated feedback loop
   for the developers to help quantify adoption rates, tracking, or point out
   things that need to be better explained or validated during configuration to
   prevent undesirable outcomes.
  </p><div id="id-1.4.8.3.13.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The telemetry module requires the Ceph Manager nodes to have the ability to push
    data over HTTPS to the upstream servers. Ensure your corporate firewalls
    permit this action.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     To enable the telemetry module:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable telemetry</pre></div><div id="id-1.4.8.3.13.6.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This command only enables you to view your data locally. This command
      does not share your data with the Ceph community.
     </p></div></li><li class="step"><p>
     To allow the telemetry module to start sharing data:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph telemetry on</pre></div></li><li class="step"><p>
     To disable telemetry data sharing:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph telemetry off</pre></div></li><li class="step"><p>
     To generate a JSON report that can be printed:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph telemetry show</pre></div></li><li class="step"><p>
     To add a contact and description to the report:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/telemetry/contact John Doe john.doe@example.com
<code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/telemetry/description 'My first Ceph cluster'</pre></div></li><li class="step"><p>
     The module compiles and sends a new report every 24 hours by default. To
     adjust this interval:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/telemetry/interval <em class="replaceable">HOURS</em></pre></div></li></ol></div></div></section></section><section class="chapter" id="cha-storage-cephx" data-id-title="Authentication with cephx"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">30 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span> <a title="Permalink" class="permalink" href="#cha-storage-cephx">#</a></h1></div></div></div><p>
  To identify clients and protect against man-in-the-middle attacks, Ceph
  provides its <code class="systemitem">cephx</code> authentication system. <span class="emphasis"><em>Clients</em></span> in
  this context are either human users—such as the admin user—or
  Ceph-related services/daemons, for example OSDs, monitors, or Object Gateways.
 </p><div id="id-1.4.8.4.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
   The <code class="systemitem">cephx</code> protocol does not address data encryption in transport, such as
   TLS/SSL.
  </p></div><section class="sect1" id="storage-cephx-arch" data-id-title="Authentication architecture"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.1 </span><span class="title-name">Authentication architecture</span> <a title="Permalink" class="permalink" href="#storage-cephx-arch">#</a></h2></div></div></div><p>
   <code class="systemitem">cephx</code> uses shared secret keys for authentication, meaning both the client
   and Ceph Monitors have a copy of the client’s secret key. The authentication
   protocol enables both parties to prove to each other that they have a copy
   of the key without actually revealing it. This provides mutual
   authentication, which means the cluster is sure the user possesses the
   secret key, and the user is sure that the cluster has a copy of the secret
   key as well.
  </p><p>
   A key scalability feature of Ceph is to avoid a centralized interface to
   the Ceph object store. This means that Ceph clients can interact with
   OSDs directly. To protect data, Ceph provides its <code class="systemitem">cephx</code> authentication
   system, which authenticates Ceph clients.
  </p><p>
   Each monitor can authenticate clients and distribute keys, so there is no
   single point of failure or bottleneck when using <code class="systemitem">cephx</code>. The monitor
   returns an authentication data structure that contains a session key for use
   in obtaining Ceph services. This session key is itself encrypted with the
   client’s permanent secret key, so that only the client can request
   services from the Ceph monitors. The client then uses the session key to
   request its desired services from the monitor, and the monitor provides the
   client with a ticket that will authenticate the client to the OSDs that
   actually handle data. Ceph monitors and OSDs share a secret, so the client
   can use the ticket provided by the monitor with any OSD or metadata server
   in the cluster. <code class="systemitem">cephx</code> tickets expire, so an attacker cannot use an expired
   ticket or session key obtained wrongfully.
  </p><p>
   To use <code class="systemitem">cephx</code>, an administrator must setup clients/users first. In the
   following diagram, the
   <code class="systemitem">client.admin</code> user invokes
   <code class="command">ceph auth get-or-create-key</code> from the command line to
   generate a user name and secret key. Ceph’s <code class="command">auth</code>
   subsystem generates the user name and key, stores a copy with the monitor(s)
   and transmits the user’s secret back to the
   <code class="systemitem">client.admin</code> user. This means that
   the client and the monitor share a secret key.
  </p><div class="figure" id="id-1.4.8.4.5.6"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring.png"><img src="images/cephx_keyring.png" width="70%" alt="Basic cephx authentication" title="Basic cephx authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 30.1: </span><span class="title-name">Basic <code class="systemitem">cephx</code> authentication </span><a title="Permalink" class="permalink" href="#id-1.4.8.4.5.6">#</a></h6></div></div><p>
   To authenticate with the monitor, the client passes the user name to the
   monitor. The monitor generates a session key and encrypts it with the secret
   key associated with the user name and transmits the encrypted ticket back to
   the client. The client then decrypts the data with the shared secret key to
   retrieve the session key. The session key identifies the user for the
   current session. The client then requests a ticket related to the user,
   which is signed by the session key. The monitor generates a ticket, encrypts
   it with the user’s secret key and transmits it back to the client. The
   client decrypts the ticket and uses it to sign requests to OSDs and metadata
   servers throughout the cluster.
  </p><div class="figure" id="id-1.4.8.4.5.8"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring2.png"><img src="images/cephx_keyring2.png" width="70%" alt="cephx authentication" title="cephx authentication"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 30.2: </span><span class="title-name"><code class="systemitem">cephx</code> authentication </span><a title="Permalink" class="permalink" href="#id-1.4.8.4.5.8">#</a></h6></div></div><p>
   The <code class="systemitem">cephx</code> protocol authenticates ongoing communications between the client
   machine and the Ceph servers. Each message sent between a client and a
   server after the initial authentication is signed using a ticket that the
   monitors, OSDs, and metadata servers can verify with their shared secret.
  </p><div class="figure" id="id-1.4.8.4.5.10"><div class="figure-contents"><div class="mediaobject"><a href="images/cephx_keyring3.png"><img src="images/cephx_keyring3.png" width="70%" alt="cephx authentication - MDS and OSD" title="cephx authentication - MDS and OSD"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 30.3: </span><span class="title-name"><code class="systemitem">cephx</code> authentication - MDS and OSD </span><a title="Permalink" class="permalink" href="#id-1.4.8.4.5.10">#</a></h6></div></div><div id="id-1.4.8.4.5.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The protection offered by this authentication is between the Ceph client
    and the Ceph cluster hosts. The authentication is not extended beyond the
    Ceph client. If the user accesses the Ceph client from a remote host,
    Ceph authentication is not applied to the connection between the user’s
    host and the client host.
   </p></div></section><section class="sect1" id="storage-cephx-keymgmt" data-id-title="Key management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">30.2 </span><span class="title-name">Key management</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt">#</a></h2></div></div></div><p>
   This section describes Ceph client users and their authentication and
   authorization with the Ceph storage cluster. <span class="emphasis"><em>Users</em></span>
   are either individuals or system actors such as applications, which use
   Ceph clients to interact with the Ceph storage cluster daemons.
  </p><p>
   When Ceph runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user (usually via the command line). If you do not
   specify a user name, Ceph will use
   <code class="systemitem">client.admin</code> as the default user
   name. If you do not specify a keyring, Ceph will look for a keyring via
   the keyring setting in the Ceph configuration file. For example, if you
   execute the <code class="command">ceph health</code> command without specifying a user
   name or keyring, Ceph interprets the command like this:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</pre></div><p>
   Alternatively, you may use the <code class="literal">CEPH_ARGS</code> environment
   variable to avoid re-entering the user name and secret.
  </p><section class="sect2" id="storage-cephx-keymgmt-backgrnd" data-id-title="Background information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.1 </span><span class="title-name">Background information</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-backgrnd">#</a></h3></div></div></div><p>
    Regardless of the type of Ceph client (for example, block device, object
    storage, file system, native API), Ceph stores all data as objects within
    <span class="emphasis"><em>pools</em></span>. Ceph users need to have access to pools in
    order to read and write data. Additionally, Ceph users must have execute
    permissions to use Ceph's administrative commands. The following concepts
    will help you understand Ceph user management.
   </p><section class="sect3" id="cephx-user" data-id-title="User"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.1 </span><span class="title-name">User</span> <a title="Permalink" class="permalink" href="#cephx-user">#</a></h4></div></div></div><p>
     A user is either an individual or a system actor such as an application.
     Creating users allows you to control who (or what) can access your Ceph
     storage cluster, its pools, and the data within pools.
    </p><p>
     Ceph uses <span class="emphasis"><em>types</em></span> of users. For the purposes of user
     management, the type will always be <code class="literal">client</code>. Ceph
     identifies users in period (.) delimited form, consisting of the user type
     and the user ID. For example, <code class="literal">TYPE.ID</code>,
     <code class="literal">client.admin</code>, or <code class="literal">client.user1</code>. The
     reason for user typing is that Ceph monitors, OSDs, and metadata servers
     also use the cephx protocol, but they are not clients. Distinguishing the
     user type helps to distinguish between client users and other users,
     streamlining access control, user monitoring, and traceability.
    </p><p>
     Sometimes Ceph’s user type may seem confusing, because the Ceph
     command line allows you to specify a user with or without the type,
     depending upon your command line usage. If you specify
     <code class="option">--user</code> or <code class="option">--id</code>, you can omit the type.
     So <code class="literal">client.user1</code> can be entered simply as
     <code class="literal">user1</code>. If you specify <code class="option">--name</code> or
     <code class="option">-n</code>, you must specify the type and name, such as
     <code class="literal">client.user1</code>. We recommend using the type and name as a
     best practice wherever possible.
    </p><div id="id-1.4.8.4.6.6.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      A Ceph storage cluster user is not the same as a Ceph object storage
      user or a Ceph file system user. The Ceph Object Gateway uses a Ceph storage
      cluster user to communicate between the gateway daemon and the storage
      cluster, but the gateway has its own user management functionality for
      end users. The Ceph file system uses POSIX semantics. The user space
      associated with it is not the same as a Ceph storage cluster user.
     </p></div></section><section class="sect3" id="authorization-capabilities-cephx" data-id-title="Authorization and capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.2 </span><span class="title-name">Authorization and capabilities</span> <a title="Permalink" class="permalink" href="#authorization-capabilities-cephx">#</a></h4></div></div></div><p>
     Ceph uses the term 'capabilities' (caps) to describe authorizing an
     authenticated user to exercise the functionality of the monitors, OSDs,
     and metadata servers. Capabilities can also restrict access to data within
     a pool or pool namespace. A Ceph administrative user sets a user's
     capabilities when creating or updating a user.
    </p><p>
     Capability syntax follows the form:
    </p><div class="verbatim-wrap"><pre class="screen"><em class="replaceable">daemon-type</em> 'allow <em class="replaceable">capability</em>' [...]</pre></div><p>
     Following is a list of capabilities for each service type:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.4.6.6.4.6.1"><span class="term">Monitor capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code> and <code class="literal">allow profile
        <em class="replaceable">cap</em></code>.
       </p><div class="verbatim-wrap"><pre class="screen">mon 'allow rwx'
mon 'allow profile osd'</pre></div></dd><dt id="id-1.4.8.4.6.6.4.6.2"><span class="term">OSD capabilities</span></dt><dd><p>
        include <code class="literal">r</code>, <code class="literal">w</code>,
        <code class="literal">x</code>, <code class="literal">class-read</code>,
        <code class="literal">class-write</code> and <code class="literal">profile osd</code>.
        Additionally, OSD capabilities also allow for pool and namespace
        settings.
       </p><div class="verbatim-wrap"><pre class="screen">osd 'allow <em class="replaceable">capability</em>' [pool=<em class="replaceable">poolname</em>] [namespace=<em class="replaceable">namespace-name</em>]</pre></div></dd><dt id="id-1.4.8.4.6.6.4.6.3"><span class="term">MDS capability</span></dt><dd><p>
        simply requires <code class="literal">allow</code>, or blank.
       </p><div class="verbatim-wrap"><pre class="screen">mds 'allow'</pre></div></dd></dl></div><p>
     The following entries describe each capability:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.4.6.6.4.8.1"><span class="term">allow</span></dt><dd><p>
        Precedes access settings for a daemon. Implies <code class="literal">rw</code>
        for MDS only.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.2"><span class="term">r</span></dt><dd><p>
        Gives the user read access. Required with monitors to retrieve the
        CRUSH map.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.3"><span class="term">w</span></dt><dd><p>
        Gives the user write access to objects.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.4"><span class="term">x</span></dt><dd><p>
        Gives the user the capability to call class methods (both read and
        write) and to conduct <code class="literal">auth</code> operations on monitors.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.5"><span class="term">class-read</span></dt><dd><p>
        Gives the user the capability to call class read methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.6"><span class="term">class-write</span></dt><dd><p>
        Gives the user the capability to call class write methods. Subset of
        <code class="literal">x</code>.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.7"><span class="term">*</span></dt><dd><p>
        Gives the user read, write, and execute permissions for a particular
        daemon/pool, and the ability to execute admin commands.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.8"><span class="term">profile osd</span></dt><dd><p>
        Gives a user permissions to connect as an OSD to other OSDs or
        monitors. Conferred on OSDs to enable OSDs to handle replication
        heartbeat traffic and status reporting.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.9"><span class="term">profile mds</span></dt><dd><p>
        Gives a user permissions to connect as an MDS to other MDSs or
        monitors.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.10"><span class="term">profile bootstrap-osd</span></dt><dd><p>
        Gives a user permissions to bootstrap an OSD. Delegated to deployment
        tools so that they have permissions to add keys when bootstrapping an
        OSD.
       </p></dd><dt id="id-1.4.8.4.6.6.4.8.11"><span class="term">profile bootstrap-mds</span></dt><dd><p>
        Gives a user permissions to bootstrap a metadata server. Delegated to
        deployment tools so they have permissions to add keys when
        bootstrapping a metadata server.
       </p></dd></dl></div></section><section class="sect3" id="cephx-pools" data-id-title="Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.1.3 </span><span class="title-name">Pools</span> <a title="Permalink" class="permalink" href="#cephx-pools">#</a></h4></div></div></div><p>
     A pool is a logical partition where users store data. In Ceph
     deployments, it is common to create a pool as a logical partition for
     similar types of data. For example, when deploying Ceph as a back-end
     for OpenStack, a typical deployment would have pools for volumes, images,
     backups and virtual machines, and users such as
     <code class="systemitem">client.glance</code> or
     <code class="systemitem">client.cinder</code>.
    </p></section></section><section class="sect2" id="storage-cephx-keymgmt-usermgmt" data-id-title="Managing users"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.2 </span><span class="title-name">Managing users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt">#</a></h3></div></div></div><p>
    User management functionality provides Ceph cluster administrators with
    the ability to create, update, and delete users directly in the Ceph
    cluster.
   </p><p>
    When you create or delete users in the Ceph cluster, you may need to
    distribute keys to clients so that they can be added to keyrings. See
    <a class="xref" href="#storage-cephx-keymgmt-keyringmgmt" title="30.2.3. Managing keyrings">Section 30.2.3, “Managing keyrings”</a> for details.
   </p><section class="sect3" id="cephx-listing-users" data-id-title="Listing users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.1 </span><span class="title-name">Listing users</span> <a title="Permalink" class="permalink" href="#cephx-listing-users">#</a></h4></div></div></div><p>
     To list the users in your cluster, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><p>
     Ceph will list all users in your cluster. For example, in a cluster with
     two nodes, <code class="command">ceph auth list</code> output looks similar to this:
    </p><div class="verbatim-wrap"><pre class="screen">installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd</pre></div><div id="id-1.4.8.4.6.7.4.6" data-id-title="TYPE.ID notation" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: TYPE.ID notation</h6><p>
      Note that the <code class="literal">TYPE.ID</code> notation for users applies such
      that <code class="literal">osd.0</code> specifies a user of type
      <code class="literal">osd</code> and its ID is <code class="literal">0</code>.
      <code class="literal">client.admin</code> is a user of type
      <code class="literal">client</code> and its ID is <code class="literal">admin</code>. Note
      also that each entry has a <code class="literal">key:
      <em class="replaceable">value</em></code> entry, and one or more
      <code class="literal">caps:</code> entries.
     </p><p>
      You may use the <code class="option">-o <em class="replaceable">filename</em></code>
      option with <code class="command">ceph auth list</code> to save the output to a
      file.
     </p></div></section><section class="sect3" id="cephx-information-users" data-id-title="Getting information about users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.2 </span><span class="title-name">Getting information about users</span> <a title="Permalink" class="permalink" href="#cephx-information-users">#</a></h4></div></div></div><p>
     To retrieve a specific user, key, and capabilities, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get <em class="replaceable">TYPE.ID</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get client.admin
exported keyring for client.admin
[client.admin]
	key = AQA19uZUqIwkHxAAFuUwvq0eJD4S173oFRxe0g==
	caps mds = "allow"
	caps mon = "allow *"
 caps osd = "allow *"</pre></div><p>
     Developers may also execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth export <em class="replaceable">TYPE.ID</em></pre></div><p>
     The <code class="command">auth export</code> command is identical to <code class="command">auth
     get</code>, but also prints the internal authentication ID.
    </p></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-useradd" data-id-title="Adding users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.3 </span><span class="title-name">Adding users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-useradd">#</a></h4></div></div></div><p>
     Adding a user creates a user name (<code class="literal">TYPE.ID</code>), a secret
     key, and any capabilities included in the command you use to create the
     user.
    </p><p>
     A user's key enables the user to authenticate with the Ceph storage
     cluster. The user's capabilities authorize the user to read, write, or
     execute on Ceph monitors (mon), Ceph OSDs (osd), or Ceph metadata
     servers (mds).
    </p><p>
     There are a few commands available to add a user:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.4.6.7.6.5.1"><span class="term"><code class="command">ceph auth add</code></span></dt><dd><p>
        This command is the canonical way to add a user. It will create the
        user, generate a key, and add any specified capabilities.
       </p></dd><dt id="id-1.4.8.4.6.7.6.5.2"><span class="term"><code class="command">ceph auth get-or-create</code></span></dt><dd><p>
        This command is often the most convenient way to create a user, because
        it returns a keyfile format with the user name (in brackets) and the
        key. If the user already exists, this command simply returns the user
        name and key in the keyfile format. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd><dt id="id-1.4.8.4.6.7.6.5.3"><span class="term"><code class="command">ceph auth get-or-create-key</code></span></dt><dd><p>
        This command is a convenient way to create a user and return the user's
        key (only). This is useful for clients that need the key only (for
        example <code class="systemitem">libvirt</code>). If the user already exists, this command simply
        returns the key. You may use the <code class="option">-o
        <em class="replaceable">filename</em></code> option to save the output
        to a file.
       </p></dd></dl></div><p>
     When creating client users, you may create a user with no capabilities. A
     user with no capabilities can authenticate but nothing more. Such client
     cannot retrieve the cluster map from the monitor. However, you can create
     a user with no capabilities if you want to defer adding capabilities later
     using the <code class="command">ceph auth caps</code> command.
    </p><p>
     A typical user has at least read capabilities on the Ceph monitor and
     read and write capabilities on Ceph OSDs. Additionally, a user's OSD
     permissions are often restricted to accessing a particular pool.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth add client.john mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.paul mon 'allow r' osd \
 'allow rw pool=liverpool'
<code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create client.george mon 'allow r' osd \
 'allow rw pool=liverpool' -o george.keyring
<code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create-key client.ringo mon 'allow r' osd \
 'allow rw pool=liverpool' -o ringo.key</pre></div><div id="id-1.4.8.4.6.7.6.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If you provide a user with capabilities to OSDs, but you <span class="emphasis"><em>do
      not</em></span> restrict access to particular pools, the user will have
      access to <span class="emphasis"><em>all</em></span> pools in the cluster.
     </p></div></section><section class="sect3" id="cephx-modifying-user-capabilities" data-id-title="Modifying user capabilities"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.4 </span><span class="title-name">Modifying user capabilities</span> <a title="Permalink" class="permalink" href="#cephx-modifying-user-capabilities">#</a></h4></div></div></div><p>
     The <code class="command">ceph auth caps</code> command allows you to specify a user
     and change the user's capabilities. Setting new capabilities will
     overwrite current ones. To view current capabilities run <code class="command">ceph
     auth get
     <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em></code>.
     To add capabilities, you also need to specify the existing capabilities
     when using the following form:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth caps <em class="replaceable">USERTYPE</em>.<em class="replaceable">USERID</em> <em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]' [<em class="replaceable">daemon</em> 'allow [r|w|x|*|...] \
     [pool=<em class="replaceable">pool-name</em>] [namespace=<em class="replaceable">namespace-name</em>]']</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get client.john
<code class="prompt user">cephuser@adm &gt; </code>ceph auth caps client.john mon 'allow r' osd 'allow rw pool=prague'
<code class="prompt user">cephuser@adm &gt; </code>ceph auth caps client.paul mon 'allow rw' osd 'allow r pool=prague'
<code class="prompt user">cephuser@adm &gt; </code>ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'</pre></div><p>
     To remove a capability, you may reset the capability. If you want the user
     to have no access to a particular daemon that was previously set, specify
     an empty string:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth caps client.ringo mon ' ' osd ' '</pre></div></section><section class="sect3" id="cephx-deleting-users" data-id-title="Deleting users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.5 </span><span class="title-name">Deleting users</span> <a title="Permalink" class="permalink" href="#cephx-deleting-users">#</a></h4></div></div></div><p>
     To delete a user, use <code class="command">ceph auth del</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth del <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     If you created users with permissions strictly for a pool that no longer
     exists, you should consider deleting those users too.
    </p></section><section class="sect3" id="cephx-printing-users-key" data-id-title="Printing a users key"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.6 </span><span class="title-name">Printing a user's key</span> <a title="Permalink" class="permalink" href="#cephx-printing-users-key">#</a></h4></div></div></div><p>
     To print a user’s authentication key to standard output, execute the
     following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth print-key <em class="replaceable">TYPE</em>.<em class="replaceable">ID</em></pre></div><p>
     where <em class="replaceable">TYPE</em> is one of <code class="literal">client</code>,
     <code class="literal">osd</code>, <code class="literal">mon</code>, or <code class="literal">mds</code>,
     and <em class="replaceable">ID</em> is the user name or ID of the daemon.
    </p><p>
     Printing a user's key is useful when you need to populate client software
     with a user's key (such as <code class="systemitem">libvirt</code>), as in the following example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mount -t ceph host:/ mount_point \
-o name=client.user,secret=`ceph auth print-key client.user`</pre></div></section><section class="sect3" id="storage-cephx-keymgmt-usermgmt-userimp" data-id-title="Importing users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.2.7 </span><span class="title-name">Importing users</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-usermgmt-userimp">#</a></h4></div></div></div><p>
     To import one or more users, use <code class="command">ceph auth import</code> and
     specify a keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><div id="id-1.4.8.4.6.7.10.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      The Ceph storage cluster will add new users, their keys and their
      capabilities and will update existing users, their keys and their
      capabilities.
     </p></div></section></section><section class="sect2" id="storage-cephx-keymgmt-keyringmgmt" data-id-title="Managing keyrings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.3 </span><span class="title-name">Managing keyrings</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-keyringmgmt">#</a></h3></div></div></div><p>
    When you access Ceph via a Ceph client, the client will look for a
    local keyring. Ceph presets the keyring setting with the following four
    keyring names by default so you do not need to set them in your Ceph
    configuration file unless you want to override the defaults:
   </p><div class="verbatim-wrap"><pre class="screen">/etc/ceph/<em class="replaceable">cluster</em>.<em class="replaceable">name</em>.keyring
/etc/ceph/<em class="replaceable">cluster</em>.keyring
/etc/ceph/keyring
/etc/ceph/keyring.bin</pre></div><p>
    The <em class="replaceable">cluster</em> metavariable is your Ceph cluster
    name as defined by the name of the Ceph configuration file.
    <code class="filename">ceph.conf</code> means that the cluster name is
    <code class="literal">ceph</code>, thus <code class="literal">ceph.keyring</code>. The
    <em class="replaceable">name</em> metavariable is the user type and user ID,
    for example <code class="literal">client.admin</code>, thus
    <code class="literal">ceph.client.admin.keyring</code>.
   </p><p>
    After you create a user (for example
    <code class="systemitem">client.ringo</code>), you must get the
    key and add it to a keyring on a Ceph client so that the user can access
    the Ceph storage cluster.
   </p><p>
    <a class="xref" href="#storage-cephx-keymgmt" title="30.2. Key management">Section 30.2, “Key management”</a> details how to list, get, add,
    modify and delete users directly in the Ceph storage cluster. However,
    Ceph also provides the <code class="command">ceph-authtool</code> utility to allow
    you to manage keyrings from a Ceph client.
   </p><section class="sect3" id="creating-keyring" data-id-title="Creating a keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.3.1 </span><span class="title-name">Creating a keyring</span> <a title="Permalink" class="permalink" href="#creating-keyring">#</a></h4></div></div></div><p>
     When you use the procedures in <a class="xref" href="#storage-cephx-keymgmt" title="30.2. Key management">Section 30.2, “Key management”</a> to
     create users, you need to provide user keys to the Ceph client(s) so
     that the client can retrieve the key for the specified user and
     authenticate with the Ceph storage cluster. Ceph clients access
     keyrings to look up a user name and retrieve the user's key:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool --create-keyring /path/to/keyring</pre></div><p>
     When creating a keyring with multiple users, we recommend using the
     cluster name (for example <em class="replaceable">cluster</em>.keyring) for
     the keyring file name and saving it in the <code class="filename">/etc/ceph</code>
     directory so that the keyring configuration default setting will pick up
     the file name without requiring you to specify it in the local copy of
     your Ceph configuration file. For example, create
     <code class="filename">ceph.keyring</code> by executing the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring</pre></div><p>
     When creating a keyring with a single user, we recommend using the cluster
     name, the user type and the user name and saving it in the
     <code class="filename">/etc/ceph</code> directory. For example,
     <code class="filename">ceph.client.admin.keyring</code> for the
     <code class="systemitem">client.admin</code> user.
    </p></section><section class="sect3" id="cephx-adding-user-keyring" data-id-title="Adding a user to a keyring"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.3.2 </span><span class="title-name">Adding a user to a keyring</span> <a title="Permalink" class="permalink" href="#cephx-adding-user-keyring">#</a></h4></div></div></div><p>
     When you add a user to the Ceph storage cluster (see
     <a class="xref" href="#storage-cephx-keymgmt-usermgmt-useradd" title="30.2.2.3. Adding users">Section 30.2.2.3, “Adding users”</a>), you can
     retrieve the user, key and capabilities, and save the user to a keyring.
    </p><p>
     If you only want to use one user per keyring, the <code class="command">ceph auth
     get</code> command with the <code class="option">-o</code> option will save the
     output in the keyring file format. For example, to create a keyring for
     the <code class="systemitem">client.admin</code> user, execute
     the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring</pre></div><p>
     When you want to import users to a keyring, you can use
     <code class="command">ceph-authtool</code> to specify the destination keyring and
     the source keyring:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring \
  --import-keyring /etc/ceph/ceph.client.admin.keyring</pre></div><div id="id-1.4.8.4.6.8.8.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      If your keyring is compromised, delete your key from the
      <code class="filename">/etc/ceph</code> directory and re-create a new key using
      the same instructions from <a class="xref" href="#creating-keyring" title="30.2.3.1. Creating a keyring">Section 30.2.3.1, “Creating a keyring”</a>.
     </p></div></section><section class="sect3" id="cephx-creating-user" data-id-title="Creating a user"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.3.3 </span><span class="title-name">Creating a user</span> <a title="Permalink" class="permalink" href="#cephx-creating-user">#</a></h4></div></div></div><p>
     Ceph provides the <code class="command">ceph auth add</code> command to create a
     user directly in the Ceph storage cluster. However, you can also create
     a user, keys and capabilities directly on a Ceph client keyring. Then,
     you can import the user to the Ceph storage cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool -n client.ringo --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.keyring</pre></div><p>
     You can also create a keyring and add a new user to the keyring
     simultaneously:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool -C /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx' --gen-key</pre></div><p>
     In the previous scenarios, the new user
     <code class="systemitem">client.ringo</code> is only in the
     keyring. To add the new user to the Ceph storage cluster, you must still
     add the new user to the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth add client.ringo -i /etc/ceph/ceph.keyring</pre></div></section><section class="sect3" id="cephx-modifying-users" data-id-title="Modifying users"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">30.2.3.4 </span><span class="title-name">Modifying users</span> <a title="Permalink" class="permalink" href="#cephx-modifying-users">#</a></h4></div></div></div><p>
     To modify the capabilities of a user record in a keyring, specify the
     keyring and the user followed by the capabilities:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-authtool /etc/ceph/ceph.keyring -n client.ringo \
  --cap osd 'allow rwx' --cap mon 'allow rwx'</pre></div><p>
     To update the modified user within the Ceph cluster environment, you
     must import the changes from the keyring to the user entry in the Ceph
     cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth import -i /etc/ceph/ceph.keyring</pre></div><p>
     See <a class="xref" href="#storage-cephx-keymgmt-usermgmt-userimp" title="30.2.2.7. Importing users">Section 30.2.2.7, “Importing users”</a> for details
     on updating a Ceph storage cluster user from a keyring.
    </p></section></section><section class="sect2" id="storage-cephx-keymgmt-cmdline" data-id-title="Command line usage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">30.2.4 </span><span class="title-name">Command line usage</span> <a title="Permalink" class="permalink" href="#storage-cephx-keymgmt-cmdline">#</a></h3></div></div></div><p>
    The <code class="command">ceph</code> command supports the following options related
    to the user name and secret manipulation:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.8.4.6.9.3.1"><span class="term"><code class="option">--id</code> or <code class="option">--user</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">id</code>, <code class="option">name</code> and <code class="option">-n</code>
       options enable you to specify the ID portion of the user name (for
       example <code class="systemitem">admin</code> or
       <code class="systemitem">user1</code>). You can specify the
       user with the --id and omit the type. For example, to specify user
       client.foo enter the following:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph --id foo --keyring /path/to/keyring health
<code class="prompt user">cephuser@adm &gt; </code>ceph --user foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.4.8.4.6.9.3.2"><span class="term"><code class="option">--name</code> or <code class="option">-n</code></span></dt><dd><p>
       Ceph identifies users with a type and an ID
       (<em class="replaceable">TYPE</em>.<em class="replaceable">ID</em>, such as
       <code class="systemitem">client.admin</code> or
       <code class="systemitem">client.user1</code>). The
       <code class="option">--name</code> and <code class="option">-n</code> options enable you to
       specify the fully qualified user name. You must specify the user type
       (typically <code class="literal">client</code>) with the user ID:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph --name client.foo --keyring /path/to/keyring health
<code class="prompt user">cephuser@adm &gt; </code>ceph -n client.foo --keyring /path/to/keyring health</pre></div></dd><dt id="id-1.4.8.4.6.9.3.3"><span class="term"><code class="option">--keyring</code></span></dt><dd><p>
       The path to the keyring containing one or more user name and secret. The
       <code class="option">--secret</code> option provides the same functionality, but it
       does not work with Object Gateway, which uses <code class="option">--secret</code> for
       another purpose. You may retrieve a keyring with <code class="command">ceph auth
       get-or-create</code> and store it locally. This is a preferred
       approach, because you can switch user names without switching the
       keyring path:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd map --id foo --keyring /path/to/keyring mypool/myimage</pre></div></dd></dl></div></section></section></section></div><section class="appendix" id="id-1.4.9" data-id-title="Ceph maintenance updates based on upstream Pacific point releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Pacific' point releases</span> <a title="Permalink" class="permalink" href="#id-1.4.9">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7.1 are based on the
  Pacific release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Pacific series, SUSE Enterprise Storage 7.1 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.4.10">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.4.10.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.4.10.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.2">#</a></span></dt><dd class="glossdef"><p>
     The host from which you run the Ceph-related commands to administer
     cluster hosts.
    </p></dd><dt id="id-1.4.10.3.3"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.3">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies the end user.
    </p></dd><dt id="id-1.4.10.3.4"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.4">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.4.10.3.5"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.5">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p></dd><dt id="id-1.4.10.3.9"><span><span class="glossterm">Ceph Client</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.9">#</a></span></dt><dd class="glossdef"><p>
     The collection of Ceph components which can access a Ceph Storage
     Cluster. These include the Object Gateway, the Ceph Block Device, the CephFS,
     and their corresponding libraries, kernel modules, and FUSE clients.
    </p></dd><dt id="id-1.4.10.3.14"><span><span class="glossterm">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.14">#</a></span></dt><dd class="glossdef"><p>
     A built-in Web-based Ceph management and monitoring application to
     administer various aspects and objects of the cluster. The dashboard is
     implemented as a Ceph Manager module.
    </p></dd><dt id="id-1.4.10.3.19"><span><span class="glossterm">Ceph Manager</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.19">#</a></span></dt><dd class="glossdef"><p>
     Ceph Manager or MGR is the Ceph manager software, which collects all the state
     from the whole cluster in one place.
    </p></dd><dt id="id-1.4.10.3.18"><span><span class="glossterm">Ceph Monitor</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.18">#</a></span></dt><dd class="glossdef"><p>
     Ceph Monitor or MON is the Ceph monitor software.
    </p></dd><dt id="id-1.4.10.3.24"><span><span class="glossterm">Ceph Object Storage</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.24">#</a></span></dt><dd class="glossdef"><p>
     The object storage "product", service or capabilities, which consists of a
     Ceph Storage Cluster and a Ceph Object Gateway.
    </p></dd><dt id="id-1.4.10.3.22"><span><span class="glossterm">Ceph OSD Daemon</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.22">#</a></span></dt><dd class="glossdef"><p>
     The <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.4.10.3.11"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.11">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data. Such a set
     consists of Ceph monitors and OSDs.
    </p></dd><dt id="id-1.4.10.3.10"><span><span class="glossterm"><code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.10">#</a></span></dt><dd class="glossdef"><p>
     Provides tooling for deploying Ceph clusters managed by cephadm using
     Salt.
    </p></dd><dt id="id-1.4.10.3.6"><span><span class="glossterm">cephadm</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.6">#</a></span></dt><dd class="glossdef"><p>
     cephadm deploys and manages a Ceph cluster by connecting to hosts from
     the manager daemon via SSH to add, remove, or update Ceph daemon
     containers.
    </p></dd><dt id="id-1.4.10.3.7"><span><span class="glossterm">CephFS</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.7">#</a></span></dt><dd class="glossdef"><p>
     The Ceph file system.
    </p></dd><dt id="id-1.4.10.3.8"><span><span class="glossterm">CephX</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.8">#</a></span></dt><dd class="glossdef"><p>
     The Ceph authentication protocol. Cephx operates like Kerberos, but it
     has no single point of failure.
    </p></dd><dt id="id-1.4.10.3.13"><span><span class="glossterm">CRUSH rule</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.13">#</a></span></dt><dd class="glossdef"><p>
     The CRUSH data placement rule that applies to a particular pool or pools.
    </p></dd><dt id="id-1.4.10.3.12"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.12">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.4.10.3.15"><span><span class="glossterm">DriveGroups</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.15">#</a></span></dt><dd class="glossdef"><p>
     DriveGroups are a declaration of one or more OSD layouts that can be mapped
     to physical drives. An OSD layout defines how Ceph physically allocates
     OSD storage on the media matching the specified criteria.
    </p></dd><dt id="id-1.4.10.3.16"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.16">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.4.10.3.17"><span><span class="glossterm">Metadata Server</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.17">#</a></span></dt><dd class="glossdef"><p>
     Metadata Server or MDS is the Ceph metadata software.
    </p></dd><dt id="id-1.4.10.3.36"><span><span class="glossterm">Multi-zone</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.36">#</a></span></dt><dd class="glossdef"/><dt id="id-1.4.10.3.20"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.20">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.4.10.3.25"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.25">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store. Also known as the
     RADOS Gateway (RGW).
    </p></dd><dt id="id-1.4.10.3.21"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.21">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Object Storage Device</em></span>: A physical or logical storage
     unit.
    </p></dd><dt id="id-1.4.10.3.23"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.23">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.4.10.3.26"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.26">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.4.10.3.27"><span><span class="glossterm">Point Release</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.27">#</a></span></dt><dd class="glossdef"><p>
     Any ad-hoc release that includes only bug or security fixes.
    </p></dd><dt id="id-1.4.10.3.28"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.28">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.4.10.3.29"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.29">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd><dt id="id-1.4.10.3.31"><span><span class="glossterm">RADOS Block Device (RBD)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.31">#</a></span></dt><dd class="glossdef"><p>
     The block storage component of Ceph. Also known as the Ceph block
     device.
    </p></dd><dt id="id-1.4.10.3.30"><span><span class="glossterm">Reliable Autonomic Distributed Object Store (RADOS)</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.30">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data (MON+OSD).
    </p></dd><dt id="id-1.4.10.3.33"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.33">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.4.10.3.32"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.32">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd><dt id="id-1.4.10.3.34"><span><span class="glossterm">Samba</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.34">#</a></span></dt><dd class="glossdef"><p>
     Windows integration software.
    </p></dd><dt id="id-1.4.10.3.35"><span><span class="glossterm">Samba Gateway</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.35">#</a></span></dt><dd class="glossdef"><p>
     The Samba Gateway joins the Active Directory in the Windows domain to authenticate
     and authorize users.
    </p></dd><dt id="id-1.4.10.3.37"><span><span class="glossterm">zonegroup</span> <a title="Permalink" class="permalink" href="#id-1.4.10.3.37">#</a></span></dt><dd class="glossdef"/></dl></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/book_storage_admin.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>