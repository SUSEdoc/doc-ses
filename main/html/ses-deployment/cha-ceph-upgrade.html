<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SES 7.1 | Deployment Guide | Upgrade from SUSE Enterprise Storage 6 to 7.1</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Upgrade from SUSE Enterprise Storage 6 to 7.1 | SES 7.1"/>
<meta name="description" content="This chapter introduces steps to upgrade SUSE Enterprise Storage 6 to version 7.1."/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7.1"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="chapter-title" content="Chapter 10. Upgrade from SUSE Enterprise Storage 6 to 7.1"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Upgrade from SUSE Enterprise Storage 6 to 7.1 | SES 7.1"/>
<meta property="og:description" content="This chapter introduces steps to upgrade SUSE Enterprise Storage 6 to version 7.1."/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Upgrade from SUSE Enterprise Storage 6 to 7.1 | SES 7.1"/>
<meta name="twitter:description" content="This chapter introduces steps to upgrade SUSE Enterprise Storage 6 to version 7.1."/>
<link rel="prev" href="ses-upgrade.html" title="Part III. Upgrading from Previous Releases"/><link rel="next" href="upgrade-to-pacific.html" title="Chapter 11. Upgrade from SUSE Enterprise Storage 7 to 7.1"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deployment Guide</a><span> / </span><a class="crumb" href="ses-upgrade.html">Upgrading from Previous Releases</a><span> / </span><a class="crumb" href="cha-ceph-upgrade.html">Upgrade from SUSE Enterprise Storage 6 to 7.1</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Deployment Guide</div><ol><li><a href="preface-deployment.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="part-ses.html" class="has-children "><span class="title-number">I </span><span class="title-name">Introducing SUSE Enterprise Storage (SES)</span></a><ol><li><a href="cha-storage-about.html" class=" "><span class="title-number">1 </span><span class="title-name">SES and Ceph</span></a></li><li><a href="storage-bp-hwreq.html" class=" "><span class="title-number">2 </span><span class="title-name">Hardware requirements and recommendations</span></a></li><li><a href="cha-admin-ha.html" class=" "><span class="title-number">3 </span><span class="title-name">Admin Node HA setup</span></a></li></ol></li><li><a href="ses-deployment.html" class="has-children "><span class="title-number">II </span><span class="title-name">Deploying Ceph Cluster</span></a><ol><li><a href="deploy-intro.html" class=" "><span class="title-number">4 </span><span class="title-name">Introduction and common tasks</span></a></li><li><a href="deploy-sles.html" class=" "><span class="title-number">5 </span><span class="title-name">Installing and configuring SUSE Linux Enterprise Server</span></a></li><li><a href="deploy-salt.html" class=" "><span class="title-number">6 </span><span class="title-name">Deploying Salt</span></a></li><li><a href="deploy-bootstrap.html" class=" "><span class="title-number">7 </span><span class="title-name">Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code></span></a></li><li><a href="deploy-core.html" class=" "><span class="title-number">8 </span><span class="title-name">Deploying the remaining core services using cephadm</span></a></li><li><a href="deploy-additional.html" class=" "><span class="title-number">9 </span><span class="title-name">Deployment of additional services</span></a></li></ol></li><li class="active"><a href="ses-upgrade.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Upgrading from Previous Releases</span></a><ol><li><a href="cha-ceph-upgrade.html" class=" you-are-here"><span class="title-number">10 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 6 to 7.1</span></a></li><li><a href="upgrade-to-pacific.html" class=" "><span class="title-number">11 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 7 to 7.1</span></a></li></ol></li><li><a href="bk01apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Pacific' point releases</span></a></li><li><a href="bk01go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-ceph-upgrade" data-id-title="Upgrade from SUSE Enterprise Storage 6 to 7.1"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7.1</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">10 </span><span class="title-name">Upgrade from SUSE Enterprise Storage 6 to 7.1</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
    This chapter introduces steps to upgrade SUSE Enterprise Storage
    6 to version 7.1.
  </p><p>
    The upgrade includes the following tasks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Upgrading from Ceph Nautilus to Pacific.
      </p></li><li class="listitem"><p>
        Switching from installing and running Ceph via RPM packages to
        running in containers.
      </p></li><li class="listitem"><p>
        Complete removal of DeepSea and replacing with <code class="systemitem">ceph-salt</code> and
        cephadm.
      </p></li></ul></div><div id="id-1.3.5.2.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
      The upgrade information in this chapter <span class="emphasis"><em>only</em></span> applies
      to upgrades from DeepSea to cephadm. Do not attempt to follow these
      instructions if you want to deploy SUSE Enterprise Storage on SUSE CaaS Platform.
    </p></div><div id="id-1.3.5.2.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
      Upgrading from SUSE Enterprise Storage versions older than 6
      is not supported. First, you must upgrade to the latest version of
      SUSE Enterprise Storage 6, and then follow the steps in this
      chapter.
    </p></div><section class="sect1" id="before-upgrade" data-id-title="Before upgrading"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.1 </span><span class="title-name">Before upgrading</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#before-upgrade">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The following tasks <span class="emphasis"><em>must</em></span> be completed before you
      start the upgrade. This can be done at any time during the SUSE Enterprise Storage
      6 lifetime.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          The OSD migration from FileStore to BlueStore
          <span class="emphasis"><em>must</em></span> happen before the upgrade as FileStore
          unsupported in SUSE Enterprise Storage 7.1. Find more details about
          BlueStore and how to migrate from FileStore at
          <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#filestore2bluestore" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#filestore2bluestore</a>.
        </p></li><li class="listitem"><p>
          If you are running an older cluster that still uses
          <code class="literal">ceph-disk</code> OSDs, you <span class="emphasis"><em>need</em></span> to
          switch to <code class="literal">ceph-volume</code> before the upgrade. Find
          more details in
          <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#upgrade-osd-deployment" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#upgrade-osd-deployment</a>.
        </p></li></ul></div><section class="sect2" id="upgrade-consider-points" data-id-title="Points to consider"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.1.1 </span><span class="title-name">Points to consider</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-consider-points">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Before upgrading, ensure you read through the following sections to
        ensure you understand all tasks that need to be executed.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            <span class="emphasis"><em>Read the release notes</em></span>. In them, you can find
            additional information on changes since the previous release of
            SUSE Enterprise Storage. Check the release notes to see whether:
          </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
                Your hardware needs special considerations.
              </p></li><li class="listitem"><p>
                Any used software packages have changed significantly.
              </p></li><li class="listitem"><p>
                Special precautions are necessary for your installation.
              </p></li></ul></div><p>
            The release notes also provide information that could not make it
            into the manual on time. They also contain notes about known
            issues.
          </p><p>
            You can find SES 7.1 release notes online at
            <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
          </p><p>
            Additionally, after having installed the package
            <span class="package">release-notes-ses</span> from the SES 7.1
            repository, find the release notes locally in the directory
            <code class="filename">/usr/share/doc/release-notes</code> or online at
            <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
          </p></li><li class="listitem"><p>
            Read <a class="xref" href="ses-deployment.html" title="Part II. Deploying Ceph Cluster">Part II, “Deploying Ceph Cluster”</a> to familiarize yourself with
            <code class="systemitem">ceph-salt</code> and the Ceph orchestrator, and in particular the
            information on service specifications.
          </p></li><li class="listitem"><p>
            The cluster upgrade may take a long time—approximately the
            time it takes to upgrade one machine multiplied by the number of
            cluster nodes.
          </p></li><li class="listitem"><p>
            You need to upgrade the Salt Master first, then replace DeepSea
            with <code class="systemitem">ceph-salt</code> and cephadm. You will <span class="emphasis"><em>not</em></span> be
            able to start using the cephadm orchestrator module until at
            least all Ceph Manager nodes are upgraded.
          </p></li><li class="listitem"><p>
            The upgrade from using Nautilus RPMs to Pacific
            containers needs to happen in a single step. This means upgrading
            an entire node at a time, not one daemon at a time.
          </p></li><li class="listitem"><p>
            The upgrade of core services (MON, MGR, OSD) happens in an orderly
            fashion. Each service is available during the upgrade. The gateway
            services (Metadata Server, Object Gateway, NFS Ganesha, iSCSI Gateway) need to be redeployed
            after the core services are upgraded. There is a certain amount of
            downtime for each of the following services:
          </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><div id="id-1.3.5.2.8.4.3.6.2.1.1" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
                  Metadata Servers and Object Gateways are down from the time the nodes are
                  upgraded from SUSE Linux Enterprise Server 15 SP1 to SUSE Linux Enterprise Server 15 SP3 until the services
                  are redeployed at the end of the upgrade procedure. This is
                  particularly important to bear in mind if these services are
                  colocated with MONs, MGRs or OSDs as they may be down for the
                  duration of the cluster upgrade. If this is going to be a
                  problem, consider deploying these services separately on
                  additional nodes before upgrading, so that they are down for
                  the shortest possible time. This is the duration of the
                  upgrade of the gateway nodes, not the duration of the upgrade
                  of the entire cluster.
                </p></div></li><li class="listitem"><p>
                NFS Ganesha and iSCSI Gateways are down only while nodes are rebooting
                during upgrade from SUSE Linux Enterprise Server 15 SP1 to SUSE Linux Enterprise Server 15 SP3, and again
                briefly when each service is redeployed on the containerized
                mode.
              </p></li></ul></div></li></ul></div></section><section class="sect2" id="upgrade-backup-config-data" data-id-title="Backing Up cluster configuration and data"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.1.2 </span><span class="title-name">Backing Up cluster configuration and data</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-backup-config-data">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        We strongly recommend backing up all cluster configuration and data
        before starting your upgrade to SUSE Enterprise Storage 7.1. For
        instructions on how to back up all your data, see
        <span class="intraxref">Book “Administration and Operations Guide”, Chapter 15 “Backup and restore”</span>.
      </p></section><section class="sect2" id="verify-previous-upgrade" data-id-title="Verifying steps from the previous upgrade"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.1.3 </span><span class="title-name">Verifying steps from the previous upgrade</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#verify-previous-upgrade">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        In case you previously upgraded from version 5, verify that the upgrade
        to version 6 was completed successfully:
      </p><p>
        Check for the existence of the
        <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.import</code>
        file.
      </p><p>
        This file is created by the engulf process during the upgrade from
        SUSE Enterprise Storage 5 to 6. The <code class="option">configuration_init:
        default-import</code> option is set in
        <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>.
      </p><p>
        If <code class="option">configuration_init</code> is still set to
        <code class="option">default-import</code>, the cluster is using
        <code class="filename">ceph.conf.import</code> as its configuration file and not
        DeepSea's default <code class="filename">ceph.conf</code>, which is compiled
        from files in
        <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>.
      </p><p>
        Therefore, you need to inspect <code class="filename">ceph.conf.import</code>
        for any custom configuration, and possibly move the configuration to
        one of the files in
        <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>.
      </p><p>
        Then remove the <code class="option">configuration_init: default-import</code>
        line from
        <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>.
      </p></section><section class="sect2" id="verify-previous-upgrade-patch" data-id-title="Updating cluster nodes and verifying cluster health"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.1.4 </span><span class="title-name">Updating cluster nodes and verifying cluster health</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#verify-previous-upgrade-patch">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Verify that all latest updates of SUSE Linux Enterprise Server 15 SP1 and SUSE Enterprise Storage
        6 are applied to all cluster nodes:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper refresh &amp;&amp; zypper patch</pre></div><div id="id-1.3.5.2.8.7.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
          Refer to
          <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates</a>
          for detailed information about updating the cluster nodes.
        </p></div><p>
        After updates are applied, restart the Salt Master, synchronize new
        Salt modules, and check the cluster health:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt '*' saltutil.sync_all
<code class="prompt user">cephuser@adm &gt; </code>ceph -s</pre></div><section class="sect3" id="upgrade-disable-insecure" data-id-title="Disable insecure clients"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.1.4.1 </span><span class="title-name">Disable insecure clients</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-disable-insecure">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Since Nautilus v14.2.20, a new health warning was
          introduced that informs you that insecure clients are allowed to join
          the cluster. This warning is <span class="emphasis"><em>on</em></span> by default. The
          Ceph Dashboard will show the cluster in the
          <code class="literal">HEALTH_WARN</code> status. The command line verifies the
          cluster status as follows:
        </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>ceph status
 cluster:
   id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
   health: HEALTH_WARN
   mons are allowing insecure global_id reclaim
 [...]</pre></div><p>
          This warning means that the Ceph Monitors are still allowing old, unpatched
          clients to connect to the cluster. This ensures existing clients can
          still connect while the cluster is being upgraded, but warns you that
          there is a problem that needs to be addressed. When the cluster and
          all clients are upgraded to the latest version of Ceph, disallow
          unpatched clients by running the following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></section><section class="sect3" id="upgrade-disable-sanity-check" data-id-title="Disable FSMap sanity check"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.1.4.2 </span><span class="title-name">Disable FSMap sanity check</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-disable-sanity-check">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          Before you start upgrading cluster nodes, disable the FSMap sanity
          check:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph config set mon mon_mds_skip_sanity true</code></pre></div></section></section><section class="sect2" id="verify-previous-upgrade-patch-repos" data-id-title="Verifying access to software repositories and container images"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.1.5 </span><span class="title-name">Verifying access to software repositories and container images</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#verify-previous-upgrade-patch-repos">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Verify that each cluster node has access to the SUSE Linux Enterprise Server 15 SP3 and
        SUSE Enterprise Storage 7.1 software repositories, as well as the
        registry of container images.
      </p><section class="sect3" id="verify-previous-upgrade-patch-repos-repos" data-id-title="Software repositories"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.1.5.1 </span><span class="title-name">Software repositories</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#verify-previous-upgrade-patch-repos-repos">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          If all nodes are registered with SCC, you will be able to use the
          <code class="command">zypper migration</code> command to upgrade. Refer to
          <a class="link" href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper" target="_blank">https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper</a>
          for more details.
        </p><p>
          If nodes are <span class="bold"><strong>not</strong></span> registered with
          SCC, disable all existing software repositories and add both the
          <code class="literal">Pool</code> and <code class="literal">Updates</code> repositories
          for each of the following extensions:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              SLE-Product-SLES/15-SP3
            </p></li><li class="listitem"><p>
              SLE-Module-Basesystem/15-SP3
            </p></li><li class="listitem"><p>
              SLE-Module-Server-Applications/15-SP3
            </p></li><li class="listitem"><p>
              SUSE-Enterprise-Storage-7.1
            </p></li></ul></div></section><section class="sect3" id="verify-previous-upgrade-patch-repos-images" data-id-title="Container images"><div class="titlepage"><div><div><div class="title-container"><h4 class="title"><span class="title-number-name"><span class="title-number">10.1.5.2 </span><span class="title-name">Container images</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#verify-previous-upgrade-patch-repos-images">#</a></h4><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
          All cluster nodes need access to the container image registry. In
          most cases, you will use the public SUSE registry at
          <code class="literal">registry.suse.com</code>. You need the following images:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              registry.suse.com/ses/7.1/ceph/ceph
            </p></li><li class="listitem"><p>
              registry.suse.com/ses/7.1/ceph/grafana
            </p></li><li class="listitem"><p>
              registry.suse.com/ses/7.1/ceph/prometheus-server
            </p></li><li class="listitem"><p>
              registry.suse.com/ses/7.1/ceph/prometheus-node-exporter
            </p></li><li class="listitem"><p>
              registry.suse.com/ses/7.1/ceph/prometheus-alertmanager
            </p></li></ul></div><p>
          Alternatively—for example, for air-gapped
          deployments—configure a local registry and verify that you have
          the correct set of container images available. Refer to
          <a class="xref" href="deploy-bootstrap.html#deploy-cephadm-configure-registry" title="7.2.10. Using the container registry">Section 7.2.10, “Using the container registry”</a> for more
          details about configuring a local container image registry.
        </p></section></section></section><section class="sect1" id="upgrade-salt-master" data-id-title="Upgrading the Salt Master"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.2 </span><span class="title-name">Upgrading the Salt Master</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-salt-master">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      The following procedure describes the process of upgrading the Salt Master:
    </p><div id="id-1.3.5.2.9.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        Before continuing, ensure the steps have been followed from
        <a class="xref" href="deploy-bootstrap.html#deploy-cephadm-configure-imagepath" title="7.2.10.2. Configuring the path to container images">Section 7.2.10.2, “Configuring the path to container images”</a>. Without this
        configuration, the podman image pull fails in cephadm, but will
        succeed in the terminal if the customer sets the following variables:
      </p><div class="verbatim-wrap"><pre class="screen">https_proxy=
http_proxy=</pre></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP3:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              For cluster whose all nodes are registered with SCC, run
              <code class="command">zypper migration</code>.
            </p></li><li class="listitem"><p>
              For cluster whose nodes have software repositories assigned
              manually, run <code class="command">zypper dup</code> followed by
              <code class="command">reboot</code>.
            </p></li></ul></div></li><li class="step"><p>
          Disable the DeepSea stages to avoid accidental use. Add the
          following content to
          <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
        </p><div class="verbatim-wrap"><pre class="screen">stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled</pre></div><p>
          Save the file and apply the changes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="step"><p>
          If you are <span class="bold"><strong>not</strong></span> using container
          images from <code class="literal">registry.suse.com</code> but rather the
          locally configured registry, edit
          <code class="filename">/srv/pillar/ceph/stack/global.yml</code> to inform
          DeepSea which Ceph container image and registry to use. For
          example, to use <code class="literal">192.168.121.1:5000/my/ceph/image</code>
          add the following lines:
        </p><div class="verbatim-wrap"><pre class="screen">ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000</pre></div><p>
          If you need to specify authentication information for the registry,
          add the <code class="literal">ses7_container_registry_auth:</code> block, for
          example:
        </p><div class="verbatim-wrap"><pre class="screen">ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
ses7_container_registry_auth:
  registry: 192.168.121.1:5000
  username: <em class="replaceable">USER_NAME</em>
  password: <em class="replaceable">PASSWORD</em></pre></div><p>
          Save the file and apply the changes:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.refresh_pillar</pre></div></li><li class="step"><p>
          Assimilate existing configuration:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config assimilate-conf -i /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
          Verify the upgrade status. Your output may differ depending on your
          cluster configuration:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 16.2.7-640-gceb23c7491b (ceb23c7491bd96ab7956111374219a4cdcf6f8f4) pacific (stable)
 os: SUSE Linux Enterprise Server 15 SP3

Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)

Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-mon-mgr-nodes" data-id-title="Upgrading the MON, MGR, and OSD nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.3 </span><span class="title-name">Upgrading the MON, MGR, and OSD nodes</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-mon-mgr-nodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Upgrade the Ceph Monitor, Ceph Manager, and OSD nodes one at a time. For each service,
      follow these steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Before adopting any OSD node, you need to perform a format conversion
          of OSD nodes to improve the accounting for OMAP data. You can do so
          by running the following commands on the Admin Node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm unit --name osd.OSD_DAEMON_ID stop
<code class="prompt user">cephuser@adm &gt; </code>cephadm shell --name osd.OSD_DAEMON_ID ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-OSD_DAEMON_ID --command quick-fix
<code class="prompt user">cephuser@adm &gt; </code>cephadm unit --name osd.OSD_DAEMON_ID start</pre></div><p>
          The conversion may take minutes to hours, depending on how much OMAP
          data the related disk contains. For more details, refer to
          <a class="link" href="https://docs.ceph.com/en/latest/releases/pacific/#upgrading-non-cephadm-clusters" target="_blank">https://docs.ceph.com/en/latest/releases/pacific/#upgrading-non-cephadm-clusters</a>.
        </p><div id="id-1.3.5.2.10.3.1.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
            You can run the above commands in parallel on multiple OSD daemons
            on the same OSD node to help accelerate the upgrade.
          </p></div></li><li class="step"><p>
          If the node you are upgrading is an OSD node, avoid having the OSD
          marked <code class="literal">out</code> during the upgrade by running the
          following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd add-noout <em class="replaceable">SHORT_NODE_NAME</em></pre></div><p>
          Replace <em class="replaceable">SHORT_NODE_NAME</em> with the short
          name of the node as it appears in the output of the <code class="command">ceph osd
          tree</code> command. In the following input, the short host names
          are <code class="literal">ses-node1</code> and <code class="literal">ses-node2</code>
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-node1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-node2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]</pre></div></li><li class="step"><p>
          Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP3:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
              If the cluster's nodes are all registered with SCC, run
              <code class="command">zypper migration</code>.
            </p></li><li class="listitem"><p>
              If the cluster's nodes have software repositories assigned
              manually, run <code class="command">zypper dup</code> followed by
              <code class="command">reboot</code>.
            </p></li></ul></div></li><li class="step"><p>
          If the node you are upgrading is an OSD node, then, after the OSD
          node with the Salt minion ID <em class="replaceable">MINION_ID</em>
          has been rebooted and is now up, run the following command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">MINION_ID</em> state.apply ceph.upgrade.ses7.adopt</pre></div></li><li class="step"><p>
          If the node you are upgrading is not an OSD node, then after the node
          is rebooted, containerize all existing MON and MGR daemons on that
          node by running the following command on the Salt Master:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">MINION_ID</em> state.apply ceph.upgrade.ses7.adopt</pre></div><p>
          Replace <em class="replaceable">MINION_ID</em> with the ID of the
          minion that you are upgrading. You can get the list of minion IDs by
          running the <code class="command">salt-key -L</code> command on the Salt Master.
        </p><div id="id-1.3.5.2.10.3.5.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
            To see the status and progress of the
            <span class="emphasis"><em>adoption</em></span>, check the Ceph Dashboard or run one of
            the following commands on the Salt Master:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph status
<code class="prompt user">root@master # </code>ceph versions
<code class="prompt user">root@master # </code>salt-run upgrade.status</pre></div></div></li><li class="step"><p>
          After the adoption has successfully finished, unset the
          <code class="literal">noout</code> flag if the node you are upgrading is an OSD
          node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd rm-noout <em class="replaceable">SHORT_NODE_NAME</em></pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-gateway-nodes" data-id-title="Upgrading gateway nodes"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.4 </span><span class="title-name">Upgrading gateway nodes</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-gateway-nodes">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Upgrade your separate gateway nodes (Samba Gateway, Metadata Server, Object Gateway, NFS Ganesha, or
      iSCSI Gateway) next. Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP3 for each node:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
          If the cluster's nodes are all registered with SUSE Customer Center, run the
          <code class="command">zypper migration</code> command.
        </p></li><li class="listitem"><p>
          If the cluster's nodes have software repositories assigned manually,
          run the <code class="command">zypper dup</code> followed by the
          <code class="command">reboot</code> commands.
        </p></li></ul></div><p>
      This step also applies to any nodes that are part of the cluster, but do
      not yet have any roles assigned (if in doubt, check the list of hosts on
      the Salt Master provided by the <code class="command">salt-key -L</code> command and
      compare it to the output of the <code class="command">salt-run
      upgrade.status</code> command).
    </p><p>
      When the OS is upgraded on all nodes in the cluster, the next step is to
      install the <span class="package">ceph-salt</span> package and apply the cluster
      configuration. The actual gateway services are redeployed in a
      containerized mode at the end of the upgrade procedure.
    </p><div id="id-1.3.5.2.11.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        To successfully upgrade the Metadata Server, ensure you reduce the Metadata Server to 1.
      </p><p>
        Run <code class="command">salt-run upgrade.status</code> to ensure that all
        Metadata Servers on standby are stopped.
      </p><p>
        Ensure you stop the Metadata Server before upgrading the Ceph Monitor nodes as it may
        otherwise result in a failed quorum.
      </p></div><div id="id-1.3.5.2.11.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
        Metadata Server and Object Gateway services are unavailable from the time of upgrading to
        SUSE Linux Enterprise Server 15 SP3 until they are redeployed at the end of the upgrade procedure.
      </p></div><div id="id-1.3.5.2.11.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
        SUSE Enterprise Storage 7.1 does not use the
        <code class="option">rgw_frontend_ssl_key</code> option. Instead, both the SSL key
        and certificate are concatenated under the
        <code class="option">rgw_frontend_ssl_certificate</code> option. If the Object Gateway
        deployment uses the <code class="option">rgw_frontend_ssl_key</code> option, it
        will not be available after the upgrade to SUSE Enterprise Storage
        7.1. In this case, the Object Gateway must be redeployed with the
        <code class="option">rgw_frontend_ssl_certificate</code> option. Refer to
        <a class="xref" href="deploy-core.html#cephadm-deploy-using-secure-ssl-access" title="8.3.4.1. Using secure SSL access">Section 8.3.4.1, “Using secure SSL access”</a> for more
        details.
      </p></div></section><section class="sect1" id="upgrade-cephsalt" data-id-title="Installing ceph-salt and applying the cluster configuration"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.5 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code> and applying the cluster configuration</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-cephsalt">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      Before you start the procedure of installing <code class="systemitem">ceph-salt</code> and applying the
      cluster configuration, check the cluster and upgrade status by running
      the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph status
<code class="prompt user">root@master # </code>ceph versions
<code class="prompt user">root@master # </code>salt-run upgrade.status</pre></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Remove the DeepSea-created <code class="literal">rbd_exporter</code> and
          <code class="literal">rgw_exporter</code> cron jobs. On the Salt Master as the
          <code class="systemitem">root</code> run the <code class="command">crontab -e</code> command to edit the
          crontab. Delete the following items if present:
        </p><div class="verbatim-wrap"><pre class="screen"># SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null</pre></div></li><li class="step"><p>
          Export cluster configuration from DeepSea, by running the following
          commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<code class="prompt user">root@master # </code>salt-run upgrade.generate_service_specs &gt; specs.yaml</pre></div></li><li class="step"><p>
          Uninstall DeepSea and install <code class="systemitem">ceph-salt</code> on the Salt Master:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper remove 'deepsea*'
<code class="prompt user">root@master # </code>zypper install ceph-salt</pre></div></li><li class="step"><p>
          Restart the Salt Master and synchronize Salt modules:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt \* saltutil.sync_all</pre></div></li><li class="step"><p>
          Import DeepSea's cluster configuration into <code class="systemitem">ceph-salt</code>:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt import ceph-salt-config.json</pre></div></li><li class="step"><p>
          Generate SSH keys for cluster node communication:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ssh generate</pre></div><div id="id-1.3.5.2.12.4.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
            Verify that the cluster configuration was imported from DeepSea
            and specify potentially missed options:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div><p>
            For a complete description of cluster configuration, refer to
            <a class="xref" href="deploy-bootstrap.html#deploy-cephadm-configure" title="7.2. Configuring cluster properties">Section 7.2, “Configuring cluster properties”</a>.
          </p></div></li><li class="step"><p>
          Apply the configuration and enable cephadm:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply</pre></div></li><li class="step"><p>
          If you need to supply local container registry URL and access
          credentials, follow the steps described in
          <a class="xref" href="deploy-bootstrap.html#deploy-cephadm-configure-registry" title="7.2.10. Using the container registry">Section 7.2.10, “Using the container registry”</a>.
        </p></li><li class="step"><ol type="a" class="substeps"><li class="step"><p>
              If you <span class="bold"><strong>are</strong></span> using container
              images from <code class="literal">registry.suse.com</code>, you need to set
              the <code class="option">container_image</code> option:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global container_image registry.suse.com/ses/7.1/ceph/ceph:latest</pre></div></li><li class="step"><p>
              If you are <span class="bold"><strong>not</strong></span> using container
              images from <code class="literal">registry.suse.com</code> but rather the
              locally-configured registry, inform Ceph which container image
              to use by running the following command:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global container_image <em class="replaceable">IMAGE_NAME</em></pre></div><p>
              For example:
            </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</pre></div></li></ol></li><li class="step"><p>
          Stop and disable the SUSE Enterprise Storage 6
          <code class="systemitem">ceph-crash</code> daemons. New
          containerized forms of these daemons are started later automatically.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' service.stop ceph-crash
<code class="prompt user">root@master # </code>salt '*' service.disable ceph-crash</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-cephsalt-monitoring" data-id-title="Upgrading and adopting the monitoring stack"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.6 </span><span class="title-name">Upgrading and adopting the monitoring stack</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-cephsalt-monitoring">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      This following procedure adopts all components of the monitoring stack
      (see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”</span> for more details).
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Pause the orchestrator:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch pause</pre></div></li><li class="step"><p>
          On whichever node is running Prometheus, Grafana and
          Alertmanager (the Salt Master by default), run the following
          commands:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name prometheus.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name grafana.$(hostname)</pre></div><div id="id-1.3.5.2.13.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
            If you are <span class="bold"><strong>not</strong></span> running the default
            container image registry <code class="literal">registry.suse.com</code>, you
            need to specify the image to use on each command, for example:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-server:2.32.1 \
  adopt --style=legacy --name prometheus.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-alertmanager:0.23.0 \
  adopt --style=legacy --name alertmanager.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/grafana:8.3.10 \
 adopt --style=legacy --name grafana.$(hostname)</pre></div><p>
            The container images required and their respective versions are
            listed in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.1 “Configuring custom or local images”</span>.
          </p></div></li><li class="step"><p>
          Remove Node-Exporter from <span class="bold"><strong>all</strong></span> nodes.
          The Node-Exporter does not need to be migrated and will be
          re-installed as a container when the <code class="filename">specs.yaml</code>
          file is applied.
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper rm golang-github-prometheus-node_exporter</pre></div><p>
          Alternatively, you can remove Node-Exporter from all nodes
          simultaneously using Salt on the admin node:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' pkg.remove golang-github-prometheus-node_exporter</pre></div></li><li class="step"><p>
          If you are using a custom container image registry that requires
          authentication, run a login command to verify that the images can be
          pulled:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph cephadm registry-login <em class="replaceable">URL</em> <em class="replaceable">USERNAME</em> <em class="replaceable">PASSWORD</em></pre></div></li><li class="step"><p>
          Apply the service specifications that you previously exported from
          DeepSea:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i specs.yaml</pre></div><div id="id-1.3.5.2.13.3.5.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
            If you are <span class="bold"><strong>not</strong></span> running the default
            container image registry <code class="literal">registry.suse.com</code>, but
            a local container registry, configure cephadm to use the
            container image from the local registry for the deployment of
            Node-Exporter before deploying the Node-Exporter. Otherwise you can
            safely skip this step and ignore the following warning.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/cephadm/container_image_node_exporter <em class="replaceable">QUALIFIED_IMAGE_PATH</em></pre></div><p>
            Make sure that all container images for monitoring services point
            to the local registry, not only the one for Node-Exporter. This
            step requires you to do so for the Node-Exporter only, but it is
            advised than you set all the monitoring container images in
            cephadm to point to the local registry at this point.
          </p><p>
            If you do not do so, new deployments of monitoring services as well
            as re-deployments will use the default cephadm configuration and
            you may end up being unable to deploy services (in the case of
            air-gapped deployments), or with services deployed with mixed
            versions.
          </p><p>
            How cephadm needs to be configured to use container images from
            the local registry is described in
            <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.1 “Configuring custom or local images”</span>.
          </p></div></li><li class="step"><p>
          Resume the orchestrator:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch resume</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-gateways" data-id-title="Gateway service redeployment"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.7 </span><span class="title-name">Gateway service redeployment</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-gateways">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect2" id="upgrade-ogw" data-id-title="Upgrading the Object Gateway"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.7.1 </span><span class="title-name">Upgrading the Object Gateway</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-ogw">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        In SUSE Enterprise Storage 7.1, the Object Gateways are always configured with
        a realm, which allows for multi-site (see
        <span class="intraxref">Book “Administration and Operations Guide”, Chapter 21 “Ceph Object Gateway”, Section 21.13 “Multisite Object Gateways”</span> for more details) in the future. If you
        used a single-site Object Gateway configuration in SUSE Enterprise Storage
        6, follow these steps to add a realm. If you do
        not plan to use the multi-site functionality, you can use
        <code class="literal">default</code> for the realm, zonegroup and zone names.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Create a new realm:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm create --rgw-realm=<em class="replaceable">REALM_NAME</em> --default</pre></div></li><li class="step"><p>
            Optionally, rename the default zone and zonegroup.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<em class="replaceable">ZONEGROUP_NAME</em>
<code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <em class="replaceable">ZONE_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em></pre></div></li><li class="step"><p>
            Configure the master zonegroup:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup modify \
 --rgw-realm=<em class="replaceable">REALM_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em> \
 --endpoints http://<em class="replaceable">RGW.EXAMPLE.COM</em>:80 \
 --master --default</pre></div></li><li class="step"><p>
            Configure the master zone. For this, you will need the ACCESS_KEY
            and SECRET_KEY of an Object Gateway user with the <code class="option">system</code>
            flag enabled. This is usually the <code class="literal">admin</code> user. To
            get the ACCESS_KEY and SECRET_KEY, run <code class="command">radosgw-admin user
            info --uid admin
            --rgw-zone=<em class="replaceable">ZONE_NAME</em></code>.
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify \
 --rgw-realm=<em class="replaceable">REALM_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE_NAME</em> \
 --endpoints http://<em class="replaceable">RGW.EXAMPLE.COM</em>:80 \
 --access-key=<em class="replaceable">ACCESS_KEY</em> \
 --secret=<em class="replaceable">SECRET_KEY</em> \
 --master --default</pre></div></li><li class="step"><p>
            Commit the updated configuration:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></li></ol></div></div><p>
        To have the Object Gateway service containerized, create its specification file
        as described in <a class="xref" href="deploy-core.html#deploy-cephadm-day2-service-ogw" title="8.3.4. Deploying Object Gateways">Section 8.3.4, “Deploying Object Gateways”</a>, and
        apply it.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">RGW</em>.yml</pre></div><div id="id-1.3.5.2.14.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
          After applying the new Object Gateway specification, run <code class="command">ceph config
          dump</code> and inspect the lines that contain
          <code class="literal">client.rgw.</code> to see if there are any old settings
          that need to be applied to the new Object Gateway instances.
        </p></div></section><section class="sect2" id="upgrade-ganesha" data-id-title="Upgrading NFS Ganesha"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.7.2 </span><span class="title-name">Upgrading NFS Ganesha</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-ganesha">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><div id="id-1.3.5.2.14.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
  NFS Ganesha supports NFS version 4.1 and newer.
  It does not support NFS version 3.
 </p></div><div id="id-1.3.5.2.14.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
  The upgrade process disables the <code class="literal">nfs</code> module in the Ceph Manager daemon. 
  You can re-enable it by executing the following command from the Admin Node:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable nfs</pre></div></div><p>
        The following demonstrates how to migrate an existing NFS Ganesha service
        running Ceph Nautilus to an NFS Ganesha container running Ceph
        Octopus.
      </p><div id="id-1.3.5.2.14.3.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
          The following documentation requires you to have already successfully
          upgraded the core Ceph services.
        </p></div><p>
        NFS Ganesha stores additional per-daemon configuration and exports
        configuration in a RADOS pool. The configured RADOS pool can be
        found on the <code class="literal">watch_url</code> line of the
        <code class="literal">RADOS_URLS</code> block in the
        <code class="filename">ganesha.conf</code> file. By default, this pool will be
        named <code class="literal">ganesha_config</code>
      </p><p>
        Before attempting any migration, we strongly recommend making a copy of
        the export and daemon configuration objects located in the RADOS pool.
        To locate the configured RADOS pool, run the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</pre></div><p>
        To list the contents of the RADOS pool:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</pre></div><p>
        To copy the RADOS objects:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<code class="prompt user">cephuser@adm &gt; </code>OBJS=$(rados $RADOS_ARGS ls)
<code class="prompt user">cephuser@adm &gt; </code>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<code class="prompt user">cephuser@adm &gt; </code>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</pre></div><p>
        On a per-node basis, any existing NFS Ganesha service needs to be stopped
        and then replaced with a container managed by cephadm.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Stop and disable the existing NFS Ganesha service:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>systemctl stop nfs-ganesha
<code class="prompt user">cephuser@adm &gt; </code>systemctl disable nfs-ganesha</pre></div></li><li class="step"><p>
            After the existing NFS Ganesha service has been stopped, a new one
            can be deployed in a container using cephadm. To do so, you need
            to create a service specification that contains a
            <code class="literal">service_id</code> that will be used to identify this
            new NFS cluster, the host name of the node we are migrating listed
            as a host in the placement specification, and the RADOS pool and
            namespace that contains the configured NFS export objects. For
            example:
          </p><div class="verbatim-wrap"><pre class="screen">service_type: nfs
service_id: <em class="replaceable">SERVICE_ID</em>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha</pre></div><p>
            For more information on creating a placement specification, see
            <a class="xref" href="deploy-core.html#cephadm-service-and-placement-specs" title="8.2. Service and placement specification">Section 8.2, “Service and placement specification”</a>.
          </p></li><li class="step"><p>
            Apply the placement specification:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">FILENAME</em>.yaml</pre></div></li><li class="step"><p>
            Confirm the NFS Ganesha daemon is running on the host:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7.1/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</pre></div></li><li class="step"><p>
            Repeat these steps for each NFS Ganesha node. You do not need to
            create a separate service specification for each node. It is
            sufficient to add each node's host name to the existing NFS service
            specification and re-apply it.
          </p></li></ol></div></div><p>
        The existing exports can be migrated in two different ways:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
            Manually re-created or re-assigned using the Ceph Dashboard.
          </p></li><li class="listitem"><p>
            Manually copy the contents of each per-daemon RADOS object into
            the newly created NFS Ganesha common configuration.
          </p></li></ul></div><div class="procedure" id="id-1.3.5.2.14.3.17" data-id-title="Manually copying exports to NFS Ganesha common configuration file"><div class="title-container"><div class="procedure-title-wrap"><div class="procedure-title"><span class="title-number-name"><span class="title-number">Procedure 10.1: </span><span class="title-name">Manually copying exports to NFS Ganesha common configuration file </span></span><a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#id-1.3.5.2.14.3.17">#</a></div></div><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Determine the list of per-daemon RADOS objects:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<code class="prompt user">cephuser@adm &gt; </code>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</pre></div></li><li class="step"><p>
            Make a copy of the per-daemon RADOS objects:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<code class="prompt user">cephuser@adm &gt; </code>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<em class="replaceable">SERVICE_ID</em>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</pre></div></li><li class="step"><p>
            Sort and merge into a single list of exports:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat conf-* | sort -u &gt; conf-nfs.<em class="replaceable">SERVICE_ID</em>
<code class="prompt user">cephuser@adm &gt; </code>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</pre></div></li><li class="step"><p>
            Write the new NFS Ganesha common configuration file:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados $RADOS_ARGS put conf-nfs.<em class="replaceable">SERVICE_ID</em> conf-nfs.<em class="replaceable">SERVICE_ID</em></pre></div></li><li class="step"><p>
            Notify the NFS Ganesha daemon:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados $RADOS_ARGS notify conf-nfs.<em class="replaceable">SERVICE_ID</em> conf-nfs.<em class="replaceable">SERVICE_ID</em></pre></div><div id="id-1.3.5.2.14.3.17.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
              This action will cause the daemon to reload the configuration.
            </p></div></li></ol></div></div><p>
        After the service has been successfully migrated, the Nautilus-based
        NFS Ganesha service can be removed.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Remove NFS Ganesha:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</pre></div></li><li class="step"><p>
            Remove the legacy cluster settings from the Ceph Dashboard:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard reset-ganesha-clusters-rados-pool-namespace</pre></div></li></ol></div></div></section><section class="sect2" id="upgrade-mds" data-id-title="Upgrading the Metadata Server"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.7.3 </span><span class="title-name">Upgrading the Metadata Server</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-mds">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        Unlike MONs, MGRs and OSDs, Metadata Server cannot be adopted in-place. Instead,
        you need to redeploy them in containers using the Ceph orchestrator.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Run the <code class="command">ceph fs ls</code> command to obtain the name of
            your file system, for example:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</pre></div></li><li class="step"><p>
            Create a new service specification file
            <code class="filename">mds.yml</code> as described in
            <a class="xref" href="deploy-core.html#deploy-cephadm-day2-service-mds" title="8.3.3. Deploying Metadata Servers">Section 8.3.3, “Deploying Metadata Servers”</a> by using the file
            system name as the <code class="option">service_id</code> and specifying the
            hosts that will run the MDS daemons. For example:
          </p><div class="verbatim-wrap"><pre class="screen">service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3</pre></div></li><li class="step"><p>
            Run the <code class="command">ceph orch apply -i mds.yml</code> command to
            apply the service specification and start the MDS daemons.
          </p></li></ol></div></div></section><section class="sect2" id="upgrade-igw" data-id-title="Upgrading the iSCSI Gateway"><div class="titlepage"><div><div><div class="title-container"><h3 class="title"><span class="title-number-name"><span class="title-number">10.7.4 </span><span class="title-name">Upgrading the iSCSI Gateway</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-igw">#</a></h3><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
        To upgrade the iSCSI Gateway, you need to redeploy it in containers using the
        Ceph orchestrator. If you have multiple iSCSI Gateways, you need to redeploy
        them one-by-one to reduce the service downtime.
      </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
            Stop and disable the existing iSCSI daemons on each iSCSI Gateway node:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop rbd-target-gw
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl disable rbd-target-gw
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop rbd-target-api
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl disable rbd-target-api</pre></div></li><li class="step"><p>
            Create a service specification for the iSCSI Gateway as described in
            <a class="xref" href="deploy-core.html#deploy-cephadm-day2-service-igw" title="8.3.5. Deploying iSCSI Gateways">Section 8.3.5, “Deploying iSCSI Gateways”</a>. For this,
            you need the <code class="option">pool</code>,
            <code class="option">trusted_ip_list</code>, and <code class="option">api_*</code>
            settings from the existing
            <code class="filename">/etc/ceph/iscsi-gateway.cfg</code> file. If you have
            SSL support enabled (<code class="literal">api_secure = true</code>), you
            also need the SSL certificate
            (<code class="filename">/etc/ceph/iscsi-gateway.crt</code>) and key
            (<code class="filename">/etc/ceph/iscsi-gateway.key</code>).
          </p><p>
            For example, if <code class="filename">/etc/ceph/iscsi-gateway.cfg</code>
            contains the following:
          </p><div class="verbatim-wrap"><pre class="screen">[config]
cluster_client_name = client.igw.ses-node5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true</pre></div><p>
            Then you need to create the following service specification file
            <code class="filename">iscsi.yml</code>:
          </p><div class="verbatim-wrap"><pre class="screen">service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-node5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----</pre></div><div id="id-1.3.5.2.14.5.3.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
              The <code class="option">pool</code>, <code class="option">trusted_ip_list</code>,
              <code class="option">api_port</code>, <code class="option">api_user</code>,
              <code class="option">api_password</code>, <code class="option">api_secure</code>
              settings are identical to the ones from the
              <code class="filename">/etc/ceph/iscsi-gateway.cfg</code> file. The
              <code class="option">ssl_cert</code> and <code class="option">ssl_key</code> values can
              be copied in from the existing SSL certificate and key files.
              Verify that they are indented correctly and the
              <span class="emphasis"><em>pipe</em></span> character <code class="literal">|</code> appears
              at the end of the <code class="literal">ssl_cert:</code> and
              <code class="literal">ssl_key:</code> lines (see the content of the
              <code class="filename">iscsi.yml</code> file above).
            </p></div></li><li class="step"><p>
            Run the <code class="command">ceph orch apply -i iscsi.yml</code> command to
            apply the service specification and start the iSCSI Gateway daemons.
          </p></li><li class="step"><p>
            Remove the old <span class="package">ceph-iscsi</span> package from each of
            the existing iSCSI gateway nodes:
          </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>zypper rm -u ceph-iscsi</pre></div></li></ol></div></div></section></section><section class="sect1" id="upgrade-post-cleanup" data-id-title="Post-upgrade Clean-up"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">10.8 </span><span class="title-name">Post-upgrade Clean-up</span></span> <a title="Permalink" class="permalink" href="cha-ceph-upgrade.html#upgrade-post-cleanup">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_upgrade.xml" title="Edit source document"> </a></div></div></div></div></div><p>
      After the upgrade, perform the following clean-up steps:
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
          Verify that the cluster was successfully upgraded by checking the
          current Ceph version:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph versions</pre></div></li><li class="step"><p>
          Check that the <code class="option">osdspec_affinity</code> entry is properly
          set for existing OSDs. If the <code class="command">ceph osd stat</code>
          command shows some OSDs as not running or unknown, refer to
          <a class="link" href="https://www.suse.com/support/kb/doc/?id=000020667" target="_blank">https://www.suse.com/support/kb/doc/?id=000020667</a>
          to get more details on properly mapp OSDs to a service specification.
        </p></li><li class="step"><p>
          Make sure that no old OSDs will join the cluster:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd require-osd-release pacific</pre></div></li><li class="step"><p>
          Set the <code class="option">pg_autoscale_mode</code> of existing pools if
          necessary:
        </p><div id="id-1.3.5.2.15.3.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
            Pools in SUSE Enterprise Storage 6 had the <code class="option">pg_autoscale_mode</code>
            set to <code class="option">warn</code> by default. This resulted in a warning
            message in case of suboptimal number of PGs, but autoscaling did
            not actually happen. The default in SUSE Enterprise Storage 7.1
            is that the <code class="option">pg_autoscale_mode</code> option is set to
            <code class="option">on</code> for new pools, and PGs will actually autoscale.
            The upgrade process does not automatically change the
            <code class="option">pg_autoscale_mode</code> of existing pools. If you want
            to change it to <code class="option">on</code> to get the full benefit of the
            autoscaler, see the instructions in
            <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.4.12 “Enabling the PG auto-scaler”</span>.
          </p></div><p>
          Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.4.12 “Enabling the PG auto-scaler”</span>.
        </p></li><li class="step"><p>
          Set the FSMap sanity checks to the default value and then remove its
          configuration:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph config set mon mon_mds_skip_sanity false</code>
<code class="prompt user">cephuser@adm &gt; </code><code class="command">ceph config rm mon mon_mds_skip_sanity</code></pre></div></li><li class="step"><p>
          Prevent pre-Luminous clients:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set-require-min-compat-client luminous</pre></div></li><li class="step"><p>
          Enable the balancer module:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer mode upmap
<code class="prompt user">cephuser@adm &gt; </code>ceph balancer on</pre></div><p>
          Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 29 “Ceph Manager modules”, Section 29.1 “Balancer”</span>.
        </p></li><li class="step"><p>
          Optionally, enable the telemetry module:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable telemetry
<code class="prompt user">cephuser@adm &gt; </code>ceph telemetry on</pre></div><p>
          Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 29 “Ceph Manager modules”, Section 29.2 “Enabling the telemetry module”</span>.
        </p></li></ol></div></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="ses-upgrade.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part III </span>Upgrading from Previous Releases</span></a> </div><div><a class="pagination-link next" href="upgrade-to-pacific.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 11 </span>Upgrade from SUSE Enterprise Storage 7 to 7.1</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-ceph-upgrade.html#before-upgrade"><span class="title-number">10.1 </span><span class="title-name">Before upgrading</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-salt-master"><span class="title-number">10.2 </span><span class="title-name">Upgrading the Salt Master</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-mon-mgr-nodes"><span class="title-number">10.3 </span><span class="title-name">Upgrading the MON, MGR, and OSD nodes</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-gateway-nodes"><span class="title-number">10.4 </span><span class="title-name">Upgrading gateway nodes</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-cephsalt"><span class="title-number">10.5 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code> and applying the cluster configuration</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-cephsalt-monitoring"><span class="title-number">10.6 </span><span class="title-name">Upgrading and adopting the monitoring stack</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-gateways"><span class="title-number">10.7 </span><span class="title-name">Gateway service redeployment</span></a></span></li><li><span class="sect1"><a href="cha-ceph-upgrade.html#upgrade-post-cleanup"><span class="title-number">10.8 </span><span class="title-name">Post-upgrade Clean-up</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2023</span></div></div></footer></body></html>