<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><title>SES 7.1 | Troubleshooting Guide | Frequently asked questions</title><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/"/>
<meta name="title" content="Frequently asked questions | SES 7.1"/>
<meta name="description" content=""/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7.1"/>
<meta name="book-title" content="Troubleshooting Guide"/>
<meta name="chapter-title" content="Chapter 14. Frequently asked questions"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta name="publisher" content="SUSE"/><meta property="og:title" content="Frequently asked questions | SES 7.1"/>
<meta property="og:description" content=""/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Frequently asked questions | SES 7.1"/>
<meta name="twitter:description" content=""/>
<link rel="prev" href="storage-tips.html" title="Chapter 13. Hints and tips"/><link rel="next" href="bk03apa.html" title="Appendix A. Ceph maintenance updates based on upstream 'Pacific' point releases"/><script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Troubleshooting Guide</a><span> / </span><a class="crumb" href="storage-faqs.html">Frequently asked questions</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Troubleshooting Guide</div><ol><li><a href="preface-troubleshooting.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="report-software.html" class=" "><span class="title-number">1 </span><span class="title-name">Reporting software problems</span></a></li><li><a href="bp-troubleshooting-logging.html" class=" "><span class="title-number">2 </span><span class="title-name">Troubleshooting logging and debugging</span></a></li><li><a href="bp-troubleshooting-cephadm.html" class=" "><span class="title-number">3 </span><span class="title-name">Troubleshooting cephadm</span></a></li><li><a href="bp-troubleshooting-osds.html" class=" "><span class="title-number">4 </span><span class="title-name">Troubleshooting OSDs</span></a></li><li><a href="bp-troubleshooting-pgs.html" class=" "><span class="title-number">5 </span><span class="title-name">Troubleshooting placement groups (PGs)</span></a></li><li><a href="bp-troubleshooting-monitors.html" class=" "><span class="title-number">6 </span><span class="title-name">Troubleshooting Ceph Monitors and Ceph Managers</span></a></li><li><a href="bp-troubleshooting-networking.html" class=" "><span class="title-number">7 </span><span class="title-name">Troubleshooting networking</span></a></li><li><a href="bp-troubleshooting-nfs.html" class=" "><span class="title-number">8 </span><span class="title-name">Troubleshooting NFS Ganesha</span></a></li><li><a href="bp-troubleshooting-status.html" class=" "><span class="title-number">9 </span><span class="title-name">Troubleshooting Ceph health status</span></a></li><li><a href="bp-troubleshooting-dashboard.html" class=" "><span class="title-number">10 </span><span class="title-name">Troubleshooting the Ceph Dashboard</span></a></li><li><a href="bp-troubleshooting-objectgateway.html" class=" "><span class="title-number">11 </span><span class="title-name">Troubleshooting Object Gateway</span></a></li><li><a href="bp-troubleshooting-cephfs.html" class=" "><span class="title-number">12 </span><span class="title-name">Troubleshooting CephFS</span></a></li><li><a href="storage-tips.html" class=" "><span class="title-number">13 </span><span class="title-name">Hints and tips</span></a></li><li><a href="storage-faqs.html" class=" you-are-here"><span class="title-number">14 </span><span class="title-name">Frequently asked questions</span></a></li><li><a href="bk03apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Pacific' point releases</span></a></li><li><a href="bk03go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="storage-faqs" data-id-title="Frequently asked questions"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7.1</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">14 </span><span class="title-name">Frequently asked questions</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><section class="sect1" id="storage-bp-tuneups-pg-num" data-id-title="How does the number of placement groups affect the cluster performance?"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.1 </span><span class="title-name">How does the number of placement groups affect the cluster performance?</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#storage-bp-tuneups-pg-num">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When your cluster is becoming 70% to 80% full, it is time to add more OSDs
   to it. When you increase the number of OSDs, you may consider increasing the
   number of placement groups as well.
  </p><div id="id-1.5.16.4.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning</div><p>
    Changing the number of PGs causes a lot of data transfer within the
    cluster.
   </p></div><p>
   To calculate the optimal value for your newly-resized cluster is a complex
   task.
  </p><p>
   A high number of PGs creates small chunks of data. This speeds up recovery
   after an OSD failure, but puts a lot of load on the monitor nodes as they
   are responsible for calculating the data location.
  </p><p>
   On the other hand, a low number of PGs takes more time and data transfer to
   recover from an OSD failure, but does not impose that much load on monitor
   nodes as they need to calculate locations for less (but larger) data chunks.
  </p><p>
   Find more information on the optimal number of PGs for your cluster in
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.4.2 “Determining the value of <em class="replaceable">PG_NUM</em>”</span>.
  </p></section><section class="sect1" id="storage-bp-tuneups-mix-ssd" data-id-title="Can I use SSDs and hard disks on the same cluster?"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.2 </span><span class="title-name">Can I use SSDs and hard disks on the same cluster?</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#storage-bp-tuneups-mix-ssd">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Solid-state drives (SSD) are generally faster than hard disks. If you mix
   the two types of disks for the same write operation, the data writing to the
   SSD disk will be slowed down by the hard disk performance. Thus, you should
   <span class="emphasis"><em>never mix SSDs and hard disks</em></span> for data writing
   following <span class="emphasis"><em>the same rule</em></span> (see
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.3 “Rule sets”</span> for more information on rules for storing
   data).
  </p><p>
   There are generally 2 cases where using SSD and hard disk on the same
   cluster makes sense:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Use each disk type for writing data following different rules. Then you
     need to have a separate rule for the SSD disk, and another rule for the
     hard disk.
    </p></li><li class="listitem"><p>
     Use each disk type for a specific purpose. For example the SSD disk for
     journal, and the hard disk for storing data.
    </p></li></ol></div></section><section class="sect1" id="storage-bp-tuneups-ssd-tradeoffs" data-id-title="What are the trade-offs of ssing a journal on SSD?"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.3 </span><span class="title-name">What are the trade-offs of ssing a journal on SSD?</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#storage-bp-tuneups-ssd-tradeoffs">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Using SSDs for OSD journal(s) is better for performance as the journal is
   usually the bottleneck of hard disk-only OSDs. SSDs are often used to share
   journals of several OSDs.
  </p><p>
   Following is a list of potential disadvantages of using SSDs for OSD
   journal:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     SSD disks are more expensive than hard disks. But as one OSD journal
     requires up to 6GB of disk space only, the price may not be so crucial.
    </p></li><li class="listitem"><p>
     SSD disk consumes storage slots which can be otherwise used by a large
     hard disk to extend the cluster capacity.
    </p></li><li class="listitem"><p>
     SSD disks have reduced write cycles compared to hard disks, but modern
     technologies are beginning to eliminate the problem.
    </p></li><li class="listitem"><p>
     If you share more journals on the same SSD disk, you risk losing all the
     related OSDs after the SSD disk fails. This will require a lot of data to
     be moved to rebalance the cluster.
    </p></li><li class="listitem"><p>
     Hotplugging disks becomes more complex as the data mapping is not 1:1 the
     failed OSD and the journal disk.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-diskfails" data-id-title="What happens when a disk fails?"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.4 </span><span class="title-name">What happens when a disk fails?</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#storage-bp-monitoring-diskfails">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The related OSD crashed and is automatically removed from the cluster.
    </p></li><li class="listitem"><p>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </p></li><li class="listitem"><p>
     Then you should remove the disk from the cluster CRUSH Map, and
     physically from the host hardware.
    </p></li></ul></div></section><section class="sect1" id="storage-bp-monitoring-journalfails" data-id-title="What happens when a journal disk fails?"><div class="titlepage"><div><div><div class="title-container"><h2 class="title"><span class="title-number-name"><span class="title-number">14.5 </span><span class="title-name">What happens when a journal disk fails?</span></span> <a title="Permalink" class="permalink" href="storage-faqs.html#storage-bp-monitoring-journalfails">#</a></h2><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/bp_troubleshooting_faqs.xml" title="Edit source document"> </a></div></div></div></div></div><p>
   Ceph can be configured to store journals or write ahead logs on devices
   separate from the OSDs. When a disk dedicated to a journal fails, the
   related OSD(s) fail as well (see
   <a class="xref" href="storage-faqs.html#storage-bp-monitoring-diskfails" title="14.4. What happens when a disk fails?">Section 14.4, “What happens when a disk fails?”</a>).
  </p><div id="id-1.5.16.8.3" data-id-title="Hosting multiple journals on one disk" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Hosting multiple journals on one disk</div><p>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </p></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="storage-tips.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 13 </span>Hints and tips</span></a> </div><div><a class="pagination-link next" href="bk03apa.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Appendix A </span>Ceph maintenance updates based on upstream 'Pacific' point releases</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="storage-faqs.html#storage-bp-tuneups-pg-num"><span class="title-number">14.1 </span><span class="title-name">How does the number of placement groups affect the cluster performance?</span></a></span></li><li><span class="sect1"><a href="storage-faqs.html#storage-bp-tuneups-mix-ssd"><span class="title-number">14.2 </span><span class="title-name">Can I use SSDs and hard disks on the same cluster?</span></a></span></li><li><span class="sect1"><a href="storage-faqs.html#storage-bp-tuneups-ssd-tradeoffs"><span class="title-number">14.3 </span><span class="title-name">What are the trade-offs of ssing a journal on SSD?</span></a></span></li><li><span class="sect1"><a href="storage-faqs.html#storage-bp-monitoring-diskfails"><span class="title-number">14.4 </span><span class="title-name">What happens when a disk fails?</span></a></span></li><li><span class="sect1"><a href="storage-faqs.html#storage-bp-monitoring-journalfails"><span class="title-number">14.5 </span><span class="title-name">What happens when a journal disk fails?</span></a></span></li></ul></div><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter/X"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2024</span></div></div></footer></body></html>