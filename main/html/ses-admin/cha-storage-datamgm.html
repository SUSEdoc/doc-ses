<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Stored data management | Administration and Operations Guide | SUSE Enterprise Storage 7.1</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Stored data management | SES 7.1"/>
<meta name="description" content="The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OS…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7.1"/>
<meta name="book-title" content="Administration and Operations Guide"/>
<meta name="chapter-title" content="Chapter 17. Stored data management"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Stored data management | SES 7.1"/>
<meta property="og:description" content="The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OS…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Stored data management | SES 7.1"/>
<meta name="twitter:description" content="The CRUSH algorithm determines how to store and retrieve data by computing data storage locations. CRUSH empowers Ceph clients to communicate with OS…"/>
<link rel="prev" href="part-storing-data.html" title="Part III. Storing Data in a Cluster"/><link rel="next" href="ceph-pools.html" title="Chapter 18. Manage storage pools"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration and Operations Guide</a><span> / </span><a class="crumb" href="part-storing-data.html">Storing Data in a Cluster</a><span> / </span><a class="crumb" href="cha-storage-datamgm.html">Stored data management</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration and Operations Guide</div><ol><li><a href="preface-admin.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">I </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">1 </span><span class="title-name">About the Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">2 </span><span class="title-name">Dashboard's Web user interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">3 </span><span class="title-name">Manage Ceph Dashboard users and roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">4 </span><span class="title-name">View cluster internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">5 </span><span class="title-name">Manage pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">6 </span><span class="title-name">Manage RADOS Block Device</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">7 </span><span class="title-name">Manage NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">8 </span><span class="title-name">Manage CephFS</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">9 </span><span class="title-name">Manage the Object Gateway</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">10 </span><span class="title-name">Manual configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">11 </span><span class="title-name">Manage users and roles on the command line</span></a></li></ol></li><li><a href="part-cluster-operation.html" class="has-children "><span class="title-number">II </span><span class="title-name">Cluster Operation</span></a><ol><li><a href="ceph-monitor.html" class=" "><span class="title-number">12 </span><span class="title-name">Determine the cluster state</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">13 </span><span class="title-name">Operational tasks</span></a></li><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">14 </span><span class="title-name">Operation of Ceph services</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">15 </span><span class="title-name">Backup and restore</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">16 </span><span class="title-name">Monitoring and alerting</span></a></li></ol></li><li class="active"><a href="part-storing-data.html" class="has-children you-are-here"><span class="title-number">III </span><span class="title-name">Storing Data in a Cluster</span></a><ol><li><a href="cha-storage-datamgm.html" class=" you-are-here"><span class="title-number">17 </span><span class="title-name">Stored data management</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">18 </span><span class="title-name">Manage storage pools</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">19 </span><span class="title-name">Erasure coded pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">20 </span><span class="title-name">RADOS Block Device</span></a></li></ol></li><li><a href="part-accessing-data.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">22 </span><span class="title-name">Ceph iSCSI gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">23 </span><span class="title-name">Clustered file system</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">24 </span><span class="title-name">Export Ceph data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">25 </span><span class="title-name">NFS Ganesha</span></a></li></ol></li><li><a href="part-integration-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">26 </span><span class="title-name"><code class="systemitem">libvirt</code> and Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph as a back-end for QEMU KVM instance</span></a></li></ol></li><li><a href="part-cluster-configuration.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Configuring a Cluster</span></a><ol><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">28 </span><span class="title-name">Ceph cluster configuration</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">29 </span><span class="title-name">Ceph Manager modules</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">30 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li></ol></li><li><a href="bk02apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Pacific' point releases</span></a></li><li><a href="bk02go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="cha-storage-datamgm" data-id-title="Stored data management"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7.1</span></div><div><h1 class="title"><span class="title-number">17 </span><span class="title-name">Stored data management</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#">#</a></h1></div></div></div><p>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers Ceph clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, Ceph
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </p><p>
  CRUSH requires a map of your cluster, and uses the CRUSH Map to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </p><p>
  CRUSH maps contain a list of OSDs, a list of 'buckets' for aggregating the
  devices into physical locations, and a list of rules that tell CRUSH how it
  should replicate data in a Ceph cluster's pools. By reflecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </p><p>
  After you deploy a Ceph cluster, a default CRUSH Map is generated. It is
  fine for your Ceph sandbox environment. However, when you deploy a
  large-scale data cluster, you should give significant consideration to
  developing a custom CRUSH Map, because it will help you manage your Ceph
  cluster, improve performance and ensure data safety.
 </p><p>
  For example, if an OSD goes down, a CRUSH Map can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </p><p>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </p><p>
  A custom CRUSH Map can also help you identify the physical locations where
  Ceph stores redundant copies of data when the placement group(s) (refer to
  <a class="xref" href="cha-storage-datamgm.html#op-pgs" title="17.4. Placement groups">Section 17.4, “Placement groups”</a>) associated with a failed host are in a degraded
  state.
 </p><p>
  There are three main sections to a CRUSH Map.
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <a class="xref" href="cha-storage-datamgm.html#datamgm-devices" title="17.1. OSD devices">OSD devices</a> consist of any
    object storage device corresponding to a <code class="systemitem">ceph-osd</code>
    daemon.
   </p></li><li class="listitem"><p>
    <a class="xref" href="cha-storage-datamgm.html#datamgm-buckets" title="17.2. Buckets">Buckets</a> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </p></li><li class="listitem"><p>
    <a class="xref" href="cha-storage-datamgm.html#datamgm-rules" title="17.3. Rule sets">Rule sets</a> consist of the
    manner of selecting buckets.
   </p></li></ul></div><section class="sect1" id="datamgm-devices" data-id-title="OSD devices"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.1 </span><span class="title-name">OSD devices</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-devices">#</a></h2></div></div></div><p>
   To map placement groups to OSDs, a CRUSH Map requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   CRUSH Map.
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device <em class="replaceable">NUM</em> osd.<em class="replaceable">OSD_NAME</em> class <em class="replaceable">CLASS_NAME</em></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3 class ssd</pre></div><p>
   As a general rule, an OSD daemon maps to a single disk.
  </p><section class="sect2" id="crush-devclasses" data-id-title="Device classes"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.1.1 </span><span class="title-name">Device classes</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#crush-devclasses">#</a></h3></div></div></div><p>
    The flexibility of the CRUSH Map in controlling data placement is one of
    the Ceph's strengths. It is also one of the most difficult parts of the
    cluster to manage. <span class="emphasis"><em>Device classes</em></span> automate the most
    common changes to CRUSH Maps that the administrator needed to do manually
    previously.
   </p><section class="sect3" id="crush-management-problem" data-id-title="The CRUSH management problem"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.1 </span><span class="title-name">The CRUSH management problem</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#crush-management-problem">#</a></h4></div></div></div><p>
     Ceph clusters are frequently built with multiple types of storage
     devices: HDD, SSD, NVMe, or even mixed classes of the above. We call these
     different types of storage devices <span class="emphasis"><em>device classes</em></span> to
     avoid confusion between the <span class="emphasis"><em>type</em></span> property of CRUSH
     buckets (for example, host, rack, row, see
     <a class="xref" href="cha-storage-datamgm.html#datamgm-buckets" title="17.2. Buckets">Section 17.2, “Buckets”</a> for more details). Ceph OSDs backed by
     SSDs are much faster than those backed by spinning disks, making them
     better suited for certain workloads. Ceph makes it easy to create RADOS
     pools for different data sets or workloads and to assign different CRUSH
     rules to control data placement for those pools.
    </p><div class="figure" id="id-1.4.5.2.12.7.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/device_classes.png"><img src="images/device_classes.png" width="70%" alt="OSDs with mixed device classes" title="OSDs with mixed device classes"/></a></div></div><div class="figure-title-wrap"><div class="figure-title"><span class="title-number">Figure 17.1: </span><span class="title-name">OSDs with mixed device classes </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.12.7.3.3">#</a></div></div></div><p>
     However, setting up the CRUSH rules to place data only on a certain class
     of device is tedious. Rules work in terms of the CRUSH hierarchy, but if
     the devices are mixed into the same hosts or racks (as in the sample
     hierarchy above), they will (by default) be mixed together and appear in
     the same sub-trees of the hierarchy. Manually separating them out into
     separate trees involved creating multiple versions of each intermediate
     node for each device class in previous versions of SUSE Enterprise Storage.
    </p></section><section class="sect3" id="osd-crush-device-classes" data-id-title="Device classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.2 </span><span class="title-name">Device classes</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#osd-crush-device-classes">#</a></h4></div></div></div><p>
     An elegant solution that Ceph offers is to add a property called
     <span class="emphasis"><em>device class</em></span> to each OSD. By default, OSDs will
     automatically set their device classes to either 'hdd', 'ssd', or 'nvme'
     based on the hardware properties exposed by the Linux kernel. These device
     classes are reported in a new column of the <code class="command">ceph osd
     tree</code> command output:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000</pre></div><p>
     If the automatic device class detection fails, for example because the
     device driver is not properly exposing information about the device via
     <code class="filename">/sys/block</code>, you can adjust device classes from the
     command line:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
<code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set-device-class ssd osd.2 osd.3
set osd(s) 2,3 to class 'ssd'</pre></div></section><section class="sect3" id="crush-placement-rules" data-id-title="Setting CRUSH placement rules"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.3 </span><span class="title-name">Setting CRUSH placement rules</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#crush-placement-rules">#</a></h4></div></div></div><p>
     CRUSH rules can restrict placement to a specific device class. For
     example, you can create a 'fast'
     <span class="bold"><strong>replicated</strong></span> pool that distributes data
     only over SSD disks by running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule create-replicated <em class="replaceable">RULE_NAME</em> <em class="replaceable">ROOT</em> <em class="replaceable">FAILURE_DOMAIN_TYPE</em> <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule create-replicated fast default host ssd</pre></div><p>
     Create a pool named 'fast_pool' and assign it to the 'fast' rule:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create fast_pool 128 128 replicated fast</pre></div><p>
     The process for creating <span class="bold"><strong>erasure code</strong></span>
     rules is slightly different. First, you create an erasure code profile
     that includes a property for your desired device class. Then, use that
     profile when creating the erasure coded pool:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd erasure-code-profile set myprofile \
 k=4 m=2 crush-device-class=ssd crush-failure-domain=host
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool create mypool 64 erasure myprofile</pre></div><p>
     In case you need to manually edit the CRUSH Map to customize your rule,
     the syntax has been extended to allow the device class to be specified.
     For example, the CRUSH rule generated by the above commands looks as
     follows:
    </p><div class="verbatim-wrap"><pre class="screen">rule ecpool {
  id 2
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default <span class="bold"><strong>class ssd</strong></span>
  step chooseleaf indep 0 type host
  step emit
}</pre></div><p>
     The important difference here is that the 'take' command includes the
     additional 'class <em class="replaceable">CLASS_NAME</em>' suffix.
    </p></section><section class="sect3" id="crush-additional-commands" data-id-title="Additional commands"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.4 </span><span class="title-name">Additional commands</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#crush-additional-commands">#</a></h4></div></div></div><p>
     To list device classes used in a CRUSH Map, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush class ls
[
  "hdd",
  "ssd"
]</pre></div><p>
     To list existing CRUSH rules, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule ls
replicated_rule
fast</pre></div><p>
     To view details of the CRUSH rule named 'fast', run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule dump fast
{
		"rule_id": 1,
		"rule_name": "fast",
		"ruleset": 1,
		"type": 1,
		"min_size": 1,
		"max_size": 10,
		"steps": [
						{
										"op": "take",
										"item": -21,
										"item_name": "default~ssd"
						},
						{
										"op": "chooseleaf_firstn",
										"num": 0,
										"type": "host"
						},
						{
										"op": "emit"
						}
		]
}</pre></div><p>
     To list OSDs that belong to an 'ssd' class, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush class ls-osd ssd
0
1</pre></div></section><section class="sect3" id="device-classes-reclassify" data-id-title="Migrating from a legacy SSD rule to device classes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.5 </span><span class="title-name">Migrating from a legacy SSD rule to device classes</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#device-classes-reclassify">#</a></h4></div></div></div><p>
     In SUSE Enterprise Storage prior to version 5, you needed to manually edit the
     CRUSH Map and maintain a parallel hierarchy for each specialized device
     type (such as SSD) in order to write rules that apply to these devices.
     Since SUSE Enterprise Storage 5, the device class feature has enabled this
     transparently.
    </p><p>
     You can transform a legacy rule and hierarchy to the new class-based rules
     by using the <code class="command">crushtool</code> command. There are several types
     of transformation possible:
    </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.12.7.7.4.1"><span class="term"><code class="command">crushtool --reclassify-root <em class="replaceable">ROOT_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This command takes everything in the hierarchy beneath
        <em class="replaceable">ROOT_NAME</em> and adjusts any rules that
        reference that root via
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em></pre></div><p>
        to instead
       </p><div class="verbatim-wrap"><pre class="screen">take <em class="replaceable">ROOT_NAME</em> class <em class="replaceable">DEVICE_CLASS</em></pre></div><p>
        It renumbers the buckets so that the old IDs are used for the specified
        class's 'shadow tree'. As a consequence, no data movement occurs.
       </p><div class="complex-example"><div class="example" id="id-1.4.5.2.12.7.7.4.1.2.6" data-id-title="crushtool --reclassify-root"><div class="example-title-wrap"><div class="example-title"><span class="title-number">Example 17.1: </span><span class="title-name"><code class="command">crushtool --reclassify-root</code> </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.12.7.7.4.1.2.6">#</a></div></div><div class="example-contents"><p>
         Consider the following existing rule:
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div><p>
         If you reclassify the root 'default' as class 'hdd', the rule will
         become
        </p><div class="verbatim-wrap"><pre class="screen">rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default class hdd
   step chooseleaf firstn 0 type rack
   step emit
}</pre></div></div></div></div></dd><dt id="id-1.4.5.2.12.7.7.4.2"><span class="term"><code class="command">crushtool --set-subtree-class <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em></code></span></dt><dd><p>
        This method marks every device in the subtree rooted at
        <em class="replaceable">BUCKET_NAME</em> with the specified device class.
       </p><p>
        <code class="option">--set-subtree-class</code> is normally used in conjunction
        with the <code class="option">--reclassify-root</code> option to ensure that all
        devices in that root are labeled with the correct class. However, some
        of those devices may intentionally have a different class, and
        therefore you do not want to relabel them. In such cases, exclude the
        <code class="option">--set-subtree-class</code> option. Keep in mind that such
        remapping will not be perfect, because the previous rule is distributed
        across devices of multiple classes but the adjusted rules will only map
        to devices of the specified device class.
       </p></dd><dt id="id-1.4.5.2.12.7.7.4.3"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">MATCH_PATTERN</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">DEFAULT_PATTERN</em></code></span></dt><dd><p>
        This method allows merging a parallel type-specific hierarchy with the
        normal hierarchy. For example, many users have CRUSH Maps similar to
        the following one:
       </p><div class="example" id="id-1.4.5.2.12.7.7.4.3.2.2" data-id-title="crushtool --reclassify-bucket"><div class="example-title-wrap"><div class="example-title"><span class="title-number">Example 17.2: </span><span class="title-name"><code class="command">crushtool --reclassify-bucket</code> </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.12.7.7.4.3.2.2">#</a></div></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">host node1 {
   id -2           # do not change unnecessarily
   # weight 109.152
   alg straw
   hash 0  # rjenkins1
   item osd.0 weight 9.096
   item osd.1 weight 9.096
   item osd.2 weight 9.096
   item osd.3 weight 9.096
   item osd.4 weight 9.096
   item osd.5 weight 9.096
   [...]
}

host node1-ssd {
   id -10          # do not change unnecessarily
   # weight 2.000
   alg straw
   hash 0  # rjenkins1
   item osd.80 weight 2.000
   [...]
}

root default {
   id -1           # do not change unnecessarily
   alg straw
   hash 0  # rjenkins1
   item node1 weight 110.967
   [...]
}

root ssd {
   id -18          # do not change unnecessarily
   # weight 16.000
   alg straw
   hash 0  # rjenkins1
   item node1-ssd weight 2.000
   [...]
}</pre></div></div></div><p>
        This function reclassifies each bucket that matches a given pattern.
        The pattern can look like <code class="literal">%suffix</code> or
        <code class="literal">prefix%</code>. In the above example, you would use the
        pattern <code class="literal">%-ssd</code>. For each matched bucket, the
        remaining portion of the name that matches the '%' wild card specifies
        the base bucket. All devices in the matched bucket are labeled with the
        specified device class and then moved to the base bucket. If the base
        bucket does not exist (for example, if 'node12-ssd' exists but 'node12'
        does not), then it is created and linked underneath the specified
        default parent bucket. The old bucket IDs are preserved for the new
        shadow buckets to prevent data movement. Rules with the
        <code class="literal">take</code> steps that reference the old buckets are
        adjusted.
       </p></dd><dt id="id-1.4.5.2.12.7.7.4.4"><span class="term"><code class="command">crushtool --reclassify-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">DEVICE_CLASS</em> <em class="replaceable">BASE_BUCKET</em></code></span></dt><dd><p>
        You can use the <code class="option">--reclassify-bucket</code> option without a
        wild card to map a single bucket. For example, in the previous example,
        we want the 'ssd' bucket to be mapped to the default bucket.
       </p><p>
        The final command to convert the map comprised of the above fragments
        would be as follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap -o original
<code class="prompt user">cephuser@adm &gt; </code>crushtool -i original --reclassify \
  --set-subtree-class default hdd \
  --reclassify-root default hdd \
  --reclassify-bucket %-ssd ssd default \
  --reclassify-bucket ssd ssd default \
  -o adjusted</pre></div><p>
        In order to verify that the conversion is correct, there is a
        <code class="option">--compare</code> option that tests a large sample of inputs
        to the CRUSH Map and compares if the same result comes back out. These
        inputs are controlled by the same options that apply to the
        <code class="option">--test</code>. For the above example, the command would be as
        follows:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent</pre></div><div id="id-1.4.5.2.12.7.7.4.4.2.6" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
         If there were differences, you would see what ratio of inputs are
         remapped in the parentheses.
        </p></div><p>
        If you are satisfied with the adjusted CRUSH Map, you can apply it to
        the cluster:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd setcrushmap -i adjusted</pre></div></dd></dl></div></section><section class="sect3" id="id-1.4.5.2.12.7.8" data-id-title="For more information"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.1.1.6 </span><span class="title-name">For more information</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.12.7.8">#</a></h4></div></div></div><p>
     Find more details on CRUSH Maps in <a class="xref" href="cha-storage-datamgm.html#op-crush" title="17.5. CRUSH Map manipulation">Section 17.5, “CRUSH Map manipulation”</a>.
    </p><p>
     Find more details on Ceph pools in general in
     <a class="xref" href="ceph-pools.html" title="Chapter 18. Manage storage pools">Chapter 18, <em>Manage storage pools</em></a>.
    </p><p>
     Find more details about erasure coded pools in
     <a class="xref" href="cha-ceph-erasure.html" title="Chapter 19. Erasure coded pools">Chapter 19, <em>Erasure coded pools</em></a>.
    </p></section></section></section><section class="sect1" id="datamgm-buckets" data-id-title="Buckets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.2 </span><span class="title-name">Buckets</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-buckets">#</a></h2></div></div></div><p>
   CRUSH maps contain a list of OSDs, which can be organized into a
   tree-structured arrangement of buckets for aggregating the devices into
   physical locations. Individual OSDs comprise the leaves on the tree.
  </p><div class="informaltable"><table style="border: none;"><colgroup><col/><col/><col/></colgroup><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        0
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        osd
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A specific device or OSD (<code class="literal">osd.1</code>,
        <code class="literal">osd.2</code>, etc.).
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        1
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        host
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        The name of a host containing one or more OSDs.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        2
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        chassis
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Identifier for which chassis in the rack contains the
        <code class="literal">host</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        3
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        rack
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A computer rack. The default is <code class="literal">unknownrack</code>.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        4
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        row
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A row in a series of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        5
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pdu
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Abbreviation for "Power Distribution Unit".
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        6
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        pod
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Abbreviation for "Point of Delivery": in this context, a group of PDUs,
        or a group of rows of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        7
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        room
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A room containing rows of racks.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        8
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        datacenter
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        A physical data center containing one or more rooms.
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        9
       </p>
      </td><td style="border-right: 1px solid ; border-bottom: 1px solid ; ">
       <p>
        region
       </p>
      </td><td style="border-bottom: 1px solid ; ">
       <p>
        Geographical region of the world (for example, NAM, LAM, EMEA, APAC
        etc.)
       </p>
      </td></tr><tr><td style="border-right: 1px solid ; ">
       <p>
        10
       </p>
      </td><td style="border-right: 1px solid ; ">
       <p>
        root
       </p>
      </td><td>
       <p>
        The root node of the tree of OSD buckets (normally set to
        <code class="literal">default</code>).
       </p>
      </td></tr></tbody></table></div><div id="id-1.4.5.2.13.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip</div><p>
    You can modify the existing types and create your own bucket types.
   </p></div><p>
   Ceph's deployment tools generate a CRUSH Map that contains a bucket for
   each host, and a root named 'default', which is useful for the default
   <code class="literal">rbd</code> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </p><p>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <code class="literal">straw2</code> by default), and
   the hash (<code class="literal">0</code> by default, reflecting CRUSH Hash
   <code class="literal">rjenkins1</code>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </p><div class="verbatim-wrap"><pre class="screen">[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</pre></div><p>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </p><div class="verbatim-wrap"><pre class="screen">host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</pre></div></section><section class="sect1" id="datamgm-rules" data-id-title="Rule sets"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.3 </span><span class="title-name">Rule sets</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-rules">#</a></h2></div></div></div><p>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default CRUSH Map has a rule for the default root. If you want more
   roots and more rules, you need to create them later or they will be created
   automatically when new pools are created.
  </p><div id="id-1.4.5.2.14.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </p></div><p>
   A rule takes the following form:
  </p><div class="verbatim-wrap"><pre class="screen">rule <em class="replaceable">rulename</em> {

        ruleset <em class="replaceable">ruleset</em>
        type <em class="replaceable">type</em>
        min_size <em class="replaceable">min-size</em>
        max_size <em class="replaceable">max-size</em>
        step <em class="replaceable">step</em>

}</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.14.6.1"><span class="term">ruleset</span></dt><dd><p>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <code class="literal">0</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.2"><span class="term">type</span></dt><dd><p>
      A string. Describes a rule for either a 'replicated' or 'erasure' coded
      pool. This option is required. Default is <code class="literal">replicated</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.3"><span class="term">min_size</span></dt><dd><p>
      An integer. If a pool group makes fewer replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">2</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.4"><span class="term">max_size</span></dt><dd><p>
      An integer. If a pool group makes more replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <code class="literal">10</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.5"><span class="term">step take <em class="replaceable">bucket</em></span></dt><dd><p>
      Takes a bucket specified by a name, and begins iterating down the tree.
      This option is required. For an explanation about iterating through the
      tree, see <a class="xref" href="cha-storage-datamgm.html#datamgm-rules-step-iterate" title="17.3.1. Iterating the node tree">Section 17.3.1, “Iterating the node tree”</a>.
     </p></dd><dt id="id-1.4.5.2.14.6.6"><span class="term">step <em class="replaceable">target</em><em class="replaceable">mode</em><em class="replaceable">num</em> type <em class="replaceable">bucket-type</em></span></dt><dd><p>
      <em class="replaceable">target</em> can either be <code class="literal">choose</code>
      or <code class="literal">chooseleaf</code>. When set to <code class="literal">choose</code>,
      a number of buckets is selected. <code class="literal">chooseleaf</code> directly
      selects the OSDs (leaf nodes) from the sub-tree of each bucket in the set
      of buckets.
     </p><p>
      <em class="replaceable">mode</em> can either be <code class="literal">firstn</code>
      or <code class="literal">indep</code>. See
      <a class="xref" href="cha-storage-datamgm.html#datamgm-rules-step-mode" title="17.3.2. firstn and indep">Section 17.3.2, “<code class="literal">firstn</code> and <code class="literal">indep</code>”</a>.
     </p><p>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <em class="replaceable">num</em> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <em class="replaceable">num</em> &lt;
      0, it means N - <em class="replaceable">num</em>; and, if
      <em class="replaceable">num</em> == 0, choose N buckets (all available).
      Follows <code class="literal">step take</code> or <code class="literal">step choose</code>.
     </p></dd><dt id="id-1.4.5.2.14.6.7"><span class="term">step emit</span></dt><dd><p>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <code class="literal">step choose</code>.
     </p></dd></dl></div><section class="sect2" id="datamgm-rules-step-iterate" data-id-title="Iterating the node tree"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.3.1 </span><span class="title-name">Iterating the node tree</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-rules-step-iterate">#</a></h3></div></div></div><p>
    The structure defined with the buckets can be viewed as a node tree.
    Buckets are nodes and OSDs are leafs in this tree.
   </p><p>
    Rules in the CRUSH Map define how OSDs are selected from this tree. A rule
    starts with a node and then iterates down the tree to return a set of OSDs.
    It is not possible to define which branch needs to be selected. Instead the
    CRUSH algorithm assures that the set of OSDs fulfills the replication
    requirements and evenly distributes the data.
   </p><p>
    With <code class="literal">step take</code> <em class="replaceable">bucket</em> the
    iteration through the node tree begins at the given bucket (not bucket
    type). If OSDs from all branches in the tree are to be returned, the bucket
    must be the root bucket. Otherwise the following steps are only iterating
    through a sub-tree.
   </p><p>
    After <code class="literal">step take</code> one or more <code class="literal">step
    choose</code> entries follow in the rule definition. Each <code class="literal">step
    choose</code> chooses a defined number of nodes (or branches) from the
    previously selected upper node.
   </p><p>
    In the end the selected OSDs are returned with <code class="literal">step
    emit</code>.
   </p><p>
    <code class="literal">step chooseleaf</code> is a convenience function that directly
    selects OSDs from branches of the given bucket.
   </p><p>
    <a class="xref" href="cha-storage-datamgm.html#datamgm-rules-step-iterate-figure" title="Example tree">Figure 17.2, “Example tree”</a> provides an example of
    how <code class="literal">step</code> is used to iterate through a tree. The orange
    arrows and numbers correspond to <code class="literal">example1a</code> and
    <code class="literal">example1b</code>, while blue corresponds to
    <code class="literal">example2</code> in the following rule definitions.
   </p><div class="figure" id="datamgm-rules-step-iterate-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-step.png"><img src="images/crush-step.png" width="100%" alt="Example tree" title="Example tree"/></a></div></div><div class="figure-title-wrap"><div class="figure-title"><span class="title-number">Figure 17.2: </span><span class="title-name">Example tree </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-rules-step-iterate-figure">#</a></div></div></div><div class="verbatim-wrap"><pre class="screen"># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</pre></div></section><section class="sect2" id="datamgm-rules-step-mode" data-id-title="firstn and indep"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.3.2 </span><span class="title-name"><code class="literal">firstn</code> and <code class="literal">indep</code></span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-rules-step-mode">#</a></h3></div></div></div><p>
    A CRUSH rule defines replacements for failed nodes or OSDs (see
    <a class="xref" href="cha-storage-datamgm.html#datamgm-rules" title="17.3. Rule sets">Section 17.3, “Rule sets”</a>). The keyword <code class="literal">step</code>
    requires either <code class="literal">firstn</code> or <code class="literal">indep</code> as
    parameter. <a class="xref" href="cha-storage-datamgm.html#datamgm-rules-step-mode-indep-figure" title="Node replacement methods">Figure 17.3, “Node replacement methods”</a> provides
    an example.
   </p><p>
    <code class="literal">firstn</code> adds replacement nodes to the end of the list of
    active nodes. In case of a failed node, the following healthy nodes are
    shifted to the left to fill the gap of the failed node. This is the default
    and desired method for <span class="emphasis"><em>replicated pools</em></span>, because a
    secondary node already has all data and therefore can take over the duties
    of the primary node immediately.
   </p><p>
    <code class="literal">indep</code> selects fixed replacement nodes for each active
    node. The replacement of a failed node does not change the order of the
    remaining nodes. This is desired for <span class="emphasis"><em>erasure coded
    pools</em></span>. In erasure coded pools the data stored on a node depends
    on its position in the node selection. When the order of nodes changes, all
    data on affected nodes needs to be relocated.
   </p><div class="figure" id="datamgm-rules-step-mode-indep-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/crush-firstn-indep.png"><img src="images/crush-firstn-indep.png" width="100%" alt="Node replacement methods" title="Node replacement methods"/></a></div></div><div class="figure-title-wrap"><div class="figure-title"><span class="title-number">Figure 17.3: </span><span class="title-name">Node replacement methods </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#datamgm-rules-step-mode-indep-figure">#</a></div></div></div></section></section><section class="sect1" id="op-pgs" data-id-title="Placement groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.4 </span><span class="title-name">Placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs">#</a></h2></div></div></div><p>
   Ceph maps objects to placement groups (PGs). Placement groups are shards
   or fragments of a logical object pool that place objects as a group into
   OSDs. Placement groups reduce the amount of per-object metadata when Ceph
   stores the data in OSDs. A larger number of placement groups—for
   example, 100 per OSD—leads to better balancing.
  </p><section class="sect2" id="op-pgs-usage" data-id-title="Using placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.1 </span><span class="title-name">Using placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-usage">#</a></h3></div></div></div><p>
    A placement group (PG) aggregates objects within a pool. The main reason is
    that tracking object placement and metadata on a per-object basis is
    computationally expensive. For example, a system with millions of objects
    cannot track placement of each of its objects directly.
   </p><div class="figure" id="id-1.4.5.2.15.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_schema.png"><img src="images/ceph_pgs_schema.png" width="70%" alt="Placement groups in a pool" title="Placement groups in a pool"/></a></div></div><div class="figure-title-wrap"><div class="figure-title"><span class="title-number">Figure 17.4: </span><span class="title-name">Placement groups in a pool </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.15.3.3">#</a></div></div></div><p>
    The Ceph client will calculate to which placement group an object will
    belong to. It does this by hashing the object ID and applying an operation
    based on the number of PGs in the defined pool and the ID of the pool.
   </p><p>
    The object's contents within a placement group are stored in a set of OSDs.
    For example, in a replicated pool of size two, each placement group will
    store objects on two OSDs:
   </p><div class="figure" id="id-1.4.5.2.15.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_pgs_osds.png"><img src="images/ceph_pgs_osds.png" width="70%" alt="Placement groups and OSDs" title="Placement groups and OSDs"/></a></div></div><div class="figure-title-wrap"><div class="figure-title"><span class="title-number">Figure 17.5: </span><span class="title-name">Placement groups and OSDs </span><a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.15.3.6">#</a></div></div></div><p>
    If OSD #2 fails, another OSD will be assigned to placement group #1 and
    will be filled with copies of all objects in OSD #1. If the pool size is
    changed from two to three, an additional OSD will be assigned to the
    placement group and will receive copies of all objects in the placement
    group.
   </p><p>
    Placement groups do not own the OSD, they share it with other placement
    groups from the same pool or even other pools. If OSD #2 fails, the
    placement group #2 will also need to restore copies of objects, using OSD
    #3.
   </p><p>
    When the number of placement groups increases, the new placement groups
    will be assigned OSDs. The result of the CRUSH function will also change
    and some objects from the former placement groups will be copied over to
    the new placement groups and removed from the old ones.
   </p></section><section class="sect2" id="op-pgs-pg-num" data-id-title="Determining the value of PG_NUM"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.2 </span><span class="title-name">Determining the value of <em class="replaceable">PG_NUM</em></span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-pg-num">#</a></h3></div></div></div><div id="id-1.4.5.2.15.4.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Since Ceph Nautilus (v14.x), you can use the Ceph Manager
     <code class="literal">pg_autoscaler</code> module to auto-scale the PGs as needed.
     If you want to enable this feature, refer to
     <span class="intraxref">Book “Deploying and Administering SUSE Enterprise Storage with Rook”, Chapter 8 “Configuration”, Section 8.1.1.1 “Default PG and PGP counts”</span>.
    </p></div><p>
    When creating a new pool, you can still choose the value of
    <em class="replaceable">PG_NUM</em> manually:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool create <em class="replaceable">POOL_NAME</em> <em class="replaceable">PG_NUM</em></pre></div><p>
    <em class="replaceable">PG_NUM</em> cannot be calculated automatically.
    Following are a few commonly used values, depending on the number of OSDs
    in the cluster:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.15.4.6.1"><span class="term">Less than 5 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 128.
      </p></dd><dt id="id-1.4.5.2.15.4.6.2"><span class="term">Between 5 and 10 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 512.
      </p></dd><dt id="id-1.4.5.2.15.4.6.3"><span class="term">Between 10 and 50 OSDs:</span></dt><dd><p>
       Set <em class="replaceable">PG_NUM</em> to 1024.
      </p></dd></dl></div><p>
    As the number of OSDs increases, choosing the right value for
    <em class="replaceable">PG_NUM</em> becomes more important.
    <em class="replaceable">PG_NUM</em> strongly affects the behavior of the
    cluster as well as the durability of the data in case of OSD failure.
   </p><section class="sect3" id="op-pgs-choosing" data-id-title="Calculating placement groups for more than 50 OSDs"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.4.2.1 </span><span class="title-name">Calculating placement groups for more than 50 OSDs</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-choosing">#</a></h4></div></div></div><p>
     If you have less than 50 OSDs, use the preselection described in
     <a class="xref" href="cha-storage-datamgm.html#op-pgs-pg-num" title="17.4.2. Determining the value of PG_NUM">Section 17.4.2, “Determining the value of <em class="replaceable">PG_NUM</em>”</a>. If you have more than 50 OSDs, we
     recommend approximately 50-100 placement groups per OSD to balance out
     resource usage, data durability, and distribution. For a single pool of
     objects, you can use the following formula to get a baseline:
    </p><div class="verbatim-wrap"><pre class="screen">total PGs = (OSDs * 100) / <em class="replaceable">POOL_SIZE</em></pre></div><p>
     Where <em class="replaceable">POOL_SIZE</em> is either the number of
     replicas for replicated pools, or the 'k'+'m' sum for erasure coded pools
     as returned by the <code class="command">ceph osd erasure-code-profile get</code>
     command. You should round the result up to the nearest power of 2.
     Rounding up is recommended for the CRUSH algorithm to evenly balance the
     number of objects among placement groups.
    </p><p>
     As an example, for a cluster with 200 OSDs and a pool size of 3 replicas,
     you would estimate the number of PGs as follows:
    </p><div class="verbatim-wrap"><pre class="screen">          (200 * 100) / 3 = 6667</pre></div><p>
     The nearest power of 2 is <span class="bold"><strong>8192</strong></span>.
    </p><p>
     When using multiple data pools for storing objects, you need to ensure
     that you balance the number of placement groups per pool with the number
     of placement groups per OSD. You need to reach a reasonable total number
     of placement groups that provides reasonably low variance per OSD without
     taxing system resources or making the peering process too slow.
    </p><p>
     For example, a cluster of 10 pools, each with 512 placement groups on 10
     OSDs, is a total of 5,120 placement groups spread over 10 OSDs, that is
     512 placement groups per OSD. Such a setup does not use too many
     resources. However, if 1000 pools were created with 512 placement groups
     each, the OSDs would handle approximately 50,000 placement groups each and
     it would require significantly more resources and time for peering.
    </p></section></section><section class="sect2" id="op-pg-set" data-id-title="Setting the number of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.3 </span><span class="title-name">Setting the number of placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-set">#</a></h3></div></div></div><div id="id-1.4.5.2.15.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
     Since Ceph Nautilus (v14.x), you can use the Ceph Manager
     <code class="literal">pg_autoscaler</code> module to auto-scale the PGs as needed.
     If you want to enable this feature, refer to
     <span class="intraxref">Book “Deploying and Administering SUSE Enterprise Storage with Rook”, Chapter 8 “Configuration”, Section 8.1.1.1 “Default PG and PGP counts”</span>.
    </p></div><p>
    If you still need to specify the number of placement groups in a pool
    manually, you need to specify them at the time of pool creation (see
    <a class="xref" href="ceph-pools.html#ceph-pools-operate-add-pool" title="18.1. Creating a pool">Section 18.1, “Creating a pool”</a>). Once you have set
    placement groups for a pool, you may increase the number of placement
    groups by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_num <em class="replaceable">PG_NUM</em></pre></div><p>
    After you increase the number of placement groups, you also need to
    increase the number of placement groups for placement
    (<code class="option">PGP_NUM</code>) before your cluster will rebalance.
    <code class="option">PGP_NUM</code> will be the number of placement groups that will
    be considered for placement by the CRUSH algorithm. Increasing
    <code class="option">PG_NUM</code> splits the placement groups but data will not be
    migrated to the newer placement groups until <code class="option">PGP_NUM</code> is
    increased. <code class="option">PGP_NUM</code> should be equal to
    <code class="option">PG_NUM</code>. To increase the number of placement groups for
    placement, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pgp_num <em class="replaceable">PGP_NUM</em></pre></div></section><section class="sect2" id="op-pg-get" data-id-title="Finding the number of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.4 </span><span class="title-name">Finding the number of placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-get">#</a></h3></div></div></div><p>
    To find out the number of placement groups in a pool, run the following
    <code class="command">get</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph osd pool get <em class="replaceable">POOL_NAME</em> pg_num</pre></div></section><section class="sect2" id="op-pg-getpgstat" data-id-title="Finding a clusters PG statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.5 </span><span class="title-name">Finding a cluster's PG statistics</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-getpgstat">#</a></h3></div></div></div><p>
    To find out the statistics for the placement groups in your cluster, run
    the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg dump [--format <em class="replaceable">FORMAT</em>]</pre></div><p>
    Valid formats are 'plain' (default) and 'json'.
   </p></section><section class="sect2" id="op-pg-getstuckstat" data-id-title="Finding statistics for stuck PGs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.6 </span><span class="title-name">Finding statistics for stuck PGs</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-getstuckstat">#</a></h3></div></div></div><p>
    To find out the statistics for all placement groups stuck in a specified
    state, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg dump_stuck <em class="replaceable">STATE</em> \
 [--format <em class="replaceable">FORMAT</em>] [--threshold <em class="replaceable">THRESHOLD</em>]</pre></div><p>
    <em class="replaceable">STATE</em> is one of 'inactive' (PGs cannot process
    reads or writes because they are waiting for an OSD with the most
    up-to-date data to come up), 'unclean' (PGs contain objects that are not
    replicated the desired number of times), 'stale' (PGs are in an unknown
    state—the OSDs that host them have not reported to the monitor
    cluster in a time interval specified by the
    <code class="option">mon_osd_report_timeout</code> option), 'undersized', or
    'degraded'.
   </p><p>
    Valid formats are 'plain' (default) and 'json'.
   </p><p>
    The threshold defines the minimum number of seconds the placement group is
    stuck before including it in the returned statistics (300 seconds by
    default).
   </p></section><section class="sect2" id="op-pgs-pgmap" data-id-title="Searching a placement group map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.7 </span><span class="title-name">Searching a placement group map</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-pgmap">#</a></h3></div></div></div><p>
    To search for the placement group map for a particular placement group, run
    the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg map <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph will return the placement group map, the placement group, and the
    OSD status:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg map 1.6c
osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</pre></div></section><section class="sect2" id="op-pg-pgstats" data-id-title="Retrieving a placement groups statistics"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.8 </span><span class="title-name">Retrieving a placement groups statistics</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-pgstats">#</a></h3></div></div></div><p>
    To retrieve statistics for a particular placement group, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg <em class="replaceable">PG_ID</em> query</pre></div></section><section class="sect2" id="op-pg-scrubpg" data-id-title="Scrubbing a placement group"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.9 </span><span class="title-name">Scrubbing a placement group</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-scrubpg">#</a></h3></div></div></div><p>
    To scrub (<a class="xref" href="cha-storage-datamgm.html#scrubbing-pgs" title="17.6. Scrubbing placement groups">Section 17.6, “Scrubbing placement groups”</a>) a placement group, run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg scrub <em class="replaceable">PG_ID</em></pre></div><p>
    Ceph checks the primary and replica nodes, generates a catalog of all
    objects in the placement group, and compares them to ensure that no objects
    are missing or mismatched and their contents are consistent. Assuming the
    replicas all match, a final semantic sweep ensures that all of the
    snapshot-related object metadata is consistent. Errors are reported via
    logs.
   </p></section><section class="sect2" id="op-pg-backfill" data-id-title="Prioritizing backfill and recovery of placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.10 </span><span class="title-name">Prioritizing backfill and recovery of placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pg-backfill">#</a></h3></div></div></div><p>
    You may run into a situation where several placement groups require
    recovery and/or back-fill, while some groups hold data more important than
    others. For example, those PGs may hold data for images used by running
    machines and other PGs may be used by inactive machines or less relevant
    data. In that case, you may want to prioritize recovery of those groups so
    that performance and availability of data stored on those groups is
    restored earlier. To mark particular placement groups as prioritized during
    backfill or recovery, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt root"># </code>ceph pg force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    This will cause Ceph to perform recovery or backfill on specified
    placement groups first, before other placement groups. This does not
    interrupt currently ongoing backfills or recovery, but causes specified PGs
    to be processed as soon as possible. If you change your mind or prioritize
    wrong groups, cancel the prioritization:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph pg cancel-force-recovery <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]
<code class="prompt root"># </code>ceph pg cancel-force-backfill <em class="replaceable">PG_ID1</em> [<em class="replaceable">PG_ID2</em> ... ]</pre></div><p>
    The <code class="command">cancel-*</code> commands remove the 'force' flag from the
    PGs so that they are processed in default order. Again, this does not
    affect placement groups currently being processed, only those that are
    still queued. The 'force' flag is cleared automatically after recovery or
    backfill of the group is done.
   </p></section><section class="sect2" id="op-pgs-revert" data-id-title="Reverting lost objects"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.11 </span><span class="title-name">Reverting lost objects</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-revert">#</a></h3></div></div></div><p>
    If the cluster has lost one or more objects and you have decided to abandon
    the search for the lost data, you need to mark the unfound objects as
    'lost'.
   </p><p>
    If the objects are still lost after having queried all possible locations,
    you may need to give up on the lost objects. This is possible given unusual
    combinations of failures that allow the cluster to learn about writes that
    were performed before the writes themselves are recovered.
   </p><p>
    Currently the only supported option is 'revert', which will either roll
    back to a previous version of the object, or forget about it entirely in
    case of a new object. To mark the 'unfound' objects as 'lost', run the
    following:
   </p><div class="verbatim-wrap"><pre class="screen">  <code class="prompt user">cephuser@adm &gt; </code>ceph pg <em class="replaceable">PG_ID</em> mark_unfound_lost revert|delete</pre></div></section><section class="sect2" id="op-pgs-autoscaler" data-id-title="Enabling the PG auto-scaler"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.4.12 </span><span class="title-name">Enabling the PG auto-scaler</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-pgs-autoscaler">#</a></h3></div></div></div><p>
    Placement groups (PGs) are an internal implementation detail of how Ceph
    distributes data. By enabling pg-autoscaling, you can allow the cluster to
    either make or automatically tune PGs based on how the cluster is used.
   </p><p>
    Each pool in the system has a <code class="option">pg_autoscale_mode</code> property
    that can be set to <code class="literal">off</code>, <code class="literal">on</code>, or
    <code class="literal">warn</code>:
   </p><p>
    The autoscaler is configured on a per-pool basis, and can run in three
    modes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.15.14.5.1"><span class="term">off</span></dt><dd><p>
       Disable autoscaling for this pool. It is up to the administrator to
       choose an appropriate PG number for each pool.
      </p></dd><dt id="id-1.4.5.2.15.14.5.2"><span class="term">on</span></dt><dd><p>
       Enable automated adjustments of the PG count for the given pool.
      </p></dd><dt id="id-1.4.5.2.15.14.5.3"><span class="term">warn</span></dt><dd><p>
       Raise health alerts when the PG count should be adjusted.
      </p></dd></dl></div><p>
    To set the autoscaling mode for existing pools:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set <em class="replaceable">POOL_NAME</em> pg_autoscale_mode <em class="replaceable">mode</em></pre></div><p>
    You can also configure the default <code class="option">pg_autoscale_mode</code> that
    is applied to any pools that are created in the future with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set global osd_pool_default_pg_autoscale_mode <em class="replaceable">MODE</em></pre></div><p>
    You can view each pool, its relative utilization, and any suggested changes
    to the PG count with this command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool autoscale-status</pre></div></section></section><section class="sect1" id="op-crush" data-id-title="CRUSH Map manipulation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.5 </span><span class="title-name">CRUSH Map manipulation</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush">#</a></h2></div></div></div><p>
   This section introduces ways to basic CRUSH Map manipulation, such as
   editing a CRUSH Map, changing CRUSH Map parameters, and
   adding/moving/removing an OSD.
  </p><section class="sect2" id="id-1.4.5.2.16.3" data-id-title="Editing a CRUSH Map"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.1 </span><span class="title-name">Editing a CRUSH Map</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#id-1.4.5.2.16.3">#</a></h3></div></div></div><p>
    To edit an existing CRUSH map, do the following:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Get a CRUSH Map. To get the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd getcrushmap -o <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will output (<code class="option">-o</code>) a compiled CRUSH Map to the
      file name you specified. Since the CRUSH Map is in a compiled form, you
      must decompile it first before you can edit it.
     </p></li><li class="step"><p>
      Decompile a CRUSH Map. To decompile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -d <em class="replaceable">compiled-crushmap-filename</em> \
 -o <em class="replaceable">decompiled-crushmap-filename</em></pre></div><p>
      Ceph will decompile (<code class="option">-d</code>) the compiled CRUSH Mapand
      output (<code class="option">-o</code>) it to the file name you specified.
     </p></li><li class="step"><p>
      Edit at least one of Devices, Buckets and Rules parameters.
     </p></li><li class="step"><p>
      Compile a CRUSH Map. To compile a CRUSH Map, execute the following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>crushtool -c <em class="replaceable">decompiled-crush-map-filename</em> \
 -o <em class="replaceable">compiled-crush-map-filename</em></pre></div><p>
      Ceph will store a compiled CRUSH Mapto the file name you specified.
     </p></li><li class="step"><p>
      Set a CRUSH Map. To set the CRUSH Map for your cluster, execute the
      following:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd setcrushmap -i <em class="replaceable">compiled-crushmap-filename</em></pre></div><p>
      Ceph will input the compiled CRUSH Map of the file name you specified
      as the CRUSH Map for the cluster.
     </p></li></ol></div></div><div id="id-1.4.5.2.16.3.4" data-id-title="Use versioning system" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Use versioning system</div><p>
     Use a versioning system—such as git or svn—for the exported
     and modified CRUSH Map files. It makes a possible rollback simple.
    </p></div><div id="id-1.4.5.2.16.3.5" data-id-title="Test the new CRUSH Map" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Test the new CRUSH Map</div><p>
     Test the new adjusted CRUSH Map using the <code class="command">crushtool
     --test</code> command, and compare to the state before applying the new
     CRUSH Map. You may find the following command switches useful:
     <code class="option">--show-statistics</code>, <code class="option">--show-mappings</code>,
     <code class="option">--show-bad-mappings</code>, <code class="option">--show-utilization</code>,
     <code class="option">--show-utilization-all</code>,
     <code class="option">--show-choose-tries</code>
    </p></div></section><section class="sect2" id="op-crush-addosd" data-id-title="Adding or moving an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.2 </span><span class="title-name">Adding or moving an OSD</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-addosd">#</a></h3></div></div></div><p>
    To add or move an OSD in the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set <em class="replaceable">id_or_name</em> <em class="replaceable">weight</em> root=<em class="replaceable">pool-name</em>
<em class="replaceable">bucket-type</em>=<em class="replaceable">bucket-name</em> ...</pre></div><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.16.4.4.1"><span class="term">id</span></dt><dd><p>
       An integer. The numeric ID of the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.2"><span class="term">name</span></dt><dd><p>
       A string. The full name of the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.3"><span class="term">weight</span></dt><dd><p>
       A double. The CRUSH weight for the OSD. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.4"><span class="term">root</span></dt><dd><p>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </p></dd><dt id="id-1.4.5.2.16.4.4.5"><span class="term">bucket-type</span></dt><dd><p>
       Key/value pairs. You may specify the OSD's location in the CRUSH
       hierarchy.
      </p></dd></dl></div><p>
    The following example adds <code class="literal">osd.0</code> to the hierarchy, or
    moves the OSD from a previous location.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-osdweight" data-id-title="Difference between ceph osd reweight and ceph osd crush reweight"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.3 </span><span class="title-name">Difference between <code class="command">ceph osd reweight</code> and <code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-osdweight">#</a></h3></div></div></div><p>
    There are two similar commands that change the 'weight' of a Ceph OSD. The
    context of their usage is different and may cause confusion.
   </p><section class="sect3" id="ceph-osd-reweight" data-id-title="ceph osd reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.5.3.1 </span><span class="title-name"><code class="command">ceph osd reweight</code></span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#ceph-osd-reweight">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd reweight</code> sets an override weight on the Ceph OSD.
     This value is in the range of 0 to 1, and forces CRUSH to reposition the
     data that would otherwise live on this drive. It does
     <span class="bold"><strong>not</strong></span> change the weights
     assigned to the buckets above the OSD, and is a corrective measure in case
     the normal CRUSH distribution is not working out quite right. For example,
     if one of your OSDs is at 90% and the others are at 40%, you could reduce
     this weight to try and compensate for it.
    </p><div id="id-1.4.5.2.16.5.3.5" data-id-title="OSD weight is temporary" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: OSD weight is temporary</div><p>
      Note that <code class="command">ceph osd reweight</code> is not a persistent
      setting. When an OSD gets marked out, its weight will be set to 0 and
      when it gets marked in again, the weight will be changed to 1.
     </p></div></section><section class="sect3" id="ceph-osd-crush-reweight" data-id-title="ceph osd crush reweight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">17.5.3.2 </span><span class="title-name"><code class="command">ceph osd crush reweight</code></span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#ceph-osd-crush-reweight">#</a></h4></div></div></div><p>
     Usage:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush reweight <em class="replaceable">OSD_NAME</em> <em class="replaceable">NEW_WEIGHT</em></pre></div><p>
     <code class="command">ceph osd crush reweight</code> sets the
     <span class="bold"><strong>CRUSH</strong></span> weight of the OSD. This
     weight is an arbitrary value—generally the size of the disk in
     TB—and controls how much data the system tries to allocate to the
     OSD.
    </p></section></section><section class="sect2" id="op-crush-osdremove" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.4 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-osdremove">#</a></h3></div></div></div><p>
    To remove an OSD from the CRUSH Map of a running cluster, execute the
    following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove <em class="replaceable">OSD_NAME</em></pre></div></section><section class="sect2" id="op-crush-addbaucket" data-id-title="Adding a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.5 </span><span class="title-name">Adding a bucket</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-addbaucket">#</a></h3></div></div></div><p>
    To add a bucket to the CRUSH Map of a running cluster, execute the
    <code class="command">ceph osd crush add-bucket</code> command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush add-bucket <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em></pre></div></section><section class="sect2" id="op-crush-movebucket" data-id-title="Moving a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.6 </span><span class="title-name">Moving a bucket</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-movebucket">#</a></h3></div></div></div><p>
    To move a bucket to a different location or position in the CRUSH Map
    hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush move <em class="replaceable">BUCKET_NAME</em> <em class="replaceable">BUCKET_TYPE</em>=<em class="replaceable">BUCKET_NAME</em> [...]</pre></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1</pre></div></section><section class="sect2" id="op-crush-rmbucket" data-id-title="Removing a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">17.5.7 </span><span class="title-name">Removing a bucket</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#op-crush-rmbucket">#</a></h3></div></div></div><p>
    To remove a bucket from the CRUSH Map hierarchy, execute the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove <em class="replaceable">BUCKET_NAME</em></pre></div><div id="id-1.4.5.2.16.9.4" data-id-title="Empty bucket only" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Empty bucket only</div><p>
     A bucket must be empty before removing it from the CRUSH hierarchy.
    </p></div></section></section><section class="sect1" id="scrubbing-pgs" data-id-title="Scrubbing placement groups"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">17.6 </span><span class="title-name">Scrubbing placement groups</span> <a title="Permalink" class="permalink" href="cha-storage-datamgm.html#scrubbing-pgs">#</a></h2></div></div></div><p>
   In addition to making multiple copies of objects, Ceph ensures data
   integrity by <span class="emphasis"><em>scrubbing</em></span> placement groups (find more
   information about placement groups in
   <span class="intraxref">Book “Deployment Guide”, Chapter 1 “SES and Ceph”, Section 1.3.2 “Placement groups”</span>). Ceph scrubbing is analogous
   to running <code class="command">fsck</code> on the object storage layer. For each
   placement group, Ceph generates a catalog of all objects and compares each
   primary object and its replicas to ensure that no objects are missing or
   mismatched. Daily light scrubbing checks the object size and attributes,
   while weekly deep scrubbing reads the data and uses checksums to ensure data
   integrity.
  </p><p>
   Scrubbing is important for maintaining data integrity, but it can reduce
   performance. You can adjust the following settings to increase or decrease
   scrubbing operations:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.4.5.2.17.4.1"><span class="term"><code class="option">osd max scrubs</code></span></dt><dd><p>
      The maximum number of simultaneous scrub operations for a Ceph OSD. Default
      is 1.
     </p></dd><dt id="id-1.4.5.2.17.4.2"><span class="term"><code class="option">osd scrub begin hour</code>, <code class="option">osd scrub end hour</code></span></dt><dd><p>
      The hours of day (0 to 24) that define a time window during which the
      scrubbing can happen. By default, begins at 0 and ends at 24.
     </p><div id="id-1.4.5.2.17.4.2.2.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
       If the placement group's scrub interval exceeds the <code class="option">osd scrub
       max interval</code> setting, the scrub will happen no matter what time
       window you define for scrubbing.
      </p></div></dd><dt id="id-1.4.5.2.17.4.3"><span class="term"><code class="option">osd scrub during recovery</code></span></dt><dd><p>
      Allows scrubs during recovery. Setting this to 'false' will disable
      scheduling new scrubs while there is an active recovery. Already running
      scrubs will continue. This option is useful for reducing load on busy
      clusters. Default is 'true'.
     </p></dd><dt id="id-1.4.5.2.17.4.4"><span class="term"><code class="option">osd scrub thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub thread times out. Default is
      60.
     </p></dd><dt id="id-1.4.5.2.17.4.5"><span class="term"><code class="option">osd scrub finalize thread timeout</code></span></dt><dd><p>
      The maximum time in seconds before a scrub finalize thread times out.
      Default is 60*10.
     </p></dd><dt id="id-1.4.5.2.17.4.6"><span class="term"><code class="option">osd scrub load threshold</code></span></dt><dd><p>
      The normalized maximum load. Ceph will not scrub when the system load
      (as defined by the ratio of <code class="literal">getloadavg()</code> / number of
      <code class="literal">online cpus</code>) is higher than this number. Default is
      0.5.
     </p></dd><dt id="id-1.4.5.2.17.4.7"><span class="term"><code class="option">osd scrub min interval</code></span></dt><dd><p>
      The minimal interval in seconds for scrubbing Ceph OSD when the Ceph
      cluster load is low. Default is 60*60*24 (once a day).
     </p></dd><dt id="id-1.4.5.2.17.4.8"><span class="term"><code class="option">osd scrub max interval</code></span></dt><dd><p>
      The maximum interval in seconds for scrubbing Ceph OSD, irrespective of
      cluster load. Default is 7*60*60*24 (once a week).
     </p></dd><dt id="id-1.4.5.2.17.4.9"><span class="term"><code class="option">osd scrub chunk min</code></span></dt><dd><p>
      The minimum number of object store chunks to scrub during a single
      operation. Ceph blocks writes to a single chunk during a scrub. Default
      is 5.
     </p></dd><dt id="id-1.4.5.2.17.4.10"><span class="term"><code class="option">osd scrub chunk max</code></span></dt><dd><p>
      The maximum number of object store chunks to scrub during a single
      operation. Default is 25.
     </p></dd><dt id="id-1.4.5.2.17.4.11"><span class="term"><code class="option">osd scrub sleep</code></span></dt><dd><p>
      Time to sleep before scrubbing the next group of chunks. Increasing this
      value slows down the whole scrub operation, while client operations are
      less impacted. Default is 0.
     </p></dd><dt id="id-1.4.5.2.17.4.12"><span class="term"><code class="option">osd deep scrub interval</code></span></dt><dd><p>
      The interval for 'deep' scrubbing (fully reading all data). The
      <code class="option">osd scrub load threshold</code> option does not affect this
      setting. Default is 60*60*24*7 (once a week).
     </p></dd><dt id="id-1.4.5.2.17.4.13"><span class="term"><code class="option">osd scrub interval randomize ratio</code></span></dt><dd><p>
      Add a random delay to the <code class="option">osd scrub min interval</code> value
      when scheduling the next scrub job for a placement group. The delay is a
      random value smaller than the result of <code class="option">osd scrub min
      interval</code> * <code class="option">osd scrub interval randomized ratio</code>.
      Therefore, the default setting practically randomly spreads the scrubs
      out in the allowed time window of [1, 1.5] * <code class="option">osd scrub min
      interval</code>. Default is 0.5.
     </p></dd><dt id="id-1.4.5.2.17.4.14"><span class="term"><code class="option">osd deep scrub stride</code></span></dt><dd><p>
      Read size when doing a deep scrub. Default is 524288 (512 kB).
     </p></dd></dl></div></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="part-storing-data.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Part III </span>Storing Data in a Cluster</span></a> </div><div><a class="pagination-link next" href="ceph-pools.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 18 </span>Manage storage pools</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="cha-storage-datamgm.html#datamgm-devices"><span class="title-number">17.1 </span><span class="title-name">OSD devices</span></a></span></li><li><span class="sect1"><a href="cha-storage-datamgm.html#datamgm-buckets"><span class="title-number">17.2 </span><span class="title-name">Buckets</span></a></span></li><li><span class="sect1"><a href="cha-storage-datamgm.html#datamgm-rules"><span class="title-number">17.3 </span><span class="title-name">Rule sets</span></a></span></li><li><span class="sect1"><a href="cha-storage-datamgm.html#op-pgs"><span class="title-number">17.4 </span><span class="title-name">Placement groups</span></a></span></li><li><span class="sect1"><a href="cha-storage-datamgm.html#op-crush"><span class="title-number">17.5 </span><span class="title-name">CRUSH Map manipulation</span></a></span></li><li><span class="sect1"><a href="cha-storage-datamgm.html#scrubbing-pgs"><span class="title-number">17.6 </span><span class="title-name">Scrubbing placement groups</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_ceph_datamgm.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>