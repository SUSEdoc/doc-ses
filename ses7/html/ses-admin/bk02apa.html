<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SES 7 | Administration and Operations Guide | Ceph maintenance updates based on upstream 'Octopus' point releases</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta name="description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="Administration and Operations Guide"/>
<meta name="chapter-title" content="Appendix A. Ceph maintenance updates based on upstream 'Octopus' point releases"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta property="og:description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta name="twitter:description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<link rel="prev" href="cha-storage-cephx.html" title="Chapter 30. Authentication with cephx"/><link rel="next" href="bk02go01.html" title="Glossary"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-ses/edit/main/xml/ceph_maintenance_updates.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Administration and Operations Guide</a><span> / </span><a class="crumb" href="bk02apa.html">Ceph maintenance updates based on upstream 'Octopus' point releases</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Administration and Operations Guide</div><ol><li><a href="preface-admin.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="part-dashboard.html" class="has-children "><span class="title-number">I </span><span class="title-name">Ceph Dashboard</span></a><ol><li><a href="dashboard-about.html" class=" "><span class="title-number">1 </span><span class="title-name">About the Ceph Dashboard</span></a></li><li><a href="dashboard-webui-general.html" class=" "><span class="title-number">2 </span><span class="title-name">Dashboard's Web user interface</span></a></li><li><a href="dashboard-user-mgmt.html" class=" "><span class="title-number">3 </span><span class="title-name">Manage Ceph Dashboard users and roles</span></a></li><li><a href="dashboard-cluster.html" class=" "><span class="title-number">4 </span><span class="title-name">View cluster internals</span></a></li><li><a href="dashboard-pools.html" class=" "><span class="title-number">5 </span><span class="title-name">Manage pools</span></a></li><li><a href="dashboard-rbds.html" class=" "><span class="title-number">6 </span><span class="title-name">Manage RADOS Block Device</span></a></li><li><a href="dash-webui-nfs.html" class=" "><span class="title-number">7 </span><span class="title-name">Manage NFS Ganesha</span></a></li><li><a href="dashboard-mds.html" class=" "><span class="title-number">8 </span><span class="title-name">Manage CephFS</span></a></li><li><a href="dashboard-ogw.html" class=" "><span class="title-number">9 </span><span class="title-name">Manage the Object Gateway</span></a></li><li><a href="dashboard-initial-configuration.html" class=" "><span class="title-number">10 </span><span class="title-name">Manual configuration</span></a></li><li><a href="dashboard-user-roles.html" class=" "><span class="title-number">11 </span><span class="title-name">Manage users and roles on the command line</span></a></li></ol></li><li><a href="part-cluster-operation.html" class="has-children "><span class="title-number">II </span><span class="title-name">Cluster Operation</span></a><ol><li><a href="ceph-monitor.html" class=" "><span class="title-number">12 </span><span class="title-name">Determine the cluster state</span></a></li><li><a href="storage-salt-cluster.html" class=" "><span class="title-number">13 </span><span class="title-name">Operational tasks</span></a></li><li><a href="cha-ceph-operating.html" class=" "><span class="title-number">14 </span><span class="title-name">Operation of Ceph services</span></a></li><li><a href="cha-deployment-backup.html" class=" "><span class="title-number">15 </span><span class="title-name">Backup and restore</span></a></li><li><a href="monitoring-alerting.html" class=" "><span class="title-number">16 </span><span class="title-name">Monitoring and alerting</span></a></li></ol></li><li><a href="part-storing-data.html" class="has-children "><span class="title-number">III </span><span class="title-name">Storing Data in a Cluster</span></a><ol><li><a href="cha-storage-datamgm.html" class=" "><span class="title-number">17 </span><span class="title-name">Stored data management</span></a></li><li><a href="ceph-pools.html" class=" "><span class="title-number">18 </span><span class="title-name">Manage storage pools</span></a></li><li><a href="cha-ceph-erasure.html" class=" "><span class="title-number">19 </span><span class="title-name">Erasure coded pools</span></a></li><li><a href="ceph-rbd.html" class=" "><span class="title-number">20 </span><span class="title-name">RADOS Block Device</span></a></li></ol></li><li><a href="part-accessing-data.html" class="has-children "><span class="title-number">IV </span><span class="title-name">Accessing Cluster Data</span></a><ol><li><a href="cha-ceph-gw.html" class=" "><span class="title-number">21 </span><span class="title-name">Ceph Object Gateway</span></a></li><li><a href="cha-ceph-iscsi.html" class=" "><span class="title-number">22 </span><span class="title-name">Ceph iSCSI gateway</span></a></li><li><a href="cha-ceph-cephfs.html" class=" "><span class="title-number">23 </span><span class="title-name">Clustered file system</span></a></li><li><a href="cha-ses-cifs.html" class=" "><span class="title-number">24 </span><span class="title-name">Export Ceph data via Samba</span></a></li><li><a href="cha-ceph-nfsganesha.html" class=" "><span class="title-number">25 </span><span class="title-name">NFS Ganesha</span></a></li></ol></li><li><a href="part-integration-virt.html" class="has-children "><span class="title-number">V </span><span class="title-name">Integration with Virtualization Tools</span></a><ol><li><a href="cha-ceph-libvirt.html" class=" "><span class="title-number">26 </span><span class="title-name"><code class="systemitem">libvirt</code> and Ceph</span></a></li><li><a href="cha-ceph-kvm.html" class=" "><span class="title-number">27 </span><span class="title-name">Ceph as a back-end for QEMU KVM instance</span></a></li></ol></li><li><a href="part-cluster-configuration.html" class="has-children "><span class="title-number">VI </span><span class="title-name">Configuring a Cluster</span></a><ol><li><a href="cha-ceph-configuration.html" class=" "><span class="title-number">28 </span><span class="title-name">Ceph cluster configuration</span></a></li><li><a href="cha-mgr-modules.html" class=" "><span class="title-number">29 </span><span class="title-name">Ceph Manager modules</span></a></li><li><a href="cha-storage-cephx.html" class=" "><span class="title-number">30 </span><span class="title-name">Authentication with <code class="systemitem">cephx</code></span></a></li></ol></li><li><a href="bk02apa.html" class=" you-are-here"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></li><li><a href="bk02go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="appendix" id="id-1.4.9" data-id-title="Ceph maintenance updates based on upstream Octopus point releases"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></span> <a title="Permalink" class="permalink" href="bk02apa.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/ceph_maintenance_updates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7 are based on the
  Octopus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Octopus series, SUSE Enterprise Storage 7 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.5"><span class="name">Octopus 15.2.11 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.12"><span class="name">Octopus 15.2.10 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.12">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The containers include an updated <code class="literal">tcmalloc</code> that avoids
    crashes seen on 15.2.9.
   </p></li><li class="listitem"><p>
    RADOS: BlueStore handling of huge (&gt;4GB) writes from RocksDB to BlueFS
    has been fixed.
   </p></li><li class="listitem"><p>
    When upgrading from a previous cephadm release,
    <code class="command">systemctl</code> may hang when trying to start or restart the
    monitoring containers. This is caused by a change in the <code class="systemitem">systemd</code> unit to
    use <code class="option">type=forking</code>.) After the upgrade, please run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy nfs
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy iscsi
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy alertmanager</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.15"><span class="name">Octopus 15.2.9 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.15">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    MGR: progress module can now be turned on/off, using the commands:
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li><li class="listitem"><p>
    OSD: PG removal has been optimized in this release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.18"><span class="name">Octopus 15.2.8 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS and includes a number of bug
  fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    OpenStack Manila use of <code class="filename">ceph_volume_client.py</code> library
    allowed tenant access to any Ceph credential’s secret.
   </p></li><li class="listitem"><p>
    <code class="command">ceph-volume</code>: The <code class="command">lvm batch</code> subcommand
    received a major rewrite. This closed a number of bugs and improves
    usability in terms of size specification and calculation, as well as
    idempotency behaviour and disk replacement process. Please refer to
    <a class="link" href="https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/" target="_blank">https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/</a>
    for more detailed information.
   </p></li><li class="listitem"><p>
    MON: The cluster log now logs health detail every
    <code class="option">mon_health_to_clog_interval</code>, which has been changed from
    1hr to 10min. Logging of health detail will be skipped if there is no
    change in health summary since last known.
   </p></li><li class="listitem"><p>
    The <code class="command">ceph df</code> command now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    The <code class="option">bluefs_preextend_wal_files</code> option has been removed.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial monitor to contact for Ceph
    tools and daemons using the <code class="option">mon_host_override</code> config
    option or <code class="option">--mon-host-override</code> command line switch. This
    generally should only be used for debugging and only affects initial
    communication with Ceph's monitor cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.21"><span class="name">Octopus 15.2.7 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.21">#</a></h2></div><p>
  This release fixes a serious bug in RGW that has been shown to cause data
  loss when a read of a large RGW object (for example, one with at least one
  tail segment) takes longer than one half the time specified in the
  configuration option <code class="option">rgw_gc_obj_min_wait</code>. The bug causes the
  tail segments of that read object to be added to the RGW garbage collection
  queue, which will in turn cause them to be deleted after a period of time.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.23"><span class="name">Octopus 15.2.6 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.23">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.25"><span class="name">Octopus 15.2.5 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.25">#</a></h2></div><p>
  The Octopus point release 15.2.5 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CephFS: Automatic static sub-tree partitioning policies may now be
    configured using the new distributed and random ephemeral pinning extended
    attributes on directories. See the following documentation for more
    information:
    <a class="link" href="https://docs.ceph.com/docs/master/cephfs/multimds/" target="_blank">https://docs.ceph.com/docs/master/cephfs/multimds/</a>
   </p></li><li class="listitem"><p>
    Monitors now have a configuration option
    <code class="option">mon_osd_warn_num_repaired</code>, which is set to 10 by default.
    If any OSD has repaired more than this many I/O errors in stored data a
    <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is generated.
   </p></li><li class="listitem"><p>
    Now, when <code class="literal">no scrub</code> and/or <code class="literal">no
    deep-scrub</code> flags are set globally or per pool, scheduled scrubs
    of the type disabled will be aborted. All user initiated scrubs are NOT
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.28"><span class="name">Octopus 15.2.4 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.28">#</a></h2></div><p>
  The Octopus point release 15.2.4 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li><li class="listitem"><p>
    Object Gateway: The <code class="command">radosgw-admin</code> sub-commands dealing with
    orphans—<code class="command">radosgw-admin orphans find</code>,
    <code class="command">radosgw-admin orphans finish</code>, and <code class="command">radosgw-admin
    orphans list-jobs</code>—have been deprecated. They had not been
    actively maintained, and since they store intermediate results on the
    cluster, they could potentially fill a nearly-full cluster. They have been
    replaced by a tool, <code class="command">rgw-orphan-list</code>, which is currently
    considered experimental.
   </p></li><li class="listitem"><p>
    RBD: The name of the RBD pool object that is used to store RBD trash purge
    schedule is changed from <code class="literal">rbd_trash_trash_purge_schedule</code>
    to <code class="literal">rbd_trash_purge_schedule</code>. Users that have already
    started using RBD trash purge schedule functionality and have per pool or
    name space schedules configured should copy the
    <code class="literal">rbd_trash_trash_purge_schedule</code> object to
    <code class="literal">rbd_trash_purge_schedule</code> before the upgrade and remove
    <code class="literal">rbd_trash_purge_schedule</code> using the following commands in
    every RBD pool and name space where a trash purge schedule was previously
    configured:
   </p><div class="verbatim-wrap"><pre class="screen">rados -p <em class="replaceable">pool-name</em> [-N namespace] cp rbd_trash_trash_purge_schedule rbd_trash_purge_schedule
rados -p <em class="replaceable">pool-name</em> [-N namespace] rm rbd_trash_trash_purge_schedule</pre></div><p>
    Alternatively, use any other convenient way to restore the schedule after
    the upgrade.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.31"><span class="name">Octopus 15.2.3 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.31">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The Octopus point release 15.2.3 was a hot-fix release to address an
    issue where WAL corruption was seen when
    <code class="option">bluefs_preextend_wal_files</code> and
    <code class="option">bluefs_buffered_io</code> were enabled at the same time. The fix
    in 15.2.3 is only a temporary measure (changing the default value of
    <code class="option">bluefs_preextend_wal_files</code> to <code class="literal">false</code>).
    The permanent fix will be to remove the
    <code class="option">bluefs_preextend_wal_files</code> option completely: this fix
    will most likely arrive in the 15.2.6 point release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.33"><span class="name">Octopus 15.2.2 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.33">#</a></h2></div><p>
  The Octopus point release 15.2.2 patched one security vulnerability:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10736: Fixed an authorization bypass in MONs and MGRs
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.4.9.36"><span class="name">Octopus 15.2.1 Point Release</span><a title="Permalink" class="permalink" href="bk02apa.html#id-1.4.9.36">#</a></h2></div><p>
  The Octopus point release 15.2.1 fixed an issue where upgrading quickly
  from Luminous (SES5.5) to Nautilus (SES6) to Octopus (SES7) caused OSDs to
  crash. In addition, it patched two security vulnerabilities that were present
  in the initial Octopus (15.2.0) release:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS because of RGW GetObject header-splitting
   </p></li></ul></div></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="cha-storage-cephx.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 30 </span>Authentication with <code class="systemitem">cephx</code></span></a> </div><div><a class="pagination-link next" href="bk02go01.html"><span class="pagination-relation">Next</span><span class="pagination-label">Glossary</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2023</span></div></div></footer></body></html>