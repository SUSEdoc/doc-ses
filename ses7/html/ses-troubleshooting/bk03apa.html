<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SES 7 | Troubleshooting Guide | Ceph maintenance updates based on upstream 'Octopus' point releases</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta name="description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="Troubleshooting Guide"/>
<meta name="chapter-title" content="Appendix A. Ceph maintenance updates based on upstream 'Octopus' point releases"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta property="og:description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Ceph maintenance updates based on upstream 'Octopus' p…"/>
<meta name="twitter:description" content="Several key packages in SUSE Enterprise Storage 7 are based on the Octopus release series of Ceph. When the Ceph project (https://github.com/ceph/cep…"/>
<link rel="prev" href="storage-faqs.html" title="Chapter 14. Frequently asked questions"/><link rel="next" href="bk03go01.html" title="Glossary"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script><meta name="edit-url" content="https://github.com/SUSE/doc-ses/edit/main/xml/ceph_maintenance_updates.xml"/></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Troubleshooting Guide</a><span> / </span><a class="crumb" href="bk03apa.html">Ceph maintenance updates based on upstream 'Octopus' point releases</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Troubleshooting Guide</div><ol><li><a href="preface-troubleshooting.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="report-software.html" class=" "><span class="title-number">1 </span><span class="title-name">Reporting software problems</span></a></li><li><a href="bp-troubleshooting-logging.html" class=" "><span class="title-number">2 </span><span class="title-name">Troubleshooting logging and debugging</span></a></li><li><a href="bp-troubleshooting-cephadm.html" class=" "><span class="title-number">3 </span><span class="title-name">Troubleshooting cephadm</span></a></li><li><a href="bp-troubleshooting-osds.html" class=" "><span class="title-number">4 </span><span class="title-name">Troubleshooting OSDs</span></a></li><li><a href="bp-troubleshooting-pgs.html" class=" "><span class="title-number">5 </span><span class="title-name">Troubleshooting placement groups (PGs)</span></a></li><li><a href="bp-troubleshooting-monitors.html" class=" "><span class="title-number">6 </span><span class="title-name">Troubleshooting Ceph Monitors and Ceph Managers</span></a></li><li><a href="bp-troubleshooting-networking.html" class=" "><span class="title-number">7 </span><span class="title-name">Troubleshooting networking</span></a></li><li><a href="bp-troubleshooting-nfs.html" class=" "><span class="title-number">8 </span><span class="title-name">Troubleshooting NFS Ganesha</span></a></li><li><a href="bp-troubleshooting-status.html" class=" "><span class="title-number">9 </span><span class="title-name">Troubleshooting Ceph health status</span></a></li><li><a href="bp-troubleshooting-dashboard.html" class=" "><span class="title-number">10 </span><span class="title-name">Troubleshooting the Ceph Dashboard</span></a></li><li><a href="bp-troubleshooting-objectgateway.html" class=" "><span class="title-number">11 </span><span class="title-name">Troubleshooting Object Gateway</span></a></li><li><a href="bp-troubleshooting-cephfs.html" class=" "><span class="title-number">12 </span><span class="title-name">Troubleshooting CephFS</span></a></li><li><a href="storage-tips.html" class=" "><span class="title-number">13 </span><span class="title-name">Hints and tips</span></a></li><li><a href="storage-faqs.html" class=" "><span class="title-number">14 </span><span class="title-name">Frequently asked questions</span></a></li><li><a href="bk03apa.html" class=" you-are-here"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></li><li><a href="bk03go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="appendix" id="id-1.5.17" data-id-title="Ceph maintenance updates based on upstream Octopus point releases"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><div class="title-container"><h1 class="title"><span class="title-number-name"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></span> <a title="Permalink" class="permalink" href="bk03apa.html#">#</a></h1><div class="icons"><a target="_blank" class="icon-reportbug" title="Report an issue"> </a><a target="_blank" class="icon-editsource" href="https://github.com/SUSE/doc-ses/edit/main/xml/ceph_maintenance_updates.xml" title="Edit source document"> </a></div></div></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7 are based on the
  Octopus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Octopus series, SUSE Enterprise Storage 7 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.5"><span class="name">Octopus 15.2.11 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.12"><span class="name">Octopus 15.2.10 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.12">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The containers include an updated <code class="literal">tcmalloc</code> that avoids
    crashes seen on 15.2.9.
   </p></li><li class="listitem"><p>
    RADOS: BlueStore handling of huge (&gt;4GB) writes from RocksDB to BlueFS
    has been fixed.
   </p></li><li class="listitem"><p>
    When upgrading from a previous cephadm release,
    <code class="command">systemctl</code> may hang when trying to start or restart the
    monitoring containers. This is caused by a change in the <code class="systemitem">systemd</code> unit to
    use <code class="option">type=forking</code>.) After the upgrade, please run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy nfs
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy iscsi
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy alertmanager</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.15"><span class="name">Octopus 15.2.9 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.15">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    MGR: progress module can now be turned on/off, using the commands:
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li><li class="listitem"><p>
    OSD: PG removal has been optimized in this release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.18"><span class="name">Octopus 15.2.8 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS and includes a number of bug
  fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    OpenStack Manila use of <code class="filename">ceph_volume_client.py</code> library
    allowed tenant access to any Ceph credential’s secret.
   </p></li><li class="listitem"><p>
    <code class="command">ceph-volume</code>: The <code class="command">lvm batch</code> subcommand
    received a major rewrite. This closed a number of bugs and improves
    usability in terms of size specification and calculation, as well as
    idempotency behaviour and disk replacement process. Please refer to
    <a class="link" href="https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/" target="_blank">https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/</a>
    for more detailed information.
   </p></li><li class="listitem"><p>
    MON: The cluster log now logs health detail every
    <code class="option">mon_health_to_clog_interval</code>, which has been changed from
    1hr to 10min. Logging of health detail will be skipped if there is no
    change in health summary since last known.
   </p></li><li class="listitem"><p>
    The <code class="command">ceph df</code> command now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    The <code class="option">bluefs_preextend_wal_files</code> option has been removed.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial monitor to contact for Ceph
    tools and daemons using the <code class="option">mon_host_override</code> config
    option or <code class="option">--mon-host-override</code> command line switch. This
    generally should only be used for debugging and only affects initial
    communication with Ceph's monitor cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.21"><span class="name">Octopus 15.2.7 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.21">#</a></h2></div><p>
  This release fixes a serious bug in RGW that has been shown to cause data
  loss when a read of a large RGW object (for example, one with at least one
  tail segment) takes longer than one half the time specified in the
  configuration option <code class="option">rgw_gc_obj_min_wait</code>. The bug causes the
  tail segments of that read object to be added to the RGW garbage collection
  queue, which will in turn cause them to be deleted after a period of time.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.23"><span class="name">Octopus 15.2.6 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.23">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.25"><span class="name">Octopus 15.2.5 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.25">#</a></h2></div><p>
  The Octopus point release 15.2.5 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CephFS: Automatic static sub-tree partitioning policies may now be
    configured using the new distributed and random ephemeral pinning extended
    attributes on directories. See the following documentation for more
    information:
    <a class="link" href="https://docs.ceph.com/docs/master/cephfs/multimds/" target="_blank">https://docs.ceph.com/docs/master/cephfs/multimds/</a>
   </p></li><li class="listitem"><p>
    Monitors now have a configuration option
    <code class="option">mon_osd_warn_num_repaired</code>, which is set to 10 by default.
    If any OSD has repaired more than this many I/O errors in stored data a
    <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is generated.
   </p></li><li class="listitem"><p>
    Now, when <code class="literal">no scrub</code> and/or <code class="literal">no
    deep-scrub</code> flags are set globally or per pool, scheduled scrubs
    of the type disabled will be aborted. All user initiated scrubs are NOT
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.28"><span class="name">Octopus 15.2.4 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.28">#</a></h2></div><p>
  The Octopus point release 15.2.4 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li><li class="listitem"><p>
    Object Gateway: The <code class="command">radosgw-admin</code> sub-commands dealing with
    orphans—<code class="command">radosgw-admin orphans find</code>,
    <code class="command">radosgw-admin orphans finish</code>, and <code class="command">radosgw-admin
    orphans list-jobs</code>—have been deprecated. They had not been
    actively maintained, and since they store intermediate results on the
    cluster, they could potentially fill a nearly-full cluster. They have been
    replaced by a tool, <code class="command">rgw-orphan-list</code>, which is currently
    considered experimental.
   </p></li><li class="listitem"><p>
    RBD: The name of the RBD pool object that is used to store RBD trash purge
    schedule is changed from <code class="literal">rbd_trash_trash_purge_schedule</code>
    to <code class="literal">rbd_trash_purge_schedule</code>. Users that have already
    started using RBD trash purge schedule functionality and have per pool or
    name space schedules configured should copy the
    <code class="literal">rbd_trash_trash_purge_schedule</code> object to
    <code class="literal">rbd_trash_purge_schedule</code> before the upgrade and remove
    <code class="literal">rbd_trash_purge_schedule</code> using the following commands in
    every RBD pool and name space where a trash purge schedule was previously
    configured:
   </p><div class="verbatim-wrap"><pre class="screen">rados -p <em class="replaceable">pool-name</em> [-N namespace] cp rbd_trash_trash_purge_schedule rbd_trash_purge_schedule
rados -p <em class="replaceable">pool-name</em> [-N namespace] rm rbd_trash_trash_purge_schedule</pre></div><p>
    Alternatively, use any other convenient way to restore the schedule after
    the upgrade.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.31"><span class="name">Octopus 15.2.3 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.31">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The Octopus point release 15.2.3 was a hot-fix release to address an
    issue where WAL corruption was seen when
    <code class="option">bluefs_preextend_wal_files</code> and
    <code class="option">bluefs_buffered_io</code> were enabled at the same time. The fix
    in 15.2.3 is only a temporary measure (changing the default value of
    <code class="option">bluefs_preextend_wal_files</code> to <code class="literal">false</code>).
    The permanent fix will be to remove the
    <code class="option">bluefs_preextend_wal_files</code> option completely: this fix
    will most likely arrive in the 15.2.6 point release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.33"><span class="name">Octopus 15.2.2 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.33">#</a></h2></div><p>
  The Octopus point release 15.2.2 patched one security vulnerability:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10736: Fixed an authorization bypass in MONs and MGRs
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.5.17.36"><span class="name">Octopus 15.2.1 Point Release</span><a title="Permalink" class="permalink" href="bk03apa.html#id-1.5.17.36">#</a></h2></div><p>
  The Octopus point release 15.2.1 fixed an issue where upgrading quickly
  from Luminous (SES5.5) to Nautilus (SES6) to Octopus (SES7) caused OSDs to
  crash. In addition, it patched two security vulnerabilities that were present
  in the initial Octopus (15.2.0) release:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS because of RGW GetObject header-splitting
   </p></li></ul></div></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="storage-faqs.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 14 </span>Frequently asked questions</span></a> </div><div><a class="pagination-link next" href="bk03go01.html"><span class="pagination-relation">Next</span><span class="pagination-label">Glossary</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2023</span></div></div></footer></body></html>