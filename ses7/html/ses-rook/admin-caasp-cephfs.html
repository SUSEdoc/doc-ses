<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>CephFS | Deploying and Administering SUSE Enterprise Storage with Rook | SUSE Enterprise Storage 7</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="CephFS | SES 7"/>
<meta name="description" content="A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared fi…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="Deploying and Administering SUSE Enterprise Storage with Rook"/>
<meta name="chapter-title" content="Chapter 5. CephFS"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="CephFS | SES 7"/>
<meta property="og:description" content="A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared fi…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CephFS | SES 7"/>
<meta name="twitter:description" content="A shared file system can be mounted with read/write permission from multiple pods. This may be useful for applications which can be clustered using a shared fi…"/>
<link rel="prev" href="admin-caasp-block-storage.html" title="Chapter 4. Block Storage"/><link rel="next" href="admin-caasp-crd.html" title="Chapter 6. Ceph cluster custom resource definitions"/>
<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft wide offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a><a href="#_bottom-pagination">Jump to page navigation: previous page [access key p]/next page [access key n]</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="index.html">Deploying and Administering SUSE Enterprise Storage with Rook</a><span> / </span><a class="crumb" href="rook-ses-admin.html">Administrating Ceph on SUSE CaaS Platform</a><span> / </span><a class="crumb" href="admin-caasp-cephfs.html">CephFS</a></div></div><main id="_content"><nav id="_side-toc-overall" class="side-toc"><div class="side-title">Deploying and Administering SUSE Enterprise Storage with Rook</div><ol><li><a href="preface-rook.html" class=" "><span class="title-number"> </span><span class="title-name">About this guide</span></a></li><li><a href="rook-ses-deployment.html" class="has-children "><span class="title-number">I </span><span class="title-name">Quick Start: Deploying Ceph on SUSE CaaS Platform</span></a><ol><li><a href="deploy-rook.html" class=" "><span class="title-number">1 </span><span class="title-name">Quick start</span></a></li></ol></li><li class="active"><a href="rook-ses-admin.html" class="has-children you-are-here"><span class="title-number">II </span><span class="title-name">Administrating Ceph on SUSE CaaS Platform</span></a><ol><li><a href="admin-intro-caasp.html" class=" "><span class="title-number">2 </span><span class="title-name">Rook-Ceph administration</span></a></li><li><a href="admin-caasp-cluster.html" class=" "><span class="title-number">3 </span><span class="title-name">Ceph cluster administration</span></a></li><li><a href="admin-caasp-block-storage.html" class=" "><span class="title-number">4 </span><span class="title-name">Block Storage</span></a></li><li><a href="admin-caasp-cephfs.html" class=" you-are-here"><span class="title-number">5 </span><span class="title-name">CephFS</span></a></li><li><a href="admin-caasp-crd.html" class=" "><span class="title-number">6 </span><span class="title-name">Ceph cluster custom resource definitions</span></a></li><li><a href="admin-caasp-cephconfig.html" class=" "><span class="title-number">7 </span><span class="title-name">Configuration</span></a></li><li><a href="admin-caasp-cephtoolbox.html" class=" "><span class="title-number">8 </span><span class="title-name">Toolboxes</span></a></li><li><a href="admin-caasp-cephosd.html" class=" "><span class="title-number">9 </span><span class="title-name">Ceph OSD management</span></a></li><li><a href="admin-caasp-ceph-examples.html" class=" "><span class="title-number">10 </span><span class="title-name">Ceph examples</span></a></li><li><a href="admin-caasp-advanced-config.html" class=" "><span class="title-number">11 </span><span class="title-name">Advanced configuration</span></a></li><li><a href="admin-caasp-object-storage.html" class=" "><span class="title-number">12 </span><span class="title-name">Object Storage</span></a></li><li><a href="admin-caasp-dashboard.html" class=" "><span class="title-number">13 </span><span class="title-name">Ceph Dashboard</span></a></li></ol></li><li><a href="rook-ses-troubleshooting.html" class="has-children "><span class="title-number">III </span><span class="title-name">Troubleshooting Ceph on SUSE CaaS Platform</span></a><ol><li><a href="atroubleshooting-caasp-debugging-rook.html" class=" "><span class="title-number">14 </span><span class="title-name">Troubleshooting</span></a></li><li><a href="admin-caasp-ceph-common-issues.html" class=" "><span class="title-number">15 </span><span class="title-name">Common issues</span></a></li></ol></li><li><a href="bk05apa.html" class=" "><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></li><li><a href="bk05go01.html" class=" "><span class="title-number"> </span><span class="title-name">Glossary</span></a></li> </ol> </nav><button id="_open-side-toc-overall" title="Contents"> </button><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section class="chapter" id="admin-caasp-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div class="version-info">Applies to  <span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#">#</a></h2></div></div></div><section class="sect1" id="rook-shared-filesystem" data-id-title="Shared File System"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Shared File System</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-shared-filesystem">#</a></h2></div></div></div><p>
   A shared file system can be mounted with read/write permission from multiple
   pods. This may be useful for applications which can be clustered using a
   shared file system.
  </p><p>
   This example runs a shared file system for the
   <a class="link" href="https://github.com/kubernetes/kubernetes/tree/release-1.18/cluster/addons" target="_blank">kube-registry</a>.
  </p><section class="sect2" id="rook-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-prerequisites">#</a></h3></div></div></div><p>
    This guide assumes you have created a Rook cluster as explained in the
    main guide: <a class="xref" href="deploy-rook.html" title="Chapter 1. Quick start">Chapter 1, <em>Quick start</em></a>.
   </p><div id="id-1.7.4.5.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     By default, only one shared file system can be created with Rook.
     Multiple file system support in Ceph is still considered experimental,
     and can be enabled with the environment variable
     <code class="literal">ROOK_ALLOW_MULTIPLE_FILESYSTEMS</code> defined in
     <code class="filename">operator.yaml</code>.
    </p></div></section><section class="sect2" id="rook-create-the-filesystem" data-id-title="Creating the File System"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.2 </span><span class="title-name">Creating the File System</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-create-the-filesystem">#</a></h3></div></div></div><p>
    Create the file system by specifying the desired settings for the metadata
    pool, data pools, and metadata server in the
    <code class="literal">CephFilesystem</code> CRD. In this example, we create the
    metadata pool with replication of three, and a single data pool with
    replication of three. For more options, see the documentation
    <a class="xref" href="admin-caasp-crd.html#rook-ceph-shared-filesystem-crd" title="6.3. Ceph shared file system CRD">Section 6.3, “Ceph shared file system CRD”</a>.
   </p><p>
    Save this shared file system definition as
    <code class="filename">filesystem.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
    dataPools:
    - replicated:
        size: 3
    preservePoolsOnDelete: true
    metadataServer:
      activeCount: 1
      activeStandby: true</pre></div><p>
    The Rook operator will create all the pools and other resources necessary
    to start the service. This may take a minute to complete.
   </p><p>
    Create the file system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f filesystem.yaml</pre></div><p>
    To confirm the file system is configured, wait for the MDS pods to start:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-mds
NAME                                      READY     STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-7d59fdfcf4-h8kw9       1/1       Running   0          12s
rook-ceph-mds-myfs-7d59fdfcf4-kgkjp       1/1       Running   0          12s</pre></div><p>
    To see detailed status of the file system, start and connect to the Rook
    toolbox. A new line will be shown with <code class="command">ceph status</code> for
    the <code class="literal">mds</code> service. In this example, there is one active
    instance of MDS which is up, with one MDS instance in
    <code class="literal">standby-replay</code> mode in case of failover.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
[...]
services:
mds: myfs-1/1/1 up {[myfs:0]=mzw58b=up:active}, 1 up:standby-replay</pre></div></section><section class="sect2" id="rook-provision-storage" data-id-title="Provisioning Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.3 </span><span class="title-name">Provisioning Storage</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-provision-storage">#</a></h3></div></div></div><p>
    Before Rook can start provisioning storage, a
    <code class="literal">StorageClass</code> needs to be created based on the file
    system. This is needed for Kubernetes to interoperate with the CSI driver to
    create persistent volumes.
   </p><div id="id-1.7.4.5.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This example uses the CSI driver, which is the preferred driver going
     forward for Kubernetes 1.13 and newer.
    </p></div><p>
    Save this storage class definition as
    <code class="filename">storageclass.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: rook-ceph

  # CephFS file system name into which the volume shall be created
  fsName: myfs

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: myfs-data0

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  # rootPath: /absolute/path

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

reclaimPolicy: Delete</pre></div><p>
    If you have deployed the Rook operator in a namespace other than
    <span class="quote">“<span class="quote">rook-ceph</span>”</span>, change the prefix in the provisioner to match the
    namespace you used. For example, if the Rook operator is running in
    <span class="quote">“<span class="quote">rook-op</span>”</span>, the provisioner value should be
    <span class="quote">“<span class="quote">rook-op.rbd.csi.ceph.com</span>”</span>.
   </p><p>
    Create the storage class:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml</pre></div><div id="id-1.7.4.5.3.6.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     The CephFS CSI driver uses quotas to enforce the PVC size requested.
     Only newer kernels support CephFS quotas (kernel version of at least
     4.17).
    </p></div></section><section class="sect2" id="rook-consume-the-shared-filesystem-k8s-registry-sample" data-id-title="Consuming the Shared File System: K8s Registry Sample"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.4 </span><span class="title-name">Consuming the Shared File System: K8s Registry Sample</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-consume-the-shared-filesystem-k8s-registry-sample">#</a></h3></div></div></div><p>
    As an example, we will start the kube-registry pod with the shared file
    system as the backing store. Save the following spec as
    <code class="filename">kube-registry.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-cephfs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-registry
  namespace: kube-system
  labels:
    k8s-app: kube-registry
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: kube-registry
  template:
    metadata:
      labels:
        k8s-app: kube-registry
        kubernetes.io/cluster-service: "true"
    spec:
      containers:
      - name: registry
        image: registry:2
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        # Configuration reference: https://docs.docker.com/registry/configuration/
        - name: REGISTRY_HTTP_ADDR
          value: :5000
        - name: REGISTRY_HTTP_SECRET
          value: "Ple4seCh4ngeThisN0tAVerySecretV4lue"
        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
          value: /var/lib/registry
        volumeMounts:
        - name: image-store
          mountPath: /var/lib/registry
        ports:
        - containerPort: 5000
          name: registry
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: registry
        readinessProbe:
          httpGet:
            path: /
            port: registry
      volumes:
      - name: image-store
        persistentVolumeClaim:
          claimName: cephfs-pvc
          readOnly: false</pre></div><p>
    Create the Kube registry deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml</pre></div><p>
    You now have a High-Availability Docker registry with persistent storage.
   </p><div id="id-1.7.4.5.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the Rook cluster has more than one file system and the application
     pod is scheduled to a node with kernel version older than 4.7,
     inconsistent results may arise, since kernels older than 4.7 do not
     support specifying file system namespaces.
    </p></div></section><section class="sect2" id="rook-consume-the-shared-filesystem-toolbox" data-id-title="Consuming the Shared File System: Toolbox"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.5 </span><span class="title-name">Consuming the Shared File System: Toolbox</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-consume-the-shared-filesystem-toolbox">#</a></h3></div></div></div><p>
    Once you have pushed an image to the registry, verify that
    <code class="filename">kube-registry</code> is using the file system that was
    configured above by mounting the shared file system in the toolbox pod.
   </p><section class="sect3" id="rook-teardown" data-id-title="Teardown"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.1.5.1 </span><span class="title-name">Teardown</span> <a title="Permalink" class="permalink" href="admin-caasp-cephfs.html#rook-teardown">#</a></h4></div></div></div><p>
     To clean up all the artifacts created by the file system demo:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl delete -f kube-registry.yaml</pre></div><p>
     To delete the file system components and backing data, delete the
     Filesystem CRD.
    </p><div id="id-1.7.4.5.3.8.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      <span class="strong"><strong>WARNING: Data will be deleted if
      <code class="option">preservePoolsOnDelete=false</code></strong></span>.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph delete cephfilesystem myfs</pre></div><p>
     Note: If the <span class="quote">“<span class="quote">preservePoolsOnDelete</span>”</span> file system attribute is
     set to true, the above command won’t delete the pools. Creating the file
     system again with the same CRD will reuse the previous pools.
    </p></section></section></section></section><nav class="bottom-pagination"><div><a class="pagination-link prev" href="admin-caasp-block-storage.html"><span class="pagination-relation">Previous</span><span class="pagination-label"><span class="title-number">Chapter 4 </span>Block Storage</span></a> </div><div><a class="pagination-link next" href="admin-caasp-crd.html"><span class="pagination-relation">Next</span><span class="pagination-label"><span class="title-number">Chapter 6 </span>Ceph cluster custom resource definitions</span></a> </div></nav></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">On this page</div><div class="toc"><ul><li><span class="sect1"><a href="admin-caasp-cephfs.html#rook-shared-filesystem"><span class="title-number">5.1 </span><span class="title-name">Shared File System</span></a></span></li></ul></div><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/admin_caasp_cephfs.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>