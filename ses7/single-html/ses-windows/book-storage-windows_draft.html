<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>SUSE Enterprise Storage for Windows guide | SUSE Enterprise Storage 7</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="SUSE Enterprise Storage for Windows guide | SES 7"/>
<meta name="description" content="This guide describes the integration, installation and configuration of Microsoft Windows environments and SUSE Enterprise Storage using the Windows …"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="SUSE Enterprise Storage for Windows guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="SUSE Enterprise Storage for Windows guide | SES 7"/>
<meta property="og:description" content="This guide describes the integration, installation and configuration of Microsoft Windows environments and SUSE Enterprise Storage using the Windows …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="SUSE Enterprise Storage for Windows guide | SES 7"/>
<meta name="twitter:description" content="This guide describes the integration, installation and configuration of Microsoft Windows environments and SUSE Enterprise Storage using the Windows …"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-windows">SUSE Enterprise Storage for Windows guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-windows" data-id-title="SUSE Enterprise Storage for Windows guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><h1 class="title">SUSE Enterprise Storage for Windows guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Mike</span> <span class="surname">Latimer</span> and <span class="firstname">Alexandra</span> <span class="surname">Settle</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>17 Oct 2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#preface-windows"><span class="title-name">About this guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.8.2.5"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.8.2.6"><span class="title-name">Giving feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.8.2.7"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.8.2.8"><span class="title-name">Support</span></a></span></li><li><span class="sect1"><a href="#id-1.8.2.9"><span class="title-name">Ceph contributors</span></a></span></li><li><span class="sect1"><a href="#id-1.8.2.10"><span class="title-name">Commands and command prompts used in this guide</span></a></span></li></ul></li><li><span class="chapter"><a href="#windows-ses"><span class="title-number">1 </span><span class="title-name">Ceph for Microsoft Windows</span></a></span><ul><li><span class="sect1"><a href="#intro-windows"><span class="title-number">1.1 </span><span class="title-name">Introduction</span></a></span></li><li><span class="sect1"><a href="#technology-preview"><span class="title-number">1.2 </span><span class="title-name">Technology preview</span></a></span></li><li><span class="sect1"><a href="#supported-platforms"><span class="title-number">1.3 </span><span class="title-name">Supported platforms</span></a></span></li><li><span class="sect1"><a href="#compatibility"><span class="title-number">1.4 </span><span class="title-name">Compatibility</span></a></span></li><li><span class="sect1"><a href="#install-config"><span class="title-number">1.5 </span><span class="title-name">Installing and configuring</span></a></span></li><li><span class="sect1"><a href="#rados-block-device"><span class="title-number">1.6 </span><span class="title-name">RADOS Block Device (RBD)</span></a></span></li><li><span class="sect1"><a href="#rbd-windows-service"><span class="title-number">1.7 </span><span class="title-name">RBD Microsoft Windows service</span></a></span></li><li><span class="sect1"><a href="#windows-cephfs"><span class="title-number">1.8 </span><span class="title-name">Configuring CephFS</span></a></span></li></ul></li><li><span class="appendix"><a href="#windows-conffile"><span class="title-number">A </span><span class="title-name">Sample configuration files</span></a></span></li><li><span class="appendix"><a href="#windows-trouble-tips"><span class="title-number">B </span><span class="title-name">Troubleshooting tips</span></a></span></li><li><span class="appendix"><a href="#win-upstream-project"><span class="title-number">C </span><span class="title-name">Upstream projects</span></a></span></li><li><span class="appendix"><a href="#id-1.8.7"><span class="title-number">D </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></span></li><li><span class="glossary"><a href="#id-1.8.8"><span class="title-name">Glossary</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.8.1.6"><p>
  Copyright © 2020–2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under Creative Commons Attribution-ShareAlike 4.0 International
  (CC-BY-SA 4.0): <a class="link" href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">https://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section xml:lang="en" class="preface" id="preface-windows" data-id-title="About this guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About this guide</span> <a title="Permalink" class="permalink" href="#preface-windows">#</a></h1></div></div></div><p>
  This guide describes the integration, installation and configuration of
  Microsoft Windows environments and SUSE Enterprise Storage using the Windows Driver.
 </p><p>
 SUSE Enterprise Storage 7 is an extension to SUSE Linux Enterprise Server 15 SP2. It combines the
 capabilities of the Ceph
 (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>)
 storage project with the enterprise engineering and support of SUSE.
 SUSE Enterprise Storage 7 provides IT organizations with the ability to
 deploy a distributed storage architecture that can support a number of use
 cases using commodity hardware platforms.
</p><section xml:lang="en" class="sect1" id="id-1.8.2.5" data-id-title="Available documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available documentation</span> <a title="Permalink" class="permalink" href="#id-1.8.2.5">#</a></h2></div></div></div><div id="id-1.8.2.5.3" data-id-title="Online documentation and latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Online documentation and latest updates</div><p>
   Documentation for our products is available at
   <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>,
   where you can also find the latest updates, and browse or download the
   documentation in various formats. The latest documentation updates can be
   found in the English language version.
  </p></div><p>
  In addition, the product documentation is available in your installed system
  under <code class="filename">/usr/share/doc/manual</code>. It is included in an RPM
  package named
  <span class="package">ses-manual_<em class="replaceable">LANG_CODE</em></span>. Install
  it if it is not already on your system, for example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper install ses-manual_en</pre></div><p>
  The following documentation is available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.8.2.5.7.1"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-deployment.html" target="_blank"><em class="citetitle">Deployment Guide</em></a></span></dt><dd><p>
     This guide focuses on deploying a basic Ceph cluster, and how to deploy
     additional services. It also cover the steps for upgrading to
     SUSE Enterprise Storage 7 from the previous product version.
    </p></dd><dt id="id-1.8.2.5.7.2"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-admin.html" target="_blank"><em class="citetitle">Administration and Operations Guide</em></a></span></dt><dd><p>
     This guide focuses on routine tasks that you as an administrator need to
     take care of after the basic Ceph cluster has been deployed (day 2
     operations). It also describes all the supported ways to access data
     stored in a Ceph cluster.
    </p></dd><dt id="id-1.8.2.5.7.3"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-security.html" target="_blank"><em class="citetitle">Security Hardening Guide</em></a></span></dt><dd><p>
     This guide focuses on how to ensure your cluster is secure.
    </p></dd><dt id="id-1.8.2.5.7.4"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-troubleshooting.html" target="_blank"><em class="citetitle">Troubleshooting Guide</em></a></span></dt><dd><p>
     This guide takes you through various common problems when running
     SUSE Enterprise Storage 7 and other related issues to relevant
     components such as Ceph or Object Gateway.
    </p></dd><dt id="id-1.8.2.5.7.5"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-windows.html" target="_blank"><em class="citetitle">SUSE Enterprise Storage for Windows Guide</em></a></span></dt><dd><p>
     This guide describes the integration, installation, and configuration of
     Microsoft Windows environments and SUSE Enterprise Storage using the Windows Driver.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.8.2.6" data-id-title="Giving feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Giving feedback</span> <a title="Permalink" class="permalink" href="#id-1.8.2.6">#</a></h2></div></div></div><p>
  We welcome feedback on, and contributions to, this documentation.
  There are several channels for this:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.8.2.6.3.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.8.2.6.3.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
     Reporting issues requires a Bugzilla account.
    </p><p>
     To simplify this process, you can use the <span class="guimenu">Report
     Documentation Bug</span> links next to headlines in the HTML
     version of this document. These preselect the right product and
     category in Bugzilla and add a link to the current section.
     You can start typing your bug report right away.
    </p></dd><dt id="id-1.8.2.6.3.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, use the <span class="guimenu">Edit Source</span>
     links next to headlines in the HTML version of this document. They
     take you to the source code on GitHub, where you can open a pull request.
     Contributing requires a GitHub account.
    </p><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README at
     <a class="link" href="https://github.com/SUSE/doc-ses" target="_blank">https://github.com/SUSE/doc-ses</a>.
    </p></dd><dt id="id-1.8.2.6.3.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.8.2.7" data-id-title="Documentation conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span> <a title="Permalink" class="permalink" href="#id-1.8.2.7">#</a></h2></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as non-privileged user.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.8.2.7.4.13.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><div class="admon-title">Warning: Warning notice</div><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.8.2.7.4.13.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important: Important notice</div><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.8.2.7.4.13.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note: Note notice</div><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.8.2.7.4.13.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Tip notice</div><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.8.2.7.4.14.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.8.2.7.4.14.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.8.2.8" data-id-title="Support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Support</span> <a title="Permalink" class="permalink" href="#id-1.8.2.8">#</a></h2></div></div></div><p>
  Find the support statement for SUSE Enterprise Storage and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.8.2.8.4" data-id-title="Support statement for SUSE Enterprise Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#id-1.8.2.8.4">#</a></h3></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offerings available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.8.2.8.4.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.8.2.8.4.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.8.2.8.4.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Enterprise Storage is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.8.2.8.5" data-id-title="Technology previews"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span> <a title="Permalink" class="permalink" href="#id-1.8.2.8.5">#</a></h3></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback!
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or in other ways
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7</a>.
  </p></section></section><section class="sect1" id="id-1.8.2.9" data-id-title="Ceph contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph contributors</span> <a title="Permalink" class="permalink" href="#id-1.8.2.9">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section><section class="sect1" id="id-1.8.2.10" data-id-title="Commands and command prompts used in this guide"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Commands and command prompts used in this guide</span> <a title="Permalink" class="permalink" href="#id-1.8.2.10">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect2" id="id-1.8.2.10.4" data-id-title="Salt-related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1 </span><span class="title-name">Salt-related commands</span> <a title="Permalink" class="permalink" href="#id-1.8.2.10.4">#</a></h3></div></div></div><p>
   These commands help you to deploy Ceph cluster nodes, run commands on
   several (or all) cluster nodes at the same time, or assist you when adding
   or removing cluster nodes. The most frequently used commands are
   <code class="command">ceph-salt</code> and <code class="command">ceph-salt config</code>. You
   need to run Salt commands on the Salt Master node as <code class="systemitem">root</code>. These
   commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div></section><section class="sect2" id="id-1.8.2.10.5" data-id-title="Ceph related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2 </span><span class="title-name">Ceph related commands</span> <a title="Permalink" class="permalink" href="#id-1.8.2.10.5">#</a></h3></div></div></div><p>
   These are lower-level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">cephadm</code>, <code class="command">rbd</code>,
   or <code class="command">radosgw-admin</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   The safer and recommended option is to create a more restrictive individual
   key for each administrator user and put it in a directory where the users
   can read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.8.2.10.5.6" data-id-title="Path to Ceph keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Path to Ceph keys</div><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph-related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephuser</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><div id="id-1.8.2.10.5.11" data-id-title="Commands for specific nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><div class="admon-title">Tip: Commands for specific nodes</div><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mon &gt; </code></pre></div></div><section class="sect3" id="id-1.8.2.10.5.12" data-id-title="Running ceph-volume"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Running <code class="command">ceph-volume</code></span> <a title="Permalink" class="permalink" href="#id-1.8.2.10.5.12">#</a></h4></div></div></div><p>
    Starting with SUSE Enterprise Storage 7, Ceph services are running containerized.
    If you need to run <code class="command">ceph-volume</code> on an OSD node, you need
    to prepend it with the <code class="command">cephadm</code> command, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm ceph-volume simple scan</pre></div></section></section><section class="sect2" id="id-1.8.2.10.6" data-id-title="General Linux commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3 </span><span class="title-name">General Linux commands</span> <a title="Permalink" class="permalink" href="#id-1.8.2.10.6">#</a></h3></div></div></div><p>
   Linux commands not related to Ceph, such as <code class="command">mount</code>,
   <code class="command">cat</code>, or <code class="command">openssl</code>, are introduced either
   with the <code class="prompt user">cephuser@adm &gt; </code> or <code class="prompt root"># </code> prompts, depending on which
   privileges the related command requires.
  </p></section><section class="sect2" id="id-1.8.2.10.7" data-id-title="Additional information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4 </span><span class="title-name">Additional information</span> <a title="Permalink" class="permalink" href="#id-1.8.2.10.7">#</a></h3></div></div></div><p>
   For more information on Ceph key management, refer to
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 30 “Authentication with <code class="systemitem">cephx</code>”, Section 30.2 “Key management”</span>.
  </p></section></section></section><section class="chapter" id="windows-ses" data-id-title="Ceph for Microsoft Windows"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">1 </span><span class="title-name">Ceph for Microsoft Windows</span> <a title="Permalink" class="permalink" href="#windows-ses">#</a></h1></div></div></div><section class="sect1" id="intro-windows" data-id-title="Introduction"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Introduction</span> <a title="Permalink" class="permalink" href="#intro-windows">#</a></h2></div></div></div><p>
   Ceph is a highly-resilient software-defined-storage offering, which has
   only been available to Microsoft Windows environments through the use of iSCSI or
   CIFS gateways. This gateway architecture introduces a single point of
   contact and limits fault-tolerance and bandwidth, in comparison to the
   native I/O paths of Ceph with RADOS.
  </p><p>
   In order to bring the benefits of native Ceph to Microsoft Windows environments,
   SUSE partnered with Cloudbase Solutions to port Ceph to the Microsoft Windows
   platform. This work is nearing completion, and provides the following
   functionality:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     RADOS Block Device (RBD)
    </p></li><li class="listitem"><p>
     CephFS
    </p></li></ul></div><p>
   You can find additional information on the background of this effort through
   the following SUSECON Digital session:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ceph in a Windows World
     (<a class="link" href="https://www.youtube.com/watch?v=BWZIwXLcNts" target="_blank">TUT-1121</a>)
     Presenters: Mike Latimer (SUSE) Alessandro Pilotti (Cloudbase Solutions)
    </p></li></ul></div></section><section class="sect1" id="technology-preview" data-id-title="Technology preview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Technology preview</span> <a title="Permalink" class="permalink" href="#technology-preview">#</a></h2></div></div></div><p>
   SUSE Enterprise Storage Driver for Windows is currently being offered as a technology
   preview. This is a necessary step toward full support as we continue work to
   ensure this driver performs well in all environments and workloads. You can
   contribute to this effort by reporting any issues you may encounter to
   SUSE Support.
  </p><p>
   CephFS functionality requires a third party FUSE wrapper provided through
   the <a class="link" href="https://github.com/dokan-dev/dokany" target="_blank">Dokany
   project</a>. This functionality should be considered experimental, and is
   not recommended for production use.
  </p></section><section class="sect1" id="supported-platforms" data-id-title="Supported platforms"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">Supported platforms</span> <a title="Permalink" class="permalink" href="#supported-platforms">#</a></h2></div></div></div><p>
   Microsoft Windows Server 2016 and 2019 are supported. Previous Microsoft Windows Server
   versions, including Microsoft Windows client versions such as Microsoft Windows 10, may work,
   but for the purpose of this document have not been thoroughly tested.
  </p><div id="id-1.8.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    Early builds of Microsoft Windows Server 2016 do not provide UNIX sockets, in which
    case the Ceph admin socket feature is unavailable.
   </p></div></section><section class="sect1" id="compatibility" data-id-title="Compatibility"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Compatibility</span> <a title="Permalink" class="permalink" href="#compatibility">#</a></h2></div></div></div><p>
   RADOS Block Device images can be exposed to the OS and host Microsoft Windows partitions or they
   can be attached to Hyper-V VMs in the same way as iSCSI disks.
  </p><div id="id-1.8.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    At the moment, the Microsoft Failover Cluster refuses to use Windows Block
    Device (WNBD) driver disks as Cluster Shared Volumes (CSVs) underlying
    storage.
   </p></div><p>
   OpenStack integration has been proposed and may be included in the next
   OpenStack release. This will allow RBD images managed by OpenStack
   Cinder to be attached to Hyper-V VMs managed by OpenStack Nova.
  </p></section><section class="sect1" id="install-config" data-id-title="Installing and configuring"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">Installing and configuring</span> <a title="Permalink" class="permalink" href="#install-config">#</a></h2></div></div></div><p>
   Ceph for Microsoft Windows can be easily installed through the
   <code class="filename">SES4Win.msi</code> setup wizard. You can download this from
   <a class="link" href="https://beta.suse.com/private/SLE15/SP2/download/SES7/SES4Win/" target="_blank">SES4Win</a>.
   This wizard performs the following functions:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Installs Ceph-related code to the <code class="filename">C:\Program
     Files\Ceph</code> directory.
    </p></li><li class="listitem"><p>
     Adds <code class="filename">C:\Program Files\Ceph\bin</code> to the
     <em class="replaceable">%PATH%</em> environment variable.
    </p></li><li class="listitem"><p>
     Creates a Ceph RBD Mapping Service to automatically map RBD devices upon
     machine restart (using <code class="filename">rbd-wnbd.exe</code>).
    </p></li></ul></div><p>
   After installing Ceph for Microsoft Windows, manual modifications are required to
   provide access to a Ceph cluster. The files which must be created or
   modified are as follows:
  </p><div class="verbatim-wrap"><pre class="screen">C:\ProgramData\ceph\ceph.conf
C:\ProgramData\ceph\keyring</pre></div><p>
   These files can be copied directly from an existing OSD node in the cluster.
   Sample configuration files are provided in
   <a class="xref" href="#windows-conffile" title="Appendix A. Sample configuration files">Appendix A, <em>Sample configuration files</em></a>.
  </p></section><section class="sect1" id="rados-block-device" data-id-title="RADOS Block Device (RBD)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.6 </span><span class="title-name">RADOS Block Device (RBD)</span> <a title="Permalink" class="permalink" href="#rados-block-device">#</a></h2></div></div></div><p>
   Support for RBD devices is possible through a combination of Ceph tools
   and Microsoft Windows WNBD. This driver is in the process of being certified by the
   Windows Hardware Quality Labs (WHQL).
  </p><p>
   Once installed, the WNBD SCSI Virtual Adapter driver can be seen in the
   <code class="literal">Device Manager</code> as a storage controller. Multiple adapters
   may be seen, in order to handle multiple RBD connections.
  </p><p>
   The <code class="command">rbd</code> command is used to create, remove, import,
   export, map, or unmap images, exactly like it is used on Linux.
  </p><section class="sect2" id="mapping-images" data-id-title="Mapping images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.6.1 </span><span class="title-name">Mapping images</span> <a title="Permalink" class="permalink" href="#mapping-images">#</a></h3></div></div></div><p>
    The behavior of the <code class="command">rbd</code> command is similar to its Linux
    counterpart, with a few notable differences:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Device paths cannot be requested. The disk number and path is picked by
      Microsoft Windows. If a device path is provided by the user when mapping an image,
      it is used as an identifier. This can also be used when unmapping the
      image.
     </p></li><li class="listitem"><p>
      The <code class="command">show</code> command was added, which describes a specific
      mapping. This can be used for retrieving the disk path.
     </p></li><li class="listitem"><p>
      The <code class="command">service</code> command was added, allowing
      <code class="literal">rbd-wnbd</code> to run as a Microsoft Windows service. All mappings are
      currently persistent and will be recreated when the service stops, unless
      they are explicitly unmapped. The service disconnects the mappings when
      being stopped.
     </p></li><li class="listitem"><p>
      The <code class="command">list</code> command also includes a
      <code class="literal">status</code> column.
     </p></li></ul></div><p>
    The mapped images can either be consumed by the host directly or exposed to
    Hyper-V VMs.
   </p></section><section class="sect2" id="hyperv-vm-disks" data-id-title="Hyper-V VM disks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.6.2 </span><span class="title-name">Hyper-V VM disks</span> <a title="Permalink" class="permalink" href="#hyperv-vm-disks">#</a></h3></div></div></div><p>
    The following sample imports an RBD image and boots a Hyper-V VM using it.
   </p><div class="verbatim-wrap"><pre class="screen">      # Feel free to use any other image. This one is convenient to use for
      # testing purposes because it's very small (~15MB) and the login prompt
      # prints the pre-configured password.
      wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img `
           -OutFile cirros-0.5.1-x86_64-disk.img

      # We'll need to make sure that the imported images are raw (so no qcow2 or vhdx).
      # You may get qemu-img from https://cloudbase.it/qemu-img-windows/
      # You can add the extracted location to $env:Path or update the path accordingly.
      qemu-img convert -O raw cirros-0.5.1-x86_64-disk.img cirros-0.5.1-x86_64-disk.raw

      rbd import cirros-0.5.1-x86_64-disk.raw
      # Let's give it a hefty 100MB size.
      rbd resize cirros-0.5.1-x86_64-disk.raw --size=100MB

      rbd-wnbd map cirros-0.5.1-x86_64-disk.raw

      # Let's have a look at the mappings.
      rbd-wnbd list
      Get-Disk

      $mappingJson = rbd-wnbd show cirros-0.5.1-x86_64-disk.raw --format=json
      $mappingJson = $mappingJson | ConvertFrom-Json

      $diskNumber = $mappingJson.disk_number

      New-VM -VMName BootFromRBD -MemoryStartupBytes 512MB
      # The disk must be turned offline before it can be passed to Hyper-V VMs
      Set-Disk -Number $diskNumber -IsOffline $true
      Add-VMHardDiskDrive -VMName BootFromRBD -DiskNumber $diskNumber
      Start-VM -VMName BootFromRBD</pre></div></section><section class="sect2" id="windows-partitions" data-id-title="Configuring Microsoft Windows partitions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.6.3 </span><span class="title-name">Configuring Microsoft Windows partitions</span> <a title="Permalink" class="permalink" href="#windows-partitions">#</a></h3></div></div></div><p>
    The following sample creates an empty RBD image, attaches it to the host
    and initializes a partition:
   </p><div class="verbatim-wrap"><pre class="screen">  rbd create blank_image --size=1G
  rbd-wnbd map blank_image

  $mappingJson = rbd-wnbd show blank_image --format=json
  $mappingJson = $mappingJson | ConvertFrom-Json

  $diskNumber = $mappingJson.disk_number

  # The disk must be online before creating or accessing partitions.
  Set-Disk -Number $diskNumber -IsOffline $false

  # Initialize the disk, partition it and create a fileystem.
  Get-Disk -Number $diskNumber | `
      Initialize-Disk -PassThru | `
      New-Partition -AssignDriveLetter -UseMaximumSize | `
      Format-Volume -Force -Confirm:$false</pre></div></section></section><section class="sect1" id="rbd-windows-service" data-id-title="RBD Microsoft Windows service"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.7 </span><span class="title-name">RBD Microsoft Windows service</span> <a title="Permalink" class="permalink" href="#rbd-windows-service">#</a></h2></div></div></div><p>
   In order to ensure that <code class="command">rbd-wnbd</code> mappings survive host
   reboots, a new Microsoft Windows service, called the Ceph RBD Mapping Service has
   been created. This service automatically maintains mappings as they are
   added using the Ceph tools. All mappings are currently persistent and are
   recreated when the service starts, unless they are explicitly unmapped. The
   service disconnects all mappings when stopped.
  </p><p>
   This service also adjusts the Microsoft Windows service start order so that RBD images
   can be mapped before starting any services that may depend on them. For
   example, VMs.
  </p><p>
   RBD maps are stored in the Microsoft Windows registry at the following location:
  </p><div class="verbatim-wrap"><pre class="screen">SYSTEM\CurrentControlSet\Services\rbd-wnbd</pre></div></section><section class="sect1" id="windows-cephfs" data-id-title="Configuring CephFS"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.8 </span><span class="title-name">Configuring CephFS</span> <a title="Permalink" class="permalink" href="#windows-cephfs">#</a></h2></div></div></div><div id="id-1.8.3.10.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
    The following feature is <span class="emphasis"><em>experimental</em></span>, and is not
    intended for use in production environments.
   </p></div><p>
   Ceph for Microsoft Windows provides CephFS support through the Dokany FUSE
   wrapper. In order to use CephFS, install Dokany v1.4.1 or newer using the
   installers available here: https://github.com/dokan-dev/dokany/releases
  </p><p>
   With Dokany installed, and <code class="filename">ceph.conf</code> and
   <code class="filename">ceph.client.admin.keyring</code> configuration files in place,
   CephFS can be mounted using the <code class="command">ceph-dokan.exe</code> command.
   For example:
  </p><div class="verbatim-wrap"><pre class="screen">ceph-dokan.exe -l x</pre></div><p>
   This command mounts the default Ceph file system using the drive letter
   <code class="literal">X</code>. If <code class="filename">ceph.conf</code> is not placed at the
   default location (<code class="filename">C:\ProgramData\ceph\ceph.conf</code>), a
   <code class="option">-c</code> parameter can be used to specify the location of
   <code class="filename">ceph.conf</code>.
  </p><p>
   The <code class="option">-l</code> argument also allows using an empty folder as a
   mountpoint instead of a drive letter.
  </p><p>
   The UID and GID used for mounting the file system defaults to
   <code class="literal">0</code> and may be changed using the following
   <code class="filename">ceph.conf</code> options:
  </p><div class="verbatim-wrap"><pre class="screen">[client]
# client_permissions = true
client_mount_uid = 1000
client_mount_gid = 1000</pre></div><div id="id-1.8.3.10.10" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    Microsoft Windows Access Control Lists (ACLs) are ignored. Portable Operating System
    Interface (POSIX) ACLs are supported but cannot be modified using the
    current CLI.
   </p></div><div id="id-1.8.3.10.11" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><div class="admon-title">Important</div><p>
    CephFS does not support mandatory file locks, which Microsoft Windows heavily
    relies upon. At the moment, we are letting Dokan handle file locks, which
    are only enforced locally.
   </p></div><p>
   For debugging purposes, <code class="option">-d</code> and <code class="option">-s</code> may be
   used. The former enables debug output and the latter enables
   <code class="literal">stderr</code> logging. By default, debug messages are sent to a
   connected debugger.
  </p><p>
   You may use <code class="option">--help</code> to get the full list of available
   options. Additional information on this <span class="emphasis"><em>experimental</em></span>
   feature may be found in the upstream Ceph documentation:
   <a class="link" href="https://docs.ceph.com/en/latest/cephfs/ceph-dokan" target="_blank">https://docs.ceph.com/en/latest/cephfs/ceph-dokan</a>
  </p></section></section><section class="appendix" id="windows-conffile" data-id-title="Sample configuration files"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Sample configuration files</span> <a title="Permalink" class="permalink" href="#windows-conffile">#</a></h1></div></div></div><p>
  <code class="filename">C:\ProgramData\ceph\ceph.conf</code>
 </p><div class="verbatim-wrap"><pre class="screen">[global]
     log to stderr = true
     ; Uncomment the following in order to use the Windows Event Log
     ; log to syslog = true

     run dir = C:/ProgramData/ceph
     crash dir = C:/ProgramData/ceph

     ; Use the following to change the cephfs client log level
     ; debug client = 2
[client]
     keyring = C:/ProgramData/ceph/keyring
     ; log file = C:/ProgramData/ceph/$name.$pid.log
     admin socket = C:/ProgramData/ceph/$name.$pid.asok

     ; client_permissions = true
     ; client_mount_uid = 1000
     ; client_mount_gid = 1000
[global]
     ; Specify IP addresses for monitor nodes as in the following example: ;
     mon host = [v2:10.1.1.1:3300,v1:10.1.1.1:6789] [v2:10.1.1.2:3300,v1:10.1.1.2:6789] [v2:10.1.1.3:3300,v1:1.1.1.3:6789]</pre></div><div id="id-1.8.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   Directory paths in the <code class="filename">ceph.conf</code> must be delimited
   using forward-slashes.
  </p></div><p>
  <code class="filename">C:\ProgramData\ceph\keyring</code>
 </p><div class="verbatim-wrap"><pre class="screen">; This file should be copied directly from /etc/ceph/ceph.client.admin.keyring
; The contents should be similar to the following example:
[client.admin]
    key = ADCyl77eBBAAABDDjX72tAljOwv04m121v/7yA==
    caps mds = "allow *"
    caps mon = "allow *"
    caps osd = "allow *"
    caps mgr = "allow *"</pre></div></section><section class="appendix" id="windows-trouble-tips" data-id-title="Troubleshooting tips"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">B </span><span class="title-name">Troubleshooting tips</span> <a title="Permalink" class="permalink" href="#windows-trouble-tips">#</a></h1></div></div></div><p>
  If you encounter installation or driver problems, the following tips may be
  helpful.
 </p><p>
  Generating an installation log:
 </p><div class="verbatim-wrap"><pre class="screen">msiexec /i C:\path\to\ses4win.msi /l*v log.txt</pre></div><p>
  You can identify driver loading issues in the Windows driver log:
 </p><div class="verbatim-wrap"><pre class="screen">C:\Windows\inf\Setupapi.dev.log</pre></div><p>
  To manually uninstall, execute the following:
 </p><div class="verbatim-wrap"><pre class="screen">msiexec /x [C:\path\to\ses4win.msi|{GUID}]</pre></div><div id="id-1.8.5.10" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><div class="admon-title">Note</div><p>
   The GUID can be found under
   <code class="filename">HKLM\Software\Microsoft\Windows\CurrentVersion\Uninstall</code>.
  </p></div><p>
  Increase WNDB logging levels through:
 </p><div class="verbatim-wrap"><pre class="screen">wnbd-client set-debug 1</pre></div><p>
  Basic I/O counters can be monitored through:
 </p><div class="verbatim-wrap"><pre class="screen">wnbd-client list
wnbd-client stats [image_name]</pre></div></section><section class="appendix" id="win-upstream-project" data-id-title="Upstream projects"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">C </span><span class="title-name">Upstream projects</span> <a title="Permalink" class="permalink" href="#win-upstream-project">#</a></h1></div></div></div><p>
  The Ceph for Microsoft Windows effort is being done entirely in Open Source, and in
  conjunction with the upstream project(s). For more development-level details,
  feel free to join the discussion in the following projects:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Ceph: <a class="link" href="https://github.com/ceph/ceph/pull/34859" target="_blank">https://github.com/ceph/ceph/pull/34859</a>
   </p></li><li class="listitem"><p>
    WNBD: <a class="link" href="https://github.com/cloudbase/wnbd" target="_blank">https://github.com/cloudbase/wnbd</a>
   </p></li><li class="listitem"><p>
    Ceph Windows Installer:
    <a class="link" href="https://github.com/cloudbase/ceph-windows-installer" target="_blank">https://github.com/cloudbase/ceph-windows-installer</a>
   </p></li></ul></div></section><section class="appendix" id="id-1.8.7" data-id-title="Ceph maintenance updates based on upstream Octopus point releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">D </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span> <a title="Permalink" class="permalink" href="#id-1.8.7">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7 are based on the
  Octopus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Octopus series, SUSE Enterprise Storage 7 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.5"><span class="name">Octopus 15.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.12"><span class="name">Octopus 15.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.12">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The containers include an updated <code class="literal">tcmalloc</code> that avoids
    crashes seen on 15.2.9.
   </p></li><li class="listitem"><p>
    RADOS: BlueStore handling of huge (&gt;4GB) writes from RocksDB to BlueFS
    has been fixed.
   </p></li><li class="listitem"><p>
    When upgrading from a previous cephadm release,
    <code class="command">systemctl</code> may hang when trying to start or restart the
    monitoring containers. This is caused by a change in the <code class="systemitem">systemd</code> unit to
    use <code class="option">type=forking</code>.) After the upgrade, please run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy nfs
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy iscsi
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy alertmanager</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.15"><span class="name">Octopus 15.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.15">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    MGR: progress module can now be turned on/off, using the commands:
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li><li class="listitem"><p>
    OSD: PG removal has been optimized in this release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.18"><span class="name">Octopus 15.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS and includes a number of bug
  fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    OpenStack Manila use of <code class="filename">ceph_volume_client.py</code> library
    allowed tenant access to any Ceph credential’s secret.
   </p></li><li class="listitem"><p>
    <code class="command">ceph-volume</code>: The <code class="command">lvm batch</code> subcommand
    received a major rewrite. This closed a number of bugs and improves
    usability in terms of size specification and calculation, as well as
    idempotency behaviour and disk replacement process. Please refer to
    <a class="link" href="https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/" target="_blank">https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/</a>
    for more detailed information.
   </p></li><li class="listitem"><p>
    MON: The cluster log now logs health detail every
    <code class="option">mon_health_to_clog_interval</code>, which has been changed from
    1hr to 10min. Logging of health detail will be skipped if there is no
    change in health summary since last known.
   </p></li><li class="listitem"><p>
    The <code class="command">ceph df</code> command now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    The <code class="option">bluefs_preextend_wal_files</code> option has been removed.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial monitor to contact for Ceph
    tools and daemons using the <code class="option">mon_host_override</code> config
    option or <code class="option">--mon-host-override</code> command line switch. This
    generally should only be used for debugging and only affects initial
    communication with Ceph's monitor cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.21"><span class="name">Octopus 15.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.21">#</a></h2></div><p>
  This release fixes a serious bug in RGW that has been shown to cause data
  loss when a read of a large RGW object (for example, one with at least one
  tail segment) takes longer than one half the time specified in the
  configuration option <code class="option">rgw_gc_obj_min_wait</code>. The bug causes the
  tail segments of that read object to be added to the RGW garbage collection
  queue, which will in turn cause them to be deleted after a period of time.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.23"><span class="name">Octopus 15.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.23">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.25"><span class="name">Octopus 15.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.25">#</a></h2></div><p>
  The Octopus point release 15.2.5 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CephFS: Automatic static sub-tree partitioning policies may now be
    configured using the new distributed and random ephemeral pinning extended
    attributes on directories. See the following documentation for more
    information:
    <a class="link" href="https://docs.ceph.com/docs/master/cephfs/multimds/" target="_blank">https://docs.ceph.com/docs/master/cephfs/multimds/</a>
   </p></li><li class="listitem"><p>
    Monitors now have a configuration option
    <code class="option">mon_osd_warn_num_repaired</code>, which is set to 10 by default.
    If any OSD has repaired more than this many I/O errors in stored data a
    <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is generated.
   </p></li><li class="listitem"><p>
    Now, when <code class="literal">no scrub</code> and/or <code class="literal">no
    deep-scrub</code> flags are set globally or per pool, scheduled scrubs
    of the type disabled will be aborted. All user initiated scrubs are NOT
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.28"><span class="name">Octopus 15.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.28">#</a></h2></div><p>
  The Octopus point release 15.2.4 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li><li class="listitem"><p>
    Object Gateway: The <code class="command">radosgw-admin</code> sub-commands dealing with
    orphans—<code class="command">radosgw-admin orphans find</code>,
    <code class="command">radosgw-admin orphans finish</code>, and <code class="command">radosgw-admin
    orphans list-jobs</code>—have been deprecated. They had not been
    actively maintained, and since they store intermediate results on the
    cluster, they could potentially fill a nearly-full cluster. They have been
    replaced by a tool, <code class="command">rgw-orphan-list</code>, which is currently
    considered experimental.
   </p></li><li class="listitem"><p>
    RBD: The name of the RBD pool object that is used to store RBD trash purge
    schedule is changed from <code class="literal">rbd_trash_trash_purge_schedule</code>
    to <code class="literal">rbd_trash_purge_schedule</code>. Users that have already
    started using RBD trash purge schedule functionality and have per pool or
    name space schedules configured should copy the
    <code class="literal">rbd_trash_trash_purge_schedule</code> object to
    <code class="literal">rbd_trash_purge_schedule</code> before the upgrade and remove
    <code class="literal">rbd_trash_purge_schedule</code> using the following commands in
    every RBD pool and name space where a trash purge schedule was previously
    configured:
   </p><div class="verbatim-wrap"><pre class="screen">rados -p <em class="replaceable">pool-name</em> [-N namespace] cp rbd_trash_trash_purge_schedule rbd_trash_purge_schedule
rados -p <em class="replaceable">pool-name</em> [-N namespace] rm rbd_trash_trash_purge_schedule</pre></div><p>
    Alternatively, use any other convenient way to restore the schedule after
    the upgrade.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.31"><span class="name">Octopus 15.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.31">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The Octopus point release 15.2.3 was a hot-fix release to address an
    issue where WAL corruption was seen when
    <code class="option">bluefs_preextend_wal_files</code> and
    <code class="option">bluefs_buffered_io</code> were enabled at the same time. The fix
    in 15.2.3 is only a temporary measure (changing the default value of
    <code class="option">bluefs_preextend_wal_files</code> to <code class="literal">false</code>).
    The permanent fix will be to remove the
    <code class="option">bluefs_preextend_wal_files</code> option completely: this fix
    will most likely arrive in the 15.2.6 point release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.33"><span class="name">Octopus 15.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.33">#</a></h2></div><p>
  The Octopus point release 15.2.2 patched one security vulnerability:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10736: Fixed an authorization bypass in MONs and MGRs
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.8.7.36"><span class="name">Octopus 15.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.8.7.36">#</a></h2></div><p>
  The Octopus point release 15.2.1 fixed an issue where upgrading quickly
  from Luminous (SES5.5) to Nautilus (SES6) to Octopus (SES7) caused OSDs to
  crash. In addition, it patched two security vulnerabilities that were present
  in the initial Octopus (15.2.0) release:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS because of RGW GetObject header-splitting
   </p></li></ul></div></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.8.8">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.8.8.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.8.8.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.2">#</a></span></dt><dd class="glossdef"><p>
     The host from which you run the Ceph-related commands to administer
     cluster hosts.
    </p></dd><dt id="id-1.8.8.3.3"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.3">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies the end user.
    </p></dd><dt id="id-1.8.8.3.4"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.4">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.8.8.3.5"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.5">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p></dd><dt id="id-1.8.8.3.6"><span><span class="glossterm">cephadm</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.6">#</a></span></dt><dd class="glossdef"><p>
     cephadm deploys and manages a Ceph cluster by connecting to hosts from
     the manager daemon via SSH to add, remove, or update Ceph daemon
     containers.
    </p></dd><dt id="id-1.8.8.3.7"><span><span class="glossterm">CephFS</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.7">#</a></span></dt><dd class="glossdef"><p>
     The Ceph file system.
    </p></dd><dt id="id-1.8.8.3.8"><span><span class="glossterm">CephX</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.8">#</a></span></dt><dd class="glossdef"><p>
     The Ceph authentication protocol. Cephx operates like Kerberos, but it
     has no single point of failure.
    </p></dd><dt id="id-1.8.8.3.9"><span><span class="glossterm">Ceph Client</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.9">#</a></span></dt><dd class="glossdef"><p>
     The collection of Ceph components which can access a Ceph Storage
     Cluster. These include the Object Gateway, the Ceph Block Device, the CephFS,
     and their corresponding libraries, kernel modules, and FUSE clients.
    </p></dd><dt id="id-1.8.8.3.10"><span><span class="glossterm"><code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.10">#</a></span></dt><dd class="glossdef"><p>
     Provides tooling for deploying Ceph clusters managed by cephadm using
     Salt.
    </p></dd><dt id="id-1.8.8.3.11"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.11">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data. Such a set
     consists of Ceph monitors and OSDs.
    </p></dd><dt id="id-1.8.8.3.12"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.12">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.8.8.3.13"><span><span class="glossterm">CRUSH rule</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.13">#</a></span></dt><dd class="glossdef"><p>
     The CRUSH data placement rule that applies to a particular pool or pools.
    </p></dd><dt id="id-1.8.8.3.14"><span><span class="glossterm">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.14">#</a></span></dt><dd class="glossdef"><p>
     A built-in Web-based Ceph management and monitoring application to
     administer various aspects and objects of the cluster. The dashboard is
     implemented as a Ceph Manager module.
    </p></dd><dt id="id-1.8.8.3.15"><span><span class="glossterm">DriveGroups</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.15">#</a></span></dt><dd class="glossdef"><p>
     DriveGroups are a declaration of one or more OSD layouts that can be mapped
     to physical drives. An OSD layout defines how Ceph physically allocates
     OSD storage on the media matching the specified criteria.
    </p></dd><dt id="id-1.8.8.3.16"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.16">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.8.8.3.17"><span><span class="glossterm">Metadata Server</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.17">#</a></span></dt><dd class="glossdef"><p>
     Metadata Server or MDS is the Ceph metadata software.
    </p></dd><dt id="id-1.8.8.3.18"><span><span class="glossterm">Ceph Monitor</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.18">#</a></span></dt><dd class="glossdef"><p>
     Ceph Monitor or MON is the Ceph monitor software.
    </p></dd><dt id="id-1.8.8.3.19"><span><span class="glossterm">Ceph Manager</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.19">#</a></span></dt><dd class="glossdef"><p>
     Ceph Manager or MGR is the Ceph manager software, which collects all the state
     from the whole cluster in one place.
    </p></dd><dt id="id-1.8.8.3.20"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.20">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.8.8.3.21"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.21">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Object Storage Device</em></span>: A physical or logical storage
     unit.
    </p></dd><dt id="id-1.8.8.3.22"><span><span class="glossterm">Ceph OSD Daemon</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.22">#</a></span></dt><dd class="glossdef"><p>
     The <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.8.8.3.23"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.23">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.8.8.3.24"><span><span class="glossterm">Ceph Object Storage</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.24">#</a></span></dt><dd class="glossdef"><p>
     The object storage "product", service or capabilities, which consists of a
     Ceph Storage Cluster and a Ceph Object Gateway.
    </p></dd><dt id="id-1.8.8.3.25"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.25">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store. Also known as the
     RADOS Gateway (RGW).
    </p></dd><dt id="id-1.8.8.3.26"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.26">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.8.8.3.27"><span><span class="glossterm">Point Release</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.27">#</a></span></dt><dd class="glossdef"><p>
     Any ad-hoc release that includes only bug or security fixes.
    </p></dd><dt id="id-1.8.8.3.28"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.28">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.8.8.3.29"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.29">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd><dt id="id-1.8.8.3.30"><span><span class="glossterm">Reliable Autonomic Distributed Object Store (RADOS)</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.30">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data (MON+OSD).
    </p></dd><dt id="id-1.8.8.3.31"><span><span class="glossterm">RADOS Block Device (RBD)</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.31">#</a></span></dt><dd class="glossdef"><p>
     The block storage component of Ceph. Also known as the Ceph block
     device.
    </p></dd><dt id="id-1.8.8.3.32"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.32">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd><dt id="id-1.8.8.3.33"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.33">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.8.8.3.34"><span><span class="glossterm">Samba</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.34">#</a></span></dt><dd class="glossdef"><p>
     Windows integration software.
    </p></dd><dt id="id-1.8.8.3.35"><span><span class="glossterm">Samba Gateway</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.35">#</a></span></dt><dd class="glossdef"><p>
     The Samba Gateway joins the Active Directory in the Windows domain to authenticate
     and authorize users.
    </p></dd><dt id="id-1.8.8.3.36"><span><span class="glossterm">Multi-zone</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.36">#</a></span></dt><dd class="glossdef"/><dt id="id-1.8.8.3.37"><span><span class="glossterm">zonegroup</span> <a title="Permalink" class="permalink" href="#id-1.8.8.3.37">#</a></span></dt><dd class="glossdef"/></dl></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/book_storage_windows.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>