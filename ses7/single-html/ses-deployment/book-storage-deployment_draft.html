<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Deployment Guide | SUSE Enterprise Storage 7</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deployment Guide | SES 7"/>
<meta name="description" content="This guide focuses on deploying a basic Ceph cluster, and how to deploy additional services. It also covers the steps for upgrading to SUSE Enterprise Storage …"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="Deployment Guide"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Deployment Guide | SES 7"/>
<meta property="og:description" content="This guide focuses on deploying a basic Ceph cluster, and how to deploy additional services. It also covers the steps for upgrading to SUSE Enterprise Storage …"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deployment Guide | SES 7"/>
<meta name="twitter:description" content="This guide focuses on deploying a basic Ceph cluster, and how to deploy additional services. It also covers the steps for upgrading to SUSE Enterprise Storage …"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.10.2.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-deployment">Deployment Guide</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-deployment" data-id-title="Deployment Guide"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><h1 class="title">Deployment Guide</h1></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Alexandra</span> <span class="surname">Settle</span>, and <span class="firstname">Liam</span> <span class="surname">Proven</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>06/27/2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#preface-deployment"><span class="title-name">About this guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.3.2.5"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.6"><span class="title-name">Giving feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.7"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.8"><span class="title-name">Support</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.9"><span class="title-name">Ceph contributors</span></a></span></li><li><span class="sect1"><a href="#id-1.3.2.10"><span class="title-name">Commands and command prompts used in this guide</span></a></span></li></ul></li><li><span class="part"><a href="#part-ses"><span class="title-number">I </span><span class="title-name">Introducing SUSE Enterprise Storage (SES)</span></a></span><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SES and Ceph</span></a></span><ul><li><span class="sect1"><a href="#storage-intro-features"><span class="title-number">1.1 </span><span class="title-name">Ceph features</span></a></span></li><li><span class="sect1"><a href="#storage-intro-core"><span class="title-number">1.2 </span><span class="title-name">Ceph core components</span></a></span></li><li><span class="sect1"><a href="#storage-intro-structure"><span class="title-number">1.3 </span><span class="title-name">Ceph storage structure</span></a></span></li><li><span class="sect1"><a href="#about-bluestore"><span class="title-number">1.4 </span><span class="title-name">BlueStore</span></a></span></li><li><span class="sect1"><a href="#storage-moreinfo"><span class="title-number">1.5 </span><span class="title-name">Additional information</span></a></span></li></ul></li><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware requirements and recommendations</span></a></span><ul><li><span class="sect1"><a href="#network-overview"><span class="title-number">2.1 </span><span class="title-name">Network overview</span></a></span></li><li><span class="sect1"><a href="#multi-architecture"><span class="title-number">2.2 </span><span class="title-name">Multiple architecture configurations</span></a></span></li><li><span class="sect1"><a href="#ses-hardware-config"><span class="title-number">2.3 </span><span class="title-name">Hardware configuration</span></a></span></li><li><span class="sect1"><a href="#deployment-osd-recommendation"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mon"><span class="title-number">2.5 </span><span class="title-name">Monitor nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-rgw"><span class="title-number">2.6 </span><span class="title-name">Object Gateway nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-mds"><span class="title-number">2.7 </span><span class="title-name">Metadata Server nodes</span></a></span></li><li><span class="sect1"><a href="#sysreq-admin-node"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span></a></span></li><li><span class="sect1"><a href="#sysreq-iscsi"><span class="title-number">2.9 </span><span class="title-name">iSCSI Gateway nodes</span></a></span></li><li><span class="sect1"><a href="#req-ses-other"><span class="title-number">2.10 </span><span class="title-name">SES and other SUSE products</span></a></span></li><li><span class="sect1"><a href="#sysreq-naming"><span class="title-number">2.11 </span><span class="title-name">Name limitations</span></a></span></li><li><span class="sect1"><a href="#ses-bp-diskshare"><span class="title-number">2.12 </span><span class="title-name">OSD and monitor sharing one server</span></a></span></li></ul></li><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Admin Node HA setup</span></a></span><ul><li><span class="sect1"><a href="#admin-ha-architecture"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA cluster for Admin Node</span></a></span></li><li><span class="sect1"><a href="#admin-ha-cluster"><span class="title-number">3.2 </span><span class="title-name">Building an HA cluster with the Admin Node</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#ses-deployment"><span class="title-number">II </span><span class="title-name">Deploying Ceph Cluster</span></a></span><ul><li><span class="chapter"><a href="#deploy-intro"><span class="title-number">4 </span><span class="title-name">Introduction and common tasks</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-install-relnotes"><span class="title-number">4.1 </span><span class="title-name">Read the release notes</span></a></span></li></ul></li><li><span class="chapter"><a href="#deploy-sles"><span class="title-number">5 </span><span class="title-name">Installing and configuring SUSE Linux Enterprise Server</span></a></span></li><li><span class="chapter"><a href="#deploy-salt"><span class="title-number">6 </span><span class="title-name">Deploying Salt</span></a></span></li><li><span class="chapter"><a href="#deploy-bootstrap"><span class="title-number">7 </span><span class="title-name">Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code></span></a></span><ul><li><span class="sect1"><a href="#deploy-cephadm-cephsalt"><span class="title-number">7.1 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code></span></a></span></li><li><span class="sect1"><a href="#deploy-cephadm-configure"><span class="title-number">7.2 </span><span class="title-name">Configuring cluster properties</span></a></span></li><li><span class="sect1"><a href="#deploy-cephadm-deploy"><span class="title-number">7.3 </span><span class="title-name">Updating nodes and bootstrap minimal cluster</span></a></span></li><li><span class="sect1"><a href="#deploy-min-cluster-final-steps"><span class="title-number">7.4 </span><span class="title-name">Reviewing final steps</span></a></span></li><li><span class="sect1"><a href="#deploy-min-cluster-disable-insecure"><span class="title-number">7.5 </span><span class="title-name">Disable insecure clients</span></a></span></li></ul></li><li><span class="chapter"><a href="#deploy-core"><span class="title-number">8 </span><span class="title-name">Deploying the remaining core services using cephadm</span></a></span><ul><li><span class="sect1"><a href="#deploy-cephadm-day2-orch"><span class="title-number">8.1 </span><span class="title-name">The <code class="command">ceph orch</code> command</span></a></span></li><li><span class="sect1"><a href="#cephadm-service-and-placement-specs"><span class="title-number">8.2 </span><span class="title-name">Service and placement specification</span></a></span></li><li><span class="sect1"><a href="#deploy-cephadm-day2-services"><span class="title-number">8.3 </span><span class="title-name">Deploy Ceph services</span></a></span></li></ul></li><li><span class="chapter"><a href="#deploy-additional"><span class="title-number">9 </span><span class="title-name">Deployment of additional services</span></a></span><ul><li><span class="sect1"><a href="#cha-ceph-as-iscsi"><span class="title-number">9.1 </span><span class="title-name">Installation of iSCSI gateway</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#ses-upgrade"><span class="title-number">III </span><span class="title-name">Upgrading from Previous Releases</span></a></span><ul><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">10 </span><span class="title-name">Upgrade from a previous release</span></a></span><ul><li><span class="sect1"><a href="#before-upgrade"><span class="title-number">10.1 </span><span class="title-name">Before upgrading</span></a></span></li><li><span class="sect1"><a href="#upgrade-salt-master"><span class="title-number">10.2 </span><span class="title-name">Upgrading the Salt Master</span></a></span></li><li><span class="sect1"><a href="#upgrade-mon-mgr-nodes"><span class="title-number">10.3 </span><span class="title-name">Upgrading the MON, MGR, and OSD nodes</span></a></span></li><li><span class="sect1"><a href="#upgrade-gateway-nodes"><span class="title-number">10.4 </span><span class="title-name">Upgrading gateway nodes</span></a></span></li><li><span class="sect1"><a href="#upgrade-cephsalt"><span class="title-number">10.5 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code> and applying the cluster configuration</span></a></span></li><li><span class="sect1"><a href="#upgrade-cephsalt-monitoring"><span class="title-number">10.6 </span><span class="title-name">Upgrading and adopting the monitoring stack</span></a></span></li><li><span class="sect1"><a href="#upgrade-gateways"><span class="title-number">10.7 </span><span class="title-name">Gateway service redeployment</span></a></span></li><li><span class="sect1"><a href="#upgrade-post-cleanup"><span class="title-number">10.8 </span><span class="title-name">Post-upgrade Clean-up</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#id-1.3.6"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></span></li><li><span class="glossary"><a href="#id-1.3.7"><span class="title-name">Glossary</span></a></span></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#storage-intro-core-rados-figure"><span class="number">1.1 </span><span class="name">Interfaces to the Ceph object store</span></a></span></li><li><span class="figure"><a href="#storage-intro-structure-example-figure"><span class="number">1.2 </span><span class="name">Small scale Ceph example</span></a></span></li><li><span class="figure"><a href="#network-overview-figure"><span class="number">2.1 </span><span class="name">Network overview</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.3.7.3.6"><span class="number">2.2 </span><span class="name">Minimum cluster configuration</span></a></span></li><li><span class="figure"><a href="#id-1.3.3.4.6.6"><span class="number">3.1 </span><span class="name">2-Node HA cluster for Admin Node</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.5.5.14"><span class="number">7.1 </span><span class="name">Deployment of a minimal cluster</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.2.5.3"><span class="number">9.1 </span><span class="name">Ceph Cluster with a Single iSCSI Gateway</span></a></span></li><li><span class="figure"><a href="#id-1.3.4.7.2.5.5"><span class="number">9.2 </span><span class="name">Ceph cluster with multiple iSCSI gateways</span></a></span></li></ul></div><div class="list-of-examples"><div class="toc-title">List of Examples</div><ul><li><span class="example"><a href="#id-1.3.4.5.4.13.4.4.7.2"><span class="number">7.1 </span><span class="name">Viewing manifest files</span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.4.13.4.4.7.3"><span class="number">7.2 </span><span class="name">Synchronize to a directory</span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.4.13.4.4.7.4"><span class="number">7.3 </span><span class="name">Synchronize Grafana images:</span></a></span></li><li><span class="example"><a href="#id-1.3.4.5.4.13.4.4.7.5"><span class="number">7.4 </span><span class="name">Synchronize latest Prometheus images</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.3.1.6"><p>
  Copyright © 2020–2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under Creative Commons Attribution-ShareAlike 4.0 International
  (CC-BY-SA 4.0): <a class="link" href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">https://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section xml:lang="en" class="preface" id="preface-deployment" data-id-title="About this guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About this guide</span> <a title="Permalink" class="permalink" href="#preface-deployment">#</a></h1></div></div></div><p>
  This guide focuses on deploying a basic Ceph cluster, and how to deploy
  additional services. It also covers the steps for upgrading to SUSE Enterprise Storage
  7 from the previous product version.
 </p><p>
 SUSE Enterprise Storage 7 is an extension to SUSE Linux Enterprise Server 15 SP2. It combines the
 capabilities of the Ceph
 (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>)
 storage project with the enterprise engineering and support of SUSE.
 SUSE Enterprise Storage 7 provides IT organizations with the ability to
 deploy a distributed storage architecture that can support a number of use
 cases using commodity hardware platforms.
</p><section xml:lang="en" class="sect1" id="id-1.3.2.5" data-id-title="Available documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available documentation</span> <a title="Permalink" class="permalink" href="#id-1.3.2.5">#</a></h2></div></div></div><div id="id-1.3.2.5.3" data-id-title="Online documentation and latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Online documentation and latest updates</h6><p>
   Documentation for our products is available at
   <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>,
   where you can also find the latest updates, and browse or download the
   documentation in various formats. The latest documentation updates can be
   found in the English language version.
  </p></div><p>
  In addition, the product documentation is available in your installed system
  under <code class="filename">/usr/share/doc/manual</code>. It is included in an RPM
  package named
  <span class="package">ses-manual_<em class="replaceable">LANG_CODE</em></span>. Install
  it if it is not already on your system, for example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper install ses-manual_en</pre></div><p>
  The following documentation is available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.5.7.1"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-deployment.html" target="_blank"><em class="citetitle">Deployment Guide</em></a></span></dt><dd><p>
     This guide focuses on deploying a basic Ceph cluster, and how to deploy
     additional services. It also cover the steps for upgrading to
     SUSE Enterprise Storage 7 from the previous product version.
    </p></dd><dt id="id-1.3.2.5.7.2"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-admin.html" target="_blank"><em class="citetitle">Administration and Operations Guide</em></a></span></dt><dd><p>
     This guide focuses on routine tasks that you as an administrator need to
     take care of after the basic Ceph cluster has been deployed (day 2
     operations). It also describes all the supported ways to access data
     stored in a Ceph cluster.
    </p></dd><dt id="id-1.3.2.5.7.3"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-security.html" target="_blank"><em class="citetitle">Security Hardening Guide</em></a></span></dt><dd><p>
     This guide focuses on how to ensure your cluster is secure.
    </p></dd><dt id="id-1.3.2.5.7.4"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-troubleshooting.html" target="_blank"><em class="citetitle">Troubleshooting Guide</em></a></span></dt><dd><p>
     This guide takes you through various common problems when running
     SUSE Enterprise Storage 7 and other related issues to relevant
     components such as Ceph or Object Gateway.
    </p></dd><dt id="id-1.3.2.5.7.5"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-windows.html" target="_blank"><em class="citetitle">SUSE Enterprise Storage for Windows Guide</em></a></span></dt><dd><p>
     This guide describes the integration, installation, and configuration of
     Microsoft Windows environments and SUSE Enterprise Storage using the Windows Driver.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.3.2.6" data-id-title="Giving feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Giving feedback</span> <a title="Permalink" class="permalink" href="#id-1.3.2.6">#</a></h2></div></div></div><p>
  We welcome feedback on, and contributions to, this documentation.
  There are several channels for this:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.6.3.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.3.2.6.3.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
     Reporting issues requires a Bugzilla account.
    </p><p>
     To simplify this process, you can use the <span class="guimenu">Report
     Documentation Bug</span> links next to headlines in the HTML
     version of this document. These preselect the right product and
     category in Bugzilla and add a link to the current section.
     You can start typing your bug report right away.
    </p></dd><dt id="id-1.3.2.6.3.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, use the <span class="guimenu">Edit Source</span>
     links next to headlines in the HTML version of this document. They
     take you to the source code on GitHub, where you can open a pull request.
     Contributing requires a GitHub account.
    </p><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README at
     <a class="link" href="https://github.com/SUSE/doc-ses" target="_blank">https://github.com/SUSE/doc-ses</a>.
    </p></dd><dt id="id-1.3.2.6.3.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.3.2.7" data-id-title="Documentation conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span> <a title="Permalink" class="permalink" href="#id-1.3.2.7">#</a></h2></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> / <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as non-privileged user.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.3.2.7.4.13.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Warning notice</h6><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.3.2.7.4.13.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Important notice</h6><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.3.2.7.4.13.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Note notice</h6><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.3.2.7.4.13.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Tip notice</h6><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.3.2.7.4.14.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.3.2.7.4.14.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.3.2.8" data-id-title="Support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Support</span> <a title="Permalink" class="permalink" href="#id-1.3.2.8">#</a></h2></div></div></div><p>
  Find the support statement for SUSE Enterprise Storage and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.3.2.8.4" data-id-title="Support statement for SUSE Enterprise Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#id-1.3.2.8.4">#</a></h3></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offerings available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.2.8.4.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.3.2.8.4.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.3.2.8.4.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Enterprise Storage is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.3.2.8.5" data-id-title="Technology previews"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span> <a title="Permalink" class="permalink" href="#id-1.3.2.8.5">#</a></h3></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback!
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or in other ways
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7</a>.
  </p></section></section><section class="sect1" id="id-1.3.2.9" data-id-title="Ceph contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph contributors</span> <a title="Permalink" class="permalink" href="#id-1.3.2.9">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section><section class="sect1" id="id-1.3.2.10" data-id-title="Commands and command prompts used in this guide"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Commands and command prompts used in this guide</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect2" id="id-1.3.2.10.4" data-id-title="Salt-related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1 </span><span class="title-name">Salt-related commands</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.4">#</a></h3></div></div></div><p>
   These commands help you to deploy Ceph cluster nodes, run commands on
   several (or all) cluster nodes at the same time, or assist you when adding
   or removing cluster nodes. The most frequently used commands are
   <code class="command">ceph-salt</code> and <code class="command">ceph-salt config</code>. You
   need to run Salt commands on the Salt Master node as <code class="systemitem">root</code>. These
   commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div></section><section class="sect2" id="id-1.3.2.10.5" data-id-title="Ceph related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2 </span><span class="title-name">Ceph related commands</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.5">#</a></h3></div></div></div><p>
   These are lower-level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">cephadm</code>, <code class="command">rbd</code>,
   or <code class="command">radosgw-admin</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   The safer and recommended option is to create a more restrictive individual
   key for each administrator user and put it in a directory where the users
   can read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.3.2.10.5.6" data-id-title="Path to Ceph keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph-related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephuser</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><div id="id-1.3.2.10.5.11" data-id-title="Commands for specific nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for specific nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mon &gt; </code></pre></div></div><section class="sect3" id="id-1.3.2.10.5.12" data-id-title="Running ceph-volume"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Running <code class="command">ceph-volume</code></span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.5.12">#</a></h4></div></div></div><p>
    Starting with SUSE Enterprise Storage 7, Ceph services are running containerized.
    If you need to run <code class="command">ceph-volume</code> on an OSD node, you need
    to prepend it with the <code class="command">cephadm</code> command, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm ceph-volume simple scan</pre></div></section></section><section class="sect2" id="id-1.3.2.10.6" data-id-title="General Linux commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3 </span><span class="title-name">General Linux commands</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.6">#</a></h3></div></div></div><p>
   Linux commands not related to Ceph, such as <code class="command">mount</code>,
   <code class="command">cat</code>, or <code class="command">openssl</code>, are introduced either
   with the <code class="prompt user">cephuser@adm &gt; </code> or <code class="prompt root"># </code> prompts, depending on which
   privileges the related command requires.
  </p></section><section class="sect2" id="id-1.3.2.10.7" data-id-title="Additional information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4 </span><span class="title-name">Additional information</span> <a title="Permalink" class="permalink" href="#id-1.3.2.10.7">#</a></h3></div></div></div><p>
   For more information on Ceph key management, refer to
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 30 “Authentication with <code class="systemitem">cephx</code>”, Section 30.2 “Key management”</span>.
  </p></section></section></section><div class="part" id="part-ses" data-id-title="Introducing SUSE Enterprise Storage (SES)"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Introducing SUSE Enterprise Storage (SES) </span><a title="Permalink" class="permalink" href="#part-ses">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-storage-about"><span class="title-number">1 </span><span class="title-name">SES and Ceph</span></a></span></li><dd class="toc-abstract"><p>SUSE Enterprise Storage is a distributed storage system designed for scalability, reliability and performance which is based on the Ceph technology. A Ceph cluster can be run on commodity servers in a common network like Ethernet. The cluster scales up well to thousands of servers (later on referred…</p></dd><li><span class="chapter"><a href="#storage-bp-hwreq"><span class="title-number">2 </span><span class="title-name">Hardware requirements and recommendations</span></a></span></li><dd class="toc-abstract"><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p></dd><li><span class="chapter"><a href="#cha-admin-ha"><span class="title-number">3 </span><span class="title-name">Admin Node HA setup</span></a></span></li><dd class="toc-abstract"><p>
  The <span class="emphasis"><em>Admin Node</em></span> is a Ceph cluster node where the Salt Master
  service runs. It manages the rest of the cluster nodes by querying and
  instructing their Salt Minion services. It usually includes other services as
  well, for example the <span class="emphasis"><em>Grafana</em></span> dashboard backed by the
  <span class="emphasis"><em>Prometheus</em></span> monitoring toolkit.
 </p></dd></ul></div><section class="chapter" id="cha-storage-about" data-id-title="SES and Ceph"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">SES and Ceph</span> <a title="Permalink" class="permalink" href="#cha-storage-about">#</a></h2></div></div></div><p>
  SUSE Enterprise Storage is a distributed storage system designed for scalability,
  reliability and performance which is based on the Ceph technology. A Ceph
  cluster can be run on commodity servers in a common network like Ethernet.
  The cluster scales up well to thousands of servers (later on referred to as
  nodes) and into the petabyte range. As opposed to conventional systems which
  have allocation tables to store and fetch data, Ceph uses a deterministic
  algorithm to allocate storage for data and has no centralized information
  structure. Ceph assumes that in storage clusters the addition or removal of
  hardware is the rule, not the exception. The Ceph cluster automates
  management tasks such as data distribution and redistribution, data
  replication, failure detection and recovery. Ceph is both self-healing and
  self-managing which results in a reduction of administrative and budget
  overhead.
 </p><p>
  This chapter provides a high level overview of SUSE Enterprise Storage 7
  and briefly describes the most important components.
 </p><section class="sect1" id="storage-intro-features" data-id-title="Ceph features"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Ceph features</span> <a title="Permalink" class="permalink" href="#storage-intro-features">#</a></h2></div></div></div><p>
   The Ceph environment has the following features:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.2.5.3.1"><span class="term">Scalability</span></dt><dd><p>
      Ceph can scale to thousands of nodes and manage storage in the range of
      petabytes.
     </p></dd><dt id="id-1.3.3.2.5.3.2"><span class="term">Commodity Hardware</span></dt><dd><p>
      No special hardware is required to run a Ceph cluster. For details, see
      <a class="xref" href="#storage-bp-hwreq" title="Chapter 2. Hardware requirements and recommendations">Chapter 2, <em>Hardware requirements and recommendations</em></a>
     </p></dd><dt id="id-1.3.3.2.5.3.3"><span class="term">Self-managing</span></dt><dd><p>
      The Ceph cluster is self-managing. When nodes are added, removed or
      fail, the cluster automatically redistributes the data. It is also aware
      of overloaded disks.
     </p></dd><dt id="id-1.3.3.2.5.3.4"><span class="term">No Single Point of Failure</span></dt><dd><p>
      No node in a cluster stores important information alone. The number of
      redundancies can be configured.
     </p></dd><dt id="id-1.3.3.2.5.3.5"><span class="term">Open Source Software</span></dt><dd><p>
      Ceph is an open source software solution and independent of specific
      hardware or vendors.
     </p></dd></dl></div></section><section class="sect1" id="storage-intro-core" data-id-title="Ceph core components"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Ceph core components</span> <a title="Permalink" class="permalink" href="#storage-intro-core">#</a></h2></div></div></div><p>
   To make full use of Ceph's power, it is necessary to understand some of
   the basic components and concepts. This section introduces some parts of
   Ceph that are often referenced in other chapters.
  </p><section class="sect2" id="storage-intro-core-rados" data-id-title="RADOS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.1 </span><span class="title-name">RADOS</span> <a title="Permalink" class="permalink" href="#storage-intro-core-rados">#</a></h3></div></div></div><p>
    The basic component of Ceph is called <span class="emphasis"><em>RADOS</em></span>
    <span class="emphasis"><em>(Reliable Autonomic Distributed Object Store)</em></span>. It is
    responsible for managing the data stored in the cluster. Data in Ceph is
    usually stored as objects. Each object consists of an identifier and the
    data.
   </p><p>
    RADOS provides the following access methods to the stored objects that
    cover many use cases:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.2.6.3.4.1"><span class="term">Object Gateway</span></dt><dd><p>
       Object Gateway is an HTTP REST gateway for the RADOS object store. It enables
       direct access to objects stored in the Ceph cluster.
      </p></dd><dt id="id-1.3.3.2.6.3.4.2"><span class="term">RADOS Block Device</span></dt><dd><p>
       RADOS Block Device (RBD) can be accessed like any other block device. These can be
       used for example in combination with <code class="systemitem">libvirt</code> for virtualization
       purposes.
      </p></dd><dt id="id-1.3.3.2.6.3.4.3"><span class="term">CephFS</span></dt><dd><p>
       The Ceph File System is a POSIX-compliant file system.
      </p></dd><dt id="id-1.3.3.2.6.3.4.4"><span class="term"><code class="systemitem">librados</code></span></dt><dd><p>
       <code class="systemitem">librados</code> is a library that can
       be used with many programming languages to create an application capable
       of directly interacting with the storage cluster.
      </p></dd></dl></div><p>
    <code class="systemitem">librados</code> is used by Object Gateway and RBD
    while CephFS directly interfaces with RADOS
    <a class="xref" href="#storage-intro-core-rados-figure" title="Interfaces to the Ceph object store">Figure 1.1, “Interfaces to the Ceph object store”</a>.
   </p><div class="figure" id="storage-intro-core-rados-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/rados-structure.png" target="_blank"><img src="images/rados-structure.png" width="" alt="Interfaces to the Ceph object store"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.1: </span><span class="title-name">Interfaces to the Ceph object store </span><a title="Permalink" class="permalink" href="#storage-intro-core-rados-figure">#</a></h6></div></div></section><section class="sect2" id="storage-intro-core-crush" data-id-title="CRUSH"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.2 </span><span class="title-name">CRUSH</span> <a title="Permalink" class="permalink" href="#storage-intro-core-crush">#</a></h3></div></div></div><p>
    At the core of a Ceph cluster is the <span class="emphasis"><em>CRUSH</em></span>
    algorithm. CRUSH is the acronym for <span class="emphasis"><em>Controlled Replication Under
    Scalable Hashing</em></span>. CRUSH is a function that handles the storage
    allocation and needs comparably few parameters. That means only a small
    amount of information is necessary to calculate the storage position of an
    object. The parameters are a current map of the cluster including the
    health state, some administrator-defined placement rules and the name of
    the object that needs to be stored or retrieved. With this information, all
    nodes in the Ceph cluster are able to calculate where an object and its
    replicas are stored. This makes writing or reading data very efficient.
    CRUSH tries to evenly distribute data over all nodes in the cluster.
   </p><p>
    The <span class="emphasis"><em>CRUSH Map</em></span> contains all storage nodes and
    administrator-defined placement rules for storing objects in the cluster.
    It defines a hierarchical structure that usually corresponds to the
    physical structure of the cluster. For example, the data-containing disks
    are in hosts, hosts are in racks, racks in rows and rows in data centers.
    This structure can be used to define <span class="emphasis"><em>failure domains</em></span>.
    Ceph then ensures that replications are stored on different branches of a
    specific failure domain.
   </p><p>
    If the failure domain is set to rack, replications of objects are
    distributed over different racks. This can mitigate outages caused by a
    failed switch in a rack. If one power distribution unit supplies a row of
    racks, the failure domain can be set to row. When the power distribution
    unit fails, the replicated data is still available on other rows.
   </p></section><section class="sect2" id="storage-intro-core-nodes" data-id-title="Ceph nodes and daemons"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.2.3 </span><span class="title-name">Ceph nodes and daemons</span> <a title="Permalink" class="permalink" href="#storage-intro-core-nodes">#</a></h3></div></div></div><p>
    In Ceph, nodes are servers working for the cluster. They can run several
    different types of daemons. We recommend running only one type of daemon on
    each node, except for Ceph Manager daemons which can be co-located with Ceph Monitors.
    Each cluster requires at least Ceph Monitor, Ceph Manager, and Ceph OSD daemons:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.2.6.5.3.1"><span class="term">Admin Node</span></dt><dd><p>
       The <span class="emphasis"><em>Admin Node</em></span> is a Ceph cluster node from which you
       run commands to manage the cluster. The Admin Node is a central point of the
       Ceph cluster because it manages the rest of the cluster nodes by
       querying and instructing their Salt Minion services.
      </p></dd><dt id="id-1.3.3.2.6.5.3.2"><span class="term">Ceph Monitor</span></dt><dd><p>
       <span class="emphasis"><em>Ceph Monitor</em></span> (often abbreviated as
       <span class="emphasis"><em>MON</em></span>) nodes maintain information about the cluster
       health state, a map of all nodes and data distribution rules (see
       <a class="xref" href="#storage-intro-core-crush" title="1.2.2. CRUSH">Section 1.2.2, “CRUSH”</a>).
      </p><p>
       If failures or conflicts occur, the Ceph Monitor nodes in the cluster decide by
       majority which information is correct. To form a qualified majority, it
       is recommended to have an odd number of Ceph Monitor nodes, and at least three
       of them.
      </p><p>
       If more than one site is used, the Ceph Monitor nodes should be distributed
       over an odd number of sites. The number of Ceph Monitor nodes per site should
       be such that more than 50% of the Ceph Monitor nodes remain functional if one
       site fails.
      </p></dd><dt id="id-1.3.3.2.6.5.3.3"><span class="term">Ceph Manager</span></dt><dd><p>
       The Ceph Manager collects the state information from the whole cluster. The
       Ceph Manager daemon runs alongside the Ceph Monitor daemons. It provides additional
       monitoring, and interfaces the external monitoring and management
       systems. It includes other services as well. For example, the
       Ceph Dashboard Web UI runs on the same node as the Ceph Manager.
      </p><p>
       The Ceph Manager requires no additional configuration, beyond ensuring it is
       running.
      </p></dd><dt id="id-1.3.3.2.6.5.3.4"><span class="term">Ceph OSD</span></dt><dd><p>
       A <span class="emphasis"><em>Ceph OSD</em></span> is a daemon handling <span class="emphasis"><em>Object
       Storage Devices</em></span> which are a physical or logical storage units
       (hard disks or partitions). Object Storage Devices can be physical
       disks/partitions or logical volumes. The daemon additionally takes care
       of data replication and rebalancing in case of added or removed nodes.
      </p><p>
       Ceph OSD daemons communicate with monitor daemons and provide them
       with the state of the other OSD daemons.
      </p></dd></dl></div><p>
    To use CephFS, Object Gateway, NFS Ganesha, or iSCSI Gateway, additional nodes are required:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.3.2.6.5.5.1"><span class="term">Metadata Server (MDS)</span></dt><dd><p>
       CephFS metadata is stored in its own RADOS pool (see
       <a class="xref" href="#storage-intro-structure-pool" title="1.3.1. Pools">Section 1.3.1, “Pools”</a>). The Metadata Servers act as a
       smart caching layer for the metadata and serializes access when needed.
       This allows concurrent access from many clients without explicit
       synchronization.
      </p></dd><dt id="id-1.3.3.2.6.5.5.2"><span class="term">Object Gateway</span></dt><dd><p>
       The Object Gateway is an HTTP REST gateway for the RADOS object store. It is
       compatible with OpenStack Swift and Amazon S3 and has its own user
       management.
      </p></dd><dt id="id-1.3.3.2.6.5.5.3"><span class="term">NFS Ganesha</span></dt><dd><p>
       NFS Ganesha provides an NFS access to either the Object Gateway or the CephFS. It
       runs in the user instead of the kernel space and directly interacts with
       the Object Gateway or CephFS.
      </p></dd><dt id="id-1.3.3.2.6.5.5.4"><span class="term">iSCSI Gateway</span></dt><dd><p>
       iSCSI is a storage network protocol that allows clients to send SCSI
       commands to SCSI storage devices (targets) on remote servers.
      </p></dd><dt id="id-1.3.3.2.6.5.5.5"><span class="term">Samba Gateway</span></dt><dd><p>
       The Samba Gateway provides a Samba access to data stored on CephFS.
      </p></dd></dl></div></section></section><section class="sect1" id="storage-intro-structure" data-id-title="Ceph storage structure"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">Ceph storage structure</span> <a title="Permalink" class="permalink" href="#storage-intro-structure">#</a></h2></div></div></div><section class="sect2" id="storage-intro-structure-pool" data-id-title="Pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.1 </span><span class="title-name">Pools</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pool">#</a></h3></div></div></div><p>
    Objects that are stored in a Ceph cluster are put into
    <span class="emphasis"><em>pools</em></span>. Pools represent logical partitions of the
    cluster to the outside world. For each pool a set of rules can be defined,
    for example, how many replications of each object must exist. The standard
    configuration of pools is called <span class="emphasis"><em>replicated pool</em></span>.
   </p><p>
    Pools usually contain objects but can also be configured to act similar to
    a RAID 5. In this configuration, objects are stored in chunks along with
    additional coding chunks. The coding chunks contain the redundant
    information. The number of data and coding chunks can be defined by the
    administrator. In this configuration, pools are referred to as
    <span class="emphasis"><em>erasure coded pools</em></span> or <span class="emphasis"><em>EC pools</em></span>.
   </p></section><section class="sect2" id="storage-intro-structure-pg" data-id-title="Placement groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.2 </span><span class="title-name">Placement groups</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-pg">#</a></h3></div></div></div><p>
    <span class="emphasis"><em>Placement Groups</em></span> (PGs) are used for the distribution
    of data within a pool. When creating a pool, a certain number of placement
    groups is set. The placement groups are used internally to group objects
    and are an important factor for the performance of a Ceph cluster. The PG
    for an object is determined by the object's name.
   </p></section><section class="sect2" id="storage-intro-structure-example" data-id-title="Example"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.3.3 </span><span class="title-name">Example</span> <a title="Permalink" class="permalink" href="#storage-intro-structure-example">#</a></h3></div></div></div><p>
    This section provides a simplified example of how Ceph manages data (see
    <a class="xref" href="#storage-intro-structure-example-figure" title="Small scale Ceph example">Figure 1.2, “Small scale Ceph example”</a>). This example
    does not represent a recommended configuration for a Ceph cluster. The
    hardware setup consists of three storage nodes or Ceph OSDs
    (<code class="literal">Host 1</code>, <code class="literal">Host 2</code>, <code class="literal">Host
    3</code>). Each node has three hard disks which are used as OSDs
    (<code class="literal">osd.1</code> to <code class="literal">osd.9</code>). The Ceph Monitor nodes are
    neglected in this example.
   </p><div id="id-1.3.3.2.7.4.3" data-id-title="Difference between Ceph OSD and OSD" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Difference between Ceph OSD and OSD</h6><p>
     While <span class="emphasis"><em>Ceph OSD</em></span> or <span class="emphasis"><em>Ceph OSD
     daemon</em></span> refers to a daemon that is run on a node, the word
     <span class="emphasis"><em>OSD</em></span> refers to the logical disk that the daemon
     interacts with.
    </p></div><p>
    The cluster has two pools, <code class="literal">Pool A</code> and <code class="literal">Pool
    B</code>. While Pool A replicates objects only two times, resilience for
    Pool B is more important and it has three replications for each object.
   </p><p>
    When an application puts an object into a pool, for example via the REST
    API, a Placement Group (<code class="literal">PG1</code> to <code class="literal">PG4</code>)
    is selected based on the pool and the object name. The CRUSH algorithm then
    calculates on which OSDs the object is stored, based on the Placement Group
    that contains the object.
   </p><p>
    In this example the failure domain is set to host. This ensures that
    replications of objects are stored on different hosts. Depending on the
    replication level set for a pool, the object is stored on two or three OSDs
    that are used by the Placement Group.
   </p><p>
    An application that writes an object only interacts with one Ceph OSD,
    the primary Ceph OSD. The primary Ceph OSD takes care of replication
    and confirms the completion of the write process after all other OSDs have
    stored the object.
   </p><p>
    If <code class="literal">osd.5</code> fails, all object in <code class="literal">PG1</code> are
    still available on <code class="literal">osd.1</code>. As soon as the cluster
    recognizes that an OSD has failed, another OSD takes over. In this example
    <code class="literal">osd.4</code> is used as a replacement for
    <code class="literal">osd.5</code>. The objects stored on <code class="literal">osd.1</code>
    are then replicated to <code class="literal">osd.4</code> to restore the replication
    level.
   </p><div class="figure" id="storage-intro-structure-example-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/data-structure-example.png" target="_blank"><img src="images/data-structure-example.png" width="" alt="Small scale Ceph example"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 1.2: </span><span class="title-name">Small scale Ceph example </span><a title="Permalink" class="permalink" href="#storage-intro-structure-example-figure">#</a></h6></div></div><p>
    If a new node with new OSDs is added to the cluster, the cluster map is
    going to change. The CRUSH function then returns different locations for
    objects. Objects that receive new locations will be relocated. This process
    results in a balanced usage of all OSDs.
   </p></section></section><section class="sect1" id="about-bluestore" data-id-title="BlueStore"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">BlueStore</span> <a title="Permalink" class="permalink" href="#about-bluestore">#</a></h2></div></div></div><p>
   BlueStore is a new default storage back-end for Ceph from SES 5. It has
   better performance than FileStore, full data check-summing, and built-in
   compression.
  </p><p>
   BlueStore manages either one, two, or three storage devices. In the
   simplest case, BlueStore consumes a single primary storage device. The
   storage device is normally partitioned into two parts:
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     A small partition named BlueFS that implements file system-like
     functionalities required by RocksDB.
    </p></li><li class="listitem"><p>
     The rest of the device is normally a large partition occupying the rest of
     the device. It is managed directly by BlueStore and contains all of the
     actual data. This primary device is normally identified by a block
     symbolic link in the data directory.
    </p></li></ol></div><p>
   It is also possible to deploy BlueStore across two additional devices:
  </p><p>
   A <span class="emphasis"><em>WAL device</em></span> can be used for BlueStore’s internal
   journal or write-ahead log. It is identified by the
   <code class="literal">block.wal</code> symbolic link in the data directory. It is only
   useful to use a separate WAL device if the device is faster than the primary
   device or the DB device, for example when:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The WAL device is an NVMe, and the DB device is an SSD, and the data
     device is either SSD or HDD.
    </p></li><li class="listitem"><p>
     Both the WAL and DB devices are separate SSDs, and the data device is an
     SSD or HDD.
    </p></li></ul></div><p>
   A <span class="emphasis"><em>DB device</em></span> can be used for storing BlueStore’s
   internal metadata. BlueStore (or rather, the embedded RocksDB) will put as
   much metadata as it can on the DB device to improve performance. Again, it
   is only helpful to provision a shared DB device if it is faster than the
   primary device.
  </p><div id="id-1.3.3.2.8.9" data-id-title="Plan for the DB size" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Plan for the DB size</h6><p>
    Plan thoroughly to ensure sufficient size of the DB device. If the DB
    device fills up, metadata will spill over to the primary device, which
    badly degrades the OSD's performance.
   </p><p>
    You can check if a WAL/DB partition is getting full and spilling over with
    the <code class="command">ceph daemon osd<em class="replaceable">.ID</em> perf
    dump</code> command. The <code class="option">slow_used_bytes</code> value shows
    the amount of data being spilled out:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph daemon osd<em class="replaceable">.ID</em> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,</pre></div></div></section><section class="sect1" id="storage-moreinfo" data-id-title="Additional information"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">Additional information</span> <a title="Permalink" class="permalink" href="#storage-moreinfo">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Ceph as a community project has its own extensive online documentation.
     For topics not found in this manual, refer to
     <a class="link" href="https://docs.ceph.com/en/octopus/" target="_blank">https://docs.ceph.com/en/octopus/</a>.
    </p></li><li class="listitem"><p>
     The original publication <span class="emphasis"><em>CRUSH: Controlled, Scalable,
     Decentralized Placement of Replicated Data</em></span> by <span class="emphasis"><em>S.A.
     Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</em></span> provides helpful
     insight into the inner workings of Ceph. Especially when deploying large
     scale clusters it is a recommended reading. The publication can be found
     at <a class="link" href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf" target="_blank">http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf</a>.
    </p></li><li class="listitem"><p>
     SUSE Enterprise Storage can be used with non-SUSE OpenStack distributions. The Ceph
     clients need to be at a level that is compatible with SUSE Enterprise Storage.
    </p><div id="id-1.3.3.2.9.2.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      SUSE supports the server component of the Ceph deployment and the
      client is supported by the OpenStack distribution vendor.
     </p></div></li></ul></div></section></section><section class="chapter" id="storage-bp-hwreq" data-id-title="Hardware requirements and recommendations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Hardware requirements and recommendations</span> <a title="Permalink" class="permalink" href="#storage-bp-hwreq">#</a></h2></div></div></div><p>
  The hardware requirements of Ceph are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </p><p>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </p><section class="sect1" id="network-overview" data-id-title="Network overview"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.1 </span><span class="title-name">Network overview</span> <a title="Permalink" class="permalink" href="#network-overview">#</a></h2></div></div></div><p>
   Ceph has several logical networks:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A front-end network called the <code class="literal">public network</code>.
    </p></li><li class="listitem"><p>
     A trusted internal network, the back-end network, called the
     <code class="literal">cluster network</code>. This is optional.
    </p></li><li class="listitem"><p>
     One or more client networks for gateways. This is optional and beyond the
     scope of this chapter.
    </p></li></ul></div><p>
   The public network is the network over which Ceph daemons communicate with
   each other and with their clients. This means that all Ceph cluster
   traffic goes over this network except in the case when a cluster network is
   configured.
  </p><p>
   The cluster network is the back-end network between the OSD nodes, for
   replication, re-balancing, and recovery. If configured, this optional
   network would ideally provide twice the bandwidth of the public network with
   default three-way replication, since the primary OSD sends two copies to
   other OSDs via this network. The public network is between clients and
   gateways on the one side to talk to monitors, managers, MDS nodes, OSD
   nodes. It is also used by monitors, managers, and MDS nodes to talk with OSD
   nodes.
  </p><div class="figure" id="network-overview-figure"><div class="figure-contents"><div class="mediaobject"><a href="images/network-overview-diagram.png" target="_blank"><img src="images/network-overview-diagram.png" width="" alt="Network overview"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.1: </span><span class="title-name">Network overview </span><a title="Permalink" class="permalink" href="#network-overview-figure">#</a></h6></div></div><section class="sect2" id="ceph-install-ceph-deploy-network" data-id-title="Network recommendations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.1.1 </span><span class="title-name">Network recommendations</span> <a title="Permalink" class="permalink" href="#ceph-install-ceph-deploy-network">#</a></h3></div></div></div><p>
    We recommend a single fault-tolerant network with enough bandwidth to
    fulfil your requirements. For the Ceph public network environment, we
    recommend two bonded 25 GbE (or faster) network interfaces bonded
    using 802.3ad (LACP). This is considered the minimal setup for Ceph. If
    you are also using a cluster network, we recommend four bonded 25 GbE
    network interfaces. Bonding two or more network interfaces provides better
    throughput via link aggregation and, given redundant links and switches,
    improved fault tolerance and maintainability.
   </p><p>
    You can also create VLANs to isolate different types of traffic over a
    bond. For example, you can create a bond to provide two VLAN interfaces,
    one for the public network, and the second for the cluster network.
    However, this is <span class="emphasis"><em>not</em></span> required when setting up Ceph
    networking. Details on bonding the interfaces can be found in
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-iface-bonding" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-iface-bonding</a>.
   </p><p>
    Fault tolerance can be enhanced through isolating the components into
    failure domains. To improve fault tolerance of the network, bonding one
    interface from two separate Network Interface Cards (NIC) offers protection
    against failure of a single NIC. Similarly, creating a bond across two
    switches protects against failure of a switch. We recommend consulting with
    the network equipment vendor in order to architect the level of fault
    tolerance required.
   </p><div id="id-1.3.3.3.5.7.5" data-id-title="Administration network not supported" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Administration network not supported</h6><p>
     Additional administration network setup—that enables for example
     separating SSH, Salt, or DNS networking—is neither tested nor
     supported.
    </p></div><div id="id-1.3.3.3.5.7.6" data-id-title="Nodes configured via DHCP" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Nodes configured via DHCP</h6><p>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various Ceph daemons start. If this happens, the Ceph MONs and OSDs
     will not start correctly (running <code class="command">systemctl status
     ceph\*</code> will result in "unable to bind" errors). To avoid this
     issue, we recommend increasing the DHCP client timeout to at least 30
     seconds on each node in your storage cluster. This can be done by changing
     the following settings on each node:
    </p><p>
     In <code class="filename">/etc/sysconfig/network/dhcp</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">DHCLIENT_WAIT_AT_BOOT="30"</pre></div><p>
     In <code class="filename">/etc/sysconfig/network/config</code>, set
    </p><div class="verbatim-wrap"><pre class="screen">WAIT_FOR_INTERFACES="60"</pre></div></div><section class="sect3" id="storage-bp-net-private" data-id-title="Adding a private network to a running cluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.1 </span><span class="title-name">Adding a private network to a running cluster</span> <a title="Permalink" class="permalink" href="#storage-bp-net-private">#</a></h4></div></div></div><p>
     If you do not specify a cluster network during Ceph deployment, it
     assumes a single public network environment. While Ceph operates fine
     with a public network, its performance and security improves when you set
     a second private cluster network. To support two networks, each Ceph
     node needs to have at least two network cards.
    </p><p>
     You need to apply the following changes to each Ceph node. It is
     relatively quick to do for a small cluster, but can be very time consuming
     if you have a cluster consisting of hundreds or thousands of nodes.
    </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Set the cluster network using the following command:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>ceph config set global cluster_network <em class="replaceable">MY_NETWORK</em></pre></div><p>
       Restart the OSDs to bind to the specified cluster network:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl restart ceph-*@osd.*.service</pre></div></li><li class="step"><p>
       Check that the private cluster network works as expected on the OS
       level.
      </p></li></ol></div></div></section><section class="sect3" id="storage-bp-net-subnets" data-id-title="Monitoring nodes on different subnets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">2.1.1.2 </span><span class="title-name">Monitoring nodes on different subnets</span> <a title="Permalink" class="permalink" href="#storage-bp-net-subnets">#</a></h4></div></div></div><p>
     If the monitor nodes are on multiple subnets, for example they are located
     in different rooms and served by different switches, you need to specify
     their public network address in CIDR notation:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon public_network "<em class="replaceable">MON_NETWORK_1</em>, <em class="replaceable">MON_NETWORK_2</em>, <em class="replaceable">MON_NETWORK_N</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon public_network "192.168.1.0/24, 10.10.0.0/16"</pre></div><div id="id-1.3.3.3.5.7.8.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      If you do specify more than one network segment for the public (or
      cluster) network as described in this section, each of these subnets must
      be capable of routing to all the others - otherwise, the MONs and other
      Ceph daemons on different network segments will not be able to
      communicate and a split cluster will ensue. Additionally, if you are
      using a firewall, make sure you include each IP address or subnet in your
      iptables and open ports for them on all nodes as necessary.
     </p></div></section></section></section><section class="sect1" id="multi-architecture" data-id-title="Multiple architecture configurations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.2 </span><span class="title-name">Multiple architecture configurations</span> <a title="Permalink" class="permalink" href="#multi-architecture">#</a></h2></div></div></div><p>
   SUSE Enterprise Storage supports both x86 and Arm architectures. When considering
   each architecture, it is important to note that from a cores per OSD,
   frequency, and RAM perspective, there is no real difference between CPU
   architectures for sizing.
  </p><p>
   As with smaller x86 processors (non-server), lower-performance Arm-based
   cores may not provide an optimal experience, especially when used for
   erasure coded pools.
  </p><div id="id-1.3.3.3.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Throughout the documentation, <em class="replaceable">SYSTEM-ARCH</em> is
    used in place of x86 or Arm.
   </p></div></section><section class="sect1" id="ses-hardware-config" data-id-title="Hardware configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.3 </span><span class="title-name">Hardware configuration</span> <a title="Permalink" class="permalink" href="#ses-hardware-config">#</a></h2></div></div></div><p>
   For the best product experience, we recommend to start with the recommended
   cluster configuration. For a test cluster or a cluster with less performance
   requirements, we document a minimal supported cluster configuration.
  </p><section class="sect2" id="ses-bp-minimum-cluster" data-id-title="Minimum cluster configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.1 </span><span class="title-name">Minimum cluster configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-minimum-cluster">#</a></h3></div></div></div><p>
    A minimal product cluster configuration consists of:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      At least four physical nodes (OSD nodes) with co-location of services
     </p></li><li class="listitem"><p>
      Dual-10 Gb Ethernet as a bonded network
     </p></li><li class="listitem"><p>
      A separate Admin Node (can be virtualized on an external node)
     </p></li></ul></div><p>
    A detailed configuration is:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Separate Admin Node with 4 GB RAM, four cores, 1 TB storage capacity. This is
      typically the Salt Master node. Ceph services and gateways, such as
      Ceph Monitor, Metadata Server, Ceph OSD, Object Gateway, or NFS Ganesha are not supported on the Admin Node
      as it needs to orchestrate the cluster update and upgrade processes
      independently.
     </p></li><li class="listitem"><p>
      At least four physical OSD nodes, with eight OSD disks each, see
      <a class="xref" href="#sysreq-osd" title="2.4.1. Minimum requirements">Section 2.4.1, “Minimum requirements”</a> for requirements.
     </p><p>
      The total capacity of the cluster should be sized so that even with one
      node unavailable, the total used capacity (including redundancy) does not
      exceed 80%.
     </p></li><li class="listitem"><p>
      Three Ceph Monitor instances. Monitors need to be run from SSD/NVMe storage, not
      HDDs, for latency reasons.
     </p></li><li class="listitem"><p>
      Monitors, Metadata Server, and gateways can be co-located on the OSD nodes, see
      <a class="xref" href="#ses-bp-diskshare" title="2.12. OSD and monitor sharing one server">Section 2.12, “OSD and monitor sharing one server”</a> for monitor co-location. If you
      co-locate services, the memory and CPU requirements need to be added up.
     </p></li><li class="listitem"><p>
      iSCSI Gateway, Object Gateway, and Metadata Server require at least incremental 4 GB RAM and four
      cores.
     </p></li><li class="listitem"><p>
      If you are using CephFS, S3/Swift, iSCSI, at least two instances of
      the respective roles (Metadata Server, Object Gateway, iSCSI) are required for redundancy
      and availability.
     </p></li><li class="listitem"><p>
      The nodes are to be dedicated to SUSE Enterprise Storage and must not be used for
      any other physical, containerized, or virtualized workload.
     </p></li><li class="listitem"><p>
      If any of the gateways (iSCSI, Object Gateway, NFS Ganesha, Metadata Server, ...) are
      deployed within VMs, these VMs must not be hosted on the physical
      machines serving other cluster roles. (This is unnecessary, as they are
      supported as collocated services.)
     </p></li><li class="listitem"><p>
      When deploying services as VMs on hypervisors outside the core physical
      cluster, failure domains must be respected to ensure redundancy.
     </p><p>
      For example, do not deploy multiple roles of the same type on the same
      hypervisor, such as multiple MONs or MDSs instances.
     </p></li><li class="listitem"><p>
      When deploying inside VMs, it is particularly crucial to ensure that the
      nodes have strong network connectivity and well working time
      synchronization.
     </p></li><li class="listitem"><p>
      The hypervisor nodes must be adequately sized to avoid interference by
      other workloads consuming CPU, RAM, network, and storage resources.
     </p></li></ul></div><div class="figure" id="id-1.3.3.3.7.3.6"><div class="figure-contents"><div class="mediaobject"><a href="images/minimal-ses.png" target="_blank"><img src="images/minimal-ses.png" width="" alt="Minimum cluster configuration"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 2.2: </span><span class="title-name">Minimum cluster configuration </span><a title="Permalink" class="permalink" href="#id-1.3.3.3.7.3.6">#</a></h6></div></div></section><section class="sect2" id="ses-bp-production-cluster" data-id-title="Recommended production cluster configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.2 </span><span class="title-name">Recommended production cluster configuration</span> <a title="Permalink" class="permalink" href="#ses-bp-production-cluster">#</a></h3></div></div></div><p>
    Once you grow your cluster, we recommend relocating Ceph Monitors, Metadata Servers, and
    Gateways to separate nodes for better fault tolerance.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Seven Object Storage Nodes
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        No single node exceeds ~15% of total storage.
       </p></li><li class="listitem"><p>
        The total capacity of the cluster should be sized so that even with one
        node unavailable, the total used capacity (including redundancy) does
        not exceed 80%.
       </p></li><li class="listitem"><p>
        25 Gb Ethernet or better, bonded for internal cluster and external
        public network each.
       </p></li><li class="listitem"><p>
        56+ OSDs per storage cluster.
       </p></li><li class="listitem"><p>
        See <a class="xref" href="#sysreq-osd" title="2.4.1. Minimum requirements">Section 2.4.1, “Minimum requirements”</a> for further recommendation.
       </p></li></ul></div></li><li class="listitem"><p>
      Dedicated physical infrastructure nodes.
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Three Ceph Monitor nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-mon" title="2.5. Monitor nodes">Section 2.5, “Monitor nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Object Gateway nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-rgw" title="2.6. Object Gateway nodes">Section 2.6, “Object Gateway nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        iSCSI Gateway nodes: 16 GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-iscsi" title="2.9. iSCSI Gateway nodes">Section 2.9, “iSCSI Gateway nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        Metadata Server nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
        RAID 1 SSDs for disk.
       </p><p>
        See <a class="xref" href="#sysreq-mds" title="2.7. Metadata Server nodes">Section 2.7, “Metadata Server nodes”</a> for further recommendation.
       </p></li><li class="listitem"><p>
        One SES Admin Node: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </p></li></ul></div></li></ul></div></section><section class="sect2" id="deployment-hw-multipath" data-id-title="Multipath configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.3.3 </span><span class="title-name">Multipath configuration</span> <a title="Permalink" class="permalink" href="#deployment-hw-multipath">#</a></h3></div></div></div><p>
    If you want to use multipath hardware, ensure that LVM sees
    <code class="literal">multipath_component_detection = 1</code> in the configuration
    file under the <code class="literal">devices</code> section. This can be checked via
    the <code class="command">lvm config</code> command.
   </p><p>
    Alternatively, ensure that LVM filters a device's mpath components via the
    LVM filter configuration. This will be host specific.
   </p><div id="id-1.3.3.3.7.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This is not recommended and should only ever be considered if
     <code class="literal">multipath_component_detection = 1</code> cannot be set.
    </p></div><p>
    For more information on multipath configuration, see
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-multipath.html#sec-multipath-lvm" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-multipath.html#sec-multipath-lvm</a>.
   </p></section></section><section class="sect1" id="deployment-osd-recommendation" data-id-title="Object Storage Nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.4 </span><span class="title-name">Object Storage Nodes</span> <a title="Permalink" class="permalink" href="#deployment-osd-recommendation">#</a></h2></div></div></div><section class="sect2" id="sysreq-osd" data-id-title="Minimum requirements"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.1 </span><span class="title-name">Minimum requirements</span> <a title="Permalink" class="permalink" href="#sysreq-osd">#</a></h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      The following CPU recommendations account for devices independent of
      usage by Ceph:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        1x 2GHz CPU Thread per spinner.
       </p></li><li class="listitem"><p>
        2x 2GHz CPU Thread per SSD.
       </p></li><li class="listitem"><p>
        4x 2GHz CPU Thread per NVMe.
       </p></li></ul></div></li><li class="listitem"><p>
      Separate 10 GbE networks (public/client and internal), required 4x 10
      GbE, recommended 2x 25 GbE.
     </p></li><li class="listitem"><p>
      Total RAM required = number of OSDs x (1 GB +
      <code class="option">osd_memory_target</code>) + 16 GB
     </p><p>
      Refer to <span class="intraxref">Book “Administration and Operations Guide”, Chapter 28 “Ceph cluster configuration”, Section 28.4.1 “Configuring automatic cache sizing”</span> for more details on
      <code class="option">osd_memory_target</code>.
     </p></li><li class="listitem"><p>
      OSD disks in JBOD configurations or individual RAID-0 configurations.
     </p></li><li class="listitem"><p>
      OSD journal can reside on OSD disk.
     </p></li><li class="listitem"><p>
      OSD disks should be exclusively used by SUSE Enterprise Storage.
     </p></li><li class="listitem"><p>
      Dedicated disk and SSD for the operating system, preferably in a RAID 1
      configuration.
     </p></li><li class="listitem"><p>
      Allocate at least an additional 4 GB of RAM if this OSD host will host
      part of a cache pool used for cache tiering.
     </p></li><li class="listitem"><p>
      Ceph Monitors, gateway and Metadata Servers can reside on Object Storage Nodes.
     </p></li><li class="listitem"><p>
      For disk performance reasons, OSD nodes are bare metal nodes. No other
      workloads should run on an OSD node unless it is a minimal setup of
      Ceph Monitors and Ceph Managers.
     </p></li><li class="listitem"><p>
      SSDs for Journal with 6:1 ratio SSD journal to OSD.
     </p></li></ul></div><div id="id-1.3.3.3.8.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure that OSD nodes do not have any networked block devices mapped, such
     as iSCSI or RADOS Block Device images.
    </p></div></section><section class="sect2" id="ses-bp-mindisk" data-id-title="Minimum disk size"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.2 </span><span class="title-name">Minimum disk size</span> <a title="Permalink" class="permalink" href="#ses-bp-mindisk">#</a></h3></div></div></div><p>
    There are two types of disk space needed to run on OSD: the space for the
    WAL/DB device, and the primary space for the stored data. The minimum (and
    default) value for the WAL/DB is 6 GB. The minimum space for data is 5 GB,
    as partitions smaller than 5 GB are automatically assigned the weight of 0.
   </p><p>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </p></section><section class="sect2" id="rec-waldb-size" data-id-title="Recommended size for the BlueStores WAL and DB device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.3 </span><span class="title-name">Recommended size for the BlueStore's WAL and DB device</span> <a title="Permalink" class="permalink" href="#rec-waldb-size">#</a></h3></div></div></div><div id="id-1.3.3.3.8.4.2" data-id-title="More Information" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: More Information</h6><p>
     Refer to <a class="xref" href="#about-bluestore" title="1.4. BlueStore">Section 1.4, “BlueStore”</a> for more information on
     BlueStore.
    </p></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      We recommend reserving 4 GB for the WAL device. While the minimal DB
      size is 64 GB for RBD-only workloads, the recommended DB size for
      Object Gateway and CephFS workloads is 2% of the main device capacity (but at
      least 196 GB).
     </p><div id="id-1.3.3.3.8.4.3.1.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       We recommend larger DB volumes for high-load deployments, especially if
       there is high RGW or CephFS usage. Reserve some capacity (slots) to
       install more hardware for more DB space if required.
      </p></div></li><li class="listitem"><p>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows Ceph to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as Ceph uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not used fully then its
      space is not wasted but used for DB operation.
     </p><p>
      To share the DB device with the WAL, do <span class="emphasis"><em>not</em></span> specify
      the WAL device, and specify only the DB device.
     </p><p>
      Find more information about specifying an OSD layout in
      <span class="intraxref">Book “Administration and Operations Guide”, Chapter 13 “Operational tasks”, Section 13.4.3 “Adding OSDs using DriveGroups specification”</span>.
     </p></li></ul></div></section><section class="sect2" id="ses-bp-share-ssd-journal" data-id-title="SSD for WAL/DB partitions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.4 </span><span class="title-name">SSD for WAL/DB partitions</span> <a title="Permalink" class="permalink" href="#ses-bp-share-ssd-journal">#</a></h3></div></div></div><p>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </p><p>
    OSDs may see a significant performance improvement by storing their WAL/DB
    partitions on an SSD and the object data on a separate hard disk.
   </p><div id="id-1.3.3.3.8.5.4" data-id-title="Sharing an SSD for Multiple WAL/DB Partitions" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing an SSD for Multiple WAL/DB Partitions</h6><p>
     As WAL/DB partitions occupy relatively little space, you can share one SSD
     disk with multiple WAL/DB partitions. Keep in mind that with each WAL/DB
     partition, the performance of the SSD disk degrades. We do not recommend
     sharing more than six WAL/DB partitions on the same SSD disk and 12 on
     NVMe disks.
    </p></div></section><section class="sect2" id="maximum-count-of-disks-osd" data-id-title="Maximum recommended number of disks"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.4.5 </span><span class="title-name">Maximum recommended number of disks</span> <a title="Permalink" class="permalink" href="#maximum-count-of-disks-osd">#</a></h3></div></div></div><p>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Network bandwidth.</em></span> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Memory.</em></span> RAM above 2 GB is used for the
      BlueStore cache. With the default <code class="option">osd_memory_target</code> of
      4 GB, the system has a reasonable starting cache size for spinning
      media. If using SSD or NVME, consider increasing the cache size and RAM
      allocation per OSD to maximize performance.
     </p></li><li class="listitem"><p>
      <span class="emphasis"><em>Fault tolerance.</em></span> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </p></li></ul></div></section></section><section class="sect1" id="sysreq-mon" data-id-title="Monitor nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.5 </span><span class="title-name">Monitor nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mon">#</a></h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     At least three MON nodes are required. The number of monitors should
     always be odd (1+2n).
    </p></li><li class="listitem"><p>
     4 GB of RAM.
    </p></li><li class="listitem"><p>
     Processor with four logical cores.
    </p></li><li class="listitem"><p>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <code class="filename">/var/lib/ceph</code> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </p></li><li class="listitem"><p>
     There must only be one monitor process per node.
    </p></li><li class="listitem"><p>
     Mixing OSD, MON, or Object Gateway nodes is only supported if sufficient hardware
     resources are available. That means that the requirements for all services
     need to be added up.
    </p></li><li class="listitem"><p>
     Two network interfaces bonded to multiple switches.
    </p></li></ul></div></section><section class="sect1" id="sysreq-rgw" data-id-title="Object Gateway nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.6 </span><span class="title-name">Object Gateway nodes</span> <a title="Permalink" class="permalink" href="#sysreq-rgw">#</a></h2></div></div></div><p>
   Object Gateway nodes should have at least six CPU cores and 32 GB of RAM. When other
   processes are co-located on the same machine, their requirements need to be
   added up.
  </p></section><section class="sect1" id="sysreq-mds" data-id-title="Metadata Server nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.7 </span><span class="title-name">Metadata Server nodes</span> <a title="Permalink" class="permalink" href="#sysreq-mds">#</a></h2></div></div></div><p>
   Proper sizing of the Metadata Server nodes depends on the specific use case.
   Generally, the more open files the Metadata Server is to handle, the more CPU and RAM
   it needs. The following are the minimum requirements:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     4 GB of RAM for each Metadata Server daemon.
    </p></li><li class="listitem"><p>
     Bonded network interface.
    </p></li><li class="listitem"><p>
     2.5 GHz CPU with at least 2 cores.
    </p></li></ul></div></section><section class="sect1" id="sysreq-admin-node" data-id-title="Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.8 </span><span class="title-name">Admin Node</span> <a title="Permalink" class="permalink" href="#sysreq-admin-node">#</a></h2></div></div></div><p>
   At least 4 GB of RAM and a quad-core CPU are required. This includes running
   the Salt Master on the Admin Node. For large clusters with hundreds of nodes, 6 GB
   of RAM is suggested.
  </p></section><section class="sect1" id="sysreq-iscsi" data-id-title="iSCSI Gateway nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.9 </span><span class="title-name">iSCSI Gateway nodes</span> <a title="Permalink" class="permalink" href="#sysreq-iscsi">#</a></h2></div></div></div><p>
   iSCSI Gateway nodes should have at least six CPU cores and 16 GB of RAM.
  </p></section><section class="sect1" id="req-ses-other" data-id-title="SES and other SUSE products"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.10 </span><span class="title-name">SES and other SUSE products</span> <a title="Permalink" class="permalink" href="#req-ses-other">#</a></h2></div></div></div><p>
   This section contains important information about integrating SES with other
   SUSE products.
  </p><section class="sect2" id="req-ses-suma" data-id-title="SUSE Manager"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">2.10.1 </span><span class="title-name">SUSE Manager</span> <a title="Permalink" class="permalink" href="#req-ses-suma">#</a></h3></div></div></div><p>
    SUSE Manager and SUSE Enterprise Storage are not integrated, therefore SUSE Manager cannot
    currently manage an SES cluster.
   </p></section></section><section class="sect1" id="sysreq-naming" data-id-title="Name limitations"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.11 </span><span class="title-name">Name limitations</span> <a title="Permalink" class="permalink" href="#sysreq-naming">#</a></h2></div></div></div><p>
   Ceph does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a Ceph
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all Ceph
   object/configuration names.
  </p></section><section class="sect1" id="ses-bp-diskshare" data-id-title="OSD and monitor sharing one server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2.12 </span><span class="title-name">OSD and monitor sharing one server</span> <a title="Permalink" class="permalink" href="#ses-bp-diskshare">#</a></h2></div></div></div><p>
   Although it is technically possible to run OSDs and MONs on the same server
   in test environments, we strongly recommend having a separate server for
   each monitor node in production. The main reason is performance—the
   more OSDs the cluster has, the more I/O operations the MON nodes need to
   perform. And when one server is shared between a MON node and OSD(s), the
   OSD I/O operations are a limiting factor for the monitor node.
  </p><p>
   Another consideration is whether to share disks between an OSD, a MON node,
   and the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </p><p>
   Although Ceph supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </p><div id="id-1.3.3.3.16.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
    If it is <span class="emphasis"><em>really</em></span> necessary to run OSD and MON node on
    the same server, run MON on a separate disk by mounting the disk to the
    <code class="filename">/var/lib/ceph/mon</code> directory for slightly better
    performance.
   </p></div></section></section><section class="chapter" id="cha-admin-ha" data-id-title="Admin Node HA setup"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Admin Node HA setup</span> <a title="Permalink" class="permalink" href="#cha-admin-ha">#</a></h2></div></div></div><p>
  The <span class="emphasis"><em>Admin Node</em></span> is a Ceph cluster node where the Salt Master
  service runs. It manages the rest of the cluster nodes by querying and
  instructing their Salt Minion services. It usually includes other services as
  well, for example the <span class="emphasis"><em>Grafana</em></span> dashboard backed by the
  <span class="emphasis"><em>Prometheus</em></span> monitoring toolkit.
 </p><p>
  In case of Admin Node failure, you usually need to provide new working hardware
  for the node and restore the complete cluster configuration stack from a
  recent backup. Such a method is time consuming and causes cluster outage.
 </p><p>
  To prevent the Ceph cluster performance downtime caused by the Admin Node
  failure, we recommend making use of a High Availability (HA) cluster for the Ceph Admin Node.
 </p><section class="sect1" id="admin-ha-architecture" data-id-title="Outline of the HA cluster for Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Outline of the HA cluster for Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-architecture">#</a></h2></div></div></div><p>
   The idea of an HA cluster is that in case of one cluster node failing, the
   other node automatically takes over its role, including the virtualized
   Admin Node. This way, other Ceph cluster nodes do not notice that the Admin Node
   failed.
  </p><p>
   The minimal HA solution for the Admin Node requires the following hardware:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Two bare metal servers able to run SUSE Linux Enterprise with the High Availability extension and
     virtualize the Admin Node.
    </p></li><li class="listitem"><p>
     Two or more redundant network communication paths, for example via Network
     Device Bonding.
    </p></li><li class="listitem"><p>
     Shared storage to host the disk image(s) of the Admin Node virtual machine. The
     shared storage needs to be accessible from both servers. It can be, for
     example, an NFS export, a Samba share, or iSCSI target.
    </p></li></ul></div><p>
   Find more details on the cluster requirements at
   <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/art-sleha-install-quick.html#sec-ha-inst-quick-req" target="_blank">https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/art-sleha-install-quick.html#sec-ha-inst-quick-req</a>.
  </p><div class="figure" id="id-1.3.3.4.6.6"><div class="figure-contents"><div class="mediaobject"><a href="images/ceph_admin_ha1.png" target="_blank"><img src="images/ceph_admin_ha1.png" width="" alt="2-Node HA cluster for Admin Node"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 3.1: </span><span class="title-name">2-Node HA cluster for Admin Node </span><a title="Permalink" class="permalink" href="#id-1.3.3.4.6.6">#</a></h6></div></div></section><section class="sect1" id="admin-ha-cluster" data-id-title="Building an HA cluster with the Admin Node"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.2 </span><span class="title-name">Building an HA cluster with the Admin Node</span> <a title="Permalink" class="permalink" href="#admin-ha-cluster">#</a></h2></div></div></div><p>
   The following procedure summarizes the most important steps of building the
   HA cluster for virtualizing the Admin Node. For details, refer to the indicated
   links.
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Set up a basic 2-node HA cluster with shared storage as described in
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/art-sleha-install-quick.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/art-sleha-install-quick.html</a>.
    </p></li><li class="step"><p>
     On both cluster nodes, install all packages required for running the KVM
     hypervisor and the <code class="systemitem">libvirt</code> toolkit as described in
     <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-vt-installation.html#sec-vt-installation-kvm" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-vt-installation.html#sec-vt-installation-kvm</a>.
    </p></li><li class="step"><p>
     On the first cluster node, create a new KVM virtual machine (VM) making
     use of <code class="systemitem">libvirt</code> as described in
     <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-kvm-inst.html#sec-libvirt-inst-virt-install" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-kvm-inst.html#sec-libvirt-inst-virt-install</a>.
     Use the preconfigured shared storage to store the disk images of the VM.
    </p></li><li class="step"><p>
     After the VM setup is complete, export its configuration to an XML file on
     the shared storage. Use the following syntax:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>virsh dumpxml <em class="replaceable">VM_NAME</em> &gt; /path/to/shared/vm_name.xml</pre></div></li><li class="step"><p>
     Create a resource for the Admin Node VM. Refer to
     <a class="link" href="https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/cha-conf-hawk2.html" target="_blank">https://documentation.suse.com/sle-ha/15-SP2/html/SLE-HA-all/cha-conf-hawk2.html</a>
     for general info on creating HA resources. Detailed info on creating
     resources for a KVM virtual machine is described in
     <a class="link" href="http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29" target="_blank">http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29</a>.
    </p></li><li class="step"><p>
     On the newly-created VM guest, deploy the Admin Node including the additional
     services you need there. Follow the relevant steps in
     <a class="xref" href="#deploy-salt" title="Chapter 6. Deploying Salt">Chapter 6, <em>Deploying Salt</em></a>. At the same time, deploy the remaining
     Ceph cluster nodes on the non-HA cluster servers.
    </p></li></ol></div></div></section></section></div><div class="part" id="ses-deployment" data-id-title="Deploying Ceph Cluster"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Deploying Ceph Cluster </span><a title="Permalink" class="permalink" href="#ses-deployment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#deploy-intro"><span class="title-number">4 </span><span class="title-name">Introduction and common tasks</span></a></span></li><dd class="toc-abstract"><p>
  Since SUSE Enterprise Storage 7, Ceph services are deployed as containers instead of
  RPM packages. The deployment process has two basic steps:
 </p></dd><li><span class="chapter"><a href="#deploy-sles"><span class="title-number">5 </span><span class="title-name">Installing and configuring SUSE Linux Enterprise Server</span></a></span></li><dd class="toc-abstract"><p>
    Install and register SUSE Linux Enterprise Server 15 SP2 on each cluster node. During installation of
    SUSE Enterprise Storage, access to the update repositories is required, therefore
    registration is mandatory. Include at least the following modules:
   </p></dd><li><span class="chapter"><a href="#deploy-salt"><span class="title-number">6 </span><span class="title-name">Deploying Salt</span></a></span></li><dd class="toc-abstract"><p>
  SUSE Enterprise Storage uses Salt and <code class="systemitem">ceph-salt</code> for the initial cluster preparation.
  Salt helps you configure and run commands on multiple cluster nodes
  simultaneously from one dedicated host called the
  <span class="emphasis"><em>Salt Master</em></span>. Before deploying Salt, consider the
  following important points:
 </p></dd><li><span class="chapter"><a href="#deploy-bootstrap"><span class="title-number">7 </span><span class="title-name">Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code></span></a></span></li><dd class="toc-abstract"><p>
  This section guides you through the process of deploying a basic Ceph
  cluster. Read the following subsections carefully and execute the included
  commands in the given order.
 </p></dd><li><span class="chapter"><a href="#deploy-core"><span class="title-number">8 </span><span class="title-name">Deploying the remaining core services using cephadm</span></a></span></li><dd class="toc-abstract"><p>
  After deploying the basic Ceph cluster, deploy core services to more
  cluster nodes. To make the cluster data accessible to clients, deploy
  additional services as well.
 </p></dd><li><span class="chapter"><a href="#deploy-additional"><span class="title-number">9 </span><span class="title-name">Deployment of additional services</span></a></span></li><dd class="toc-abstract"><p>iSCSI is a storage area network (SAN) protocol that allows clients (called initiators) to send SCSI commands to SCSI storage devices (targets) on remote servers. SUSE Enterprise Storage 7 includes a facility that opens Ceph storage management to heterogeneous clients, such as Microsoft Windows* and …</p></dd></ul></div><section class="chapter" id="deploy-intro" data-id-title="Introduction and common tasks"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Introduction and common tasks</span> <a title="Permalink" class="permalink" href="#deploy-intro">#</a></h2></div></div></div><p>
  Since SUSE Enterprise Storage 7, Ceph services are deployed as containers instead of
  RPM packages. The deployment process has two basic steps:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.2.4.1"><span class="term">Deploying bootstrap cluster</span></dt><dd><p>
     This phase is called <span class="emphasis"><em>Day 1 deployment</em></span> and consists of
     the following tasks: It includes installing the underlying operating
     system, configuring the Salt infrastructure, and deploying the minimal
     cluster that consist of one MON and one MGR service.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Install and do basic configuration of the underlying operating
       system—SUSE Linux Enterprise Server 15 SP2—on all cluster nodes.
      </p></li><li class="listitem"><p>
       Deploy the Salt infrastructure on all cluster nodes for performing the
       initial deployment preparations via <code class="systemitem">ceph-salt</code>.
      </p></li><li class="listitem"><p>
       Configure the basic properties of the cluster via <code class="systemitem">ceph-salt</code> and deploy
       it.
      </p></li></ul></div></dd><dt id="id-1.3.4.2.4.2"><span class="term">Deploying additional services</span></dt><dd><p>
     During <span class="emphasis"><em>Day 2 deployment</em></span>, additional core and non-core
     Ceph services, for example gateways and monitoring stack, are deployed.
    </p></dd></dl></div><div id="id-1.3.4.2.5" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
   Note that the Ceph community documentation uses the <code class="command">cephadm
   bootstrap</code> command during initial deployment. <code class="systemitem">ceph-salt</code> calls the
   <code class="command">cephadm bootstrap</code> command automatically. The
   <code class="command">cephadm bootstrap</code> command should not be run directly. Any
   Ceph cluster deployment manually using the <code class="command">cephadm
   bootstrap</code> will be unsupported.
  </p></div><section class="sect1" id="cha-ceph-install-relnotes" data-id-title="Read the release notes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Read the release notes</span> <a title="Permalink" class="permalink" href="#cha-ceph-install-relnotes">#</a></h2></div></div></div><p>
   In the release notes you can find additional information on changes since
   the previous release of SUSE Enterprise Storage. Check the release notes to see
   whether:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     your hardware needs special considerations.
    </p></li><li class="listitem"><p>
     any used software packages have changed significantly.
    </p></li><li class="listitem"><p>
     special precautions are necessary for your installation.
    </p></li></ul></div><p>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </p><p>
   After having installed the package <span class="package">release-notes-ses</span>,
   find the release notes locally in the directory
   <code class="filename">/usr/share/doc/release-notes</code> or online at
   <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
  </p></section></section><section class="chapter" id="deploy-sles" data-id-title="Installing and configuring SUSE Linux Enterprise Server"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Installing and configuring SUSE Linux Enterprise Server</span> <a title="Permalink" class="permalink" href="#deploy-sles">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Install and register SUSE Linux Enterprise Server 15 SP2 on each cluster node. During installation of
    SUSE Enterprise Storage, access to the update repositories is required, therefore
    registration is mandatory. Include at least the following modules:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Basesystem Module
     </p></li><li class="listitem"><p>
      Server Applications Module
     </p></li></ul></div><p>
    Find more details on how to install SUSE Linux Enterprise Server in
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-install.html" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-install.html</a>.
   </p></li><li class="step"><p>
    Install the <span class="emphasis"><em>SUSE Enterprise Storage 7</em></span> extension on
    each cluster node.
   </p><div id="id-1.3.4.3.2.2.2" data-id-title="Install SUSE Enterprise Storage together with SUSE Linux Enterprise Server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Install SUSE Enterprise Storage together with SUSE Linux Enterprise Server</h6><p>
     You can either install the SUSE Enterprise Storage 7 extension
     separately after you have installed SUSE Linux Enterprise Server 15 SP2, or you can add it during the
     SUSE Linux Enterprise Server 15 SP2 installation procedure.
    </p></div><p>
    Find more details on how to install extensions in
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html</a>.
   </p></li><li class="step"><p>
    Configure network settings including proper DNS name resolution on each
    node. For more information on configuring a network, see
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-yast" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-yast</a>
    For more information on configuring a DNS server, see
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-dns.html" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-dns.html</a>.
   </p></li></ol></div></div></section><section class="chapter" id="deploy-salt" data-id-title="Deploying Salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Deploying Salt</span> <a title="Permalink" class="permalink" href="#deploy-salt">#</a></h2></div></div></div><p>
  SUSE Enterprise Storage uses Salt and <code class="systemitem">ceph-salt</code> for the initial cluster preparation.
  Salt helps you configure and run commands on multiple cluster nodes
  simultaneously from one dedicated host called the
  <span class="emphasis"><em>Salt Master</em></span>. Before deploying Salt, consider the
  following important points:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <span class="emphasis"><em>Salt Minions</em></span> are the nodes controlled by a dedicated
    node called Salt Master.
   </p></li><li class="listitem"><p>
    If the Salt Master host should be part of the Ceph cluster, it needs to run
    its own Salt Minion, but this is not a requirement.
   </p><div id="id-1.3.4.4.3.2.2" data-id-title="Sharing multiple roles per server" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Sharing multiple roles per server</h6><p>
     You will get the best performance from your Ceph cluster when each role
     is deployed on a separate node. But real deployments sometimes require
     sharing one node for multiple roles. To avoid trouble with performance and
     the upgrade procedure, do not deploy the Ceph OSD, Metadata Server, or Ceph Monitor role to
     the Admin Node.
    </p></div></li><li class="listitem"><p>
    Salt Minions need to correctly resolve the Salt Master's host name over the
    network. By default, they look for the <code class="systemitem">salt</code> host
    name, but you can specify any other network-reachable host name in the
    <code class="filename">/etc/salt/minion</code> file.
   </p></li></ul></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
    Install the <code class="literal">salt-master</code> on the Salt Master node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper in salt-master</pre></div></li><li class="step"><p>
    Check that the <code class="systemitem">salt-master</code> service is enabled and
    started, and enable and start it if needed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl enable salt-master.service
<code class="prompt user">root@master # </code>systemctl start salt-master.service</pre></div></li><li class="step"><p>
    If you intend to use the firewall, verify that the Salt Master node has ports
    4505 and 4506 open to all Salt Minion nodes. If the ports are closed, you can
    open them using the <code class="command">yast2 firewall</code> command by allowing
    the <span class="guimenu">salt-master</span> service for the appropriate zone. For
    example, <code class="literal">public</code>.
   </p></li><li class="step"><p>
    Install the package <code class="literal">salt-minion</code> on all minion nodes.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>zypper in salt-minion</pre></div></li><li class="step"><p>
    Edit <code class="filename">/etc/salt/minion</code> and uncomment the following
    line:
   </p><div class="verbatim-wrap"><pre class="screen">#log_level_logfile: warning</pre></div><p>
    Change the <code class="literal">warning</code> log level to <code class="literal">info</code>.
   </p><div id="id-1.3.4.4.4.5.4" data-id-title="log_level_logfile and log_level" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: <code class="option">log_level_logfile</code> and <code class="option">log_level</code></h6><p>
     While <code class="option">log_level</code> controls which log messages will be
     displayed on the screen, <code class="option">log_level_logfile</code> controls which
     log messages will be written to <code class="filename">/var/log/salt/minion</code>.
    </p></div><div id="id-1.3.4.4.4.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure you change the log level on <span class="emphasis"><em>all</em></span> cluster
     (minion) nodes.
    </p></div></li><li class="step"><p>
    Make sure that the <span class="emphasis"><em>fully qualified domain name</em></span> of each
    node can be resolved to an IP address on the public cluster network by all
    the other nodes.
   </p></li><li class="step"><p>
    Configure all minions to connect to the master. If your Salt Master is not
    reachable by the host name <code class="literal">salt</code>, edit the file
    <code class="filename">/etc/salt/minion</code> or create a new file
    <code class="filename">/etc/salt/minion.d/master.conf</code> with the following
    content:
   </p><div class="verbatim-wrap"><pre class="screen">master: <em class="replaceable">host_name_of_salt_master</em></pre></div><p>
    If you performed any changes to the configuration files mentioned above,
    restart the Salt service on all related Salt Minions:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>systemctl restart salt-minion.service</pre></div></li><li class="step"><p>
    Check that the <code class="systemitem">salt-minion</code> service is enabled and
    started on all nodes. Enable and start it if needed:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>systemctl enable salt-minion.service
<code class="prompt root"># </code>systemctl start salt-minion.service</pre></div></li><li class="step"><p>
    Verify each Salt Minion's fingerprint and accept all salt keys on the
    Salt Master if the fingerprints match.
   </p><div id="id-1.3.4.4.4.9.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the Salt Minion fingerprint comes back empty, make sure the Salt Minion has
     a Salt Master configuration and that it can communicate with the Salt Master.
    </p></div><p>
    View each minion's fingerprint:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@minion &gt; </code>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
    After gathering fingerprints of all the Salt Minions, list fingerprints of
    all unaccepted minion keys on the Salt Master:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</pre></div><p>
    If the minions' fingerprints match, accept them:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --accept-all</pre></div></li><li class="step"><p>
    Verify that the keys have been accepted:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-key --list-all</pre></div></li><li class="step"><p>
    Test whether all Salt Minions respond:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run manage.status</pre></div></li></ol></div></div></section><section class="chapter" id="deploy-bootstrap" data-id-title="Deploying the bootstrap cluster using ceph-salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7 </span><span class="title-name">Deploying the bootstrap cluster using <code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#deploy-bootstrap">#</a></h2></div></div></div><p>
  This section guides you through the process of deploying a basic Ceph
  cluster. Read the following subsections carefully and execute the included
  commands in the given order.
 </p><section class="sect1" id="deploy-cephadm-cephsalt" data-id-title="Installing ceph-salt"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#deploy-cephadm-cephsalt">#</a></h2></div></div></div><p>
   <code class="systemitem">ceph-salt</code> provides tools for deploying Ceph clusters managed by
   cephadm. <code class="systemitem">ceph-salt</code> uses the Salt infrastructure to perform OS
   management—for example, software updates or time
   synchronization—and defining roles for Salt Minions.
  </p><p>
   On the Salt Master, install the <span class="package">ceph-salt</span> package:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper install ceph-salt</pre></div><p>
   The above command installed <span class="package">ceph-salt-formula</span> as a
   dependency which modified the Salt Master configuration by inserting
   additional files in the <code class="filename">/etc/salt/master.d</code> directory.
   To apply the changes, restart
   <code class="systemitem">salt-master.service</code> and synchronize
   Salt modules:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt \* saltutil.sync_all</pre></div></section><section class="sect1" id="deploy-cephadm-configure" data-id-title="Configuring cluster properties"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.2 </span><span class="title-name">Configuring cluster properties</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure">#</a></h2></div></div></div><p>
   Use the <code class="command">ceph-salt config</code> command to configure the basic
   properties of the cluster.
  </p><div id="id-1.3.4.5.4.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The <code class="filename">/etc/ceph/ceph.conf</code> file is managed by cephadm
    and users <span class="emphasis"><em>should not</em></span> edit it. Ceph configuration
    parameters should be set using the new <code class="command">ceph config</code>
    command. See <span class="intraxref">Book “Administration and Operations Guide”, Chapter 28 “Ceph cluster configuration”, Section 28.2 “Configuration database”</span> for more
    information.
   </p></div><section class="sect2" id="deploy-cephadm-configure-shell" data-id-title="Using the ceph-salt shell"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.1 </span><span class="title-name">Using the <code class="systemitem">ceph-salt</code> shell</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-shell">#</a></h3></div></div></div><p>
    If you run <code class="command">ceph-salt config</code> without any path or
    subcommand, you will enter an interactive <code class="systemitem">ceph-salt</code> shell. The shell is
    convenient if you need to configure multiple properties in one batch and do
    not want type the full command syntax.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config
<code class="prompt user">/&gt;</code> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]</pre></div><p>
    As you can see from the output of <code class="systemitem">ceph-salt</code>'s <code class="command">ls</code>
    command, the cluster configuration is organized in a tree structure. To
    configure a specific property of the cluster in the <code class="systemitem">ceph-salt</code> shell, you
    have two options:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Run the command from the current position and enter the absolute path to
      the property as the first argument:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">/&gt;</code> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<code class="prompt user">/&gt; /cephadm_bootstrap/dashboard/username set ceph-admin</code>
Value set.</pre></div></li><li class="listitem"><p>
      Change to the path whose property you need to configure and run the
      command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">/&gt;</code> cd /cephadm_bootstrap/dashboard/
<code class="prompt user">/ceph_cluster/minions&gt;</code> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]</pre></div></li></ul></div><div id="id-1.3.4.5.4.4.6" data-id-title="Autocompletion of configuration snippets" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Autocompletion of configuration snippets</h6><p>
     While in a <code class="systemitem">ceph-salt</code> shell, you can use the autocompletion feature
     similar to a normal Linux shell (Bash) autocompletion. It completes
     configuration paths, subcommands, or Salt Minion names. When autocompleting
     a configuration path, you have two options:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       To let the shell finish a path relative to your current position,press
       the TAB key <span class="keycap">→|</span> twice.
      </p></li><li class="listitem"><p>
       To let the shell finish an absolute path, enter <span class="keycap">/</span> and
       press the TAB key <span class="keycap">→|</span> twice.
      </p></li></ul></div></div><div id="id-1.3.4.5.4.4.7" data-id-title="Navigating with the cursor keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Navigating with the cursor keys</h6><p>
     If you enter <code class="command">cd</code> from the <code class="systemitem">ceph-salt</code> shell without any
     path, the command will print a tree structure of the cluster configuration
     with the line of the current path active. You can use the up and down
     cursor keys to navigate through individual lines. After you confirm with
     <span class="keycap">Enter</span>, the configuration path will change to
     the last active one.
    </p></div><div id="id-1.3.4.5.4.4.8" data-id-title="Convention" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Convention</h6><p>
     To keep the documentation consistent, we will use a single command syntax
     without entering the <code class="systemitem">ceph-salt</code> shell. For example, you can list the
     cluster configuration tree by using the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div></div></section><section class="sect2" id="deploy-cephadm-configure-minions" data-id-title="Adding Salt Minions"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.2 </span><span class="title-name">Adding Salt Minions</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-minions">#</a></h3></div></div></div><p>
    Include all or a subset of Salt Minions that we deployed and accepted in
    <a class="xref" href="#deploy-salt" title="Chapter 6. Deploying Salt">Chapter 6, <em>Deploying Salt</em></a> to the Ceph cluster configuration. You can
    either specify the Salt Minions by their full names, or use a glob
    expressions '*' and '?' to include multiple Salt Minions at once. Use the
    <code class="command">add</code> subcommand under the
    <code class="literal">/ceph_cluster/minions</code> path. The following command
    includes all accepted Salt Minions:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/minions add '*'</pre></div><p>
    Verify that the specified Salt Minions were added:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]</pre></div></section><section class="sect2" id="deploy-cephadm-configure-cephadm" data-id-title="Specifying Salt Minions managed by cephadm"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.3 </span><span class="title-name">Specifying Salt Minions managed by cephadm</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-cephadm">#</a></h3></div></div></div><p>
    Specify which nodes will belong to the Ceph cluster and will be managed
    by cephadm. Include all nodes that will run Ceph services as well as
    the Admin Node:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/cephadm add '*'</pre></div></section><section class="sect2" id="deploy-cephadm-configure-admin" data-id-title="Specifying Admin Node"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.4 </span><span class="title-name">Specifying Admin Node</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-admin">#</a></h3></div></div></div><p>
    The Admin Node is the node where the <code class="filename">ceph.conf</code>
    configuration file and the Ceph admin keyring is installed. You usually
    run Ceph related commands on the Admin Node.
   </p><div id="id-1.3.4.5.4.7.3" data-id-title="Salt Master and Admin Node on the Same Node" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Salt Master and Admin Node on the Same Node</h6><p>
     In a homogeneous environment where all or most hosts belong to
     SUSE Enterprise Storage, we recommend having the Admin Node on the same host as the
     Salt Master.
    </p><p>
     In a heterogeneous environment where one Salt infrastructure hosts more
     than one cluster, for example, SUSE Enterprise Storage together with SUSE Manager, do
     <span class="emphasis"><em>not</em></span> place the Admin Node on the same host as Salt Master.
    </p></div><p>
    To specify the Admin Node, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
<code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]</pre></div><div id="id-1.3.4.5.4.7.6" data-id-title="Install ceph.conf and the admin keyring on multiple nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Install <code class="filename">ceph.conf</code> and the admin keyring on multiple nodes</h6><p>
     You can install the Ceph configuration file and admin keyring on
     multiple nodes if your deployment requires it. For security reasons, avoid
     installing them on all the cluster's nodes.
    </p></div></section><section class="sect2" id="deploy-cephadm-configure-mon" data-id-title="Specifying first MON/MGR node"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.5 </span><span class="title-name">Specifying first MON/MGR node</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-mon">#</a></h3></div></div></div><p>
    You need to specify which of the cluster's Salt Minions will bootstrap the
    cluster. This minion will become the first one running Ceph Monitor and Ceph Manager
    services.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
<code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]</pre></div><p>
    Additionally, you need to specify the bootstrap MON's IP address on the
    public network to ensure that the <code class="option">public_network</code> parameter
    is set correctly, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20</pre></div></section><section class="sect2" id="deploy-cephadm-tuned-profiles" data-id-title="Specifying tuned profiles"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.6 </span><span class="title-name">Specifying tuned profiles</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-tuned-profiles">#</a></h3></div></div></div><p>
    You need to specify which of the cluster's minions have actively tuned
    profiles. To do so, add these roles explicitly with the following commands:
   </p><div id="id-1.3.4.5.4.9.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     One minion cannot have both the <code class="literal">latency</code> and
     <code class="literal">throughput</code> roles.
    </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
<code class="prompt user">root@master # </code>ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.</pre></div></section><section class="sect2" id="deploy-cephadm-configure-ssh" data-id-title="Generating an SSH key pair"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.7 </span><span class="title-name">Generating an SSH key pair</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-ssh">#</a></h3></div></div></div><p>
    cephadm uses the SSH protocol to communicate with cluster nodes. A user
    account named <code class="literal">cephadm</code> is automatically created and used
    for SSH communication.
   </p><p>
    You need to generate the private and public part of the SSH key pair:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ssh generate
Key pair generated.
<code class="prompt user">root@master # </code>ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]</pre></div></section><section class="sect2" id="deploy-cephadm-configure-ntp" data-id-title="Configuring the time server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.8 </span><span class="title-name">Configuring the time server</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-ntp">#</a></h3></div></div></div><p>
    All cluster nodes need to have their time synchronized with a reliable time
    source. There are several scenarios to approach time synchronization:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If all cluster nodes are already configured to synchronize their time
      using an NTP service of choice, disable time server handling completely:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /time_server disable</pre></div></li><li class="listitem"><p>
      If your site already has a single source of time, specify the host name
      of the time source:
     </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">root@master # </code>ceph-salt config /time_server/servers add <em class="replaceable">time-server.example.com</em></pre></div></li><li class="listitem"><p>
      Alternatively, <code class="systemitem">ceph-salt</code> has the ability to configure one of the
      Salt Minion to serve as the time server for the rest of the cluster. This
      is sometimes referred to as an "internal time server". In this scenario,
      <code class="systemitem">ceph-salt</code> will configure the internal time server (which should be one
      of the Salt Minion) to synchronize its time with an external time server,
      such as <code class="literal">pool.ntp.org</code>, and configure all the other
      minions to get their time from the internal time server. This can be
      achieved as follows:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /time_server/servers add ses-master.example.com
<code class="prompt user">root@master # </code>ceph-salt config /time_server/external_servers add pool.ntp.org</pre></div><p>
      The <code class="option">/time_server/subnet</code> option specifies the subnet from
      which NTP clients are allowed to access the NTP server. It is
      automatically set when you specify <code class="option">/time_server/servers</code>.
      If you need to change it or specify it manually, run:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /time_server/subnet set 10.20.6.0/24</pre></div></li></ul></div><p>
    Check the time server settings:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]</pre></div><p>
    Find more information on setting up time synchronization in
    <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast</a>.
   </p></section><section class="sect2" id="deploy-cephadm-configure-dashboardlogin" data-id-title="Configuring the Ceph Dashboard login credentials"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.9 </span><span class="title-name">Configuring the Ceph Dashboard login credentials</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-dashboardlogin">#</a></h3></div></div></div><p>
    Ceph Dashboard will be available after the basic cluster is deployed. To
    access it, you need to set a valid user name and password, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/dashboard/username set admin
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/dashboard/password set <em class="replaceable">PWD</em></pre></div><div id="id-1.3.4.5.4.12.4" data-id-title="Forcing password update" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Forcing password update</h6><p>
     By default, the first dashboard user will be forced to change their
     password on first login to the dashboard. To disable this feature, run the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</pre></div></div></section><section class="sect2" id="deploy-cephadm-configure-registry" data-id-title="Using the container registry"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.10 </span><span class="title-name">Using the container registry</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-registry">#</a></h3></div></div></div><p>
    The Ceph cluster needs to have access to a container registry so that it
    can download and deploy containerized Ceph services. There are two ways
    to access the registry:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      If your cluster can access the default registry at
      <code class="literal">registry.suse.com</code> (directly or via proxy), you can
      point <code class="systemitem">ceph-salt</code> directly to this URL without creating a local registry.
      Continue by following the steps in
      <a class="xref" href="#deploy-cephadm-configure-imagepath" title="7.2.10.2. Configuring the path to container images">Section 7.2.10.2, “Configuring the path to container images”</a>.
     </p></li><li class="listitem"><p>
      If your cluster cannot access the default registry—for example, for
      an air-gapped deployment—you need to configure a local container
      registry. After the local registry is created and configured, you need to
      point <code class="systemitem">ceph-salt</code> to it.
     </p></li></ul></div><section class="sect3" id="updating-ceph-local-registry" data-id-title="Creating and configuring the local registry (optional)"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.2.10.1 </span><span class="title-name">Creating and configuring the local registry (optional)</span> <a title="Permalink" class="permalink" href="#updating-ceph-local-registry">#</a></h4></div></div></div><div id="id-1.3.4.5.4.13.4.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      There are numerous methods of creating a local registry. The instructions
      in this section are examples of creating secure and insecure registries.
      For general information on running a container image registry, refer to
      <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-registry-installation.html#sec-docker-registry-installation" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-registry-installation.html#sec-docker-registry-installation</a>.
     </p></div><div id="id-1.3.4.5.4.13.4.3" data-id-title="Placement and port usage" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Placement and port usage</h6><p>
      Deploy the registry on a machine accessible by all nodes in the cluster.
      We recommend the Admin Node. By default, the registry listens on port 5000.
     </p><p>
      On the registry node, use the following command to ensure that the port
      is free:
     </p><div class="verbatim-wrap"><pre class="screen">ss -tulpn | grep :5000</pre></div><p>
      If other processes (such as <code class="literal">iscsi-tcmu</code>) are already
      listening on port 5000, determine another free port which can be used to
      map to port 5000 in the registry container.
     </p></div><div class="procedure" id="id-1.3.4.5.4.13.4.4" data-id-title="Creating the local registry"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.1: </span><span class="title-name">Creating the local registry </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.4">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Verify that the <span class="package">Containers Module</span> extension is
       enabled:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP2 x86_64 (Activated)</pre></div></li><li class="step"><p>
       Verify that the following packages are installed:
       <span class="package">apache2-utils</span> (if enabling a secure registry),
       <span class="package">cni</span>, <span class="package">cni-plugins</span>,
       <span class="package">podman</span>, <span class="package">podman-cni-config</span>, and
       <span class="package">skopeo</span>.
      </p></li><li class="step"><p>
       Gather the following information:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Fully qualified domain name of the registry host
         (<code class="option">REG_HOST_FQDN</code>).
        </p></li><li class="listitem"><p>
         An available port number used to map to the registry container port of
         5000 (<code class="option">REG_HOST_PORT</code>).
        </p></li><li class="listitem"><p>
         Whether the registry will be secure or insecure
         (<code class="option">insecure=[true|false]</code>).
        </p></li></ul></div></li><li class="step"><p>
       To start an insecure registry (without SSL encryption), follow these
       steps:
      </p><ol type="a" class="substeps"><li class="step"><p>
         Configure <code class="systemitem">ceph-salt</code> for the insecure registry:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config containers/registries_conf enable
<code class="prompt user">cephuser@adm &gt; </code>ceph-salt config containers/registries_conf/registries \
 add prefix=<code class="option">REG_HOST_FQDN</code> insecure=true \
 location=<code class="option">REG_HOST_PORT</code>:5000</pre></div></li><li class="step"><p>
         Start the insecure registry by creating the necessary directory (for
         example, <code class="filename">/var/lib/registry</code>) and starting the
         registry with the <code class="command">podman</code> command:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir -p /var/lib/registry
<code class="prompt root"># </code>podman run --privileged -d --name registry \
 -p <code class="option">REG_HOST_PORT</code>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2</pre></div></li><li class="step"><p>
         To have the registry start after a reboot, create a <code class="systemitem">systemd</code> unit
         file for it and enable it:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> podman generate systemd --files --name registry
<code class="prompt user">&gt; </code><code class="command">sudo</code> mv container-registry.service /etc/systemd/system/
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl enable container-registry.service</pre></div></li></ol></li><li class="step"><p>
       To start a secure registry, follow these steps:
      </p><ol type="a" class="substeps"><li class="step"><p>
         Create the necessary directories:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>mkdir -p /var/lib/registry/{auth,certs}</pre></div></li><li class="step"><p>
         Generate an SSL certificate:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt</pre></div><div id="id-1.3.4.5.4.13.4.4.6.2.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
          Set the <code class="literal">CN=[value]</code> value to the fully qualified
          domain name of the host ([<code class="option">REG_HOST_FQDN</code>]).
         </p></div></li><li class="step"><p>
         Copy the certificate to all cluster nodes and refresh the certificate
         cache:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
<code class="prompt root"># </code>salt '*' cmd.shell "update-ca-certificates"</pre></div></li><li class="step"><p>
         Generate a username and password combination for authentication to the
         registry:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <code class="option">REG_USERNAME</code> <code class="option">REG_PASSWORD</code></pre></div></li><li class="step"><p>
         Start the secure registry. Use the
         <code class="option">REGISTRY_STORAGE_DELETE_ENABLED=true</code> flag so that you
         can delete images afterwards with the <code class="command">skopeo delete</code>
         command.
        </p><div class="verbatim-wrap"><pre class="screen">podman run --name myregistry -p <code class="option">REG_HOST_PORT</code>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_STORAGE_DELETE_ENABLED=true \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2</pre></div></li><li class="step"><p>
         Test secure access to the registry:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>curl https://<code class="option">REG_HOST_FQDN</code>:<code class="option">REG_HOST_PORT</code>/v2/_catalog \
 -u <code class="option">REG_USERNAME</code>:<code class="option">REG_PASSWORD</code></pre></div></li></ol></li><li class="step"><p>
       When the local registry is created, you need to synchronize container
       images from the official SUSE registry at
       <code class="literal">registry.suse.com</code> to the local one. You can use the
       <code class="command">skopeo sync</code> command found in the
       <span class="package">skopeo</span> package for that purpose. For more details,
       refer to the manual page (<code class="command">man 1 skopeo-sync</code>).
       Consider the following examples:
      </p><div class="example" id="id-1.3.4.5.4.13.4.4.7.2" data-id-title="Viewing manifest files"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.1: </span><span class="title-name">Viewing manifest files </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.4.7.2">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">skopeo inspect docker://registry.suse.com/ses/7/ceph/ceph | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7/ceph/grafana | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7/ceph/prometheus-server:2.27.1 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7/ceph/prometheus-node-exporter:1.1.2 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7/ceph/prometheus-alertmanager:0.21.0 | jq .RepoTags</pre></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.4.13.4.4.7.3" data-id-title="Synchronize to a directory"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.2: </span><span class="title-name">Synchronize to a directory </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.4.7.3">#</a></h6></div><div class="example-contents"><p>
        Synchronize all Ceph images:
       </p><div class="verbatim-wrap"><pre class="screen">skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/ceph /root/images/</pre></div><p>
        Synchronize just the latest images:
       </p><div class="verbatim-wrap"><pre class="screen">skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/ceph:latest /root/images/</pre></div></div></div></div><div class="complex-example"><div class="example" id="id-1.3.4.5.4.13.4.4.7.4" data-id-title="Synchronize Grafana images:"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.3: </span><span class="title-name">Synchronize Grafana images: </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.4.7.4">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/grafana /root/images/</pre></div><p>
        Synchronize the latest Grafana images only:
       </p><div class="verbatim-wrap"><pre class="screen">skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/grafana:latest /root/images/</pre></div></div></div></div><div class="example" id="id-1.3.4.5.4.13.4.4.7.5" data-id-title="Synchronize latest Prometheus images"><div class="example-title-wrap"><h6 class="example-title"><span class="title-number">Example 7.4: </span><span class="title-name">Synchronize latest Prometheus images </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.4.7.5">#</a></h6></div><div class="example-contents"><div class="verbatim-wrap"><pre class="screen">skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/prometheus-server:2.27.1 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/prometheus-node-exporter:1.1.2 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7/ceph/prometheus-alertmanager:0.21.0 /root/images/</pre></div></div></div></li></ol></div></div><div class="procedure" id="id-1.3.4.5.4.13.4.5" data-id-title="Configure the local registry and access credentials"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 7.2: </span><span class="title-name">Configure the local registry and access credentials </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.4.13.4.5">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
       Configure the URL of the local registry:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /containers/registry_auth/registry set <em class="replaceable">REG_HOST_URL</em></pre></div></li><li class="step"><p>
       Configure the user name and password to access the local registry:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /containers/registry_auth/username set <em class="replaceable">REG_USERNAME</em></pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-salt config /containers/registry_auth/password set <em class="replaceable">REG_PASSWORD</em></pre></div></li></ol></div></div><div id="id-1.3.4.5.4.13.4.6" data-id-title="Registry cache" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Registry cache</h6><p>
      To avoid re-syncing the local registry when new updated containers
      appear, you can configure a <span class="emphasis"><em>registry cache</em></span>.
     </p></div></section><section class="sect3" id="deploy-cephadm-configure-imagepath" data-id-title="Configuring the path to container images"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.2.10.2 </span><span class="title-name">Configuring the path to container images</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-imagepath">#</a></h4></div></div></div><div id="id-1.3.4.5.4.13.5.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      This section helps you configure the path to container images of the
      bootstrap cluster (deployment of the first Ceph Monitor and Ceph Manager pair). The
      path does not apply to container images of additional services, for
      example the monitoring stack.
     </p></div><div id="id-1.3.4.5.4.13.5.3" data-id-title="Configuring HTTPS proxy" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Configuring HTTPS proxy</h6><p>
      If you need to use a proxy to communicate with the container registry
      server, perform the following configuration steps on all cluster nodes:
     </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Copy the configuration file for containers:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> cp /usr/share/containers/containers.conf /etc/containers/containers.conf</pre></div></li><li class="step"><p>
        Edit the newly-copied file and add the <code class="option">http_proxy</code>
        setting to its <code class="literal">[engine]</code> section, for example:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]</pre></div></li></ol></div></div></div><p>
     cephadm needs to know a valid URI path to container images. Verify the
     default setting by executing
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_image_path ls</pre></div><p>
     If you do not need an alternative or local registry, specify the default
     SUSE container registry:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7/ceph/ceph</pre></div><p>
     If your deployment requires a specific path, for example, a path to a
     local registry, configure it as follows:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_image_path set <em class="replaceable">LOCAL_REGISTRY_PATH</em></pre></div></section></section><section class="sect2" id="deploy-cephadm-inflight-encryption" data-id-title="Enabling data in-flight encryption (msgr2)"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.11 </span><span class="title-name">Enabling data in-flight encryption (msgr2)</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-inflight-encryption">#</a></h3></div></div></div><p>
    The Messenger v2 protocol (MSGR2) is Ceph's on-wire protocol. It provides
    a security mode that encrypts all data passing over the network,
    encapsulation of authentication payloads, and the enabling of future
    integration of new authentication modes (such as Kerberos).
   </p><div id="id-1.3.4.5.4.14.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     msgr2 is not currently supported by Linux kernel Ceph clients, such as
     CephFS and RADOS Block Device.
    </p></div><p>
    Ceph daemons can bind to multiple ports, allowing both legacy Ceph
    clients and new v2-capable clients to connect to the same cluster. By
    default, MONs now bind to the new IANA-assigned port 3300 (CE4h or 0xCE4)
    for the new v2 protocol, while also binding to the old default port 6789
    for the legacy v1 protocol.
   </p><p>
    The v2 protocol (MSGR2) supports two connection modes:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.4.14.6.1"><span class="term">crc mode</span></dt><dd><p>
       A strong initial authentication when the connection is established and a
       CRC32C integrity check.
      </p></dd><dt id="id-1.3.4.5.4.14.6.2"><span class="term">secure mode</span></dt><dd><p>
       A strong initial authentication when the connection is established and
       full encryption of all post-authentication traffic, including a
       cryptographic integrity check.
      </p></dd></dl></div><p>
    For most connections, there are options that control which modes are used:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.4.14.8.1"><span class="term">ms_cluster_mode</span></dt><dd><p>
       The connection mode (or permitted modes) used for intra-cluster
       communication between Ceph daemons. If multiple modes are listed, the
       modes listed first are preferred.
      </p></dd><dt id="id-1.3.4.5.4.14.8.2"><span class="term">ms_service_mode</span></dt><dd><p>
       A list of permitted modes for clients to use when connecting to the
       cluster.
      </p></dd><dt id="id-1.3.4.5.4.14.8.3"><span class="term">ms_client_mode</span></dt><dd><p>
       A list of connection modes, in order of preference, for clients to use
       (or allow) when talking to a Ceph cluster.
      </p></dd></dl></div><p>
    There are a parallel set of options that apply specifically to monitors,
    allowing administrators to set different (usually more secure) requirements
    on communication with the monitors.
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.5.4.14.10.1"><span class="term">ms_mon_cluster_mode</span></dt><dd><p>
       The connection mode (or permitted modes) to use between monitors.
      </p></dd><dt id="id-1.3.4.5.4.14.10.2"><span class="term">ms_mon_service_mode</span></dt><dd><p>
       A list of permitted modes for clients or other Ceph daemons to use
       when connecting to monitors.
      </p></dd><dt id="id-1.3.4.5.4.14.10.3"><span class="term">ms_mon_client_mode</span></dt><dd><p>
       A list of connection modes, in order of preference, for clients or
       non-monitor daemons to use when connecting to monitors.
      </p></dd></dl></div><p>
    In order to enable MSGR2 encryption mode during the deployment, you need to
    add some configuration options to the <code class="systemitem">ceph-salt</code> configuration before
    running <code class="command">ceph-salt apply</code>.
   </p><p>
    To use <code class="literal">secure</code> mode, run the following commands.
   </p><p>
    Add the global section to <code class="filename">ceph_conf</code> in the <code class="systemitem">ceph-salt</code>
    configuration tool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf add global</pre></div><p>
    Set the following options:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"</pre></div><div id="id-1.3.4.5.4.14.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure <code class="literal">secure</code> precedes <code class="literal">crc</code>.
    </p></div><p>
    To <span class="emphasis"><em>force</em></span> <code class="literal">secure</code> mode, run the
    following commands:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure</pre></div><div id="update-inflight-encryption-settings" data-id-title="Updating settings" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Updating settings</h6><p>
     If you want to change any of the above settings, set the configuration
     changes in the monitor configuration store. This is achieved using the
     <code class="command">ceph config set</code> command.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global <em class="replaceable">CONNECTION_OPTION</em> <em class="replaceable">CONNECTION_MODE</em> [--force]</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global ms_cluster_mode "secure crc"</pre></div><p>
     If you want to check the current value, including default value, run the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config get <em class="replaceable">CEPH_COMPONENT</em> <em class="replaceable">CONNECTION_OPTION</em></pre></div><p>
     For example, to get the <code class="literal">ms_cluster_mode</code> for OSD's, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config get osd ms_cluster_mode</pre></div></div></section><section class="sect2" id="deploy-cephadm-enable-network" data-id-title="Configuring the cluster network"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.12 </span><span class="title-name">Configuring the cluster network</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-enable-network">#</a></h3></div></div></div><p>
    Optionally, if you are running a separate cluster network, you may need to
    set the cluster network IP address followed by the subnet mask part after
    the slash sign, for example <code class="literal">192.168.10.22/24</code>.
   </p><p>
    Run the following commands to enable <code class="literal">cluster_network</code>:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf add global
<code class="prompt user">root@master # </code>ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <em class="replaceable">NETWORK_ADDR</em></pre></div></section><section class="sect2" id="deploy-cephadm-configure-verify" data-id-title="Verifying the cluster configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.13 </span><span class="title-name">Verifying the cluster configuration</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-verify">#</a></h3></div></div></div><p>
    The minimal cluster configuration is finished. Inspect it for obvious
    errors:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path ............... [registry.suse.com/ses/7/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]</pre></div><div id="id-1.3.4.5.4.16.4" data-id-title="Status of cluster configuration" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Status of cluster configuration</h6><p>
     You can check if the configuration of the cluster is valid by running the
     following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK</pre></div></div></section><section class="sect2" id="deploy-cephadm-configure-export" data-id-title="Exporting cluster configurations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.2.14 </span><span class="title-name">Exporting cluster configurations</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-configure-export">#</a></h3></div></div></div><p>
    After you have configured the basic cluster and its configuration is valid,
    it is a good idea to export its configuration to a file:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt export &gt; cluster.json</pre></div><div id="id-1.3.4.5.4.17.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     The output of the <code class="command">ceph-salt export</code> includes the SSH
     private key. If you are concerned about the security implications, do not
     execute this command without taking appropriate precautions.
    </p></div><p>
    In case you break the cluster configuration and need to revert to a backup
    state, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt import cluster.json</pre></div></section></section><section class="sect1" id="deploy-cephadm-deploy" data-id-title="Updating nodes and bootstrap minimal cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.3 </span><span class="title-name">Updating nodes and bootstrap minimal cluster</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-deploy">#</a></h2></div></div></div><p>
   Before you deploy the cluster, update all software packages on all nodes:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt update</pre></div><p>
   If a node reports <code class="literal">Reboot is needed</code> during the update,
   important OS packages—such as the kernel—were updated to a newer
   version and you need to reboot the node to apply the changes.
  </p><p>
   To reboot all nodes that require rebooting, either append the
   <code class="option">--reboot</code> option
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt update --reboot</pre></div><p>
   Or, reboot them in a separate step:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt reboot</pre></div><div id="id-1.3.4.5.5.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    The Salt Master is never rebooted by <code class="command">ceph-salt update
    --reboot</code> or <code class="command">ceph-salt reboot</code> commands. If the
    Salt Master needs rebooting, you need to reboot it manually.
   </p></div><p>
   After the nodes are updated, bootstrap the minimal cluster:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply</pre></div><div id="id-1.3.4.5.5.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    When bootstrapping is complete, the cluster will have one Ceph Monitor and one
    Ceph Manager.
   </p></div><p>
   The above command will open an interactive user interface that shows the
   current progress of each minion.
  </p><div class="figure" id="id-1.3.4.5.5.14"><div class="figure-contents"><div class="mediaobject"><a href="images/cephadm_deploy.png" target="_blank"><img src="images/cephadm_deploy.png" width="" alt="Deployment of a minimal cluster"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 7.1: </span><span class="title-name">Deployment of a minimal cluster </span><a title="Permalink" class="permalink" href="#id-1.3.4.5.5.14">#</a></h6></div></div><div id="id-1.3.4.5.5.15" data-id-title="Non-interactive mode" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Non-interactive mode</h6><p>
    If you need to apply the configuration from a script, there is also a
    non-interactive mode of deployment. This is also useful when deploying the
    cluster from a remote machine because constant updating of the progress
    information on the screen over the network may become distracting:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply --non-interactive</pre></div></div></section><section class="sect1" id="deploy-min-cluster-final-steps" data-id-title="Reviewing final steps"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.4 </span><span class="title-name">Reviewing final steps</span> <a title="Permalink" class="permalink" href="#deploy-min-cluster-final-steps">#</a></h2></div></div></div><p>
   After the <code class="command">ceph-salt apply</code> command has completed, you
   should have one Ceph Monitor and one Ceph Manager. You should be able to run the
   <code class="command">ceph status</code> command successfully on any of the minions
   that were given the <code class="literal">admin</code> role as <code class="literal">root</code>
   or the <code class="literal">cephadm</code> user using <code class="literal">sudo</code>.
  </p><p>
   The next steps involve using the cephadm to deploy additional Ceph Monitor,
   Ceph Manager, OSDs, the Monitoring Stack, and Gateways.
  </p><p>
   Before you continue, review your new cluster's network settings. At this
   point, the <code class="literal">public_network</code> setting has been populated
   based on what was entered for <code class="literal">/cephadm_bootstrap/mon_ip</code>
   in the <code class="literal">ceph-salt</code> configuration. However, this setting was
   only applied to Ceph Monitor. You can review this setting with the following
   command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config get mon public_network</pre></div><p>
   This is the minimum that Ceph requires to work, but we recommend making
   this <code class="literal">public_network</code> setting <code class="literal">global</code>,
   which means it will apply to all types of Ceph daemons, and not only to
   MONs:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global public_network "$(ceph config get mon public_network)"</pre></div><div id="id-1.3.4.5.6.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    This step is not required. However, if you do not use this setting, the
    Ceph OSDs and other daemons (except Ceph Monitor) will listen on <span class="emphasis"><em>all
    addresses</em></span>.
   </p><p>
    If you want your OSDs to communicate amongst themselves using a completely
    separate network, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global cluster_network "<em class="replaceable">cluster_network_in_cidr_notation</em>"</pre></div><p>
    Executing this command will ensure that the OSDs created in your deployment
    will use the intended cluster network from the start.
   </p></div><p>
   If your cluster is set to have dense nodes (greater than 62 OSDs per host),
   make sure to assign sufficient ports for Ceph OSDs. The default range
   (6800-7300) currently allows for no more than 62 OSDs per host. For a
   cluster with dense nodes, adjust the setting
   <code class="literal">ms_bind_port_max</code> to a suitable value. Each OSD will
   consume eight additional ports. For example, given a host that is set to run
   96 OSDs, 768 ports will be needed. <code class="literal">ms_bind_port_max</code>
   should be set at least to 7568 by running the following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set osd.* ms_bind_port_max 7568</pre></div><p>
   You will need to adjust your firewall settings accordingly for this to work.
   See <span class="intraxref">Book “Troubleshooting Guide”, Chapter 13 “Hints and tips”, Section 13.7 “Firewall settings for Ceph”</span> for more information.
  </p></section><section class="sect1" id="deploy-min-cluster-disable-insecure" data-id-title="Disable insecure clients"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.5 </span><span class="title-name">Disable insecure clients</span> <a title="Permalink" class="permalink" href="#deploy-min-cluster-disable-insecure">#</a></h2></div></div></div><p>
   Since Octopus v15.2.11, a new health warning was introduced that informs
   you that insecure clients are allowed to join the cluster. This warning is
   <span class="emphasis"><em>on</em></span> by default. The Ceph Dashboard will show the cluster in
   the <code class="literal">HEALTH_WARN</code> status and verifying the cluster status
   on the command line informs you as follows:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]</pre></div><p>
   This warning means that the Ceph Monitors are still allowing old, unpatched clients
   to connect to the cluster. This ensures existing clients can still connect
   while the cluster is being upgraded, but warns you that there is a problem
   that needs to be addressed. When the cluster and all clients are upgraded to
   the latest version of Ceph, disallow unpatched clients by running the
   following command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></section></section><section class="chapter" id="deploy-core" data-id-title="Deploying the remaining core services using cephadm"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8 </span><span class="title-name">Deploying the remaining core services using cephadm</span> <a title="Permalink" class="permalink" href="#deploy-core">#</a></h2></div></div></div><p>
  After deploying the basic Ceph cluster, deploy core services to more
  cluster nodes. To make the cluster data accessible to clients, deploy
  additional services as well.
 </p><p>
  Currently, we support deployment of Ceph services on the command line by
  using the Ceph orchestrator (<code class="command">ceph orch</code> subcommands).
 </p><section class="sect1" id="deploy-cephadm-day2-orch" data-id-title="The ceph orch command"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">The <code class="command">ceph orch</code> command</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-orch">#</a></h2></div></div></div><p>
   The Ceph orchestrator command <code class="command">ceph orch</code>—which is
   an interface to the cephadm module—will take care of listing cluster
   components and deploying Ceph services on new cluster nodes.
  </p><section class="sect2" id="deploy-cephadm-day2-orch-status" data-id-title="Displaying the orchestrator status"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.1.1 </span><span class="title-name">Displaying the orchestrator status</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-orch-status">#</a></h3></div></div></div><p>
    The following command shows the current mode and status of the Ceph
    orchestrator.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch status</pre></div></section><section class="sect2" id="deploy-cephadm-day2-orch-list" data-id-title="Listing devices, services, and daemons"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.1.2 </span><span class="title-name">Listing devices, services, and daemons</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-orch-list">#</a></h3></div></div></div><p>
    To list all disk devices, run the following:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]</pre></div><div id="id-1.3.4.6.4.4.4" data-id-title="Services and daemons" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Services and daemons</h6><p>
     <span class="emphasis"><em>Service</em></span> is a general term for a Ceph service of a
     specific type, for example Ceph Manager.
    </p><p>
     <span class="emphasis"><em>Daemon</em></span> is a specific instance of a service, for
     example a process <code class="literal">mgr.ses-min1.gdlcik</code> running on a node
     called <code class="literal">ses-min1</code>.
    </p></div><p>
    To list all services known to cephadm, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd</pre></div><div id="id-1.3.4.6.4.4.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     You can limit the list to services on a particular node with the optional
     <code class="option">-–host</code> parameter, and services of a particular type with
     the optional <code class="option">--service-type</code> parameter. Acceptable types
     are <code class="literal">mon</code>, <code class="literal">osd</code>,
     <code class="literal">mgr</code>, <code class="literal">mds</code>, and
     <code class="literal">rgw</code>.
    </p></div><p>
    To list all running daemons deployed by cephadm, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369</pre></div><div id="id-1.3.4.6.4.4.10" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     To query the status of a particular daemon, use
     <code class="option">--daemon_type</code> and <code class="option">--daemon_id</code>. For OSDs,
     the ID is the numeric OSD ID. For MDS, the ID is the file system name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --daemon_type osd --daemon_id 0
<code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --daemon_type mds --daemon_id my_cephfs</pre></div></div></section></section><section class="sect1" id="cephadm-service-and-placement-specs" data-id-title="Service and placement specification"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.2 </span><span class="title-name">Service and placement specification</span> <a title="Permalink" class="permalink" href="#cephadm-service-and-placement-specs">#</a></h2></div></div></div><p>
   The recommended way to specify the deployment of Ceph services is to
   create a YAML-formatted file with the specification of the services that you
   intend to deploy.
  </p><section class="sect2" id="cephadm-service-spec" data-id-title="Creating service specifications"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.1 </span><span class="title-name">Creating service specifications</span> <a title="Permalink" class="permalink" href="#cephadm-service-spec">#</a></h3></div></div></div><p>
    You can create a separate specification file for each type of service, for
    example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>cat nfs.yml
service_type: nfs
service_id: <em class="replaceable">EXAMPLE_NFS</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <em class="replaceable">EXAMPLE_POOL</em>
  namespace: <em class="replaceable">EXAMPLE_NAMESPACE</em></pre></div><p>
    Alternatively, you can specify multiple (or all) service types in one
    file—for example, <code class="filename">cluster.yml</code>—that
    describes which nodes will run specific services. Remember to separate
    individual service types with three dashes (<code class="literal">---</code>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat cluster.yml
service_type: nfs
service_id: <em class="replaceable">EXAMPLE_NFS</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <em class="replaceable">EXAMPLE_POOL</em>
  namespace: <em class="replaceable">EXAMPLE_NAMESPACE</em>
---
service_type: rgw
service_id: <em class="replaceable">REALM_NAME</em>.<em class="replaceable">ZONE_NAME</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]</pre></div><p>
    The aforementioned properties have the following meaning:
   </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.6.5.3.7.1"><span class="term"><code class="literal">service_type</code></span></dt><dd><p>
       The type of the service. It can be either a Ceph service
       (<code class="literal">mon</code>, <code class="literal">mgr</code>, <code class="literal">mds</code>,
       <code class="literal">crash</code>, <code class="literal">osd</code>, or
       <code class="literal">rbd-mirror</code>), a gateway (<code class="literal">nfs</code> or
       <code class="literal">rgw</code>), or part of the monitoring stack
       (<code class="literal">alertmanager</code>, <code class="literal">grafana</code>,
       <code class="literal">node-exporter</code>, or <code class="literal">prometheus</code>).
      </p></dd><dt id="id-1.3.4.6.5.3.7.2"><span class="term"><code class="literal">service_id</code></span></dt><dd><p>
       The name of the service. Specifications of type <code class="literal">mon</code>,
       <code class="literal">mgr</code>, <code class="literal">alertmanager</code>,
       <code class="literal">grafana</code>, <code class="literal">node-exporter</code>, and
       <code class="literal">prometheus</code> do not require the
       <code class="literal">service_id</code> property.
      </p></dd><dt id="id-1.3.4.6.5.3.7.3"><span class="term"><code class="literal">placement</code></span></dt><dd><p>
       Specifies which nodes will be running the service. Refer to
       <a class="xref" href="#cephadm-placement-specs" title="8.2.2. Creating placement specification">Section 8.2.2, “Creating placement specification”</a> for more details.
      </p></dd><dt id="id-1.3.4.6.5.3.7.4"><span class="term"><code class="literal">spec</code></span></dt><dd><p>
       Additional specification relevant for the service type.
      </p></dd></dl></div><div id="id-1.3.4.6.5.3.8" data-id-title="Applying specific services" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Applying specific services</h6><p>
     Ceph cluster services have usually a number of properties specific to
     them. For examples and details of individual services' specification,
     refer to <a class="xref" href="#deploy-cephadm-day2-services" title="8.3. Deploy Ceph services">Section 8.3, “Deploy Ceph services”</a>.
    </p></div></section><section class="sect2" id="cephadm-placement-specs" data-id-title="Creating placement specification"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.2 </span><span class="title-name">Creating placement specification</span> <a title="Permalink" class="permalink" href="#cephadm-placement-specs">#</a></h3></div></div></div><p>
    To deploy Ceph services, cephadm needs to know on which nodes to deploy
    them. Use the <code class="literal">placement</code> property and list the short host
    names of the nodes that the service applies to:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]</pre></div></section><section class="sect2" id="cephadm-apply-cluster-specs" data-id-title="Applying cluster specification"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.3 </span><span class="title-name">Applying cluster specification</span> <a title="Permalink" class="permalink" href="#cephadm-apply-cluster-specs">#</a></h3></div></div></div><p>
    After you have created a full <code class="filename">cluster.yml</code> file with
    specifications of all services and their placement, you can apply the
    cluster by running the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i cluster.yml</pre></div><p>
    To view the status of the cluster, run the <code class="command">ceph orch
    status</code> command. For more details, see
    <a class="xref" href="#deploy-cephadm-day2-orch-status" title="8.1.1. Displaying the orchestrator status">Section 8.1.1, “Displaying the orchestrator status”</a>.
   </p></section><section class="sect2" id="cephadm-apply-cluster-specs-" data-id-title="Exporting the specification of a running cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.2.4 </span><span class="title-name">Exporting the specification of a running cluster</span> <a title="Permalink" class="permalink" href="#cephadm-apply-cluster-specs-">#</a></h3></div></div></div><p>
    Although you deployed services to the Ceph cluster by using the
    specification files as described in
    <a class="xref" href="#cephadm-service-and-placement-specs" title="8.2. Service and placement specification">Section 8.2, “Service and placement specification”</a>, the
    configuration of the cluster may diverge from the original specification
    during its operation. Also, you may have removed the specification files
    accidentally.
   </p><p>
    To retrieve a complete specification of a running cluster, run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]</pre></div><div id="id-1.3.4.6.5.6.5" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     You can append the <code class="option">--format</code> option to change the default
     <code class="literal">yaml</code> output format. You can select from
     <code class="literal">json</code>, <code class="literal">json-pretty</code>, or
     <code class="literal">yaml</code>. For example:
    </p><div class="verbatim-wrap"><pre class="screen">ceph orch ls --export --format json</pre></div></div></section></section><section class="sect1" id="deploy-cephadm-day2-services" data-id-title="Deploy Ceph services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.3 </span><span class="title-name">Deploy Ceph services</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-services">#</a></h2></div></div></div><p>
   After the basic cluster is running, you can deploy Ceph services to
   additional nodes.
  </p><section class="sect2" id="deploy-cephadm-day2-service-mon" data-id-title="Deploying Ceph Monitors and Ceph Managers"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.1 </span><span class="title-name">Deploying Ceph Monitors and Ceph Managers</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-mon">#</a></h3></div></div></div><p>
    Ceph cluster has three or five MONs deployed across different nodes. If
    there are five or more nodes in the cluster, we recommend deploying five
    MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
   </p><div id="id-1.3.4.6.6.3.3" data-id-title="Include Bootstrap MON" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Include Bootstrap MON</h6><p>
     When deploying MONs and MGRs, remember to include the first MON that you
     added when configuring the basic cluster in
     <a class="xref" href="#deploy-cephadm-configure-mon" title="7.2.5. Specifying first MON/MGR node">Section 7.2.5, “Specifying first MON/MGR node”</a>.
    </p></div><p>
    To deploy MONs, apply the following specification:
   </p><div class="verbatim-wrap"><pre class="screen">service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3</pre></div><div id="id-1.3.4.6.6.3.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If you need to add another node, append the host name to the same YAML
     list. For example:
    </p><div class="verbatim-wrap"><pre class="screen">service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4</pre></div></div><p>
    Similarly, to deploy MGRs, apply the following specification:
   </p><div id="id-1.3.4.6.6.3.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Ensure your deployment has at least three Ceph Managers in each deployment.
    </p></div><div class="verbatim-wrap"><pre class="screen">service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3</pre></div><div id="id-1.3.4.6.6.3.10" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     If MONs or MGRs are <span class="emphasis"><em>not</em></span> on the same subnet, you need
     to append the subnet addresses. For example:
    </p><div class="verbatim-wrap"><pre class="screen">service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24</pre></div></div></section><section class="sect2" id="deploy-cephadm-day2-service-osd" data-id-title="Deploying Ceph OSDs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.2 </span><span class="title-name">Deploying Ceph OSDs</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-osd">#</a></h3></div></div></div><div id="id-1.3.4.6.6.4.2" data-id-title="When Storage Device is Available" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: When Storage Device is Available</h6><p>
     A storage device is considered <span class="emphasis"><em>available</em></span> if all of
     the following conditions are met:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The device has no partitions.
      </p></li><li class="listitem"><p>
       The device does not have any LVM state.
      </p></li><li class="listitem"><p>
       The device is not be mounted.
      </p></li><li class="listitem"><p>
       The device does not contain a file system.
      </p></li><li class="listitem"><p>
       The device does not contain a BlueStore OSD.
      </p></li><li class="listitem"><p>
       The device is larger than 5 GB.
      </p></li></ul></div><p>
     If the above conditions are not met, Ceph refuses to provision such
     OSDs.
    </p></div><p>
    There are two ways you can deploy OSDs:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Tell Ceph to consume all available and unused storage devices:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply osd --all-available-devices</pre></div></li><li class="listitem"><p>
      Use DriveGroups (see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 13 “Operational tasks”, Section 13.4.3 “Adding OSDs using DriveGroups specification”</span>) to create OSD
      specification describing devices that will be deployed based on their
      properties, such as device type (SSD or HDD), device model names, size,
      or the nodes on which the devices exist. Then apply the specification by
      running the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply osd -i drive_groups.yml</pre></div></li></ul></div></section><section class="sect2" id="deploy-cephadm-day2-service-mds" data-id-title="Deploying Metadata Servers"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.3 </span><span class="title-name">Deploying Metadata Servers</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-mds">#</a></h3></div></div></div><p>
    CephFS requires one or more Metadata Server (MDS) services. To create a CephFS,
    first create MDS servers by applying the following specification:
   </p><div id="id-1.3.4.6.6.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure you have at least two pools, one for CephFS data and one for
     CephFS metadata, created before applying the following specification.
    </p></div><div class="verbatim-wrap"><pre class="screen">service_type: mds
service_id: <em class="replaceable">CEPHFS_NAME</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3</pre></div><p>
    After MDSs are functional, create the CephFS:
   </p><div class="verbatim-wrap"><pre class="screen">ceph fs new <em class="replaceable">CEPHFS_NAME</em> <em class="replaceable">metadata_pool</em> <em class="replaceable">data_pool</em></pre></div></section><section class="sect2" id="deploy-cephadm-day2-service-ogw" data-id-title="Deploying Object Gateways"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.4 </span><span class="title-name">Deploying Object Gateways</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-ogw">#</a></h3></div></div></div><p>
    cephadm deploys an Object Gateway as a collection of daemons that manage a
    particular <span class="emphasis"><em>realm</em></span> and <span class="emphasis"><em>zone</em></span>.
   </p><p>
    You can either relate an Object Gateway service to already existing realm and zone,
    (refer to <span class="intraxref">Book “Administration and Operations Guide”, Chapter 21 “Ceph Object Gateway”, Section 21.13 “Multisite Object Gateways”</span> for more details), or you can
    specify a non-existing <em class="replaceable">REALM_NAME</em> and
    <em class="replaceable">ZONE_NAME</em> and they will be created automatically
    after you apply the following configuration:
   </p><div class="verbatim-wrap"><pre class="screen">service_type: rgw
service_id: <em class="replaceable">REALM_NAME</em>.<em class="replaceable">ZONE_NAME</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <em class="replaceable">RGW_REALM</em>
  rgw_zone: <em class="replaceable">RGW_ZONE</em></pre></div><section class="sect3" id="cephadm-deploy-using-secure-ssl-access" data-id-title="Using secure SSL access"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">8.3.4.1 </span><span class="title-name">Using secure SSL access</span> <a title="Permalink" class="permalink" href="#cephadm-deploy-using-secure-ssl-access">#</a></h4></div></div></div><p>
     To use a secure SSL connection to the Object Gateway, you need a pair of valid SSL
     certificate and key files (see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 21 “Ceph Object Gateway”, Section 21.7 “Enable HTTPS/SSL for Object Gateways”</span> for more
     details). You need to enable SSL, specify a port number for SSL
     connections, and the SSL certificate and key files.
    </p><p>
     To enable SSL and specify the port number, include the following in your
     specification:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  ssl: true
  rgw_frontend_port: 443</pre></div><p>
     To specify the SSL certificate and key, you can paste their contents
     directly into the YAML specification file. The pipe sign
     (<code class="literal">|</code>) at the end of line tells the parser to expect a
     multi-line string as a value. For example:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----</pre></div><div id="id-1.3.4.6.6.6.5.7" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      Instead of pasting the content of SSL certificate and key files, you can
      omit the <code class="literal">rgw_frontend_ssl_certificate:</code> and
      <code class="literal">rgw_frontend_ssl_key:</code> keywords and upload them to the
      configuration database:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config-key set rgw/cert/<em class="replaceable">REALM_NAME</em>/<em class="replaceable">ZONE_NAME</em>.crt \
 -i <em class="replaceable">SSL_CERT_FILE</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph config-key set rgw/cert/<em class="replaceable">REALM_NAME</em>/<em class="replaceable">ZONE_NAME</em>.key \
 -i <em class="replaceable">SSL_KEY_FILE</em></pre></div></div><section class="sect4" id="cephadm-deploy-ogw-ports" data-id-title="Configure the Object Gateway to listen on both ports 443 and 80"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">8.3.4.1.1 </span><span class="title-name">Configure the Object Gateway to listen on both ports 443 and 80</span> <a title="Permalink" class="permalink" href="#cephadm-deploy-ogw-ports">#</a></h5></div></div></div><p>
      To configure the Object Gateway to listen on both ports 443 (HTTPS) and 80 (HTTP),
      follow these steps:
     </p><div id="id-1.3.4.6.6.6.5.8.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The commands in the procedure use realm and zone
       <code class="literal">default</code>.
      </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
        Deploy the Object Gateway by supplying a specification file. Refer to
        <a class="xref" href="#deploy-cephadm-day2-service-ogw" title="8.3.4. Deploying Object Gateways">Section 8.3.4, “Deploying Object Gateways”</a> for more
        details on the Object Gateway specification. Use the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">SPEC_FILE</em></pre></div></li><li class="step"><p>
        If SSL certificates are not supplied in the specification file, add
        them by using the following command:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<code class="prompt user">cephuser@adm &gt; </code>ceph config-key set rgw/cert/default/default.key -i key.pem</pre></div></li><li class="step"><p>
        Change the default value of the <code class="option">rgw_frontends</code> option:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"</pre></div></li><li class="step"><p>
        Restart Object Gateways:
       </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch restart rgw.default.default</pre></div></li></ol></div></div></section></section><section class="sect3" id="cephadm-deploy-with-subcluster" data-id-title="Deploying with a subcluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">8.3.4.2 </span><span class="title-name">Deploying with a subcluster</span> <a title="Permalink" class="permalink" href="#cephadm-deploy-with-subcluster">#</a></h4></div></div></div><p>
     <span class="emphasis"><em>Subclusters</em></span> help you organize the nodes in your
     clusters to isolate workloads and make elastic scaling easier. If you are
     deploying with a subcluster, apply the following configuration:
    </p><div class="verbatim-wrap"><pre class="screen">service_type: rgw
service_id: <em class="replaceable">REALM_NAME</em>.<em class="replaceable">ZONE_NAME</em>.<em class="replaceable">SUBCLUSTER</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <em class="replaceable">RGW_REALM</em>
  rgw_zone: <em class="replaceable">RGW_ZONE</em>
  subcluster: <em class="replaceable">SUBCLUSTER</em></pre></div></section></section><section class="sect2" id="deploy-cephadm-day2-service-igw" data-id-title="Deploying iSCSI Gateways"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.5 </span><span class="title-name">Deploying iSCSI Gateways</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-igw">#</a></h3></div></div></div><p>
    cephadm deploys an iSCSI Gateway which is a storage area network (SAN) protocol
    that allows clients (called initiators) to send SCSI commands to SCSI
    storage devices (targets) on remote servers.
   </p><p>
    Apply the following configuration to deploy. Ensure
    <code class="literal">trusted_ip_list</code> contains the IP addresses of all iSCSI Gateway
    and Ceph Manager nodes (see the example output below).
   </p><div id="id-1.3.4.6.6.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure the pool is created before applying the following specification.
    </p></div><div class="verbatim-wrap"><pre class="screen">service_type: iscsi
service_id: <em class="replaceable">EXAMPLE_ISCSI</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <em class="replaceable">EXAMPLE_POOL</em>
  api_user: <em class="replaceable">EXAMPLE_USER</em>
  api_password: <em class="replaceable">EXAMPLE_PASSWORD</em>
  trusted_ip_list: "<em class="replaceable">IP_ADDRESS_1</em>,<em class="replaceable">IP_ADDRESS_2</em>"</pre></div><div id="id-1.3.4.6.6.7.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Ensure the IPs listed for <code class="literal">trusted_ip_list</code> do
     <span class="emphasis"><em>not</em></span> have a space after the comma separation.
    </p></div><section class="sect3" id="id-1.3.4.6.6.7.7" data-id-title="Secure SSL configuration"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">8.3.5.1 </span><span class="title-name">Secure SSL configuration</span> <a title="Permalink" class="permalink" href="#id-1.3.4.6.6.7.7">#</a></h4></div></div></div><p>
     To use a secure SSL connection between the Ceph Dashboard and the iSCSI
     target API, you need a pair of valid SSL certificate and key files. These
     can be either CA-issued or self-signed (see
     <span class="intraxref">Book “Administration and Operations Guide”, Chapter 10 “Manual configuration”, Section 10.1.1 “Creating self-signed certificates”</span>). To enable SSL, include
     the <code class="literal">api_secure: true</code> setting in your specification
     file:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  api_secure: true</pre></div><p>
     To specify the SSL certificate and key, you can paste the content directly
     into the YAML specification file. The pipe sign (<code class="literal">|</code>) at
     the end of line tells the parser to expect a multi-line string as a value.
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----</pre></div></section></section><section class="sect2" id="deploy-cephadm-day2-service-nfs" data-id-title="Deploying NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.6 </span><span class="title-name">Deploying NFS Ganesha</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-nfs">#</a></h3></div></div></div><div id="id-1.3.4.6.6.8.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
  NFS Ganesha supports NFS version 4.1 and newer.
  It does not support NFS version 3.
 </p></div><p>
    cephadm deploys NFS Ganesha using a pre-defined RADOS pool and an
    optional name-space. To deploy NFS Ganesha, apply the following
    specification:
   </p><div id="id-1.3.4.6.6.8.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     You need to have a pre-defined RADOS pool otherwise the <code class="command">ceph
     orch apply</code> operation will fail. For more information on creating
     a pool, see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 18 “Manage storage pools”, Section 18.1 “Creating a pool”</span>.
    </p></div><div class="verbatim-wrap"><pre class="screen">service_type: nfs
service_id: <em class="replaceable">EXAMPLE_NFS</em>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <em class="replaceable">EXAMPLE_POOL</em>
  namespace: <em class="replaceable">EXAMPLE_NAMESPACE</em></pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <em class="replaceable">EXAMPLE_NFS</em> with an arbitrary string that
      identifies the NFS export.
     </p></li><li class="listitem"><p>
      <em class="replaceable">EXAMPLE_POOL</em> with the name of the pool where
      the NFS Ganesha RADOS configuration object will be stored.
     </p></li><li class="listitem"><p>
      <em class="replaceable">EXAMPLE_NAMESPACE</em> (optional) with the desired
      Object Gateway NFS namespace (for example, <code class="literal">ganesha</code>).
     </p></li></ul></div></section><section class="sect2" id="deploy-cephadm-day2-service-rbdmirror" data-id-title="Deploying rbd-mirror"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.7 </span><span class="title-name">Deploying <code class="systemitem">rbd-mirror</code></span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-rbdmirror">#</a></h3></div></div></div><p>
    The <code class="systemitem">rbd-mirror</code> service takes care of synchronizing RADOS Block Device images between
    two Ceph clusters (for more details, see
    <span class="intraxref">Book “Administration and Operations Guide”, Chapter 20 “RADOS Block Device”, Section 20.4 “RBD image mirrors”</span>). To deploy <code class="systemitem">rbd-mirror</code>, use the
    following specification:
   </p><div class="verbatim-wrap"><pre class="screen">service_type: rbd-mirror
service_id: <em class="replaceable">EXAMPLE_RBD_MIRROR</em>
placement:
  hosts:
  - ses-min3</pre></div></section><section class="sect2" id="deploy-cephadm-day2-service-monitoring" data-id-title="Deploying the monitoring stack"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.3.8 </span><span class="title-name">Deploying the monitoring stack</span> <a title="Permalink" class="permalink" href="#deploy-cephadm-day2-service-monitoring">#</a></h3></div></div></div><p>
    The monitoring stack consists of Prometheus, Prometheus exporters,
    Prometheus Alertmanager, and Grafana. Ceph Dashboard makes use of these
    components to store and visualize detailed metrics on cluster usage and
    performance.
   </p><div id="id-1.3.4.6.6.10.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     If your deployment requires custom or locally served container images of
     the monitoring stack services, refer to
     <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.1 “Configuring custom or local images”</span>.
    </p></div><p>
    To deploy the monitoring stack, follow these steps:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Enable the <code class="literal">prometheus</code> module in the Ceph Manager daemon. This
      exposes the internal Ceph metrics so that Prometheus can read them:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable prometheus</pre></div><div id="id-1.3.4.6.6.10.5.1.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       Ensure this command is run before Prometheus is deployed. If the
       command was not run before the deployment, you must redeploy
       Prometheus to update Prometheus' configuration:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus</pre></div></div></li><li class="step"><p>
      Create a specification file (for example
      <code class="filename">monitoring.yaml</code>) with a content similar to the
      following:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3</pre></div></li><li class="step"><p>
      Apply monitoring services by running:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i monitoring.yaml</pre></div><p>
      It may take a minute or two for the monitoring services to be deployed.
     </p></li></ol></div></div><div id="id-1.3.4.6.6.10.6" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     Prometheus, Grafana, and the Ceph Dashboard are all automatically
     configured to talk to each other, resulting in a fully functional
     Grafana integration in the Ceph Dashboard when deployed as described above.
    </p><p>
     The only exception to this rule is monitoring with RBD images. See
     <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.5.4 “Enabling RBD-image monitoring”</span> for more information.
    </p></div></section></section></section><section class="chapter" id="deploy-additional" data-id-title="Deployment of additional services"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9 </span><span class="title-name">Deployment of additional services</span> <a title="Permalink" class="permalink" href="#deploy-additional">#</a></h2></div></div></div><section class="sect1" id="cha-ceph-as-iscsi" data-id-title="Installation of iSCSI gateway"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Installation of iSCSI gateway</span> <a title="Permalink" class="permalink" href="#cha-ceph-as-iscsi">#</a></h2></div></div></div><p>
   iSCSI is a storage area network (SAN) protocol that allows clients (called
   <span class="emphasis"><em>initiators</em></span>) to send SCSI commands to SCSI storage
   devices (<span class="emphasis"><em>targets</em></span>) on remote servers. SUSE Enterprise Storage
   7 includes a facility that opens Ceph storage management to
   heterogeneous clients, such as Microsoft Windows* and VMware* vSphere, through the
   iSCSI protocol. Multipath iSCSI access enables availability and scalability
   for these clients, and the standardized iSCSI protocol also provides an
   additional layer of security isolation between clients and the SUSE Enterprise Storage
   7 cluster. The configuration facility is named <code class="systemitem">ceph-iscsi</code>.
   Using <code class="systemitem">ceph-iscsi</code>, Ceph storage administrators can define thin-provisioned,
   replicated, highly-available volumes supporting read-only snapshots,
   read-write clones, and automatic resizing with Ceph RADOS Block Device
   (RBD). Administrators can then export volumes either via a single <code class="systemitem">ceph-iscsi</code>
   gateway host, or via multiple gateway hosts supporting multipath failover.
   Linux, Microsoft Windows, and VMware hosts can connect to volumes using the iSCSI
   protocol, which makes them available like any other SCSI block device. This
   means SUSE Enterprise Storage 7 customers can effectively run a complete
   block-storage infrastructure subsystem on Ceph that provides all the
   features and benefits of a conventional SAN, enabling future growth.
  </p><p>
   This chapter introduces detailed information to set up a Ceph cluster
   infrastructure together with an iSCSI gateway so that the client hosts can
   use remotely stored data as local storage devices using the iSCSI protocol.
  </p><section class="sect2" id="ceph-iscsi-iscsi" data-id-title="iSCSI block storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.1 </span><span class="title-name">iSCSI block storage</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi">#</a></h3></div></div></div><p>
    iSCSI is an implementation of the Small Computer System Interface (SCSI)
    command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
    is implemented as a service where a client (the initiator) talks to a
    server (the target) via a session on TCP port 3260. An iSCSI target's IP
    address and port are called an <span class="emphasis"><em>iSCSI portal</em></span>, where a
    target can be exposed through one or more portals. The combination of a
    target and one or more portals is called the <span class="emphasis"><em>target portal
    group</em></span> (TPG).
   </p><p>
    The underlying data link layer protocol for iSCSI is most often Ethernet.
    More specifically, modern iSCSI infrastructures use 10 GigE Ethernet or
    faster networks for optimal throughput. 10 Gigabit Ethernet connectivity
    between the iSCSI gateway and the back-end Ceph cluster is strongly
    recommended.
   </p><section class="sect3" id="ceph-iscsi-iscsi-target" data-id-title="The Linux kernel iSCSI target"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.1 </span><span class="title-name">The Linux kernel iSCSI target</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-target">#</a></h4></div></div></div><p>
     The Linux kernel iSCSI target was originally named LIO for
     <code class="literal">linux-iscsi.org</code>, the project's original domain and Web
     site. For some time, no fewer than four competing iSCSI target
     implementations were available for the Linux platform, but LIO ultimately
     prevailed as the single iSCSI reference target. The mainline kernel code
     for LIO uses the simple, but somewhat ambiguous name "target",
     distinguishing between "target core" and a variety of front-end and
     back-end target modules.
    </p><p>
     The most commonly used front-end module is arguably iSCSI. However, LIO
     also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
     several other front-end protocols. At this time, only the iSCSI protocol
     is supported by SUSE Enterprise Storage.
    </p><p>
     The most frequently used target back-end module is one that is capable of
     simply re-exporting any available block device on the target host. This
     module is named <span class="emphasis"><em>iblock</em></span>. However, LIO also has an
     RBD-specific back-end module supporting parallelized multipath I/O access
     to RBD images.
    </p></section><section class="sect3" id="ceph-iscsi-iscsi-initiators" data-id-title="iSCSI initiators"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.1.2 </span><span class="title-name">iSCSI initiators</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-iscsi-initiators">#</a></h4></div></div></div><p>
     This section introduces brief information on iSCSI initiators used on
     Linux, Microsoft Windows, and VMware platforms.
    </p><section class="sect4" id="ceph-iscsi-initiators-linux" data-id-title="Linux"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.1.2.1 </span><span class="title-name">Linux</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-initiators-linux">#</a></h5></div></div></div><p>
      The standard initiator for the Linux platform is
      <code class="systemitem">open-iscsi</code>. <code class="systemitem">open-iscsi</code>
      launches a daemon, <code class="systemitem">iscsid</code>, which the user can
      then use to discover iSCSI targets on any given portal, log in to
      targets, and map iSCSI volumes. <code class="systemitem">iscsid</code>
      communicates with the SCSI mid layer to create in-kernel block devices
      that the kernel can then treat like any other SCSI block device on the
      system. The <code class="systemitem">open-iscsi</code> initiator can be deployed
      in conjunction with the Device Mapper Multipath
      (<code class="systemitem">dm-multipath</code>) facility to provide a highly
      available iSCSI block device.
     </p></section><section class="sect4" id="ceph-iscsi-mswin-hyperv" data-id-title="Microsoft Windows and Hyper-V"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.1.2.2 </span><span class="title-name">Microsoft Windows and Hyper-V</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-mswin-hyperv">#</a></h5></div></div></div><p>
      The default iSCSI initiator for the Microsoft Windows operating system is the
      Microsoft iSCSI initiator. The iSCSI service can be configured via a
      graphical user interface (GUI), and supports multipath I/O for high
      availability.
     </p></section><section class="sect4" id="ceph-iscsi-vmware" data-id-title="VMware"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.1.2.3 </span><span class="title-name">VMware</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-vmware">#</a></h5></div></div></div><p>
      The default iSCSI initiator for VMware vSphere and ESX is the VMware
      ESX software iSCSI initiator, <code class="systemitem">vmkiscsi</code>. When
      enabled, it can be configured either from the vSphere client, or using
      the <code class="command">vmkiscsi-tool</code> command. You can then format storage
      volumes connected through the vSphere iSCSI storage adapter with VMFS,
      and use them like any other VM storage device. The VMware initiator
      also supports multipath I/O for high availability.
     </p></section></section></section><section class="sect2" id="ceph-iscsi-lrbd" data-id-title="General information about ceph-iscsi"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.2 </span><span class="title-name">General information about <code class="systemitem">ceph-iscsi</code></span> <a title="Permalink" class="permalink" href="#ceph-iscsi-lrbd">#</a></h3></div></div></div><p>
    <code class="systemitem">ceph-iscsi</code> combines the benefits of RADOS Block Devices with the ubiquitous
    versatility of iSCSI. By employing <code class="systemitem">ceph-iscsi</code> on an iSCSI target host (known
    as the iSCSI Gateway), any application that needs to make use of block storage can
    benefit from Ceph, even if it does not speak any Ceph client protocol.
    Instead, users can use iSCSI or any other target front-end protocol to
    connect to an LIO target, which translates all target I/O to RBD storage
    operations.
   </p><div class="figure" id="id-1.3.4.7.2.5.3"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme1.png" target="_blank"><img src="images/lrbd_scheme1.png" width="" alt="Ceph Cluster with a Single iSCSI Gateway"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.1: </span><span class="title-name">Ceph Cluster with a Single iSCSI Gateway </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.2.5.3">#</a></h6></div></div><p>
    <code class="systemitem">ceph-iscsi</code> is inherently highly-available and supports multipath operations.
    Thus, downstream initiator hosts can use multiple iSCSI gateways for both
    high availability and scalability. When communicating with an iSCSI
    configuration with more than one gateway, initiators may load-balance iSCSI
    requests across multiple gateways. In the event of a gateway failing, being
    temporarily unreachable, or being disabled for maintenance, I/O will
    transparently continue via another gateway.
   </p><div class="figure" id="id-1.3.4.7.2.5.5"><div class="figure-contents"><div class="mediaobject"><a href="images/lrbd_scheme2.png" target="_blank"><img src="images/lrbd_scheme2.png" width="" alt="Ceph cluster with multiple iSCSI gateways"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 9.2: </span><span class="title-name">Ceph cluster with multiple iSCSI gateways </span><a title="Permalink" class="permalink" href="#id-1.3.4.7.2.5.5">#</a></h6></div></div></section><section class="sect2" id="ceph-iscsi-deploy" data-id-title="Deployment considerations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.3 </span><span class="title-name">Deployment considerations</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-deploy">#</a></h3></div></div></div><p>
    A minimum configuration of SUSE Enterprise Storage 7 with <code class="systemitem">ceph-iscsi</code>
    consists of the following components:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      A Ceph storage cluster. The Ceph cluster consists of a minimum of
      four physical servers hosting at least eight object storage daemons
      (OSDs) each. In such a configuration, three OSD nodes also double as a
      monitor (MON) host.
     </p></li><li class="listitem"><p>
      An iSCSI target server running the LIO iSCSI target, configured via
      <code class="systemitem">ceph-iscsi</code>.
     </p></li><li class="listitem"><p>
      An iSCSI initiator host, running <code class="systemitem">open-iscsi</code>
      (Linux), the Microsoft iSCSI Initiator (Microsoft Windows), or any other compatible
      iSCSI initiator implementation.
     </p></li></ul></div><p>
    A recommended production configuration of SUSE Enterprise Storage 7
    with <code class="systemitem">ceph-iscsi</code> consists of:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      A Ceph storage cluster. A production Ceph cluster consists of any
      number of (typically more than 10) OSD nodes, each typically running
      10-12 object storage daemons (OSDs), with a minimum of three dedicated
      MON hosts.
     </p></li><li class="listitem"><p>
      Several iSCSI target servers running the LIO iSCSI target, configured via
      <code class="systemitem">ceph-iscsi</code>. For iSCSI failover and load-balancing, these servers must run
      a kernel supporting the <code class="systemitem">target_core_rbd</code> module.
      Update packages are available from the SUSE Linux Enterprise Server maintenance channel.
     </p></li><li class="listitem"><p>
      Any number of iSCSI initiator hosts, running
      <code class="systemitem">open-iscsi</code> (Linux), the Microsoft iSCSI
      Initiator (Microsoft Windows), or any other compatible iSCSI initiator
      implementation.
     </p></li></ul></div></section><section class="sect2" id="ceph-iscsi-install" data-id-title="Installation and configuration"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.4 </span><span class="title-name">Installation and configuration</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-install">#</a></h3></div></div></div><p>
    This section describes steps to install and configure an iSCSI Gateway on top of
    SUSE Enterprise Storage.
   </p><section class="sect3" id="ceph-iscsi-install-igw-ceph-cluster" data-id-title="Deploy the iSCSI Gateway to a Ceph cluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.1 </span><span class="title-name">Deploy the iSCSI Gateway to a Ceph cluster</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-install-igw-ceph-cluster">#</a></h4></div></div></div><p>
     The Ceph iSCSI Gateway deployment follows the same procedure as the deployment
     of other Ceph services—by means of cephadm. For more details, see
     <a class="xref" href="#deploy-cephadm-day2-service-igw" title="8.3.5. Deploying iSCSI Gateways">Section 8.3.5, “Deploying iSCSI Gateways”</a>.
    </p></section><section class="sect3" id="ceph-iscsi-rbd-images" data-id-title="Creating RBD images"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.2 </span><span class="title-name">Creating RBD images</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-images">#</a></h4></div></div></div><p>
     RBD images are created in the Ceph store and subsequently exported to
     iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
     You can create a volume from any host that is able to connect to your
     storage cluster using the Ceph <code class="command">rbd</code> command line
     utility. This requires the client to have at least a minimal
     <code class="filename">ceph.conf</code> configuration file, and appropriate CephX
     authentication credentials.
    </p><p>
     To create a new volume for subsequent export via iSCSI, use the
     <code class="command">rbd create</code> command, specifying the volume size in
     megabytes. For example, in order to create a 100 GB volume named
     <code class="literal">testvol</code> in the pool named
     <code class="literal">iscsi-images</code>, run:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rbd --pool iscsi-images create --size=102400 <em class="replaceable">testvol</em></pre></div></section><section class="sect3" id="ceph-iscsi-rbd-export" data-id-title="Exporting RBD images via iSCSI"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.3 </span><span class="title-name">Exporting RBD images via iSCSI</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-export">#</a></h4></div></div></div><p>
     To export RBD images via iSCSI, you can use either Ceph Dashboard Web
     interface or the <code class="systemitem">ceph-iscsi</code> gwcli utility. In this section, we will focus
     on gwcli only, demonstrating how to create an iSCSI target that exports
     an RBD image using the command line.
    </p><div id="id-1.3.4.7.2.7.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      RBD images with the following properties cannot be exported via iSCSI:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        images with the <code class="option">journaling</code> feature enabled
       </p></li><li class="listitem"><p>
        images with a <code class="option">stripe unit</code> less than 4096 bytes
       </p></li></ul></div></div><p>
     As <code class="systemitem">root</code>, enter the iSCSI Gateway container:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>cephadm enter --name <em class="replaceable">CONTAINER_NAME</em></pre></div><p>
     As <code class="systemitem">root</code>, start the iSCSI Gateway command line interface:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>gwcli</pre></div><p>
     Go to <code class="literal">iscsi-targets</code> and create a target with the name
     <code class="literal">iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets
<code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; create iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol</pre></div><p>
     Create the iSCSI gateways by specifying the gateway
     <code class="literal">name</code> and <code class="literal">ip</code> address:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; cd iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/gateways
<code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; create iscsi1 192.168.124.104
<code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; create iscsi2 192.168.124.105</pre></div><div id="id-1.3.4.7.2.7.5.12" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      Use the <code class="literal">help</code> command to show the list of available
      commands in the current configuration node.
     </p></div><p>
     Add the RBD image with the name <code class="literal">testvol</code> in the pool
     <code class="literal">iscsi-images</code>::
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...tvol/gateways&gt; cd /disks
<code class="prompt user">gwcli &gt; </code> /disks&gt; attach iscsi-images/testvol</pre></div><p>
     Map the RBD image to the target:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/disks
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/disks&gt; add iscsi-images/testvol</pre></div><div id="id-1.3.4.7.2.7.5.17" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You can use lower-level tools, such as <code class="command">targetcli</code>, to
      query the local configuration, but not to modify it.
     </p></div><div id="id-1.3.4.7.2.7.5.18" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      You can use the <code class="command">ls</code> command to review the
      configuration. Some configuration nodes also support the
      <code class="command">info</code> command, which can be used to display more
      detailed information.
     </p></div><p>
     Note that, by default, ACL authentication is enabled so this target is not
     accessible yet. Check <a class="xref" href="#iscsi-lrbd-authentication" title="9.1.4.4. Authentication and access control">Section 9.1.4.4, “Authentication and access control”</a> for
     more information about authentication and access control.
    </p></section><section class="sect3" id="iscsi-lrbd-authentication" data-id-title="Authentication and access control"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.4 </span><span class="title-name">Authentication and access control</span> <a title="Permalink" class="permalink" href="#iscsi-lrbd-authentication">#</a></h4></div></div></div><p>
     iSCSI authentication is flexible and covers many authentication
     possibilities.
    </p><section class="sect4" id="iscsi-no-auth" data-id-title="Disabling ACL authentication"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.4.1 </span><span class="title-name">Disabling ACL authentication</span> <a title="Permalink" class="permalink" href="#iscsi-no-auth">#</a></h5></div></div></div><p>
      <span class="emphasis"><em>No Authentication</em></span> means that any initiator will be
      able to access any LUNs on the corresponding target. You can enable
      <span class="emphasis"><em>No Authentication</em></span> by disabling the ACL
      authentication:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/hosts&gt; auth disable_acl</pre></div></section><section class="sect4" id="iscsi-acl-auth" data-id-title="Using ACL authentication"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.4.2 </span><span class="title-name">Using ACL authentication</span> <a title="Permalink" class="permalink" href="#iscsi-acl-auth">#</a></h5></div></div></div><p>
      When using initiator-name-based ACL authentication, only the defined
      initiators are allowed to connect. You can define an initiator by doing:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts
<code class="prompt user">gwcli &gt; </code> /iscsi-target...testvol/hosts&gt; create iqn.1996-04.de.suse:01:e6ca28cc9f20</pre></div><p>
      Defined initiators will be able to connect, but will only have access to
      the RBD images that were explicitly added to the initiator:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...:e6ca28cc9f20&gt; disk add rbd/testvol</pre></div></section><section class="sect4" id="chap-auth-password" data-id-title="Enabling CHAP authentication"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.4.3 </span><span class="title-name">Enabling CHAP authentication</span> <a title="Permalink" class="permalink" href="#chap-auth-password">#</a></h5></div></div></div><p>
      In addition to the ACL, you can enable CHAP authentication by specifying
      a user name and password for each initiator:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol/hosts/iqn.1996-04.de.suse:01:e6ca28cc9f20
<code class="prompt user">gwcli &gt; </code> /iscsi-target...:e6ca28cc9f20&gt; auth username=common12 password=pass12345678</pre></div><div id="id-1.3.4.7.2.7.6.5.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       User names must have a length of 8 to 64 characters and can contain
       alphanumeric characters, <code class="literal">.</code>, <code class="literal">@</code>,
       <code class="literal">-</code>, <code class="literal">_</code> or <code class="literal">:</code>.
      </p><p>
       Passwords must have a length of 12 to 16 characters and can contain
       alphanumeric characters, <code class="literal">@</code>, <code class="literal">-</code>,
       <code class="literal">_</code> or <code class="literal">/</code>..
      </p></div><p>
      Optionally, you can also enable CHAP mutual authentication by specifying
      the <code class="option">mutual_username</code> and <code class="option">mutual_password</code>
      parameters in the <code class="command">auth</code> command.
     </p></section><section class="sect4" id="iscsi-discovery-mutual-auth" data-id-title="Configuring discovery and mutual authentication"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.4.4 </span><span class="title-name">Configuring discovery and mutual authentication</span> <a title="Permalink" class="permalink" href="#iscsi-discovery-mutual-auth">#</a></h5></div></div></div><p>
      <span class="emphasis"><em>Discovery authentication</em></span> is independent of the
      previous authentication methods. It requires credentials for browsing, it
      is optional, and can be configured by:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets
<code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; discovery_auth username=du123456 password=dp1234567890</pre></div><div id="id-1.3.4.7.2.7.6.6.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       User names must have a length of 8 to 64 characters and can only contain
       letters, <code class="literal">.</code>, <code class="literal">@</code>,
       <code class="literal">-</code>, <code class="literal">_</code> or <code class="literal">:</code>.
      </p><p>
       Passwords must have a length of 12 to 16 characters and can only contain
       letters, <code class="literal">@</code>, <code class="literal">-</code>,
       <code class="literal">_</code> or <code class="literal">/</code>.
      </p></div><p>
      Optionally, you can also specify the <code class="option">mutual_username</code> and
      <code class="option">mutual_password</code> parameters in the
      <code class="command">discovery_auth</code> command.
     </p><p>
      Discovery authentication can be disabled by using the following command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-targets&gt; discovery_auth nochap</pre></div></section></section><section class="sect3" id="ceph-iscsi-rbd-advanced" data-id-title="Configuring advanced settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.5 </span><span class="title-name">Configuring advanced settings</span> <a title="Permalink" class="permalink" href="#ceph-iscsi-rbd-advanced">#</a></h4></div></div></div><p>
     <code class="systemitem">ceph-iscsi</code> can be configured with advanced parameters which are
     subsequently passed on to the LIO I/O target. The parameters are divided
     up into <code class="literal">target</code> and <code class="literal">disk</code> parameters.
    </p><div id="id-1.3.4.7.2.7.7.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Unless otherwise noted, changing these parameters from the default
      setting is not recommended.
     </p></div><section class="sect4" id="iscsi-target-settings" data-id-title="Viewing target settings"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.5.1 </span><span class="title-name">Viewing target settings</span> <a title="Permalink" class="permalink" href="#iscsi-target-settings">#</a></h5></div></div></div><p>
      You can view the value of these settings by using the
      <code class="command">info</code> command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /iscsi-targets/iqn.2003-01.org.linux-iscsi.iscsi.<em class="replaceable">SYSTEM-ARCH</em>:testvol
<code class="prompt user">gwcli &gt; </code> /iscsi-target...i.<em class="replaceable">SYSTEM-ARCH</em>:testvol&gt; info</pre></div><p>
      And change a setting using the <code class="command">reconfigure</code> command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /iscsi-target...i.<em class="replaceable">SYSTEM-ARCH</em>:testvol&gt; reconfigure login_timeout 20</pre></div><p>
      The available <code class="literal">target</code> settings are:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.2.7.7.4.7.1"><span class="term">default_cmdsn_depth</span></dt><dd><p>
         Default CmdSN (Command Sequence Number) depth. Limits the amount of
         requests that an iSCSI initiator can have outstanding at any moment.
        </p></dd><dt id="id-1.3.4.7.2.7.7.4.7.2"><span class="term">default_erl</span></dt><dd><p>
         Default error recovery level.
        </p></dd><dt id="id-1.3.4.7.2.7.7.4.7.3"><span class="term">login_timeout</span></dt><dd><p>
         Login timeout value in seconds.
        </p></dd><dt id="id-1.3.4.7.2.7.7.4.7.4"><span class="term">netif_timeout</span></dt><dd><p>
         NIC failure timeout in seconds.
        </p></dd><dt id="id-1.3.4.7.2.7.7.4.7.5"><span class="term">prod_mode_write_protect</span></dt><dd><p>
         If set to <code class="literal">1</code>, prevents writes to LUNs.
        </p></dd></dl></div></section><section class="sect4" id="iscsi-disk-settings" data-id-title="Viewing disk settings"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">9.1.4.5.2 </span><span class="title-name">Viewing disk settings</span> <a title="Permalink" class="permalink" href="#iscsi-disk-settings">#</a></h5></div></div></div><p>
      You can view the value of these settings by using the
      <code class="command">info</code> command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /&gt; cd /disks/rbd/testvol
<code class="prompt user">gwcli &gt; </code> /disks/rbd/testvol&gt; info</pre></div><p>
      And change a setting using the <code class="command">reconfigure</code> command:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks/rbd/testvol&gt; reconfigure rbd/testvol emulate_pr 0</pre></div><p>
      The available <code class="literal">disk</code> settings are:
     </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.3.4.7.2.7.7.5.7.1"><span class="term">block_size</span></dt><dd><p>
         Block size of the underlying device.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.2"><span class="term">emulate_3pc</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Third Party Copy.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.3"><span class="term">emulate_caw</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Compare and Write.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.4"><span class="term">emulate_dpo</span></dt><dd><p>
         If set to 1, turns on Disable Page Out.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.5"><span class="term">emulate_fua_read</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Force Unit Access read.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.6"><span class="term">emulate_fua_write</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Force Unit Access write.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.7"><span class="term">emulate_model_alias</span></dt><dd><p>
         If set to <code class="literal">1</code>, uses the back-end device name for the
         model alias.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.8"><span class="term">emulate_pr</span></dt><dd><p>
         If set to 0, support for SCSI Reservations, including Persistent Group
         Reservations, is disabled. While disabled, the SES iSCSI Gateway can
         ignore reservation state, resulting in improved request latency.
        </p><div id="id-1.3.4.7.2.7.7.5.7.8.2.2" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
          Setting <code class="literal">backstore_emulate_pr</code> to
          <code class="literal">0</code> is recommended if iSCSI initiators do not
          require SCSI Reservation support.
         </p></div></dd><dt id="id-1.3.4.7.2.7.7.5.7.9"><span class="term">emulate_rest_reord</span></dt><dd><p>
         If set to <code class="literal">0</code>, the Queue Algorithm Modifier has
         Restricted Reordering.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.10"><span class="term">emulate_tas</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Task Aborted Status.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.11"><span class="term">emulate_tpu</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Thin Provisioning Unmap.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.12"><span class="term">emulate_tpws</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Thin Provisioning Write Same.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.13"><span class="term">emulate_ua_intlck_ctrl</span></dt><dd><p>
         If set to <code class="literal">1</code>, enables Unit Attention Interlock.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.14"><span class="term">emulate_write_cache</span></dt><dd><p>
         If set to <code class="literal">1</code>, turns on Write Cache Enable.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.15"><span class="term">enforce_pr_isids</span></dt><dd><p>
         If set to <code class="literal">1</code>, enforces persistent reservation ISIDs.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.16"><span class="term">is_nonrot</span></dt><dd><p>
         If set to <code class="literal">1</code>, the backstore is a non-rotational
         device.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.17"><span class="term">max_unmap_block_desc_count</span></dt><dd><p>
         Maximum number of block descriptors for UNMAP.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.18"><span class="term">max_unmap_lba_count:</span></dt><dd><p>
         Maximum number of LBAs for UNMAP.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.19"><span class="term">max_write_same_len</span></dt><dd><p>
         Maximum length for WRITE_SAME.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.20"><span class="term">optimal_sectors</span></dt><dd><p>
         Optimal request size in sectors.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.21"><span class="term">pi_prot_type</span></dt><dd><p>
         DIF protection type.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.22"><span class="term">queue_depth</span></dt><dd><p>
         Queue depth.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.23"><span class="term">unmap_granularity</span></dt><dd><p>
         UNMAP granularity.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.24"><span class="term">unmap_granularity_alignment</span></dt><dd><p>
         UNMAP granularity alignment.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.25"><span class="term">force_pr_aptpl</span></dt><dd><p>
         When enabled, LIO will always write out the <span class="emphasis"><em>persistent
         reservation</em></span> state to persistent storage, regardless of
         whether the client has requested it via <code class="option">aptpl=1</code>. This
         has no effect with the kernel RBD back-end for LIO—it always
         persists PR state. Ideally, the <code class="option">target_core_rbd</code>
         option should force it to '1' and throw an error if someone tries to
         disable it via configuration.
        </p></dd><dt id="id-1.3.4.7.2.7.7.5.7.26"><span class="term">unmap_zeroes_data</span></dt><dd><p>
         Affects whether LIO will advertise LBPRZ to SCSI initiators,
         indicating that zeros will be read back from a region following UNMAP
         or WRITE SAME with an unmap bit.
        </p></dd></dl></div></section></section></section><section class="sect2" id="iscsi-tcmu" data-id-title="Exporting RADOS Block Device images using tcmu-runner"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.5 </span><span class="title-name">Exporting RADOS Block Device images using <code class="systemitem">tcmu-runner</code></span> <a title="Permalink" class="permalink" href="#iscsi-tcmu">#</a></h3></div></div></div><p>
    The <code class="systemitem">ceph-iscsi</code> supports both <code class="option">rbd</code> (kernel-based) and
    <code class="option">user:rbd</code> (tcmu-runner) backstores, making all the
    management transparent and independent of the backstore.
   </p><div id="id-1.3.4.7.2.8.3" data-id-title="Technology preview" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Technology preview</h6><p>
     <code class="systemitem">tcmu-runner</code> based iSCSI Gateway deployments are currently
     a technology preview.
    </p></div><p>
    Unlike kernel-based iSCSI Gateway deployments, <code class="systemitem">tcmu-runner</code>
    based iSCSI Gateways do not offer support for multipath I/O or SCSI Persistent
    Reservations.
   </p><p>
    To export an RADOS Block Device image using <code class="systemitem">tcmu-runner</code>, all
    you need to do is specify the <code class="option">user:rbd</code> backstore when
    attaching the disk:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">gwcli &gt; </code> /disks&gt; attach rbd/testvol backstore=user:rbd</pre></div><div id="id-1.3.4.7.2.8.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     When using <code class="systemitem">tcmu-runner</code>, the exported RBD image
     must have the <code class="option">exclusive-lock</code> feature enabled.
    </p></div></section></section></section></div><div class="part" id="ses-upgrade" data-id-title="Upgrading from Previous Releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Upgrading from Previous Releases </span><a title="Permalink" class="permalink" href="#ses-upgrade">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#cha-ceph-upgrade"><span class="title-number">10 </span><span class="title-name">Upgrade from a previous release</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage 6 to
  version 7.
 </p></dd></ul></div><section class="chapter" id="cha-ceph-upgrade" data-id-title="Upgrade from a previous release"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10 </span><span class="title-name">Upgrade from a previous release</span> <a title="Permalink" class="permalink" href="#cha-ceph-upgrade">#</a></h2></div></div></div><p>
  This chapter introduces steps to upgrade SUSE Enterprise Storage 6 to
  version 7.
 </p><p>
  The upgrade includes the following tasks:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    Upgrading from Ceph Nautilus to Octopus.
   </p></li><li class="listitem"><p>
    Switching from installing and running Ceph via RPM packages to running in
    containers.
   </p></li><li class="listitem"><p>
    Complete removal of DeepSea and replacing with <code class="systemitem">ceph-salt</code> and cephadm.
   </p></li></ul></div><div id="id-1.3.5.2.6" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
   The upgrade information in this chapter <span class="emphasis"><em>only</em></span> applies to
   upgrades from DeepSea to cephadm. Do not attempt to follow these
   instructions if you want to deploy SUSE Enterprise Storage on SUSE CaaS Platform.
  </p></div><div id="id-1.3.5.2.7" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
   Upgrading from SUSE Enterprise Storage versions older than 6 is not
   supported. First, you must upgrade to the latest version of SUSE Enterprise Storage
   6, and then follow the steps in this chapter.
  </p></div><section class="sect1" id="before-upgrade" data-id-title="Before upgrading"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Before upgrading</span> <a title="Permalink" class="permalink" href="#before-upgrade">#</a></h2></div></div></div><p>
   The following tasks <span class="emphasis"><em>must</em></span> be completed before you start
   the upgrade. This can be done at any time during the SUSE Enterprise Storage
   6 life time.
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The OSD migration from FileStore to BlueStore
     <span class="emphasis"><em>must</em></span> happen before the upgrade as FileStore
     unsupported in SUSE Enterprise Storage 7. Find more details about
     BlueStore and how to migrate from FileStore at
     <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#filestore2bluestore" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#filestore2bluestore</a>.
    </p></li><li class="listitem"><p>
     If you are running an older cluster that still uses
     <code class="literal">ceph-disk</code> OSDs, you <span class="emphasis"><em>need</em></span> to switch
     to <code class="literal">ceph-volume</code> before the upgrade. Find more details in
     <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#upgrade-osd-deployment" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-upgrade.html#upgrade-osd-deployment</a>.
    </p></li></ul></div><section class="sect2" id="upgrade-consider-points" data-id-title="Points to consider"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.1 </span><span class="title-name">Points to consider</span> <a title="Permalink" class="permalink" href="#upgrade-consider-points">#</a></h3></div></div></div><p>
    Before upgrading, ensure you read through the following sections to ensure
    you understand all tasks that need to be executed.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <span class="emphasis"><em>Read the release notes</em></span>. In them, you can find
      additional information on changes since the previous release of
      SUSE Enterprise Storage. Check the release notes to see whether:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        Your hardware needs special considerations.
       </p></li><li class="listitem"><p>
        Any used software packages have changed significantly.
       </p></li><li class="listitem"><p>
        Special precautions are necessary for your installation.
       </p></li></ul></div><p>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </p><p>
      You can find SES 7 release notes online at
      <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
     </p><p>
      Additionally, after having installed the package
      <span class="package">release-notes-ses</span> from the SES 7
      repository, find the release notes locally in the directory
      <code class="filename">/usr/share/doc/release-notes</code> or online at
      <a class="link" href="https://www.suse.com/releasenotes/" target="_blank">https://www.suse.com/releasenotes/</a>.
     </p></li><li class="listitem"><p>
      Read <a class="xref" href="#ses-deployment" title="Part II. Deploying Ceph Cluster">Part II, “Deploying Ceph Cluster”</a> to familiarise yourself with
      <code class="systemitem">ceph-salt</code> and the Ceph orchestrator, and in particular the information
      on service specifications.
     </p></li><li class="listitem"><p>
      The cluster upgrade may take a long time—approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </p></li><li class="listitem"><p>
      You need to upgrade the Salt Master first, then replace DeepSea with
      <code class="systemitem">ceph-salt</code> and cephadm. You will <span class="emphasis"><em>not</em></span> be able to
      start using the cephadm orchestrator module until at least all Ceph Manager
      nodes are upgraded.
     </p></li><li class="listitem"><p>
      The upgrade from using Nautilus RPMs to Octopus containers needs
      to happen in a single step. This means upgrading an entire node at a
      time, not one daemon at a time.
     </p></li><li class="listitem"><p>
      The upgrade of core services (MON, MGR, OSD) happens in an orderly
      fashion. Each service is available during the upgrade. The gateway
      services (Metadata Server, Object Gateway, NFS Ganesha, iSCSI Gateway) need to be redeployed after the
      core services are upgraded. There is a certain amount of downtime for
      each of the following services:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><div id="id-1.3.5.2.8.4.3.6.2.1.1" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
         Metadata Servers and Object Gateways are down from the time the nodes are upgraded from
         SUSE Linux Enterprise Server 15 SP1 to SUSE Linux Enterprise Server 15 SP2 until the services are redeployed at the end
         of the upgrade procedure. This is particularly important to bear in
         mind if these services are colocated with MONs, MGRs or OSDs, because
         in this case they may be down for the entire duration of the cluster
         upgrade. If this is going to be a problem, consider deploying these
         services separately on additional nodes before upgrading, so that they
         are down for the shortest possible time. This is the duration of the
         upgrade of the gateway nodes, not the duration of the upgrade of the
         entire cluster.
        </p></div></li><li class="listitem"><p>
        NFS Ganesha and iSCSI Gateways are down only while nodes are rebooting during
        upgrade from SUSE Linux Enterprise Server 15 SP1 to SUSE Linux Enterprise Server 15 SP2, and again briefly when each
        service is redeployed on the containerized mode.
       </p></li></ul></div></li></ul></div></section><section class="sect2" id="upgrade-backup-config-data" data-id-title="Backing Up cluster configuration and data"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.2 </span><span class="title-name">Backing Up cluster configuration and data</span> <a title="Permalink" class="permalink" href="#upgrade-backup-config-data">#</a></h3></div></div></div><p>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to SUSE Enterprise Storage 7. For instructions on
    how to back up all your data, see
    <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/cha-deployment-backup.html" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/cha-deployment-backup.html</a>.
   </p></section><section class="sect2" id="verify-previous-upgrade" data-id-title="Verifying steps from the previous upgrade"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.3 </span><span class="title-name">Verifying steps from the previous upgrade</span> <a title="Permalink" class="permalink" href="#verify-previous-upgrade">#</a></h3></div></div></div><p>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </p><p>
    Check for the existence of the
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.import</code>
    file.
   </p><p>
    This file is created by the engulf process during the upgrade from
    SUSE Enterprise Storage 5 to 6. The <code class="option">configuration_init:
    default-import</code> option is set in
    <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>.
   </p><p>
    If <code class="option">configuration_init</code> is still set to
    <code class="option">default-import</code>, the cluster is using
    <code class="filename">ceph.conf.import</code> as its configuration file and not
    DeepSea's default <code class="filename">ceph.conf</code>, which is compiled from
    files in
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>.
   </p><p>
    Therefore, you need to inspect <code class="filename">ceph.conf.import</code> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <code class="filename">/srv/salt/ceph/configuration/files/ceph.conf.d/</code>.
   </p><p>
    Then remove the <code class="option">configuration_init: default-import</code> line
    from
    <code class="filename">/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</code>.
   </p></section><section class="sect2" id="verify-previous-upgrade-patch" data-id-title="Updating cluster nodes and verifying cluster health"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.4 </span><span class="title-name">Updating cluster nodes and verifying cluster health</span> <a title="Permalink" class="permalink" href="#verify-previous-upgrade-patch">#</a></h3></div></div></div><p>
    Verify that all latest updates of SUSE Linux Enterprise Server 15 SP1 and SUSE Enterprise Storage
    6 are applied to all cluster nodes:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper refresh &amp;&amp; zypper patch</pre></div><div id="id-1.3.5.2.8.7.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
     Refer to
     <a class="link" href="https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates" target="_blank">https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates</a>
     for detailed information about updating the cluster nodes.
    </p></div><p>
    After updates are applied, restart the Salt Master, synchronize new Salt
    modules, and check the cluster health:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt '*' saltutil.sync_all
<code class="prompt user">cephuser@adm &gt; </code>ceph -s</pre></div><section class="sect3" id="upgrade-disable-insecure" data-id-title="Disable insecure clients"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.1 </span><span class="title-name">Disable insecure clients</span> <a title="Permalink" class="permalink" href="#upgrade-disable-insecure">#</a></h4></div></div></div><p>
     Since Nautilus v14.2.20, a new health warning was introduced that
     informs you that insecure clients are allowed to join the cluster. This
     warning is <span class="emphasis"><em>on</em></span> by default. The Ceph Dashboard will show
     the cluster in the <code class="literal">HEALTH_WARN</code> status and verifying the
     cluster status on the command line informs you as follows:
    </p><div class="verbatim-wrap"><pre class="screen"> <code class="prompt user">cephuser@adm &gt; </code>ceph status
 cluster:
   id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
   health: HEALTH_WARN
   mons are allowing insecure global_id reclaim
 [...]</pre></div><p>
     This warning means that the Ceph Monitors are still allowing old, unpatched
     clients to connect to the cluster. This ensures existing clients can still
     connect while the cluster is being upgraded, but warns you that there is a
     problem that needs to be addressed. When the cluster and all clients are
     upgraded to the latest version of Ceph, disallow unpatched clients by
     running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div></section></section><section class="sect2" id="verify-previous-upgrade-patch-repos" data-id-title="Verifying access to software repositories and container images"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.5 </span><span class="title-name">Verifying access to software repositories and container images</span> <a title="Permalink" class="permalink" href="#verify-previous-upgrade-patch-repos">#</a></h3></div></div></div><p>
    Verify that each cluster node has access to the SUSE Linux Enterprise Server 15 SP2 and SUSE Enterprise Storage
    7 software repositories, as well as the registry of container
    images.
   </p><section class="sect3" id="verify-previous-upgrade-patch-repos-repos" data-id-title="Software repositories"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.5.1 </span><span class="title-name">Software repositories</span> <a title="Permalink" class="permalink" href="#verify-previous-upgrade-patch-repos-repos">#</a></h4></div></div></div><p>
     If all nodes are registered with SCC, you will be able to use the
     <code class="command">zypper migration</code> command to upgrade. Refer to
     <a class="link" href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper" target="_blank">https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper</a>
     for more details.
    </p><p>
     If nodes are <span class="bold"><strong>not</strong></span> registered with SCC,
     disable all existing software repositories and add both the
     <code class="literal">Pool</code> and <code class="literal">Updates</code> repositories for
     each of the following extensions:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       SLE-Product-SLES/15-SP2
      </p></li><li class="listitem"><p>
       SLE-Module-Basesystem/15-SP2
      </p></li><li class="listitem"><p>
       SLE-Module-Server-Applications/15-SP2
      </p></li><li class="listitem"><p>
       SUSE-Enterprise-Storage-7
      </p></li></ul></div></section><section class="sect3" id="verify-previous-upgrade-patch-repos-images" data-id-title="Container images"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.5.2 </span><span class="title-name">Container images</span> <a title="Permalink" class="permalink" href="#verify-previous-upgrade-patch-repos-images">#</a></h4></div></div></div><p>
     All cluster nodes need access to the container image registry. In most
     cases, you will use the public SUSE registry at
     <code class="literal">registry.suse.com</code>. You need the following images:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       registry.suse.com/ses/7/ceph/ceph
      </p></li><li class="listitem"><p>
       registry.suse.com/ses/7/ceph/grafana
      </p></li><li class="listitem"><p>
       registry.suse.com/ses/7/ceph/prometheus-server
      </p></li><li class="listitem"><p>
       registry.suse.com/ses/7/ceph/prometheus-node-exporter
      </p></li><li class="listitem"><p>
       registry.suse.com/ses/7/ceph/prometheus-alertmanager
      </p></li></ul></div><p>
     Alternatively—for example, for air-gapped
     deployments—configure a local registry and verify that you have the
     correct set of container images available. Refer to
     <a class="xref" href="#deploy-cephadm-configure-registry" title="7.2.10. Using the container registry">Section 7.2.10, “Using the container registry”</a> for more details
     about configuring a local container image registry.
    </p></section></section></section><section class="sect1" id="upgrade-salt-master" data-id-title="Upgrading the Salt Master"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.2 </span><span class="title-name">Upgrading the Salt Master</span> <a title="Permalink" class="permalink" href="#upgrade-salt-master">#</a></h2></div></div></div><p>
   The following procedure describes the process of upgrading the Salt Master:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP2:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       For cluster whose all nodes are registered with SCC, run <code class="command">zypper
       migration</code>.
      </p></li><li class="listitem"><p>
       For cluster whose nodes have software repositories assigned manually,
       run <code class="command">zypper dup</code> followed by <code class="command">reboot</code>.
      </p></li></ul></div></li><li class="step"><p>
     Disable the DeepSea stages to avoid accidental use. Add the following
     content to <code class="filename">/srv/pillar/ceph/stack/global.yml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled</pre></div><p>
     Save the file and apply the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.pillar_refresh</pre></div></li><li class="step"><p>
     If you are <span class="bold"><strong>not</strong></span> using container images
     from <code class="literal">registry.suse.com</code> but rather the locally
     configured registry, edit
     <code class="filename">/srv/pillar/ceph/stack/global.yml</code> to inform DeepSea
     which Ceph container image and registry to use. For example, to use
     <code class="literal">192.168.121.1:5000/my/ceph/image</code> add the following
     lines:
    </p><div class="verbatim-wrap"><pre class="screen">ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000</pre></div><p>
     If you need to specify authentication information for the registry, add
     the <code class="literal">ses7_container_registry_auth:</code> block, for example:
    </p><div class="verbatim-wrap"><pre class="screen">ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
ses7_container_registry_auth:
  registry: 192.168.121.1:5000
  username: <em class="replaceable">USER_NAME</em>
  password: <em class="replaceable">PASSWORD</em></pre></div><p>
     Save the file and apply the changes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' saltutil.refresh_pillar</pre></div></li><li class="step"><p>
     Assimilate existing configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config assimilate-conf -i /etc/ceph/ceph.conf</pre></div></li><li class="step"><p>
     Verify the upgrade status. Your output may differ depending on your
     cluster configuration:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-mon-mgr-nodes" data-id-title="Upgrading the MON, MGR, and OSD nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.3 </span><span class="title-name">Upgrading the MON, MGR, and OSD nodes</span> <a title="Permalink" class="permalink" href="#upgrade-mon-mgr-nodes">#</a></h2></div></div></div><p>
   Upgrade the Ceph Monitor, Ceph Manager, and OSD nodes one at a time. For each service,
   follow these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     If the node you are upgrading is an OSD node, avoid having the OSD marked
     <code class="literal">out</code> during the upgrade by running the following
     command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd add-noout <em class="replaceable">SHORT_NODE_NAME</em></pre></div><p>
     Replace <em class="replaceable">SHORT_NODE_NAME</em> with the short name of
     the node as it appears in the output of the <code class="command">ceph osd
     tree</code> command. In the following input, the short host names are
     <code class="literal">ses-min1</code> and <code class="literal">ses-min2</code>
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]</pre></div></li><li class="step"><p>
     Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP2:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If the cluster's nodes are all registered with SCC, run <code class="command">zypper
       migration</code>.
      </p></li><li class="listitem"><p>
       If the cluster's nodes have software repositories assigned manually, run
       <code class="command">zypper dup</code> followed by <code class="command">reboot</code>.
      </p></li></ul></div></li><li class="step"><p>
     After the node is rebooted, containerize all existing MON, MGR, and OSD
     daemons on that node by running the following command on the Salt Master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt <em class="replaceable">MINION_ID</em> state.apply ceph.upgrade.ses7.adopt</pre></div><p>
     Replace <em class="replaceable">MINION_ID</em> with the ID of the minion
     that you are upgrading. You can get the list of minion IDs by running the
     <code class="command">salt-key -L</code> command on the Salt Master.
    </p><div id="id-1.3.5.2.10.3.3.4" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      To see the status and progress of the <span class="emphasis"><em>adoption</em></span>,
      check the Ceph Dashboard or run one of the following commands on the
      Salt Master:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph status
<code class="prompt user">root@master # </code>ceph versions
<code class="prompt user">root@master # </code>salt-run upgrade.status</pre></div></div></li><li class="step"><p>
     After the adoption has successfully finished, unset the
     <code class="literal">noout</code> flag if the node you are upgrading is an OSD
     node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd rm-noout <em class="replaceable">SHORT_NODE_NAME</em></pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-gateway-nodes" data-id-title="Upgrading gateway nodes"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.4 </span><span class="title-name">Upgrading gateway nodes</span> <a title="Permalink" class="permalink" href="#upgrade-gateway-nodes">#</a></h2></div></div></div><p>
   Upgrade your separate gateway nodes (Samba Gateway, Metadata Server, Object Gateway, NFS Ganesha, or
   iSCSI Gateway) next. Upgrade the underlying OS to SUSE Linux Enterprise Server 15 SP2 for each node:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     If the cluster's nodes are all registered with SUSE Customer Center, run the
     <code class="command">zypper migration</code> command.
    </p></li><li class="listitem"><p>
     If the cluster's nodes have software repositories assigned manually, run
     the <code class="command">zypper dup</code> followed by the
     <code class="command">reboot</code> commands.
    </p></li></ul></div><p>
   This step also applies for any nodes that are part of the cluster, but do
   not yet have any roles assigned (if in doubt, check the list of hosts on the
   Salt Master provided by the <code class="command">salt-key -L</code> command and compare
   it to the output of the <code class="command">salt-run upgrade.status</code> command).
  </p><p>
   Once the OS is upgraded on all nodes in the cluster, the next step is to
   install the <span class="package">ceph-salt</span> package and apply the cluster
   configuration. The actual gateway services are redeployed in a containerized
   mode at the end of the upgrade procedure.
  </p><div id="id-1.3.5.2.11.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Metadata Server and Object Gateway services are unavailable from the time of upgrading to
    SUSE Linux Enterprise Server 15 SP2 until they are redeployed at the end of the upgrade procedure.
   </p></div></section><section class="sect1" id="upgrade-cephsalt" data-id-title="Installing ceph-salt and applying the cluster configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.5 </span><span class="title-name">Installing <code class="systemitem">ceph-salt</code> and applying the cluster configuration</span> <a title="Permalink" class="permalink" href="#upgrade-cephsalt">#</a></h2></div></div></div><p>
   Before you start the procedure of installing <code class="systemitem">ceph-salt</code> and applying the
   cluster configuration, check the cluster and upgrade status by running the
   following commands:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph status
<code class="prompt user">root@master # </code>ceph versions
<code class="prompt user">root@master # </code>salt-run upgrade.status</pre></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Remove the DeepSea-created <code class="literal">rbd_exporter</code> and
     <code class="literal">rgw_exporter</code> cron jobs. On the Salt Master as the
     <code class="systemitem">root</code> run the <code class="command">crontab -e</code> command to edit the
     crontab. Delete the following items if present:
    </p><div class="verbatim-wrap"><pre class="screen"># SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null</pre></div></li><li class="step"><p>
     Export cluster configuration from DeepSea, by running the following
     commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<code class="prompt user">root@master # </code>salt-run upgrade.generate_service_specs &gt; specs.yaml</pre></div></li><li class="step"><p>
     Uninstall DeepSea and install <code class="systemitem">ceph-salt</code> on the Salt Master:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>zypper remove 'deepsea*'
<code class="prompt user">root@master # </code>zypper install ceph-salt</pre></div></li><li class="step"><p>
     Restart the Salt Master and synchronize Salt modules:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>systemctl restart salt-master.service
<code class="prompt user">root@master # </code>salt \* saltutil.sync_all</pre></div></li><li class="step"><p>
     Import DeepSea's cluster configuration into <code class="systemitem">ceph-salt</code>:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt import ceph-salt-config.json</pre></div></li><li class="step"><p>
     Generate SSH keys for cluster node communication:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config /ssh generate</pre></div><div id="id-1.3.5.2.12.4.6.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      Verify that the cluster configuration was imported from DeepSea and
      specify potentially missed options:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div><p>
      For a complete description of cluster configuration, refer to
      <a class="xref" href="#deploy-cephadm-configure" title="7.2. Configuring cluster properties">Section 7.2, “Configuring cluster properties”</a>.
     </p></div></li><li class="step"><p>
     Apply the configuration and enable cephadm:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt apply</pre></div></li><li class="step"><p>
     If you need to supply local container registry URL and access credentials,
     follow the steps described in
     <a class="xref" href="#deploy-cephadm-configure-registry" title="7.2.10. Using the container registry">Section 7.2.10, “Using the container registry”</a>.
    </p></li><li class="step"><p>
     If you are <span class="bold"><strong>not</strong></span> using container images
     from <code class="literal">registry.suse.com</code> but rather the
     locally-configured registry, inform Ceph which container image to use by
     running
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global container_image <em class="replaceable">IMAGE_NAME</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</pre></div></li><li class="step"><p>
     Stop and disable the SUSE Enterprise Storage 6
     <code class="systemitem">ceph-crash</code> daemons. New
     containerized forms of these daemons are started later automatically.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' service.stop ceph-crash
<code class="prompt user">root@master # </code>salt '*' service.disable ceph-crash</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-cephsalt-monitoring" data-id-title="Upgrading and adopting the monitoring stack"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.6 </span><span class="title-name">Upgrading and adopting the monitoring stack</span> <a title="Permalink" class="permalink" href="#upgrade-cephsalt-monitoring">#</a></h2></div></div></div><p>
   This following procedure adopts all components of the monitoring stack (see
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”</span> for more details).
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Pause the orchestrator:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch pause</pre></div></li><li class="step"><p>
     On whichever node is running Prometheus, Grafana and Alertmanager
     (the Salt Master by default), run the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name prometheus.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm adopt --style=legacy --name grafana.$(hostname)</pre></div><div id="id-1.3.5.2.13.3.2.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you are <span class="bold"><strong>not</strong></span> running the default
      container image registry <code class="literal">registry.suse.com</code>, you need
      to specify the image to use on each command, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7/ceph/prometheus-server:2.27.1 \
  adopt --style=legacy --name prometheus.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7/ceph/prometheus-alertmanager:0.21.0 \
  adopt --style=legacy --name alertmanager.$(hostname)
<code class="prompt user">cephuser@adm &gt; </code>cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.3.1 \
 adopt --style=legacy --name grafana.$(hostname)</pre></div><p>
      The container images required and their respective versions are listed in
      <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.1 “Configuring custom or local images”</span>.
     </p></div></li><li class="step"><p>
     Remove Node-Exporter from <span class="bold"><strong>all</strong></span> nodes. The
     Node-Exporter does not need to be migrated and will be re-installed as a
     container when the <code class="filename">specs.yaml</code> file is applied.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> zypper rm golang-github-prometheus-node_exporter</pre></div><p>
     Alternatively, you can remove Node-Exporter from all nodes simultaneously
     using Salt on the admin node:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>salt '*' pkg.remove golang-github-prometheus-node_exporter</pre></div></li><li class="step"><p>
     If you are using a custom container image registry that requires
     authentication, run a login command to verify that the images can be
     pulled:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph cephadm registry-login <em class="replaceable">URL</em> <em class="replaceable">USERNAME</em> <em class="replaceable">PASSWORD</em></pre></div></li><li class="step"><p>
     Apply the service specifications that you previously exported from
     DeepSea:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i specs.yaml</pre></div><div id="id-1.3.5.2.13.3.5.3" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      If you are <span class="bold"><strong>not</strong></span> running the default
      container image registry <code class="literal">registry.suse.com</code>, but a
      local container registry, configure cephadm to use the container image
      from the local registry for the deployment of Node-Exporter before
      deploying the Node-Exporter. Otherwise you can safely skip this step and
      ignore the following warning.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mgr mgr/cephadm/container_image_node_exporter <em class="replaceable">QUALIFIED_IMAGE_PATH</em></pre></div><p>
      Make sure that all container images for monitoring services point to the
      local registry, not only the one for Node-Exporter. This step requires
      you to do so for the Node-Exporter only, but it is advised than you set
      all the monitoring container images in cephadm to point to the local
      registry at this point.
     </p><p>
      If you do not do so, new deployments of monitoring services as well as
      re-deployments will use the default cephadm configuration and you may
      end up being unable to deploy services (in the case of air-gapped
      deployments), or with services deployed with mixed versions.
     </p><p>
      How cephadm needs to be configured to use container images from the
      local registry is described in
      <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”, Section 16.1 “Configuring custom or local images”</span>.
     </p></div></li><li class="step"><p>
     Resume the orchestrator:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch resume</pre></div></li></ol></div></div></section><section class="sect1" id="upgrade-gateways" data-id-title="Gateway service redeployment"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.7 </span><span class="title-name">Gateway service redeployment</span> <a title="Permalink" class="permalink" href="#upgrade-gateways">#</a></h2></div></div></div><section class="sect2" id="upgrade-ogw" data-id-title="Upgrading the Object Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.7.1 </span><span class="title-name">Upgrading the Object Gateway</span> <a title="Permalink" class="permalink" href="#upgrade-ogw">#</a></h3></div></div></div><p>
    In SUSE Enterprise Storage 7, the Object Gateways are always configured with a
    realm, which allows for multi-site (see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 21 “Ceph Object Gateway”, Section 21.13 “Multisite Object Gateways”</span> for
    more details) in the future. If you used a single-site Object Gateway configuration
    in SUSE Enterprise Storage 6, follow these steps to add a realm. If
    you do not plan to actually use the multi-site functionality, it is fine to
    use <code class="literal">default</code> for the realm, zonegroup and zone names.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Create a new realm:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin realm create --rgw-realm=<em class="replaceable">REALM_NAME</em> --default</pre></div></li><li class="step"><p>
      Optionally, rename the default zone and zonegroup.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<em class="replaceable">ZONEGROUP_NAME</em>
<code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <em class="replaceable">ZONE_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em></pre></div></li><li class="step"><p>
      Configure the master zonegroup:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zonegroup modify \
 --rgw-realm=<em class="replaceable">REALM_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em> \
 --endpoints http://<em class="replaceable">RGW.EXAMPLE.COM</em>:80 \
 --master --default</pre></div></li><li class="step"><p>
      Configure the master zone. For this, you will need the ACCESS_KEY and
      SECRET_KEY of an Object Gateway user with the <code class="option">system</code> flag
      enabled. This is usually the <code class="literal">admin</code> user. To get the
      ACCESS_KEY and SECRET_KEY, run <code class="command">radosgw-admin user info --uid
      admin --rgw-zone=<em class="replaceable">ZONE_NAME</em></code>.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin zone modify \
 --rgw-realm=<em class="replaceable">REALM_NAME</em> \
 --rgw-zonegroup=<em class="replaceable">ZONEGROUP_NAME</em> \
 --rgw-zone=<em class="replaceable">ZONE_NAME</em> \
 --endpoints http://<em class="replaceable">RGW.EXAMPLE.COM</em>:80 \
 --access-key=<em class="replaceable">ACCESS_KEY</em> \
 --secret=<em class="replaceable">SECRET_KEY</em> \
 --master --default</pre></div></li><li class="step"><p>
      Commit the updated configuration:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>radosgw-admin period update --commit</pre></div></li></ol></div></div><p>
    To have the Object Gateway service containerized, create its specification file as
    described in <a class="xref" href="#deploy-cephadm-day2-service-ogw" title="8.3.4. Deploying Object Gateways">Section 8.3.4, “Deploying Object Gateways”</a>, and apply
    it.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">RGW</em>.yml</pre></div></section><section class="sect2" id="upgrade-ganesha" data-id-title="Upgrading NFS Ganesha"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.7.2 </span><span class="title-name">Upgrading NFS Ganesha</span> <a title="Permalink" class="permalink" href="#upgrade-ganesha">#</a></h3></div></div></div><div id="id-1.3.5.2.14.3.2" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
  NFS Ganesha supports NFS version 4.1 and newer.
  It does not support NFS version 3.
 </p></div><p>
    The following demonstrates how to migrate an existing NFS Ganesha service
    running Ceph Nautilus to an NFS Ganesha container running Ceph Octopus.
   </p><div id="id-1.3.5.2.14.3.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     The following documentation requires you to have already successfully
     upgraded the core Ceph services.
    </p></div><p>
    NFS Ganesha stores additional per-daemon configuration and exports
    configuration in a RADOS pool. The configured RADOS pool can be found
    on the <code class="literal">watch_url</code> line of the
    <code class="literal">RADOS_URLS</code> block in the
    <code class="filename">ganesha.conf</code> file. By default, this pool will be named
    <code class="literal">ganesha_config</code>
   </p><p>
    Before attempting any migration, we strongly recommend making a copy of the
    export and daemon configuration objects located in the RADOS pool. To
    locate the configured RADOS pool, run the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</pre></div><p>
    To list the contents of the RADOS pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</pre></div><p>
    To copy the RADOS objects:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<code class="prompt user">cephuser@adm &gt; </code>OBJS=$(rados $RADOS_ARGS ls)
<code class="prompt user">cephuser@adm &gt; </code>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<code class="prompt user">cephuser@adm &gt; </code>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</pre></div><p>
    On a per-node basis, any existing NFS Ganesha service needs to be stopped and
    then replaced with a container managed by cephadm.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop and disable the existing NFS Ganesha service:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>systemctl stop nfs-ganesha
<code class="prompt user">cephuser@adm &gt; </code>systemctl disable nfs-ganesha</pre></div></li><li class="step"><p>
      After the existing NFS Ganesha service has been stopped, a new one can be
      deployed in a container using cephadm. To do so, you need to create a
      service specification that contains a <code class="literal">service_id</code> that
      will be used to identify this new NFS cluster, the host name of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
      For example:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: nfs
service_id: <em class="replaceable">SERVICE_ID</em>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha</pre></div><p>
      For more information on creating a placement specification, see
      <a class="xref" href="#cephadm-service-and-placement-specs" title="8.2. Service and placement specification">Section 8.2, “Service and placement specification”</a>.
     </p></li><li class="step"><p>
      Apply the placement specification:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch apply -i <em class="replaceable">FILENAME</em>.yaml</pre></div></li><li class="step"><p>
      Confirm the NFS Ganesha daemon is running on the host:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</pre></div></li><li class="step"><p>
      Repeat these steps for each NFS Ganesha node. You do not need to create a
      separate service specification for each node. It is sufficient to add
      each node's host name to the existing NFS service specification and
      re-apply it.
     </p></li></ol></div></div><p>
    The existing exports can be migrated in two different ways:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Manually re-created or re-assigned using the Ceph Dashboard.
     </p></li><li class="listitem"><p>
      Manually copy the contents of each per-daemon RADOS object into the
      newly created NFS Ganesha common configuration.
     </p></li></ul></div><div class="procedure" id="id-1.3.5.2.14.3.16" data-id-title="Manually copying exports to NFS Ganesha common configuration file"><div class="procedure-title-wrap"><h6 class="procedure-title"><span class="title-number">Procedure 10.1: </span><span class="title-name">Manually copying exports to NFS Ganesha common configuration file </span><a title="Permalink" class="permalink" href="#id-1.3.5.2.14.3.16">#</a></h6></div><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Determine the list of per-daemon RADOS objects:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<code class="prompt user">cephuser@adm &gt; </code>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</pre></div></li><li class="step"><p>
      Make a copy of the per-daemon RADOS objects:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<code class="prompt user">cephuser@adm &gt; </code>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<em class="replaceable">SERVICE_ID</em>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</pre></div></li><li class="step"><p>
      Sort and merge into a single list of exports:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cat conf-* | sort -u &gt; conf-nfs.<em class="replaceable">SERVICE_ID</em>
<code class="prompt user">cephuser@adm &gt; </code>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</pre></div></li><li class="step"><p>
      Write the new NFS Ganesha common configuration file:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados $RADOS_ARGS put conf-nfs.<em class="replaceable">SERVICE_ID</em> conf-nfs.<em class="replaceable">SERVICE_ID</em></pre></div></li><li class="step"><p>
      Notify the NFS Ganesha daemon:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>rados $RADOS_ARGS notify conf-nfs.<em class="replaceable">SERVICE_ID</em> conf-nfs.<em class="replaceable">SERVICE_ID</em></pre></div><div id="id-1.3.5.2.14.3.16.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       This action will cause the daemon to reload the configuration.
      </p></div></li></ol></div></div><p>
    After the service has been successfully migrated, the Nautilus-based
    NFS Ganesha service can be removed.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Remove NFS Ganesha:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</pre></div></li><li class="step"><p>
      Remove the legacy cluster settings from the Ceph Dashboard:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph dashboard reset-ganesha-clusters-rados-pool-namespace</pre></div></li></ol></div></div></section><section class="sect2" id="upgrade-mds" data-id-title="Upgrading the Metadata Server"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.7.3 </span><span class="title-name">Upgrading the Metadata Server</span> <a title="Permalink" class="permalink" href="#upgrade-mds">#</a></h3></div></div></div><p>
    Unlike MONs, MGRs and OSDs, Metadata Server cannot be adopted in-place. Instead, you
    need to redeploy them in containers using the Ceph orchestrator.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run the <code class="command">ceph fs ls</code> command to obtain the name of your
      file system, for example:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</pre></div></li><li class="step"><p>
      Create a new service specification file <code class="filename">mds.yml</code> as
      described in <a class="xref" href="#deploy-cephadm-day2-service-mds" title="8.3.3. Deploying Metadata Servers">Section 8.3.3, “Deploying Metadata Servers”</a> by using
      the file system name as the <code class="option">service_id</code> and specifying
      the hosts that will run the MDS daemons. For example:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3</pre></div></li><li class="step"><p>
      Run the <code class="command">ceph orch apply -i mds.yml</code> command to apply
      the service specification and start the MDS daemons.
     </p></li></ol></div></div></section><section class="sect2" id="upgrade-igw" data-id-title="Upgrading the iSCSI Gateway"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.7.4 </span><span class="title-name">Upgrading the iSCSI Gateway</span> <a title="Permalink" class="permalink" href="#upgrade-igw">#</a></h3></div></div></div><p>
    To upgrade the iSCSI Gateway, you need to redeploy it in containers using the
    Ceph orchestrator. If you have multiple iSCSI Gateways, you need to redeploy them
    one-by-one to reduce the service downtime.
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Stop and disable the existing iSCSI daemons on each iSCSI Gateway node:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop rbd-target-gw
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl disable rbd-target-gw
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl stop rbd-target-api
<code class="prompt user">&gt; </code><code class="command">sudo</code> systemctl disable rbd-target-api</pre></div></li><li class="step"><p>
      Create a service specification for the iSCSI Gateway as described in
      <a class="xref" href="#deploy-cephadm-day2-service-igw" title="8.3.5. Deploying iSCSI Gateways">Section 8.3.5, “Deploying iSCSI Gateways”</a>. For this, you
      need the <code class="option">pool</code>, <code class="option">trusted_ip_list</code>, and
      <code class="option">api_*</code> settings from the existing
      <code class="filename">/etc/ceph/iscsi-gateway.cfg</code> file. If you have SSL
      support enabled (<code class="literal">api_secure = true</code>), you also need the
      SSL certificate (<code class="filename">/etc/ceph/iscsi-gateway.crt</code>) and
      key (<code class="filename">/etc/ceph/iscsi-gateway.key</code>).
     </p><p>
      For example, if <code class="filename">/etc/ceph/iscsi-gateway.cfg</code> contains
      the following:
     </p><div class="verbatim-wrap"><pre class="screen">[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true</pre></div><p>
      Then you need to create the following service specification file
      <code class="filename">iscsi.yml</code>:
     </p><div class="verbatim-wrap"><pre class="screen">service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----</pre></div><div id="id-1.3.5.2.14.5.3.2.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The <code class="option">pool</code>, <code class="option">trusted_ip_list</code>,
       <code class="option">api_port</code>, <code class="option">api_user</code>,
       <code class="option">api_password</code>, <code class="option">api_secure</code> settings are
       identical to the ones from the
       <code class="filename">/etc/ceph/iscsi-gateway.cfg</code> file. The
       <code class="option">ssl_cert</code> and <code class="option">ssl_key</code> values can be
       copied in from the existing SSL certificate and key files. Verify that
       they are indented correctly and the <span class="emphasis"><em>pipe</em></span> character
       <code class="literal">|</code> appears at the end of the
       <code class="literal">ssl_cert:</code> and <code class="literal">ssl_key:</code> lines (see
       the content of the <code class="filename">iscsi.yml</code> file above).
      </p></div></li><li class="step"><p>
      Run the <code class="command">ceph orch apply -i iscsi.yml</code> command to apply
      the service specification and start the iSCSI Gateway daemons.
     </p></li><li class="step"><p>
      Remove the old <span class="package">ceph-iscsi</span> package from each of the
      existing iSCSI gateway nodes:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>zypper rm -u ceph-iscsi</pre></div></li></ol></div></div></section></section><section class="sect1" id="upgrade-post-cleanup" data-id-title="Post-upgrade Clean-up"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.8 </span><span class="title-name">Post-upgrade Clean-up</span> <a title="Permalink" class="permalink" href="#upgrade-post-cleanup">#</a></h2></div></div></div><p>
   After the upgrade, perform the following clean-up steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Verify that the cluster was successfully upgraded by checking the current
     Ceph version:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph versions</pre></div></li><li class="step"><p>
     Make sure that no old OSDs will join the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd require-osd-release octopus</pre></div></li><li class="step"><p>
     Enable the autoscaler module:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable pg_autoscaler</pre></div><div id="id-1.3.5.2.15.3.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      Pools in SUSE Enterprise Storage 6 had the <code class="option">pg_autoscale_mode</code> set
      to <code class="option">warn</code> by default. This resulted in a warning message
      in case of suboptimal number of PGs, but autoscaling did not actually
      happen. The default in SUSE Enterprise Storage 7 is that the
      <code class="option">pg_autoscale_mode</code> option is set to <code class="option">on</code>
      for new pools, and PGs will actually autoscale. The upgrade process does
      not automatically change the <code class="option">pg_autoscale_mode</code> of
      existing pools. If you want to change it to <code class="option">on</code> to get
      the full benefit of the autoscaler, see the instructions in
      <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.4.12 “Enabling the PG auto-scaler”</span>.
     </p></div><p>
     Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”, Section 17.4.12 “Enabling the PG auto-scaler”</span>.
    </p></li><li class="step"><p>
     Prevent pre-Luminous clients:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd set-require-min-compat-client luminous</pre></div></li><li class="step"><p>
     Enable the balancer module:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph balancer mode upmap
<code class="prompt user">cephuser@adm &gt; </code>ceph balancer on</pre></div><p>
     Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 29 “Ceph Manager modules”, Section 29.1 “Balancer”</span>.
    </p></li><li class="step"><p>
     Optionally, enable the telemetry module:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph mgr module enable telemetry
<code class="prompt user">cephuser@adm &gt; </code>ceph telemetry on</pre></div><p>
     Find more details in <span class="intraxref">Book “Administration and Operations Guide”, Chapter 29 “Ceph Manager modules”, Section 29.2 “Enabling the telemetry module”</span>.
    </p></li></ol></div></div></section></section></div><section class="appendix" id="id-1.3.6" data-id-title="Ceph maintenance updates based on upstream Octopus point releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span> <a title="Permalink" class="permalink" href="#id-1.3.6">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7 are based on the
  Octopus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Octopus series, SUSE Enterprise Storage 7 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.5"><span class="name">Octopus 15.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.12"><span class="name">Octopus 15.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.12">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The containers include an updated <code class="literal">tcmalloc</code> that avoids
    crashes seen on 15.2.9.
   </p></li><li class="listitem"><p>
    RADOS: BlueStore handling of huge (&gt;4GB) writes from RocksDB to BlueFS
    has been fixed.
   </p></li><li class="listitem"><p>
    When upgrading from a previous cephadm release,
    <code class="command">systemctl</code> may hang when trying to start or restart the
    monitoring containers. This is caused by a change in the <code class="systemitem">systemd</code> unit to
    use <code class="option">type=forking</code>.) After the upgrade, please run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy nfs
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy iscsi
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy alertmanager</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.15"><span class="name">Octopus 15.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.15">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    MGR: progress module can now be turned on/off, using the commands:
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li><li class="listitem"><p>
    OSD: PG removal has been optimized in this release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.18"><span class="name">Octopus 15.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS and includes a number of bug
  fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    OpenStack Manila use of <code class="filename">ceph_volume_client.py</code> library
    allowed tenant access to any Ceph credential’s secret.
   </p></li><li class="listitem"><p>
    <code class="command">ceph-volume</code>: The <code class="command">lvm batch</code> subcommand
    received a major rewrite. This closed a number of bugs and improves
    usability in terms of size specification and calculation, as well as
    idempotency behaviour and disk replacement process. Please refer to
    <a class="link" href="https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/" target="_blank">https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/</a>
    for more detailed information.
   </p></li><li class="listitem"><p>
    MON: The cluster log now logs health detail every
    <code class="option">mon_health_to_clog_interval</code>, which has been changed from
    1hr to 10min. Logging of health detail will be skipped if there is no
    change in health summary since last known.
   </p></li><li class="listitem"><p>
    The <code class="command">ceph df</code> command now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    The <code class="option">bluefs_preextend_wal_files</code> option has been removed.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial monitor to contact for Ceph
    tools and daemons using the <code class="option">mon_host_override</code> config
    option or <code class="option">--mon-host-override</code> command line switch. This
    generally should only be used for debugging and only affects initial
    communication with Ceph's monitor cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.21"><span class="name">Octopus 15.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.21">#</a></h2></div><p>
  This release fixes a serious bug in RGW that has been shown to cause data
  loss when a read of a large RGW object (for example, one with at least one
  tail segment) takes longer than one half the time specified in the
  configuration option <code class="option">rgw_gc_obj_min_wait</code>. The bug causes the
  tail segments of that read object to be added to the RGW garbage collection
  queue, which will in turn cause them to be deleted after a period of time.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.23"><span class="name">Octopus 15.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.23">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.25"><span class="name">Octopus 15.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.25">#</a></h2></div><p>
  The Octopus point release 15.2.5 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CephFS: Automatic static sub-tree partitioning policies may now be
    configured using the new distributed and random ephemeral pinning extended
    attributes on directories. See the following documentation for more
    information:
    <a class="link" href="https://docs.ceph.com/docs/master/cephfs/multimds/" target="_blank">https://docs.ceph.com/docs/master/cephfs/multimds/</a>
   </p></li><li class="listitem"><p>
    Monitors now have a configuration option
    <code class="option">mon_osd_warn_num_repaired</code>, which is set to 10 by default.
    If any OSD has repaired more than this many I/O errors in stored data a
    <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is generated.
   </p></li><li class="listitem"><p>
    Now, when <code class="literal">no scrub</code> and/or <code class="literal">no
    deep-scrub</code> flags are set globally or per pool, scheduled scrubs
    of the type disabled will be aborted. All user initiated scrubs are NOT
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.28"><span class="name">Octopus 15.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.28">#</a></h2></div><p>
  The Octopus point release 15.2.4 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li><li class="listitem"><p>
    Object Gateway: The <code class="command">radosgw-admin</code> sub-commands dealing with
    orphans—<code class="command">radosgw-admin orphans find</code>,
    <code class="command">radosgw-admin orphans finish</code>, and <code class="command">radosgw-admin
    orphans list-jobs</code>—have been deprecated. They had not been
    actively maintained, and since they store intermediate results on the
    cluster, they could potentially fill a nearly-full cluster. They have been
    replaced by a tool, <code class="command">rgw-orphan-list</code>, which is currently
    considered experimental.
   </p></li><li class="listitem"><p>
    RBD: The name of the RBD pool object that is used to store RBD trash purge
    schedule is changed from <code class="literal">rbd_trash_trash_purge_schedule</code>
    to <code class="literal">rbd_trash_purge_schedule</code>. Users that have already
    started using RBD trash purge schedule functionality and have per pool or
    name space schedules configured should copy the
    <code class="literal">rbd_trash_trash_purge_schedule</code> object to
    <code class="literal">rbd_trash_purge_schedule</code> before the upgrade and remove
    <code class="literal">rbd_trash_purge_schedule</code> using the following commands in
    every RBD pool and name space where a trash purge schedule was previously
    configured:
   </p><div class="verbatim-wrap"><pre class="screen">rados -p <em class="replaceable">pool-name</em> [-N namespace] cp rbd_trash_trash_purge_schedule rbd_trash_purge_schedule
rados -p <em class="replaceable">pool-name</em> [-N namespace] rm rbd_trash_trash_purge_schedule</pre></div><p>
    Alternatively, use any other convenient way to restore the schedule after
    the upgrade.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.31"><span class="name">Octopus 15.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.31">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The Octopus point release 15.2.3 was a hot-fix release to address an
    issue where WAL corruption was seen when
    <code class="option">bluefs_preextend_wal_files</code> and
    <code class="option">bluefs_buffered_io</code> were enabled at the same time. The fix
    in 15.2.3 is only a temporary measure (changing the default value of
    <code class="option">bluefs_preextend_wal_files</code> to <code class="literal">false</code>).
    The permanent fix will be to remove the
    <code class="option">bluefs_preextend_wal_files</code> option completely: this fix
    will most likely arrive in the 15.2.6 point release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.33"><span class="name">Octopus 15.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.33">#</a></h2></div><p>
  The Octopus point release 15.2.2 patched one security vulnerability:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10736: Fixed an authorization bypass in MONs and MGRs
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.3.6.36"><span class="name">Octopus 15.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.3.6.36">#</a></h2></div><p>
  The Octopus point release 15.2.1 fixed an issue where upgrading quickly
  from Luminous (SES5.5) to Nautilus (SES6) to Octopus (SES7) caused OSDs to
  crash. In addition, it patched two security vulnerabilities that were present
  in the initial Octopus (15.2.0) release:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS because of RGW GetObject header-splitting
   </p></li></ul></div></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.3.7">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.3.7.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.3.7.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.2">#</a></span></dt><dd class="glossdef"><p>
     The host from which you run the Ceph-related commands to administer
     cluster hosts.
    </p></dd><dt id="id-1.3.7.3.3"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.3">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies the end user.
    </p></dd><dt id="id-1.3.7.3.4"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.4">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.3.7.3.5"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.5">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p></dd><dt id="id-1.3.7.3.9"><span><span class="glossterm">Ceph Client</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.9">#</a></span></dt><dd class="glossdef"><p>
     The collection of Ceph components which can access a Ceph Storage
     Cluster. These include the Object Gateway, the Ceph Block Device, the CephFS,
     and their corresponding libraries, kernel modules, and FUSE clients.
    </p></dd><dt id="id-1.3.7.3.14"><span><span class="glossterm">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.14">#</a></span></dt><dd class="glossdef"><p>
     A built-in Web-based Ceph management and monitoring application to
     administer various aspects and objects of the cluster. The dashboard is
     implemented as a Ceph Manager module.
    </p></dd><dt id="id-1.3.7.3.19"><span><span class="glossterm">Ceph Manager</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.19">#</a></span></dt><dd class="glossdef"><p>
     Ceph Manager or MGR is the Ceph manager software, which collects all the state
     from the whole cluster in one place.
    </p></dd><dt id="id-1.3.7.3.18"><span><span class="glossterm">Ceph Monitor</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.18">#</a></span></dt><dd class="glossdef"><p>
     Ceph Monitor or MON is the Ceph monitor software.
    </p></dd><dt id="id-1.3.7.3.24"><span><span class="glossterm">Ceph Object Storage</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.24">#</a></span></dt><dd class="glossdef"><p>
     The object storage "product", service or capabilities, which consists of a
     Ceph Storage Cluster and a Ceph Object Gateway.
    </p></dd><dt id="id-1.3.7.3.22"><span><span class="glossterm">Ceph OSD Daemon</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.22">#</a></span></dt><dd class="glossdef"><p>
     The <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.3.7.3.11"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.11">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data. Such a set
     consists of Ceph monitors and OSDs.
    </p></dd><dt id="id-1.3.7.3.10"><span><span class="glossterm"><code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.10">#</a></span></dt><dd class="glossdef"><p>
     Provides tooling for deploying Ceph clusters managed by cephadm using
     Salt.
    </p></dd><dt id="id-1.3.7.3.6"><span><span class="glossterm">cephadm</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.6">#</a></span></dt><dd class="glossdef"><p>
     cephadm deploys and manages a Ceph cluster by connecting to hosts from
     the manager daemon via SSH to add, remove, or update Ceph daemon
     containers.
    </p></dd><dt id="id-1.3.7.3.7"><span><span class="glossterm">CephFS</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.7">#</a></span></dt><dd class="glossdef"><p>
     The Ceph file system.
    </p></dd><dt id="id-1.3.7.3.8"><span><span class="glossterm">CephX</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.8">#</a></span></dt><dd class="glossdef"><p>
     The Ceph authentication protocol. Cephx operates like Kerberos, but it
     has no single point of failure.
    </p></dd><dt id="id-1.3.7.3.13"><span><span class="glossterm">CRUSH rule</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.13">#</a></span></dt><dd class="glossdef"><p>
     The CRUSH data placement rule that applies to a particular pool or pools.
    </p></dd><dt id="id-1.3.7.3.12"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.12">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.3.7.3.15"><span><span class="glossterm">DriveGroups</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.15">#</a></span></dt><dd class="glossdef"><p>
     DriveGroups are a declaration of one or more OSD layouts that can be mapped
     to physical drives. An OSD layout defines how Ceph physically allocates
     OSD storage on the media matching the specified criteria.
    </p></dd><dt id="id-1.3.7.3.16"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.16">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.3.7.3.17"><span><span class="glossterm">Metadata Server</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.17">#</a></span></dt><dd class="glossdef"><p>
     Metadata Server or MDS is the Ceph metadata software.
    </p></dd><dt id="id-1.3.7.3.36"><span><span class="glossterm">Multi-zone</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.36">#</a></span></dt><dd class="glossdef"/><dt id="id-1.3.7.3.20"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.20">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.3.7.3.25"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.25">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store. Also known as the
     RADOS Gateway (RGW).
    </p></dd><dt id="id-1.3.7.3.21"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.21">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Object Storage Device</em></span>: A physical or logical storage
     unit.
    </p></dd><dt id="id-1.3.7.3.23"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.23">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.3.7.3.26"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.26">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.3.7.3.27"><span><span class="glossterm">Point Release</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.27">#</a></span></dt><dd class="glossdef"><p>
     Any ad-hoc release that includes only bug or security fixes.
    </p></dd><dt id="id-1.3.7.3.28"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.28">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.3.7.3.29"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.29">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd><dt id="id-1.3.7.3.31"><span><span class="glossterm">RADOS Block Device (RBD)</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.31">#</a></span></dt><dd class="glossdef"><p>
     The block storage component of Ceph. Also known as the Ceph block
     device.
    </p></dd><dt id="id-1.3.7.3.30"><span><span class="glossterm">Reliable Autonomic Distributed Object Store (RADOS)</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.30">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data (MON+OSD).
    </p></dd><dt id="id-1.3.7.3.33"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.33">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.3.7.3.32"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.32">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd><dt id="id-1.3.7.3.34"><span><span class="glossterm">Samba</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.34">#</a></span></dt><dd class="glossdef"><p>
     Windows integration software.
    </p></dd><dt id="id-1.3.7.3.35"><span><span class="glossterm">Samba Gateway</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.35">#</a></span></dt><dd class="glossdef"><p>
     The Samba Gateway joins the Active Directory in the Windows domain to authenticate
     and authorize users.
    </p></dd><dt id="id-1.3.7.3.37"><span><span class="glossterm">zonegroup</span> <a title="Permalink" class="permalink" href="#id-1.3.7.3.37">#</a></span></dt><dd class="glossdef"/></dl></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-reportbug" href="#" rel="nofollow" target="_blank">Report an issue</a></li><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/book_storage_deployment.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>