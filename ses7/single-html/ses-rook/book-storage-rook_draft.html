<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/><title>Deploying and Administering SUSE Enterprise Storage with Rook | SUSE Enterprise Storage 7</title><link rel="stylesheet" type="text/css" href="static/css/style.css"/>
<meta name="title" content="Deploying and Administering SUSE Enterprise Storage wi…"/>
<meta name="description" content="Deployment of containerized Ceph clusters on SUSE CaaS Platform is released under limited availability for the SUSE Enterprise Storage 7 release. For…"/>
<meta name="product-name" content="SUSE Enterprise Storage"/>
<meta name="product-number" content="7"/>
<meta name="book-title" content="Deploying and Administering SUSE Enterprise Storage with Rook"/>
<meta name="tracker-url" content="https://github.com/SUSE/doc-ses/issues/new"/>
<meta name="tracker-type" content="gh"/>
<meta name="tracker-gh-assignee" content="tbazant"/>
<meta name="tracker-gh-template" content="&#10;     ### Type of report&#10;   - [ ] bug&#10;   - [ ] feature request&#10;&#10;### Source Document&#10;@@source@@&#10;&#10;### Summary&#10;&lt;!-- A clear and concise description of what the bug/feature request is. --&gt;&#10;&#10;### Steps To Reproduce&#10;&lt;!-- Steps to reproduce the behavior if you ran through steps in the document. --&gt;&#10;&#10;### Expected Results&#10;&lt;!-- A clear and concise description of what you expected to happen. --&gt;&#10;&#10;### Actual Results&#10;&lt;!-- Explain what actually happened if steps were executed, and if applicable, add screenshots to help explain your problem. --&gt;&#10;&#10;### Notes&#10;&lt;!-- Add any other context about the problem here. --&gt;&#10;&#10;    "/>
<meta name="tracker-gh-labels" content="bug,question,feature request"/>
<meta property="og:title" content="Deploying and Administering SUSE Enterprise Storage wi…"/>
<meta property="og:description" content="Deployment of containerized Ceph clusters on SUSE CaaS Platform is released under limited availability for the SUSE Enterprise Storage 7 release. For…"/>
<meta property="og:type" content="article"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deploying and Administering SUSE Enterprise Storage wi…"/>
<meta name="twitter:description" content="Deployment of containerized Ceph clusters on SUSE CaaS Platform is released under limited availability for the SUSE Enterprise Storage 7 release. For…"/>

<script type="text/javascript">

if ( window.location.protocol.toLowerCase() != 'file:' ) {
  document.write('<link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"></link>');
};

</script><noscript><link rel="stylesheet" type="text/css" href="https://documentation.suse.com/docserv/res/fonts/poppins/poppins.css"/></noscript><script src="static/js/jquery-1.12.4.min.js" type="text/javascript"> </script><script src="static/js/script.js" type="text/javascript"> </script><script src="static/js/highlight.min.js" type="text/javascript"> </script><script>

$(document).ready(function() {
  $('.verbatim-wrap.highlight').each(function(i, block) {
    hljs.highlightBlock(block);
  });
});
hljs.configure({
  useBR: false
});

</script></head><body class="draft single normal offline js-off" onload="$('#betawarn-button-wrap').toggle();if (document.cookie.length > 0) {if (document.cookie.indexOf('betawarn=closed') != -1){$('#betawarn').toggle()}};"><div id="betawarn" style="position:fixed;bottom:0;z-index:9025;background-color:#FDE8E8;padding:1em;margin-left:10%;margin-right:10%;display:block;border-top:.75em solid #E11;width:80%"><p style="color:#333;margin:1em 0;padding:0;">This is a draft document that was built and uploaded automatically. It may document beta software and be incomplete or even incorrect. <strong>Use this document at your own risk.</strong></p> <div id="betawarn-button-wrap" style="display:none;margin:0;padding:0;"><a href="#" onclick="$('#betawarn').toggle();var d=new Date();d.setTime(d.getTime()+(0.5*24*60*60*1000));document.cookie='betawarn=closed; expires='+d.toUTCString()+'; path=/'; return false;" style="color:#333;text-decoration:underline;float:left;margin-top:.5em;padding:1em;display:block;background-color:#FABEBE;">I understand this is a draft</a></div></div><div class="bypass-block"><a href="#_content">Jump to content</a></div><header id="_mainnav"><div class="growth-inhibitor"><img src="static/images/logo.svg" alt="Logo" class="logo"/></div></header><div class="crumbs"><div class="growth-inhibitor"><a class="crumb" href="#book-storage-rook">Deploying and Administering SUSE Enterprise Storage with Rook</a></div></div><main id="_content"><nav class="side-toc placebo" id="_side-toc-overall"> </nav><article class="documentation"><button id="_unfold-side-toc-page">On this page</button><section xml:lang="en" class="book" id="book-storage-rook" data-id-title="Deploying and Administering SUSE Enterprise Storage with Rook"><div class="titlepage"><div><div class="big-version-info"><span class="productname">SUSE Enterprise Storage</span> <span class="productnumber">7</span></div><div><h1 class="title">Deploying and Administering SUSE Enterprise Storage with Rook</h1></div><div class="abstract"><p>
    Deployment of containerized Ceph clusters on SUSE CaaS Platform is released under
    limited availability for the SUSE Enterprise Storage 7 release.
   </p><p>
    For more details about the SUSE CaaS Platform product, see
    <a class="link" href="https://documentation.suse.com/suse-caasp/4.5/" target="_blank">https://documentation.suse.com/suse-caasp/4.5/</a>.
   </p></div><div class="authorgroup"><div><span class="imprint-label">Authors: </span><span class="firstname">Tomáš</span> <span class="surname">Bažant</span>, <span class="firstname">Alexandra</span> <span class="surname">Settle</span>, and <span class="firstname">Liam</span> <span class="surname">Proven</span></div></div><div class="date"><span class="imprint-label">Publication Date: </span>12 Sep 2022</div></div></div><div class="toc"><ul><li><span class="preface"><a href="#preface-rook"><span class="title-name">About this guide</span></a></span><ul><li><span class="sect1"><a href="#id-1.7.2.7"><span class="title-name">Available documentation</span></a></span></li><li><span class="sect1"><a href="#id-1.7.2.8"><span class="title-name">Giving feedback</span></a></span></li><li><span class="sect1"><a href="#id-1.7.2.9"><span class="title-name">Documentation conventions</span></a></span></li><li><span class="sect1"><a href="#id-1.7.2.10"><span class="title-name">Support</span></a></span></li><li><span class="sect1"><a href="#id-1.7.2.11"><span class="title-name">Ceph contributors</span></a></span></li><li><span class="sect1"><a href="#id-1.7.2.12"><span class="title-name">Commands and command prompts used in this guide</span></a></span></li></ul></li><li><span class="part"><a href="#rook-ses-deployment"><span class="title-number">I </span><span class="title-name">Quick Start: Deploying Ceph on SUSE CaaS Platform</span></a></span><ul><li><span class="chapter"><a href="#deploy-rook"><span class="title-number">1 </span><span class="title-name">Quick start</span></a></span><ul><li><span class="sect1"><a href="#rook-deploy-hardware-specs"><span class="title-number">1.1 </span><span class="title-name">Recommended hardware specifications</span></a></span></li><li><span class="sect1"><a href="#rook-deploy-before-begin"><span class="title-number">1.2 </span><span class="title-name">Prerequisites</span></a></span></li><li><span class="sect1"><a href="#getting-started-rook"><span class="title-number">1.3 </span><span class="title-name">Getting started with Rook</span></a></span></li><li><span class="sect1"><a href="#rook-deploy-ceph"><span class="title-number">1.4 </span><span class="title-name">Deploying Ceph with Rook</span></a></span></li><li><span class="sect1"><a href="#rook-config-ceph"><span class="title-number">1.5 </span><span class="title-name">Configuring the Ceph cluster</span></a></span></li><li><span class="sect1"><a href="#updating-rook-images"><span class="title-number">1.6 </span><span class="title-name">Updating local images</span></a></span></li><li><span class="sect1"><a href="#uninstalling-rook"><span class="title-number">1.7 </span><span class="title-name">Uninstalling</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#rook-ses-admin"><span class="title-number">II </span><span class="title-name">Administrating Ceph on SUSE CaaS Platform</span></a></span><ul><li><span class="chapter"><a href="#admin-intro-caasp"><span class="title-number">2 </span><span class="title-name">Rook-Ceph administration</span></a></span></li><li><span class="chapter"><a href="#admin-caasp-cluster"><span class="title-number">3 </span><span class="title-name">Ceph cluster administration</span></a></span><ul><li><span class="sect1"><a href="#admin-caasp-cluster-shutdown"><span class="title-number">3.1 </span><span class="title-name">Shutting down and restarting the cluster</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-block-storage"><span class="title-number">4 </span><span class="title-name">Block Storage</span></a></span><ul><li><span class="sect1"><a href="#rook-block-storage-provision-storage"><span class="title-number">4.1 </span><span class="title-name">Provisioning Block Storage</span></a></span></li><li><span class="sect1"><a href="#consume-the-storage-wordpress-sample"><span class="title-number">4.2 </span><span class="title-name">Consuming storage: WordPress sample</span></a></span></li><li><span class="sect1"><a href="#consume-the-storage-toolbox"><span class="title-number">4.3 </span><span class="title-name">Consuming the storage: Toolbox</span></a></span></li><li><span class="sect1"><a href="#rook-block-storage-teardown"><span class="title-number">4.4 </span><span class="title-name">Teardown</span></a></span></li><li><span class="sect1"><a href="#advanced-example-erasure-coded-block-storage"><span class="title-number">4.5 </span><span class="title-name">Advanced Example: Erasure-Coded Block Storage</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-cephfs"><span class="title-number">5 </span><span class="title-name">CephFS</span></a></span><ul><li><span class="sect1"><a href="#rook-shared-filesystem"><span class="title-number">5.1 </span><span class="title-name">Shared File System</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-crd"><span class="title-number">6 </span><span class="title-name">Ceph cluster custom resource definitions</span></a></span><ul><li><span class="sect1"><a href="#rook-ceph-cluster-crd"><span class="title-number">6.1 </span><span class="title-name">Ceph cluster CRD</span></a></span></li><li><span class="sect1"><a href="#rook-ceph-block-pool-crd"><span class="title-number">6.2 </span><span class="title-name">Ceph block pool CRD</span></a></span></li><li><span class="sect1"><a href="#rook-ceph-shared-filesystem-crd"><span class="title-number">6.3 </span><span class="title-name">Ceph shared file system CRD</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-cephconfig"><span class="title-number">7 </span><span class="title-name">Configuration</span></a></span><ul><li><span class="sect1"><a href="#configuration"><span class="title-number">7.1 </span><span class="title-name">Ceph configuration</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-cephtoolbox"><span class="title-number">8 </span><span class="title-name">Toolboxes</span></a></span><ul><li><span class="sect1"><a href="#rook-toolbox"><span class="title-number">8.1 </span><span class="title-name">Rook toolbox</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-cephosd"><span class="title-number">9 </span><span class="title-name">Ceph OSD management</span></a></span><ul><li><span class="sect1"><a href="#ceph-osd-management"><span class="title-number">9.1 </span><span class="title-name">Ceph OSD management</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-ceph-examples"><span class="title-number">10 </span><span class="title-name">Ceph examples</span></a></span><ul><li><span class="sect1"><a href="#ceph-examples"><span class="title-number">10.1 </span><span class="title-name">Ceph examples</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-advanced-config"><span class="title-number">11 </span><span class="title-name">Advanced configuration</span></a></span><ul><li><span class="sect1"><a href="#advanced-configuration"><span class="title-number">11.1 </span><span class="title-name">Performing advanced configuration tasks</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-object-storage"><span class="title-number">12 </span><span class="title-name">Object Storage</span></a></span><ul><li><span class="sect1"><a href="#rook-object-storage"><span class="title-number">12.1 </span><span class="title-name">Object Storage</span></a></span></li><li><span class="sect1"><a href="#ceph-object-store-crd"><span class="title-number">12.2 </span><span class="title-name">Ceph Object Storage CRD</span></a></span></li><li><span class="sect1"><a href="#ceph-object-bucket-claim"><span class="title-number">12.3 </span><span class="title-name">Ceph object bucket claim</span></a></span></li><li><span class="sect1"><a href="#ceph-object-store-user-crd"><span class="title-number">12.4 </span><span class="title-name">Ceph Object Storage user custom resource definitions (CRD)</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-dashboard"><span class="title-number">13 </span><span class="title-name">Ceph Dashboard</span></a></span><ul><li><span class="sect1"><a href="#caasp-ceph-dashboard"><span class="title-number">13.1 </span><span class="title-name">Ceph Dashboard</span></a></span></li></ul></li></ul></li><li><span class="part"><a href="#rook-ses-troubleshooting"><span class="title-number">III </span><span class="title-name">Troubleshooting Ceph on SUSE CaaS Platform</span></a></span><ul><li><span class="chapter"><a href="#atroubleshooting-caasp-debugging-rook"><span class="title-number">14 </span><span class="title-name">Troubleshooting</span></a></span><ul><li><span class="sect1"><a href="#debugging-caasp-rook-methods"><span class="title-number">14.1 </span><span class="title-name">Debugging Rook</span></a></span></li></ul></li><li><span class="chapter"><a href="#admin-caasp-ceph-common-issues"><span class="title-number">15 </span><span class="title-name">Common issues</span></a></span><ul><li><span class="sect1"><a href="#ceph-common-issues"><span class="title-number">15.1 </span><span class="title-name">Ceph common issues</span></a></span></li></ul></li></ul></li><li><span class="appendix"><a href="#id-1.7.6"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span></a></span></li><li><span class="glossary"><a href="#id-1.7.7"><span class="title-name">Glossary</span></a></span></li></ul></div><div class="list-of-figures"><div class="toc-title">List of Figures</div><ul><li><span class="figure"><a href="#id-1.7.4.13.3.3"><span class="number">13.1 </span><span class="name">The Ceph Dashboard</span></a></span></li></ul></div><div><div xml:lang="en" class="legalnotice" id="id-1.7.1.6"><p>
  Copyright © 2020–2022

  SUSE LLC and contributors. All rights reserved.
 </p><p>
  Except where otherwise noted, this document is licensed under Creative Commons Attribution-ShareAlike 4.0 International
  (CC-BY-SA 4.0): <a class="link" href="https://creativecommons.org/licenses/by-sa/4.0/legalcode" target="_blank">https://creativecommons.org/licenses/by-sa/4.0/legalcode</a>.
 </p><p>
  For SUSE trademarks, see
  <a class="link" href="http://www.suse.com/company/legal/" target="_blank">http://www.suse.com/company/legal/</a>. All
  third-party trademarks are the property of their respective owners. Trademark
  symbols (®, ™ etc.) denote trademarks of SUSE and its affiliates.
  Asterisks (*) denote third-party trademarks.
 </p><p>
  All information found in this book has been compiled with utmost attention to
  detail. However, this does not guarantee complete accuracy. Neither
  SUSE LLC, its affiliates, the authors nor the translators shall be
  held liable for possible errors or the consequences thereof.
 </p></div></div><section xml:lang="en" class="preface" id="preface-rook" data-id-title="About this guide"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">About this guide</span> <a title="Permalink" class="permalink" href="#preface-rook">#</a></h1></div></div></div><p>
  Part of the SUSE Enterprise Storage family is the Rook deployment tool, which runs on
  SUSE CaaS Platform. Rook allows you to deploy and run Ceph on top of Kubernetes, in
  order to provide container workloads with all their storage needs.
 </p><p>
  Deployment using Rook is currently in <span class="emphasis"><em>limited
  availability</em></span>, meaning that it is only available to nominated and
  approved customers. For information on how to get so nominated, contact your
  SUSE sales team.
 </p><p>
  Rook is a so-called storage operator: it automates many steps that you need
  to do manually in a "traditional" setup with cephadm. This guide explains
  how to install Rook after you installed SUSE CaaS Platform, and how to administer it.
 </p><p>
 SUSE Enterprise Storage 7 is an extension to SUSE Linux Enterprise Server 15 SP2. It combines the
 capabilities of the Ceph
 (<a class="link" href="http://ceph.com/" target="_blank">http://ceph.com/</a>)
 storage project with the enterprise engineering and support of SUSE.
 SUSE Enterprise Storage 7 provides IT organizations with the ability to
 deploy a distributed storage architecture that can support a number of use
 cases using commodity hardware platforms.
</p><section xml:lang="en" class="sect1" id="id-1.7.2.7" data-id-title="Available documentation"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1 </span><span class="title-name">Available documentation</span> <a title="Permalink" class="permalink" href="#id-1.7.2.7">#</a></h2></div></div></div><div id="id-1.7.2.7.3" data-id-title="Online documentation and latest updates" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Online documentation and latest updates</h6><p>
   Documentation for our products is available at
   <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>,
   where you can also find the latest updates, and browse or download the
   documentation in various formats. The latest documentation updates can be
   found in the English language version.
  </p></div><p>
  In addition, the product documentation is available in your installed system
  under <code class="filename">/usr/share/doc/manual</code>. It is included in an RPM
  package named
  <span class="package">ses-manual_<em class="replaceable">LANG_CODE</em></span>. Install
  it if it is not already on your system, for example:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper install ses-manual_en</pre></div><p>
  The following documentation is available for this product:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.2.7.7.1"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-deployment.html" target="_blank"><em class="citetitle">Deployment Guide</em></a></span></dt><dd><p>
     This guide focuses on deploying a basic Ceph cluster, and how to deploy
     additional services. It also cover the steps for upgrading to
     SUSE Enterprise Storage 7 from the previous product version.
    </p></dd><dt id="id-1.7.2.7.7.2"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-admin.html" target="_blank"><em class="citetitle">Administration and Operations Guide</em></a></span></dt><dd><p>
     This guide focuses on routine tasks that you as an administrator need to
     take care of after the basic Ceph cluster has been deployed (day 2
     operations). It also describes all the supported ways to access data
     stored in a Ceph cluster.
    </p></dd><dt id="id-1.7.2.7.7.3"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-security.html" target="_blank"><em class="citetitle">Security Hardening Guide</em></a></span></dt><dd><p>
     This guide focuses on how to ensure your cluster is secure.
    </p></dd><dt id="id-1.7.2.7.7.4"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-troubleshooting.html" target="_blank"><em class="citetitle">Troubleshooting Guide</em></a></span></dt><dd><p>
     This guide takes you through various common problems when running
     SUSE Enterprise Storage 7 and other related issues to relevant
     components such as Ceph or Object Gateway.
    </p></dd><dt id="id-1.7.2.7.7.5"><span class="term"><a class="link" href="https://documentation.suse.com/ses/html/ses-all/book-storage-windows.html" target="_blank"><em class="citetitle">SUSE Enterprise Storage for Windows Guide</em></a></span></dt><dd><p>
     This guide describes the integration, installation, and configuration of
     Microsoft Windows environments and SUSE Enterprise Storage using the Windows Driver.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.7.2.8" data-id-title="Giving feedback"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">2 </span><span class="title-name">Giving feedback</span> <a title="Permalink" class="permalink" href="#id-1.7.2.8">#</a></h2></div></div></div><p>
  We welcome feedback on, and contributions to, this documentation.
  There are several channels for this:
 </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.2.8.3.1"><span class="term">Service requests and support</span></dt><dd><p>
     For services and support options available for your product, see
     <a class="link" href="http://www.suse.com/support/" target="_blank">http://www.suse.com/support/</a>.
    </p><p>
     To open a service request, you need a SUSE subscription registered at
     SUSE Customer Center.
     Go to <a class="link" href="https://scc.suse.com/support/requests" target="_blank">https://scc.suse.com/support/requests</a>, log
     in, and click <span class="guimenu">Create New</span>.
    </p></dd><dt id="id-1.7.2.8.3.2"><span class="term">Bug reports</span></dt><dd><p>
     Report issues with the documentation at <a class="link" href="https://bugzilla.suse.com/" target="_blank">https://bugzilla.suse.com/</a>.
     Reporting issues requires a Bugzilla account.
    </p><p>
     To simplify this process, you can use the <span class="guimenu">Report
     Documentation Bug</span> links next to headlines in the HTML
     version of this document. These preselect the right product and
     category in Bugzilla and add a link to the current section.
     You can start typing your bug report right away.
    </p></dd><dt id="id-1.7.2.8.3.3"><span class="term">Contributions</span></dt><dd><p>
     To contribute to this documentation, use the <span class="guimenu">Edit Source</span>
     links next to headlines in the HTML version of this document. They
     take you to the source code on GitHub, where you can open a pull request.
     Contributing requires a GitHub account.
    </p><p>
     For more information about the documentation environment used for this
     documentation, see the repository's README at
     <a class="link" href="https://github.com/SUSE/doc-ses" target="_blank">https://github.com/SUSE/doc-ses</a>.
    </p></dd><dt id="id-1.7.2.8.3.4"><span class="term">Mail</span></dt><dd><p>
     You can also report errors and send feedback concerning the
     documentation to &lt;<a class="email" href="mailto:doc-team@suse.com">doc-team@suse.com</a>&gt;. Include the
     document title, the product version, and the publication date of the
     document. Additionally, include the relevant section number and title (or
     provide the URL) and provide a concise description of the problem.
    </p></dd></dl></div></section><section xml:lang="en" class="sect1" id="id-1.7.2.9" data-id-title="Documentation conventions"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3 </span><span class="title-name">Documentation conventions</span> <a title="Permalink" class="permalink" href="#id-1.7.2.9">#</a></h2></div></div></div><p>
  The following notices and typographic conventions are used in this
  document:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    <code class="filename">/etc/passwd</code>: Directory names and file names
   </p></li><li class="listitem"><p>
    <em class="replaceable">PLACEHOLDER</em>: Replace
    <em class="replaceable">PLACEHOLDER</em> with the actual value
   </p></li><li class="listitem"><p>
    <code class="envar">PATH</code>: An environment variable
   </p></li><li class="listitem"><p>
    <code class="command">ls</code>, <code class="option">--help</code>: Commands, options, and
    parameters
   </p></li><li class="listitem"><p>
    <code class="systemitem">user</code>: The name of user or group
   </p></li><li class="listitem"><p>
    <span class="package">package_name</span>: The name of a software package
   </p></li><li class="listitem"><p>
    <span class="keycap">Alt</span>, <span class="keycap">Alt</span><span class="key-connector">–</span><span class="keycap">F1</span>: A key to press or a key combination. Keys
    are shown in uppercase as on a keyboard.
   </p></li><li class="listitem"><p>
    <span class="guimenu">File</span>, <span class="guimenu">File</span> › <span class="guimenu">Save
    As</span>: menu items, buttons
   </p></li><li class="listitem"><p><strong class="arch-arrow-start">AMD/Intel</strong>
    This paragraph is only relevant for the AMD64/Intel 64 architectures. The
    arrows mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p><p><strong class="arch-arrow-start">IBM Z, POWER</strong>
    This paragraph is only relevant for the architectures
    <code class="literal">IBM Z</code> and <code class="literal">POWER</code>. The arrows
    mark the beginning and the end of the text block.
   <strong class="arch-arrow-end"> </strong></p></li><li class="listitem"><p>
    <em class="citetitle">Chapter 1, <span class="quote">“<span class="quote">Example chapter</span>”</span></em>:
    A cross-reference to another chapter in this guide.
   </p></li><li class="listitem"><p>
    Commands that must be run with <code class="systemitem">root</code> privileges. Often you can also
    prefix these commands with the <code class="command">sudo</code> command to run them
    as non-privileged user.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code><code class="command">command</code>
<code class="prompt user">&gt; </code><code class="command">sudo</code> <code class="command">command</code></pre></div></li><li class="listitem"><p>
    Commands that can be run by non-privileged users.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code><code class="command">command</code></pre></div></li><li class="listitem"><p>
    Notices
   </p><div id="id-1.7.2.9.4.13.2" data-id-title="Warning notice" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning: Warning notice</h6><p>
     Vital information you must be aware of before proceeding. Warns you about
     security issues, potential loss of data, damage to hardware, or physical
     hazards.
    </p></div><div id="id-1.7.2.9.4.13.3" data-id-title="Important notice" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important: Important notice</h6><p>
     Important information you should be aware of before proceeding.
    </p></div><div id="id-1.7.2.9.4.13.4" data-id-title="Note notice" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note: Note notice</h6><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.7.2.9.4.13.5" data-id-title="Tip notice" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Tip notice</h6><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li><li class="listitem"><p>
    Compact Notices
   </p><div id="id-1.7.2.9.4.14.2" class="admonition note compact"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><p>
     Additional information, for example about differences in software
     versions.
    </p></div><div id="id-1.7.2.9.4.14.3" class="admonition tip compact"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><p>
     Helpful information, like a guideline or a piece of practical advice.
    </p></div></li></ul></div></section><section xml:lang="en" class="sect1" id="id-1.7.2.10" data-id-title="Support"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4 </span><span class="title-name">Support</span> <a title="Permalink" class="permalink" href="#id-1.7.2.10">#</a></h2></div></div></div><p>
  Find the support statement for SUSE Enterprise Storage and general information about
  technology previews below.
  For details about the product lifecycle, see
  <a class="link" href="https://www.suse.com/lifecycle" target="_blank">https://www.suse.com/lifecycle</a>.
 </p><p>
  If you are entitled to support, find details on how to collect information
  for a support ticket at
  <a class="link" href="https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html" target="_blank">https://documentation.suse.com/sles-15/html/SLES-all/cha-adm-support.html</a>.
 </p><section class="sect2" id="id-1.7.2.10.4" data-id-title="Support statement for SUSE Enterprise Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.1 </span><span class="title-name">Support statement for SUSE Enterprise Storage</span> <a title="Permalink" class="permalink" href="#id-1.7.2.10.4">#</a></h3></div></div></div><p>
   To receive support, you need an appropriate subscription with SUSE.
   To view the specific support offerings available to you, go to
   <a class="link" href="https://www.suse.com/support/" target="_blank">https://www.suse.com/support/</a> and select your product.
  </p><p>
   The support levels are defined as follows:
  </p><div class="variablelist"><dl class="variablelist"><dt id="id-1.7.2.10.4.4.1"><span class="term">L1</span></dt><dd><p>
      Problem determination, which means technical support designed to provide
      compatibility information, usage support, ongoing maintenance,
      information gathering and basic troubleshooting using available
      documentation.
     </p></dd><dt id="id-1.7.2.10.4.4.2"><span class="term">L2</span></dt><dd><p>
      Problem isolation, which means technical support designed to analyze
      data, reproduce customer problems, isolate problem area and provide a
      resolution for problems not resolved by Level 1 or prepare for
      Level 3.
     </p></dd><dt id="id-1.7.2.10.4.4.3"><span class="term">L3</span></dt><dd><p>
      Problem resolution, which means technical support designed to resolve
      problems by engaging engineering to resolve product defects which have
      been identified by Level 2 Support.
     </p></dd></dl></div><p>
   For contracted customers and partners, SUSE Enterprise Storage is delivered with L3
   support for all packages, except for the following:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews.
    </p></li><li class="listitem"><p>
     Sound, graphics, fonts, and artwork.
    </p></li><li class="listitem"><p>
     Packages that require an additional customer contract.
    </p></li><li class="listitem"><p>
     Some packages shipped as part of the module <span class="emphasis"><em>Workstation
     Extension</em></span> are L2-supported only.
    </p></li><li class="listitem"><p>
     Packages with names ending in <span class="package">-devel</span> (containing header
     files and similar developer resources) will only be supported together
     with their main packages.
    </p></li></ul></div><p>
   SUSE will only support the usage of original packages.
   That is, packages that are unchanged and not recompiled.
  </p></section><section class="sect2" id="id-1.7.2.10.5" data-id-title="Technology previews"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.2 </span><span class="title-name">Technology previews</span> <a title="Permalink" class="permalink" href="#id-1.7.2.10.5">#</a></h3></div></div></div><p>
   Technology previews are packages, stacks, or features delivered by SUSE
   to provide glimpses into upcoming innovations.
   Technology previews are included for your convenience to give you a chance
   to test new technologies within your environment.
   We would appreciate your feedback!
   If you test a technology preview, please contact your SUSE representative
   and let them know about your experience and use cases.
   Your input is helpful for future development.
  </p><p>
   Technology previews have the following limitations:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Technology previews are still in development.
     Therefore, they may be functionally incomplete, unstable, or in other ways
     <span class="emphasis"><em>not</em></span> suitable for production use.
    </p></li><li class="listitem"><p>
     Technology previews are <span class="emphasis"><em>not</em></span> supported.
    </p></li><li class="listitem"><p>
     Technology previews may only be available for specific hardware
     architectures.
    </p></li><li class="listitem"><p>
     Details and functionality of technology previews are subject to change.
     As a result, upgrading to subsequent releases of a technology preview may
     be impossible and require a fresh installation.
    </p></li><li class="listitem"><p>
     SUSE may discover that a preview does not meet customer or market needs,
     or does not comply with enterprise standards.
     Technology previews can be removed from a product at any time.
     SUSE does not commit to providing a supported version of such
     technologies in the future.
    </p></li></ul></div><p>
   For an overview of technology previews shipped with your product, see the
   release notes at <a class="link" href="https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7" target="_blank">https://www.suse.com/releasenotes/x86_64/SUSE-Enterprise-Storage/7</a>.
  </p></section></section><section class="sect1" id="id-1.7.2.11" data-id-title="Ceph contributors"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5 </span><span class="title-name">Ceph contributors</span> <a title="Permalink" class="permalink" href="#id-1.7.2.11">#</a></h2></div></div></div><p>
  The Ceph project and its documentation is a result of the work of hundreds
  of contributors and organizations. See
  <a class="link" href="https://ceph.com/contributors/" target="_blank">https://ceph.com/contributors/</a> for more details.
 </p></section><section class="sect1" id="id-1.7.2.12" data-id-title="Commands and command prompts used in this guide"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6 </span><span class="title-name">Commands and command prompts used in this guide</span> <a title="Permalink" class="permalink" href="#id-1.7.2.12">#</a></h2></div></div></div><p>
  As a Ceph cluster administrator, you will be configuring and adjusting the
  cluster behavior by running specific commands. There are several types of
  commands you will need:
 </p><section class="sect2" id="id-1.7.2.12.4" data-id-title="Salt-related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1 </span><span class="title-name">Salt-related commands</span> <a title="Permalink" class="permalink" href="#id-1.7.2.12.4">#</a></h3></div></div></div><p>
   These commands help you to deploy Ceph cluster nodes, run commands on
   several (or all) cluster nodes at the same time, or assist you when adding
   or removing cluster nodes. The most frequently used commands are
   <code class="command">ceph-salt</code> and <code class="command">ceph-salt config</code>. You
   need to run Salt commands on the Salt Master node as <code class="systemitem">root</code>. These
   commands are introduced with the following prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">root@master # </code>ceph-salt config ls</pre></div></section><section class="sect2" id="id-1.7.2.12.5" data-id-title="Ceph related commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2 </span><span class="title-name">Ceph related commands</span> <a title="Permalink" class="permalink" href="#id-1.7.2.12.5">#</a></h3></div></div></div><p>
   These are lower-level commands to configure and fine tune all aspects of the
   cluster and its gateways on the command line, for example
   <code class="command">ceph</code>, <code class="command">cephadm</code>, <code class="command">rbd</code>,
   or <code class="command">radosgw-admin</code>.
  </p><p>
   To run Ceph related commands, you need to have read access to a Ceph
   key. The key's capabilities then define your privileges within the Ceph
   environment. One option is to run Ceph commands as <code class="systemitem">root</code> (or via
   <code class="command">sudo</code>) and use the unrestricted default keyring
   'ceph.client.admin.key'.
  </p><p>
   The safer and recommended option is to create a more restrictive individual
   key for each administrator user and put it in a directory where the users
   can read it, for example:
  </p><div class="verbatim-wrap"><pre class="screen">~/.ceph/ceph.client.<em class="replaceable">USERNAME</em>.keyring</pre></div><div id="id-1.7.2.12.5.6" data-id-title="Path to Ceph keys" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Path to Ceph keys</h6><p>
    To use a custom admin user and keyring, you need to specify the user name
    and path to the key each time you run the <code class="command">ceph</code> command
    using the <code class="option">-n client.<em class="replaceable">USER_NAME</em></code>
    and <code class="option">--keyring <em class="replaceable">PATH/TO/KEYRING</em></code>
    options.
   </p><p>
    To avoid this, include these options in the <code class="varname">CEPH_ARGS</code>
    variable in the individual users' <code class="filename">~/.bashrc</code> files.
   </p></div><p>
   Although you can run Ceph-related commands on any cluster node, we
   recommend running them on the Admin Node. This documentation uses the <code class="systemitem">cephuser</code>
   user to run the commands, therefore they are introduced with the following
   prompt:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code></pre></div><p>
   For example:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth list</pre></div><div id="id-1.7.2.12.5.11" data-id-title="Commands for specific nodes" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip: Commands for specific nodes</h6><p>
    If the documentation instructs you to run a command on a cluster node with
    a specific role, it will be addressed by the prompt. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@mon &gt; </code></pre></div></div><section class="sect3" id="id-1.7.2.12.5.12" data-id-title="Running ceph-volume"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Running <code class="command">ceph-volume</code></span> <a title="Permalink" class="permalink" href="#id-1.7.2.12.5.12">#</a></h4></div></div></div><p>
    Starting with SUSE Enterprise Storage 7, Ceph services are running containerized.
    If you need to run <code class="command">ceph-volume</code> on an OSD node, you need
    to prepend it with the <code class="command">cephadm</code> command, for example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>cephadm ceph-volume simple scan</pre></div></section></section><section class="sect2" id="id-1.7.2.12.6" data-id-title="General Linux commands"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3 </span><span class="title-name">General Linux commands</span> <a title="Permalink" class="permalink" href="#id-1.7.2.12.6">#</a></h3></div></div></div><p>
   Linux commands not related to Ceph, such as <code class="command">mount</code>,
   <code class="command">cat</code>, or <code class="command">openssl</code>, are introduced either
   with the <code class="prompt user">cephuser@adm &gt; </code> or <code class="prompt root"># </code> prompts, depending on which
   privileges the related command requires.
  </p></section><section class="sect2" id="id-1.7.2.12.7" data-id-title="Additional information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.4 </span><span class="title-name">Additional information</span> <a title="Permalink" class="permalink" href="#id-1.7.2.12.7">#</a></h3></div></div></div><p>
   For more information on Ceph key management, refer to
   <span class="intraxref">Book “Administration and Operations Guide”, Chapter 30 “Authentication with <code class="systemitem">cephx</code>”, Section 30.2 “Key management”</span>.
  </p></section></section></section><div class="part" id="rook-ses-deployment" data-id-title="Quick Start: Deploying Ceph on SUSE CaaS Platform"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part I </span><span class="title-name">Quick Start: Deploying Ceph on SUSE CaaS Platform </span><a title="Permalink" class="permalink" href="#rook-ses-deployment">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#deploy-rook"><span class="title-number">1 </span><span class="title-name">Quick start</span></a></span></li><dd class="toc-abstract"><p>
  SUSE Enterprise Storage is a distributed storage system designed for scalability,
  reliability, and performance, which is based on the Ceph technology. The
  traditional way to run a Ceph cluster is setting up a dedicated cluster to
  provide block, file, and object storage to a variety of clients.
 </p></dd></ul></div><section class="chapter" id="deploy-rook" data-id-title="Quick start"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">1 </span><span class="title-name">Quick start</span> <a title="Permalink" class="permalink" href="#deploy-rook">#</a></h1></div></div></div><p>
  SUSE Enterprise Storage is a distributed storage system designed for scalability,
  reliability, and performance, which is based on the Ceph technology. The
  traditional way to run a Ceph cluster is setting up a dedicated cluster to
  provide block, file, and object storage to a variety of clients.
 </p><p>
  Rook manages Ceph as a containerized application on Kubernetes and allows a
  hyper-converged setup, in which a single Kubernetes cluster runs applications and
  storage together. The primary purpose of SUSE Enterprise Storage deployed with Rook
  is to provide storage to other applications running in the Kubernetes cluster.
  This can be block, file, or object storage.
 </p><p>
  This chapter describes how to quickly deploy containerized SUSE Enterprise Storage
  7 on top of a SUSE CaaS Platform 4.5 Kubernetes cluster.
 </p><section class="sect1" id="rook-deploy-hardware-specs" data-id-title="Recommended hardware specifications"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.1 </span><span class="title-name">Recommended hardware specifications</span> <a title="Permalink" class="permalink" href="#rook-deploy-hardware-specs">#</a></h2></div></div></div><p>
   For SUSE Enterprise Storage deployed with Rook, the minimal configuration is
   preliminary, we will update it based on real customer needs.
   
  </p><p>
   For the purpose of this document, consider the following minimum
   configuration:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     A highly available Kubernetes cluster with 3 master nodes
    </p></li><li class="listitem"><p>
     Four physical Kubernetes worker nodes, each with two OSD disks and 5GB of RAM
     per OSD disk
    </p></li><li class="listitem"><p>
     Allow additional 4GB of RAM per additional daemon deployed on a node
    </p></li><li class="listitem"><p>
     Dual-10 Gb ethernet as bonded network
    </p></li><li class="listitem"><p>
     If you are running a hyper-converged infrastructure (HCI), ensure you add
     any additional requirements for your workloads.
    </p></li></ul></div></section><section class="sect1" id="rook-deploy-before-begin" data-id-title="Prerequisites"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.2 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#rook-deploy-before-begin">#</a></h2></div></div></div><p>
   Ensure the following prerequisites are met before continuing with this
   quickstart guide:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     Installation of SUSE CaaS Platform 4.5. See the SUSE CaaS Platform documentation for more
     details on how to install:
     <a class="link" href="https://documentation.suse.com/en-us/suse-caasp/4.5/" target="_blank">https://documentation.suse.com/en-us/suse-caasp/4.5/</a>.
    </p></li><li class="listitem"><p>
     Ensure <code class="literal">ceph-csi</code> (and required sidecars) are running in
     your Kubernetes cluster.
    </p></li><li class="listitem"><p>
     Installation of the LVM2 package on the host where the OSDs are running.
    </p></li><li class="listitem"><p>
     Ensure you have one of the following storage options to configure Ceph
     properly:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Raw devices (no partitions or formatted file systems)
      </p></li><li class="listitem"><p>
       Raw partitions (no formatted file system)
      </p></li></ul></div></li><li class="listitem"><p>
     Ensure the SUSE CaaS Platform 4.5 repository is enabled for the installation of Helm
     3.
    </p></li></ul></div></section><section class="sect1" id="getting-started-rook" data-id-title="Getting started with Rook"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.3 </span><span class="title-name">Getting started with Rook</span> <a title="Permalink" class="permalink" href="#getting-started-rook">#</a></h2></div></div></div><div id="id-1.7.3.2.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    The following instructions are designed for a quick start deployment only.
    For more information on installing Helm, see
    <a class="link" href="https://documentation.suse.com/en-us/suse-caasp/4.5/" target="_blank">https://documentation.suse.com/en-us/suse-caasp/4.5/</a>.
   </p></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Install Helm v3:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code>zypper in helm3</pre></div></li><li class="step"><p>
     On a node with access to the Kubernetes cluster, execute the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>export HELM_EXPERIMENTAL_OCI=1</pre></div></li><li class="step"><p>
     Create a local copy of the Helm chart to your local registry:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 chart pull registry.suse.com/ses/7/charts/rook-ceph:latest</pre></div></li><li class="step"><p>
     Export the Helm charts to a Rook-Ceph sub-directory under your current
     working directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 chart export registry.suse.com/ses/7/charts/rook-ceph:latest</pre></div></li><li class="step"><p>
     Create a file named <code class="filename">myvalues.yaml</code> based off the
     <code class="filename">rook-ceph/values.yaml file</code>.
    </p></li><li class="step"><p>
     Set local parameters in <code class="filename">myvalues.yaml</code>.
    </p></li><li class="step"><p>
     Create the namespace:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create namespace rook-ceph</pre></div></li><li class="step"><p>
     Install the helm charts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 install -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</pre></div></li><li class="step"><p>
     Verify the <code class="literal">rook-operator</code> is running:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-operator</pre></div></li></ol></div></div></section><section class="sect1" id="rook-deploy-ceph" data-id-title="Deploying Ceph with Rook"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.4 </span><span class="title-name">Deploying Ceph with Rook</span> <a title="Permalink" class="permalink" href="#rook-deploy-ceph">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     You need to apply labels to your Kubernetes nodes before deploying your Ceph
     cluster. The key <code class="literal">node-role.rook-ceph/cluster</code> accepts
     one of the following values:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">any</code>
      </p></li><li class="listitem"><p>
       <code class="literal">mon</code>
      </p></li><li class="listitem"><p>
       <code class="literal">mon-mgr</code>
      </p></li><li class="listitem"><p>
       <code class="literal">mon-mgr-osd</code>
      </p></li></ul></div><p>
     Run the following the get the names of your cluster's nodes:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl get nodes</pre></div></li><li class="step"><p>
     On the Master node, run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl label nodes <em class="replaceable">node-name</em> <em class="replaceable">label-key</em>=<em class="replaceable">label-value</em></pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl label node <em class="replaceable">k8s-worker-node-1</em> <em class="replaceable">node-role.rook-ceph/cluster</em>=<em class="replaceable">any</em></pre></div></li><li class="step"><p>
     Verify the application of the label by re-running the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl get nodes --show-labels</pre></div><p>
     You can also use the <code class="command">describe</code> command to get the full
     list of labels given to the node. For example:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl describe node <em class="replaceable">node-name</em></pre></div></li><li class="step"><p>
     Next, you need to apply a Ceph cluster manifest file, for example,
     <code class="filename">cluster.yaml</code>, to your Kubernetes cluster. You can apply
     the example <code class="filename">cluster.yaml</code> as is without any additional
     services or requirements from the Rook Helm chart.
    </p><p>
     To apply the example Ceph cluster manifest to your Kubernetes cluster, run
     the following command:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>kubectl create -f rook-ceph/examples/cluster.yaml</pre></div></li></ol></div></div></section><section class="sect1" id="rook-config-ceph" data-id-title="Configuring the Ceph cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.5 </span><span class="title-name">Configuring the Ceph cluster</span> <a title="Permalink" class="permalink" href="#rook-config-ceph">#</a></h2></div></div></div><p>
   You can have two types of integration with your SUSE Enterprise Storage intregrated
   cluster. These types are: CephFS or RADOS Block Device (RBD).
  </p><p>
   Before you start the SUSE CaaS Platform and SUSE Enterprise Storage integration, ensure you have
   met the following prerequisites:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     The SUSE CaaS Platform cluster must have <code class="literal">ceph-common</code> and
     <code class="literal">xfsprogs</code> installed on all nodes. You can check this by
     running the <code class="command">rpm -q ceph-common</code> command or the
     <code class="command">rpm -q xfsprogs</code> command.
    </p></li><li class="listitem"><p>
     That the SUSE Enterprise Storage cluster has a pool with a RBD device or CephFS
     enabled.
    </p></li></ul></div><section class="sect2" id="config-cephfs" data-id-title="Configure CephFS"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.5.1 </span><span class="title-name">Configure CephFS</span> <a title="Permalink" class="permalink" href="#config-cephfs">#</a></h3></div></div></div><p>
    For more information on configuring CephFS, see
    <a class="link" href="https://documentation.suse.com/en-us/suse-caasp/4.5/" target="_blank">https://documentation.suse.com/en-us/suse-caasp/4.5/</a>
    for steps and more information. This section will also provide the
    necessary procedure on attaching a pod to either an CephFS static or
    dynamic volume.
   </p></section><section class="sect2" id="config-persistent-volumes" data-id-title="Configure RADOS block device"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">1.5.2 </span><span class="title-name">Configure RADOS block device</span> <a title="Permalink" class="permalink" href="#config-persistent-volumes">#</a></h3></div></div></div><p>
    For instructions on configuring the RADOS Block Device (RBD) in a pod,
    see
    <a class="link" href="https://documentation.suse.com/en-us/suse-caasp/4.5/" target="_blank">https://documentation.suse.com/en-us/suse-caasp/4.5/</a>
    for more information. This section will also provide the necessary
    procedure on attaching a pod to either an RBD static or dynamic volume.
   </p></section></section><section class="sect1" id="updating-rook-images" data-id-title="Updating local images"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.6 </span><span class="title-name">Updating local images</span> <a title="Permalink" class="permalink" href="#updating-rook-images">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     To update your local image to the latest tag, apply the new parameters in
     <code class="filename">myvalues.yaml</code>:
    </p><div class="verbatim-wrap"><pre class="screen">image:
refix: rook
repository: registry.suse.com/ses/7/rook/ceph
tag: <em class="replaceable">LATEST_TAG</em>
pullPolicy: IfNotPresent</pre></div></li><li class="step"><p>
     Re-pull a new local copy of the Helm chart to your local registry:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 chart pull <em class="replaceable">REGISTRY_URL</em></pre></div></li><li class="step"><p>
     Export the Helm charts to a Rook-Ceph sub-directory under your current
     working directory:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 chart export <em class="replaceable">REGISTRY_URL</em></pre></div></li><li class="step"><p>
     Upgrade the Helm charts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 upgrade -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</pre></div></li></ol></div></div></section><section class="sect1" id="uninstalling-rook" data-id-title="Uninstalling"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">1.7 </span><span class="title-name">Uninstalling</span> <a title="Permalink" class="permalink" href="#uninstalling-rook">#</a></h2></div></div></div><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Delete any Kubernetes applications that are consuming Rook storage.
    </p></li><li class="step"><p>
     Delete all object, file, and block storage artifacts.
    </p></li><li class="step"><p>
     Remove the CephCluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>&gt;kubectl delete -f cluster.yaml</pre></div></li><li class="step"><p>
     Uninstall the operator:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>helm3 uninstall <em class="replaceable">REGISTRY_URL</em></pre></div></li><li class="step"><p>
     Delete any data on the hosts:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>rm -rf /var/lib/rook</pre></div></li><li class="step"><p>
     Wipe the disks if necessary.
    </p></li><li class="step"><p>
     Delete the namespace:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">&gt; </code>kubectl delete namespace rook-ceph</pre></div></li></ol></div></div></section></section></div><div class="part" id="rook-ses-admin" data-id-title="Administrating Ceph on SUSE CaaS Platform"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part II </span><span class="title-name">Administrating Ceph on SUSE CaaS Platform </span><a title="Permalink" class="permalink" href="#rook-ses-admin">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#admin-intro-caasp"><span class="title-number">2 </span><span class="title-name">Rook-Ceph administration</span></a></span></li><dd class="toc-abstract"><p>
  This part of the guide focuses on routine tasks that you as an administrator
  need to take care of after the basic Ceph cluster has been deployed ("day
  two operations"). It also describes all the supported ways to access data
  stored in a Ceph cluster.
 </p></dd><li><span class="chapter"><a href="#admin-caasp-cluster"><span class="title-number">3 </span><span class="title-name">Ceph cluster administration</span></a></span></li><dd class="toc-abstract"><p>
  This chapter introduces tasks that are performed on the whole cluster.
 </p></dd><li><span class="chapter"><a href="#admin-caasp-block-storage"><span class="title-number">4 </span><span class="title-name">Block Storage</span></a></span></li><dd class="toc-abstract"><p>
  Block Storage allows a single pod to mount storage. This guide shows how to
  create a simple, multi-tier web application on Kubernetes using persistent
  volumes enabled by Rook.
 </p></dd><li><span class="chapter"><a href="#admin-caasp-cephfs"><span class="title-number">5 </span><span class="title-name">CephFS</span></a></span></li><dd class="toc-abstract"><p>
   A shared file system can be mounted with read/write permission from multiple
   pods. This may be useful for applications which can be clustered using a
   shared file system.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-crd"><span class="title-number">6 </span><span class="title-name">Ceph cluster custom resource definitions</span></a></span></li><dd class="toc-abstract"><p>
   Rook allows the creation and customization of storage clusters through
   Custom Resource Definitions (CRDs). There are two different methods of
   cluster creation, depending on whether the storage on which to base the
   Ceph cluster can be dynamically provisioned.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-cephconfig"><span class="title-number">7 </span><span class="title-name">Configuration</span></a></span></li><dd class="toc-abstract"><p>
   For almost any Ceph cluster, the user will want—and may need—
   to change some Ceph configurations. These changes often may be warranted
   in order to alter performance to meet SLAs, or to update default data
   resiliency settings.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-cephtoolbox"><span class="title-number">8 </span><span class="title-name">Toolboxes</span></a></span></li><dd class="toc-abstract"><p>
   The Rook toolbox is a container with common tools used for rook debugging
   and testing. The toolbox is based on SUSE Linux Enterprise Server, so more
   tools of your choosing can be installed with <code class="command">zypper</code>.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-cephosd"><span class="title-number">9 </span><span class="title-name">Ceph OSD management</span></a></span></li><dd class="toc-abstract"><p>Ceph Object Storage Daemons (OSDs) are the heart and soul of the Ceph storage platform. Each OSD manages a local device and together they provide the distributed storage. Rook will automate creation and management of OSDs to hide the complexity based on the desired state in the CephCluster CR as muc…</p></dd><li><span class="chapter"><a href="#admin-caasp-ceph-examples"><span class="title-number">10 </span><span class="title-name">Ceph examples</span></a></span></li><dd class="toc-abstract"><p>Configuration for Rook and Ceph can be configured in multiple ways to provide block devices, shared file system volumes, or object storage in a Kubernetes namespace. We have provided several examples to simplify storage setup, but remember there are many tunables and you will need to decide what set…</p></dd><li><span class="chapter"><a href="#admin-caasp-advanced-config"><span class="title-number">11 </span><span class="title-name">Advanced configuration</span></a></span></li><dd class="toc-abstract"><p>
   These examples show how to perform advanced configuration tasks on your Rook
   storage cluster.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-object-storage"><span class="title-number">12 </span><span class="title-name">Object Storage</span></a></span></li><dd class="toc-abstract"><p>
   Object Storage exposes an S3 API to the storage cluster for applications to
   <code class="literal">put</code> and <code class="literal">get</code> data.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-dashboard"><span class="title-number">13 </span><span class="title-name">Ceph Dashboard</span></a></span></li><dd class="toc-abstract"><p>The Ceph Dashboard is a helpful tool to give you an overview of the status of your Ceph cluster, including overall health, status of the MOPN quorum, status of the MGR, OSD, and other Ceph daemons, view pools and PG status, show logs for the daemons, and more. Rook makes it simple to enable the dash…</p></dd></ul></div><section class="chapter" id="admin-intro-caasp" data-id-title="Rook-Ceph administration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">2 </span><span class="title-name">Rook-Ceph administration</span> <a title="Permalink" class="permalink" href="#admin-intro-caasp">#</a></h1></div></div></div><p>
  This part of the guide focuses on routine tasks that you as an administrator
  need to take care of after the basic Ceph cluster has been deployed ("day
  two operations"). It also describes all the supported ways to access data
  stored in a Ceph cluster.
 </p><p>
  The chapters in this part contain links to additional documentation
  resources. These include additional documentation that is available on the
  system, as well as documentation available on the Internet.
 </p><p>
  For an overview of the documentation available for your product and the
  latest documentation updates, refer to
  <a class="link" href="https://documentation.suse.com" target="_blank">https://documentation.suse.com</a>.
 </p></section><section class="chapter" id="admin-caasp-cluster" data-id-title="Ceph cluster administration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">3 </span><span class="title-name">Ceph cluster administration</span> <a title="Permalink" class="permalink" href="#admin-caasp-cluster">#</a></h1></div></div></div><p>
  This chapter introduces tasks that are performed on the whole cluster.
 </p><section class="sect1" id="admin-caasp-cluster-shutdown" data-id-title="Shutting down and restarting the cluster"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">3.1 </span><span class="title-name">Shutting down and restarting the cluster</span> <a title="Permalink" class="permalink" href="#admin-caasp-cluster-shutdown">#</a></h2></div></div></div><p>
   To shut down the whole Ceph cluster for planned maintenance tasks, follow
   these steps:
  </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
     Stop all clients that are using the cluster.
    </p></li><li class="step"><p>
     Verify that the cluster is in a healthy state. Use the following commands:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
<code class="prompt user">cephuser@adm &gt; </code>ceph health</pre></div></li><li class="step"><p>
     Set the following OSD flags:
    </p><div class="verbatim-wrap"><pre class="screen">    <code class="prompt user">cephuser@adm &gt; </code>ceph osd set noout
    <code class="prompt user">cephuser@adm &gt; </code>ceph osd set nobackfill
    <code class="prompt user">cephuser@adm &gt; </code>ceph osd set norecover</pre></div></li><li class="step"><p>
     Shutdown service nodes one by one (non-storage workers).
    </p></li><li class="step"><p>
     Shutdown Ceph Monitor nodes one by one (masters by default).
    </p></li><li class="step"><p>
     Shutdown Admin Node (masters).
    </p></li></ol></div></div><p>
   After you finish the maintenance, you can start the cluster again by running
   the above procedure in reverse order.
  </p></section></section><section class="chapter" id="admin-caasp-block-storage" data-id-title="Block Storage"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">4 </span><span class="title-name">Block Storage</span> <a title="Permalink" class="permalink" href="#admin-caasp-block-storage">#</a></h1></div></div></div><p>
  Block Storage allows a single pod to mount storage. This guide shows how to
  create a simple, multi-tier web application on Kubernetes using persistent
  volumes enabled by Rook.
 </p><section class="sect1" id="rook-block-storage-provision-storage" data-id-title="Provisioning Block Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.1 </span><span class="title-name">Provisioning Block Storage</span> <a title="Permalink" class="permalink" href="#rook-block-storage-provision-storage">#</a></h2></div></div></div><p>
   Before Rook can provision storage, a <code class="literal">StorageClass</code> and a
   <code class="literal">CephBlockPool</code> need to be created. This will allow Kubernetes
   to interoperate with Rook when provisioning persistent volumes.
  </p><div id="id-1.7.4.4.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    This sample requires <span class="emphasis"><em>at least one OSD per node</em></span>, with
    each OSD located on <span class="emphasis"><em>three different nodes</em></span>.
   </p></div><p>
   Each OSD must be located on a different node, because the
   <a class="link" href="https://github.com/rook/rook/blob/release-1.4/Documentation/ceph-pool-crd.md#spec" target="_blank"><code class="literal">failureDomain</code></a>
   is set to <code class="literal">host</code> and the <code class="literal">replicated.size</code>
   is set to <code class="literal">3</code>.
  </p><div id="id-1.7.4.4.4.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    This example uses the CSI driver, which is the preferred driver going
    forward for Kubernetes 1.13 and newer. Examples are found in the
    <a class="link" href="https://github.com/rook/rook/tree/release-1.4/cluster/examples/kubernetes/ceph/csi/rbd" target="_blank">CSI
    RBD</a> directory.
   </p></div><p>
   Save this <code class="literal">StorageClass</code> definition as
   <code class="literal">storageclass.yaml</code>:
  </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # Ceph pool into which the RBD image shall be created
    pool: replicapool

    # RBD image format. Defaults to "2".
    imageFormat: "2"

    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete</pre></div><p>
   If you have deployed the Rook operator in a namespace other than
   <span class="quote">“<span class="quote">rook-ceph</span>”</span>, change the prefix in the provisioner to match the
   namespace you used. For example, if the Rook operator is running in the
   namespace <span class="quote">“<span class="quote">my-namespace</span>”</span> the provisioner value should be
   <span class="quote">“<span class="quote">my-namespace.rbd.csi.ceph.com</span>”</span>.
  </p><p>
   Create the storage class.
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml</pre></div><div id="id-1.7.4.4.4.11" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    As
    <a class="link" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain" target="_blank">specified
    by Kubernetes</a>, when using the <code class="literal">Retain</code> reclaim policy,
    any Ceph RBD image that is backed by a
    <code class="literal">PersistentVolume</code> will continue to exist even after the
    <code class="literal">PersistentVolume</code> has been deleted. These Ceph RBD
    images will need to be cleaned up manually using <code class="literal">rbd rm</code>.
   </p></div></section><section class="sect1" id="consume-the-storage-wordpress-sample" data-id-title="Consuming storage: WordPress sample"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.2 </span><span class="title-name">Consuming storage: WordPress sample</span> <a title="Permalink" class="permalink" href="#consume-the-storage-wordpress-sample">#</a></h2></div></div></div><p>
   In this example, we will create a sample application to consume the block
   storage provisioned by Rook with the classic WordPress and MySQL apps.
   Both of these applications will make use of block volumes provisioned by
   Rook.
  </p><p>
   Start MySQL and WordPress from the
   <code class="literal">cluster/examples/kubernetes</code> folder:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f mysql.yaml
kubectl create -f wordpress.yaml</pre></div><p>
   Both of these applications create a block volume, and mount it to their
   respective pod. You can see the Kubernetes volume claims by running the
   following:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m</pre></div><p>
   Once the WordPress and MySQL pods are in the <code class="literal">Running</code>
   state, get the cluster IP of the WordPress app and enter it in your browser:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl get svc wordpress
NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m</pre></div><p>
   You should see the WordPress application running.
  </p><p>
   If you are using Minikube, the WordPress URL can be retrieved with this
   one-line command:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')</pre></div><div id="id-1.7.4.4.5.12" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    When running in a Vagrant environment, there will be no external IP address
    to reach WordPress with. You will only be able to reach WordPress via the
    <code class="literal">CLUSTER-IP</code> from inside the Kubernetes cluster.
   </p></div></section><section class="sect1" id="consume-the-storage-toolbox" data-id-title="Consuming the storage: Toolbox"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.3 </span><span class="title-name">Consuming the storage: Toolbox</span> <a title="Permalink" class="permalink" href="#consume-the-storage-toolbox">#</a></h2></div></div></div><p>
   With the pool that was created above, we can also create a block image and
   mount it directly in a pod.
  </p></section><section class="sect1" id="rook-block-storage-teardown" data-id-title="Teardown"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.4 </span><span class="title-name">Teardown</span> <a title="Permalink" class="permalink" href="#rook-block-storage-teardown">#</a></h2></div></div></div><p>
   To clean up all the artifacts created by the block-storage demonstration:
  </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl delete -f wordpress.yaml
<code class="prompt user">kubectl@adm &gt; </code>kubectl delete -f mysql.yaml
<code class="prompt user">kubectl@adm &gt; </code>kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
<code class="prompt user">kubectl@adm &gt; </code>kubectl delete storageclass rook-ceph-block</pre></div></section><section class="sect1" id="advanced-example-erasure-coded-block-storage" data-id-title="Advanced Example: Erasure-Coded Block Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">4.5 </span><span class="title-name">Advanced Example: Erasure-Coded Block Storage</span> <a title="Permalink" class="permalink" href="#advanced-example-erasure-coded-block-storage">#</a></h2></div></div></div><p>
   If you want to use erasure-coded pools with RBD, your OSDs must use
   <code class="literal">bluestore</code> as their <code class="literal">storeType</code>.
   Additionally, the nodes that will mount the erasure-coded RBD block storage
   must have Linux kernel <code class="literal">4.11</code> or above.
  </p><p>
   This example requires <span class="emphasis"><em>at least three bluestore OSDs</em></span>,
   with each OSD located on a <span class="emphasis"><em>different node</em></span>.
  </p><p>
   The OSDs must be located on different nodes, because the
   <code class="literal">failureDomain</code> is set to <code class="literal">host</code> and the
   <code class="literal">erasureCoded</code> chunk settings require at least three
   different OSDs (two <code class="literal">dataChunks</code> plus one
   <code class="literal">codingChunk</code>).
  </p><p>
   To be able to use an erasure-coded pool, you need to create two pools (as
   seen below in the definitions): one erasure-coded, and one replicated.
  </p><section class="sect2" id="erasure-coded-csi-driver" data-id-title="Erasure coded CSI driver"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">4.5.1 </span><span class="title-name">Erasure coded CSI driver</span> <a title="Permalink" class="permalink" href="#erasure-coded-csi-driver">#</a></h3></div></div></div><p>
    The erasure-coded pool must be set as the <code class="literal">dataPool</code>
    parameter in
    <a class="link" href="https://github.com/rook/rook/blob/release-1.4/cluster/examples/kubernetes/ceph/csi/rbd/storageclass-ec.yaml" target="_blank"><code class="literal">storageclass-ec.yaml</code></a>
    It is used for the data of the RBD images.
   </p></section></section></section><section class="chapter" id="admin-caasp-cephfs" data-id-title="CephFS"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">5 </span><span class="title-name">CephFS</span> <a title="Permalink" class="permalink" href="#admin-caasp-cephfs">#</a></h1></div></div></div><section class="sect1" id="rook-shared-filesystem" data-id-title="Shared File System"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">5.1 </span><span class="title-name">Shared File System</span> <a title="Permalink" class="permalink" href="#rook-shared-filesystem">#</a></h2></div></div></div><p>
   A shared file system can be mounted with read/write permission from multiple
   pods. This may be useful for applications which can be clustered using a
   shared file system.
  </p><p>
   This example runs a shared file system for the
   <a class="link" href="https://github.com/kubernetes/kubernetes/tree/release-1.18/cluster/addons" target="_blank">kube-registry</a>.
  </p><section class="sect2" id="rook-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#rook-prerequisites">#</a></h3></div></div></div><p>
    This guide assumes you have created a Rook cluster as explained in the
    main guide: <a class="xref" href="#deploy-rook" title="Chapter 1. Quick start">Chapter 1, <em>Quick start</em></a>.
   </p><div id="id-1.7.4.5.3.4.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     By default, only one shared file system can be created with Rook.
     Multiple file system support in Ceph is still considered experimental,
     and can be enabled with the environment variable
     <code class="literal">ROOK_ALLOW_MULTIPLE_FILESYSTEMS</code> defined in
     <code class="filename">operator.yaml</code>.
    </p></div></section><section class="sect2" id="rook-create-the-filesystem" data-id-title="Creating the File System"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.2 </span><span class="title-name">Creating the File System</span> <a title="Permalink" class="permalink" href="#rook-create-the-filesystem">#</a></h3></div></div></div><p>
    Create the file system by specifying the desired settings for the metadata
    pool, data pools, and metadata server in the
    <code class="literal">CephFilesystem</code> CRD. In this example, we create the
    metadata pool with replication of three, and a single data pool with
    replication of three. For more options, see the documentation
    <a class="xref" href="#rook-ceph-shared-filesystem-crd" title="6.3. Ceph shared file system CRD">Section 6.3, “Ceph shared file system CRD”</a>.
   </p><p>
    Save this shared file system definition as
    <code class="filename">filesystem.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: myfs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 3
    dataPools:
    - replicated:
        size: 3
    preservePoolsOnDelete: true
    metadataServer:
      activeCount: 1
      activeStandby: true</pre></div><p>
    The Rook operator will create all the pools and other resources necessary
    to start the service. This may take a minute to complete.
   </p><p>
    Create the file system:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f filesystem.yaml</pre></div><p>
    To confirm the file system is configured, wait for the MDS pods to start:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-mds
NAME                                      READY     STATUS    RESTARTS   AGE
rook-ceph-mds-myfs-7d59fdfcf4-h8kw9       1/1       Running   0          12s
rook-ceph-mds-myfs-7d59fdfcf4-kgkjp       1/1       Running   0          12s</pre></div><p>
    To see detailed status of the file system, start and connect to the Rook
    toolbox. A new line will be shown with <code class="command">ceph status</code> for
    the <code class="literal">mds</code> service. In this example, there is one active
    instance of MDS which is up, with one MDS instance in
    <code class="literal">standby-replay</code> mode in case of failover.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
[...]
services:
mds: myfs-1/1/1 up {[myfs:0]=mzw58b=up:active}, 1 up:standby-replay</pre></div></section><section class="sect2" id="rook-provision-storage" data-id-title="Provisioning Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.3 </span><span class="title-name">Provisioning Storage</span> <a title="Permalink" class="permalink" href="#rook-provision-storage">#</a></h3></div></div></div><p>
    Before Rook can start provisioning storage, a
    <code class="literal">StorageClass</code> needs to be created based on the file
    system. This is needed for Kubernetes to interoperate with the CSI driver to
    create persistent volumes.
   </p><div id="id-1.7.4.5.3.6.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This example uses the CSI driver, which is the preferred driver going
     forward for Kubernetes 1.13 and newer.
    </p></div><p>
    Save this storage class definition as
    <code class="filename">storageclass.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-cephfs
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.cephfs.csi.ceph.com
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: rook-ceph

  # CephFS file system name into which the volume shall be created
  fsName: myfs

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: myfs-data0

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  # rootPath: /absolute/path

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

reclaimPolicy: Delete</pre></div><p>
    If you have deployed the Rook operator in a namespace other than
    <span class="quote">“<span class="quote">rook-ceph</span>”</span>, change the prefix in the provisioner to match the
    namespace you used. For example, if the Rook operator is running in
    <span class="quote">“<span class="quote">rook-op</span>”</span>, the provisioner value should be
    <span class="quote">“<span class="quote">rook-op.rbd.csi.ceph.com</span>”</span>.
   </p><p>
    Create the storage class:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml</pre></div><div id="id-1.7.4.5.3.6.9" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     The CephFS CSI driver uses quotas to enforce the PVC size requested.
     Only newer kernels support CephFS quotas (kernel version of at least
     4.17).
    </p></div></section><section class="sect2" id="rook-consume-the-shared-filesystem-k8s-registry-sample" data-id-title="Consuming the Shared File System: K8s Registry Sample"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.4 </span><span class="title-name">Consuming the Shared File System: K8s Registry Sample</span> <a title="Permalink" class="permalink" href="#rook-consume-the-shared-filesystem-k8s-registry-sample">#</a></h3></div></div></div><p>
    As an example, we will start the kube-registry pod with the shared file
    system as the backing store. Save the following spec as
    <code class="filename">kube-registry.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cephfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: rook-cephfs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-registry
  namespace: kube-system
  labels:
    k8s-app: kube-registry
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 3
  selector:
    matchLabels:
      k8s-app: kube-registry
  template:
    metadata:
      labels:
        k8s-app: kube-registry
        kubernetes.io/cluster-service: "true"
    spec:
      containers:
      - name: registry
        image: registry:2
        imagePullPolicy: Always
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        # Configuration reference: https://docs.docker.com/registry/configuration/
        - name: REGISTRY_HTTP_ADDR
          value: :5000
        - name: REGISTRY_HTTP_SECRET
          value: "Ple4seCh4ngeThisN0tAVerySecretV4lue"
        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
          value: /var/lib/registry
        volumeMounts:
        - name: image-store
          mountPath: /var/lib/registry
        ports:
        - containerPort: 5000
          name: registry
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: registry
        readinessProbe:
          httpGet:
            path: /
            port: registry
      volumes:
      - name: image-store
        persistentVolumeClaim:
          claimName: cephfs-pvc
          readOnly: false</pre></div><p>
    Create the Kube registry deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f cluster/examples/kubernetes/ceph/csi/cephfs/kube-registry.yaml</pre></div><p>
    You now have a High-Availability Docker registry with persistent storage.
   </p><div id="id-1.7.4.5.3.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If the Rook cluster has more than one file system and the application
     pod is scheduled to a node with kernel version older than 4.7,
     inconsistent results may arise, since kernels older than 4.7 do not
     support specifying file system namespaces.
    </p></div></section><section class="sect2" id="rook-consume-the-shared-filesystem-toolbox" data-id-title="Consuming the Shared File System: Toolbox"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">5.1.5 </span><span class="title-name">Consuming the Shared File System: Toolbox</span> <a title="Permalink" class="permalink" href="#rook-consume-the-shared-filesystem-toolbox">#</a></h3></div></div></div><p>
    Once you have pushed an image to the registry, verify that
    <code class="filename">kube-registry</code> is using the file system that was
    configured above by mounting the shared file system in the toolbox pod.
   </p><section class="sect3" id="rook-teardown" data-id-title="Teardown"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">5.1.5.1 </span><span class="title-name">Teardown</span> <a title="Permalink" class="permalink" href="#rook-teardown">#</a></h4></div></div></div><p>
     To clean up all the artifacts created by the file system demo:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl delete -f kube-registry.yaml</pre></div><p>
     To delete the file system components and backing data, delete the
     Filesystem CRD.
    </p><div id="id-1.7.4.5.3.8.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      <span class="strong"><strong>WARNING: Data will be deleted if
      <code class="option">preservePoolsOnDelete=false</code></strong></span>.
     </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph delete cephfilesystem myfs</pre></div><p>
     Note: If the <span class="quote">“<span class="quote">preservePoolsOnDelete</span>”</span> file system attribute is
     set to true, the above command won’t delete the pools. Creating the file
     system again with the same CRD will reuse the previous pools.
    </p></section></section></section></section><section class="chapter" id="admin-caasp-crd" data-id-title="Ceph cluster custom resource definitions"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">6 </span><span class="title-name">Ceph cluster custom resource definitions</span> <a title="Permalink" class="permalink" href="#admin-caasp-crd">#</a></h1></div></div></div><section class="sect1" id="rook-ceph-cluster-crd" data-id-title="Ceph cluster CRD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.1 </span><span class="title-name">Ceph cluster CRD</span> <a title="Permalink" class="permalink" href="#rook-ceph-cluster-crd">#</a></h2></div></div></div><p>
   Rook allows the creation and customization of storage clusters through
   Custom Resource Definitions (CRDs). There are two different methods of
   cluster creation, depending on whether the storage on which to base the
   Ceph cluster can be dynamically provisioned.
  </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
     Specify the host paths and raw devices.
    </p></li><li class="listitem"><p>
     Specify the storage class Rook should use to consume storage via PVCs.
    </p></li></ol></div><p>
   Examples for each of these approaches follow.
  </p><section class="sect2" id="rook-host-based-cluster" data-id-title="Host-based cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1.1 </span><span class="title-name">Host-based cluster</span> <a title="Permalink" class="permalink" href="#rook-host-based-cluster">#</a></h3></div></div></div><p>
    To get you started, here is a simple example of a CRD to configure a Ceph
    cluster with all nodes and all devices. In the next example, the MONs and
    OSDs are backed by PVCs.
   </p><div id="id-1.7.4.6.3.5.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     In addition to your CephCluster object, you need to create the namespace,
     service accounts, and RBAC rules for the namespace in which you will
     create the CephCluster. These resources are defined in the example
     <code class="filename">common.yaml</code> file.
    </p></div><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # see the "Cluster Settings" section below for more details on which image of Ceph to run
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: true
  storage:
    useAllNodes: true
    useAllDevices: true</pre></div></section><section class="sect2" id="rook-pvc-based-cluster" data-id-title="PVC-based cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1.2 </span><span class="title-name">PVC-based cluster</span> <a title="Permalink" class="permalink" href="#rook-pvc-based-cluster">#</a></h3></div></div></div><div id="id-1.7.4.6.3.6.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Kubernetes version 1.13.0 or greater is required to provision OSDs on PVCs.
    </p></div><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # see the "Cluster Settings" section below for more details on which image of Ceph to run
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  storage:
   storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      encrypted: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: local-storage
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div><p>
    For more advanced scenarios, such as adding a dedicated device, please
    refer to
    <a class="xref" href="#rook-dedicated-metadata-and-wal-device-for-osd-on-pvc" title="6.1.4.8. Dedicated metadata and WAL device for OSD on PVC">Section 6.1.4.8, “Dedicated metadata and WAL device for OSD on PVC”</a>.
   </p></section><section class="sect2" id="rook-settings" data-id-title="Settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1.3 </span><span class="title-name">Settings</span> <a title="Permalink" class="permalink" href="#rook-settings">#</a></h3></div></div></div><p>
    Settings can be specified at the global level to apply to the cluster as a
    whole, while other settings can be specified at more fine-grained levels.
    If any setting is unspecified, a suitable default will be used
    automatically.
   </p><section class="sect3" id="rook-cluster-metadata" data-id-title="Cluster metadata"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.1 </span><span class="title-name">Cluster metadata</span> <a title="Permalink" class="permalink" href="#rook-cluster-metadata">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name that will be used internally for the
       Ceph cluster. Most commonly, the name is the same as the namespace
       since multiple clusters are not supported in the same namespace.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>: The Kubernetes namespace that will be created
       for the Rook cluster. The services, pods, and other resources created
       by the operator will be added to this namespace. The common scenario is
       to create a single Rook cluster. If multiple clusters are created,
       they must not have conflicting devices or host paths.
      </p></li></ul></div></section><section class="sect3" id="rook-cluster-settings" data-id-title="Cluster settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.2 </span><span class="title-name">Cluster settings</span> <a title="Permalink" class="permalink" href="#rook-cluster-settings">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">external</code>:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">enable</code>: if <code class="literal">true</code>, the cluster
         will not be managed by Rook but via an external entity. This mode is
         intended to connect to an existing cluster. In this case, Rook will
         only consume the external cluster. However, if an image is provided,
         Rook will be able to deploy various daemons in Kubernetes, such as
         object gateways, MDS and NFS. If an image is not provided, it will
         refuse. If this setting is enabled,
         <span class="strong"><strong>all</strong></span> the other options will be
         ignored except <code class="literal">cephVersion.image</code> and
         <code class="literal">dataDirHostPath</code>. See
         <a class="xref" href="#rook-external-cluster" title="6.1.4.9. External cluster">Section 6.1.4.9, “External cluster”</a>. If
         <code class="literal">cephVersion.image</code> is left blank, Rook will refuse
         the creation of extra CRs such as object, file and NFS.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">cephVersion</code>: The version information for launching
       the Ceph daemons.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">image</code>: The image used for running the Ceph
         daemons. For example, <code class="literal">ceph/ceph:v14.2.10</code> or
         <code class="literal">ceph/ceph:v15.2.4</code>. To ensure a consistent version
         of the image is running across all nodes in the cluster, we recommend
         to use a very specific image version. Tags also exist that would give
         the latest version, but they are only recommended for test
         environments. For example, the tag <code class="literal">v14</code> will be
         updated each time a new nautilus build is released. Using the
         <code class="literal">v14</code> or similar tag is not recommended in production
         because it may lead to inconsistent versions of the image running
         across different nodes in the cluster.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">dataDirHostPath</code>: The path on the host where config
       and data should be stored for each of the services. If the directory
       does not exist, it will be created. Because this directory persists on
       the host, it will remain after pods are deleted. You
       <span class="strong"><strong> must not</strong></span> use the following paths and
       any of their subpaths: <code class="command">/etc/ceph</code>,
       <code class="command">/rook</code> or <code class="command">/var/log/ceph</code>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         On <span class="strong"><strong>Minikube</strong></span> environments, use
         <code class="command">/data/rook</code>. Minikube boots into a <code class="literal">tmpfs
         </code> but it provides some directories where files can persist
         across reboots. Using one of these directories will ensure that
         Rook’s data and configuration files persist and that enough storage
         space is available.
        </p><div id="id-1.7.4.6.3.7.4.2.3.2.1.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
          <span class="strong"><strong>WARNING</strong></span>: For test scenarios, if
          you delete a cluster and start a new cluster on the same hosts, the
          path used by <code class="literal">dataDirHostPath</code> must be deleted.
          Otherwise, stale keys and other configuration will remain from the
          previous cluster and the new MONs will fail to start. If this value
          is empty, each pod will get an ephemeral directory to store their
          config files that is tied to the lifetime of the pod running on that
          node.
         </p></div></li></ul></div></li><li class="listitem"><p>
       <code class="literal">continueUpgradeAfterChecksEvenIfNotHealthy</code>: if set to
       <code class="literal">true</code>, Rook will continue the OSD daemon upgrade
       process even if the PGs are not clean, or continue with the MDS upgrade
       even the file system is not healthy.
      </p></li><li class="listitem"><p>
       <code class="literal">dashboard</code>: Settings for the Ceph Dashboard. To view the
       dashboard in your browser, see <span class="intraxref">Book “Administration and Operations Guide”</span>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">enabled</code>: Whether to enable the dashboard to view
         cluster status.
        </p></li><li class="listitem"><p>
         <code class="literal">urlPrefix</code>: Allows serving the dashboard under a
         subpath (useful when you are accessing the dashboard via a reverse
         proxy).
        </p></li><li class="listitem"><p>
         <code class="literal">port</code>: Allows changing the default port where the
         dashboard is served.
        </p></li><li class="listitem"><p>
         <code class="literal">ssl</code>: Whether to serve the dashboard via SSL;
         ignored on Ceph versions older than <code class="literal">13.2.2</code>.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">monitoring</code>: Settings for monitoring Ceph using
       Prometheus. To enable monitoring on your cluster, see the
       <span class="intraxref">Book “Administration and Operations Guide”, Chapter 16 “Monitoring and alerting”</span>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">enabled</code>: Whether to enable-Prometheus based
         monitoring for this cluster.
        </p></li><li class="listitem"><p>
         <code class="literal">rulesNamespace</code>: Namespace to deploy
         <code class="literal">prometheusRule</code>. If empty, the namespace of the
         cluster will be used. We recommend:
        </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
           If you have a single Rook Ceph cluster, set the
           <code class="literal">rulesNamespace</code> to the same namespace as the
           cluster, or leave it empty.
          </p></li><li class="listitem"><p>
           If you have multiple Rook Ceph clusters in the same Kubernetes
           cluster, choose the same namespace to set
           <code class="literal">rulesNamespace</code> for all the clusters (ideally,
           namespace with Prometheus deployed). Otherwise, you will get
           duplicate alerts with duplicate alert definitions.
          </p></li></ul></div></li></ul></div></li><li class="listitem"><p>
       <code class="literal">network</code>: For the network settings for the cluster,
       refer to <a class="xref" href="#rook-network-configuration-settings" title="6.1.3.5. Network configuration settings">Section 6.1.3.5, “Network configuration settings”</a>.
      </p></li><li class="listitem"><p>
       <code class="literal">mon</code>: contains MON related options
       <a class="xref" href="#rook-mon-settings" title="6.1.3.3. MON settings">Section 6.1.3.3, “MON settings”</a>.
      </p></li><li class="listitem"><p>
       <code class="literal">mgr</code>: manager top level section.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">modules</code>: is the list of Ceph Manager modules to enable.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">crashCollector</code>: The settings for crash collector
       daemon(s).
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">disable</code>: if set to <code class="literal">true</code>, the
         crash collector will not run on any node where a Ceph daemon runs.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">annotations</code>:
       <a class="xref" href="#rook-annotations-and-labels" title="6.1.3.10. Annotations and labels">Section 6.1.3.10, “Annotations and labels”</a>
      </p></li><li class="listitem"><p>
       <code class="literal">labels</code>: <a class="xref" href="#rook-annotations-and-labels" title="6.1.3.10. Annotations and labels">Section 6.1.3.10, “Annotations and labels”</a>
      </p></li><li class="listitem"><p>
       <code class="literal">placement</code>:
       <a class="xref" href="#rook-placement-configuration-settings" title="6.1.3.11. Placement configuration settings">Section 6.1.3.11, “Placement configuration settings”</a>
      </p></li><li class="listitem"><p>
       <code class="literal">resources</code>:
       <a class="xref" href="#rook-cluster-wide-resources-configuration-settings" title="6.1.3.12. Cluster-wide resources configuration settings">Section 6.1.3.12, “Cluster-wide resources configuration settings”</a>
      </p></li><li class="listitem"><p>
       <code class="literal">priorityClassNames</code>:
       <a class="xref" href="#rook-priority-class-names-configuration-settings" title="6.1.3.14. Priority class names configuration settings">Section 6.1.3.14, “Priority class names configuration settings”</a>
      </p></li><li class="listitem"><p>
       <code class="literal">storage</code>: Storage selection and configuration that
       will be used across the cluster. Note that these settings can be
       overridden for specific nodes.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">useAllNodes</code>: <code class="literal">true</code> or
         <code class="literal">false</code>, indicating if all nodes in the cluster
         should be used for storage according to the cluster level storage
         selection and configuration values. If individual nodes are specified
         under the <code class="literal">nodes</code> field, then
         <code class="literal">useAllNodes</code> must be set to
         <code class="literal">false</code>.
        </p></li><li class="listitem"><p>
         <code class="literal">nodes</code>: Names of individual nodes in the cluster
         that should have their storage included in accordance with either the
         cluster level configuration specified above or any node specific
         overrides described in the next section below.
         <code class="literal">useAllNodes</code> must be set to <code class="literal">false</code>
         to use specific nodes and their configuration. See
         <a class="xref" href="#rook-node-settings" title="6.1.3.6. Node settings">Section 6.1.3.6, “Node settings”</a> below.
        </p></li><li class="listitem"><p>
         <code class="literal">config</code>: Config settings applied to all OSDs on the
         node unless overridden by <code class="literal">devices</code>.
        </p></li><li class="listitem"><p>
         <a class="xref" href="#rook-storage-selection-settings" title="6.1.3.7. Storage selection settings">Section 6.1.3.7, “Storage selection settings”</a>
        </p></li><li class="listitem"><p>
         <a class="xref" href="#rook-storage-class-device-sets" title="6.1.3.8. Storage class device sets">Section 6.1.3.8, “Storage class device sets”</a>
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">disruptionManagement</code>: The section for configuring
       management of daemon disruptions
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">managePodBudgets</code>: if <code class="literal">true</code>, the
         operator will create and manage
         <code class="literal">PodDisruptionBudgets</code> for OSD, MON, RGW, and MDS
         daemons. The operator will block eviction of OSDs by default and
         unblock them safely when drains are detected.
        </p></li><li class="listitem"><p>
         <code class="literal">osdMaintenanceTimeout</code>: is a duration in minutes
         that determines how long an entire failure domain like
         <code class="literal">region/zone/host</code> will be held in
         <code class="literal">noout</code> (in addition to the default DOWN/OUT
         interval) when it is draining. This is only relevant when
         <code class="literal">managePodBudgets</code> is <code class="literal">true</code>. The
         default value is <code class="literal">30</code> minutes.
        </p></li><li class="listitem"><p>
         <code class="literal">manageMachineDisruptionBudgets</code>: if
         <code class="literal">true</code>, the operator will create and manage
         <code class="literal">MachineDisruptionBudgets</code> to ensure OSDs are only
         fenced when the cluster is healthy. Only available on OpenShift.
        </p></li><li class="listitem"><p>
         <code class="literal">machineDisruptionBudgetNamespace</code>: the namespace in
         which to watch the <code class="literal">MachineDisruptionBudgets</code>.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">removeOSDsIfOutAndSafeToRemove</code>: If
       <code class="literal">true</code> the operator will remove the OSDs that are down
       and whose data has been restored to other OSDs.
      </p></li><li class="listitem"><p>
       <code class="literal">cleanupPolicy</code>: <a class="xref" href="#rook-cleanup-policy" title="6.1.4.10. Cleanup policy">Section 6.1.4.10, “Cleanup policy”</a>
      </p></li></ul></div></section><section class="sect3" id="rook-mon-settings" data-id-title="MON settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.3 </span><span class="title-name">MON settings</span> <a title="Permalink" class="permalink" href="#rook-mon-settings">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">count</code>: Set the number of MONs to be started. This
       should be an odd number between one and nine. If not specified, the
       default is set to three, and <code class="literal">allowMultiplePerNode</code> is
       also set to <code class="literal">true</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">allowMultiplePerNode</code>: Enable
       (<code class="literal">true</code>) or disable (<code class="literal">false</code>) the
       placement of multiple MONs on one node. Default is
       <code class="literal">false</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">volumeClaimTemplate</code>: A
       <code class="literal">PersistentVolumeSpec</code> used by Rook to create PVCs
       for monitor storage. This field is optional, and when not provided,
       HostPath volume mounts are used. The current set of fields from template
       that are used are <code class="literal">storageClassName</code> and the
       <code class="literal">storage</code> resource request and limit. The default
       storage size request for new PVCs is <code class="literal">10Gi</code>. Ensure
       that associated storage class is configured to use
       <code class="literal">volumeBindingMode: WaitForFirstConsumer</code>. This setting
       only applies to new monitors that are created when the requested number
       of monitors increases, or when a monitor fails and is recreated.
      </p></li></ul></div><p>
     If these settings are changed in the CRD, the operator will update the
     number of MONs during a periodic check of the MON health, which by default
     is every 45 seconds.
    </p><p>
     To change the defaults that the operator uses to determine the MON health
     and whether to failover a MON, refer to the
     <a class="xref" href="#rook-health-settings" title="6.1.3.15. Health settings">Section 6.1.3.15, “Health settings”</a>. The intervals should be small
     enough that you have confidence the MONs will maintain quorum, while also
     being long enough to ignore network blips where MONs are failed over too
     often.
    </p></section><section class="sect3" id="rook-mgr-settings" data-id-title="Ceph Manager settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.4 </span><span class="title-name">Ceph Manager settings</span> <a title="Permalink" class="permalink" href="#rook-mgr-settings">#</a></h4></div></div></div><p>
     You can use the cluster CR to enable or disable any manager module. For
     example, this can be configured:
    </p><div class="verbatim-wrap"><pre class="screen">mgr:
  modules:
  - name: &lt;name of the module&gt;
    enabled: true</pre></div><p>
     Some modules will have special configuration to ensure the module is fully
     functional after being enabled. Specifically, the
     <code class="literal">pg_autoscaler</code>—Rook will configure all new pools
     with PG autoscaling by setting:
     <code class="literal">osd_pool_default_pg_autoscale_mode = on</code>
    </p></section><section class="sect3" id="rook-network-configuration-settings" data-id-title="Network configuration settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.5 </span><span class="title-name">Network configuration settings</span> <a title="Permalink" class="permalink" href="#rook-network-configuration-settings">#</a></h4></div></div></div><p>
     If not specified, the default SDN will be used. Configure the network that
     will be enabled for the cluster and services.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">provider</code>: Specifies the network provider that will be
       used to connect the network interface.
      </p></li><li class="listitem"><p>
       <code class="literal">selectors</code>: List the network selector(s) that will be
       used associated by a key.
      </p></li></ul></div><div id="id-1.7.4.6.3.7.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Changing networking configuration after a Ceph cluster has been
      deployed is not supported and will result in a non-functioning cluster.
     </p></div><p>
     To use host networking, set <code class="literal">provider: host</code>.
    </p></section><section class="sect3" id="rook-node-settings" data-id-title="Node settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.6 </span><span class="title-name">Node settings</span> <a title="Permalink" class="permalink" href="#rook-node-settings">#</a></h4></div></div></div><p>
     In addition to the cluster level settings specified above, each individual
     node can also specify configuration to override the cluster level settings
     and defaults. If a node does not specify any configuration, then it will
     inherit the cluster level settings.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name of the node, which should match its
       <code class="literal">kubernetes.io/hostname</code> label.
      </p></li><li class="listitem"><p>
       <code class="literal">config</code>: Configuration settings applied to all OSDs on
       the node unless overridden by <code class="literal">devices</code>.
      </p></li><li class="listitem"><p>
       <a class="xref" href="#rook-storage-selection-settings" title="6.1.3.7. Storage selection settings">Section 6.1.3.7, “Storage selection settings”</a>
      </p></li></ul></div><p>
     When <code class="literal">useAllNodes</code> is set to <code class="literal">true</code>,
     Rook attempts to make Ceph cluster management as hands-off as possible
     while still maintaining reasonable data safety. If a usable node comes
     online, Rook will begin to use it automatically. To maintain a balance
     between hands-off usability and data safety, nodes are removed from Ceph
     as OSD hosts only (1) if the node is deleted from Kubernetes itself or (2) if
     the node has its taints or affinities modified in such a way that the node
     is no longer usable by Rook. Any changes to taints or affinities,
     intentional or unintentional, may affect the data reliability of the
     Ceph cluster. In order to help protect against this somewhat, deletion
     of nodes by taint or affinity modifications must be confirmed by deleting
     the Rook-Ceph operator pod and allowing the operator deployment to
     restart the pod.
    </p><p>
     For production clusters, we recommend that <code class="literal">useAllNodes</code>
     is set to <code class="literal">false</code> to prevent the Ceph cluster from
     suffering reduced data reliability unintentionally due to a user mistake.
     When <code class="literal">useAllNodes</code> is set to <code class="literal">false</code>,
     Rook relies on the user to be explicit about when nodes are added to or
     removed from the Ceph cluster. Nodes are only added to the Ceph
     cluster if the node is added to the Ceph cluster resource. Similarly,
     nodes are only removed if the node is removed from the Ceph cluster
     resource.
    </p><section class="sect4" id="rook-node-updates" data-id-title="Node updates"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">6.1.3.6.1 </span><span class="title-name">Node updates</span> <a title="Permalink" class="permalink" href="#rook-node-updates">#</a></h5></div></div></div><p>
      Nodes can be added and removed over time by updating the cluster CRD
      —for example, with the following command:
     </p><div class="verbatim-wrap"><pre class="screen">kubectl -n rook-ceph edit cephcluster rook-ceph</pre></div><p>
      This will bring up your default text editor and allow you to add and
      remove storage nodes from the cluster. This feature is only available
      when <code class="literal">useAllNodes</code> has been set to
      <code class="literal">false</code>.
     </p></section></section><section class="sect3" id="rook-storage-selection-settings" data-id-title="Storage selection settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.7 </span><span class="title-name">Storage selection settings</span> <a title="Permalink" class="permalink" href="#rook-storage-selection-settings">#</a></h4></div></div></div><p>
     Below are the settings available, both at the cluster and individual node
     level, for selecting which storage resources will be included in the
     cluster.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">useAllDevices</code>: <code class="literal">true</code> or
       <code class="literal">false</code>, indicating whether all devices found on nodes
       in the cluster should be automatically consumed by OSDs. This is
       <span class="strong"><strong>Not recommended</strong></span> unless you have a
       very controlled environment where you will not risk formatting of
       devices with existing data. When <code class="literal">true</code>, all
       devices/partitions will be used. Is overridden by
       <code class="literal">deviceFilter</code> if specified.
      </p></li><li class="listitem"><p>
       <code class="literal">deviceFilter</code>: A regular expression for short kernel
       names of devices (for example, <code class="literal">sda</code>) that allows
       selection of devices to be consumed by OSDs. If individual devices have
       been specified for a node then this filter will be ignored. For example:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">sdb</code>: Selects only the <code class="literal">sdb</code> device
         (if found).
        </p></li><li class="listitem"><p>
         <code class="literal">^sd</code>: Selects all devices starting with
         <code class="literal">sd</code>.
        </p></li><li class="listitem"><p>
         <code class="literal">^sd[a-d]</code>: Selects devices starting with
         <code class="literal">sda</code>, <code class="literal">sdb</code>,
         <code class="literal">sdc</code>, and <code class="literal">sdd</code> (if found).
        </p></li><li class="listitem"><p>
         <code class="literal">^s</code>: Selects all devices that start with
         <code class="literal">s</code>.
        </p></li><li class="listitem"><p>
         <code class="literal">^[^r]</code>: Selects all devices that do
         <span class="emphasis"><em>not</em></span> start with <code class="literal">r</code>
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">devicePathFilter</code>: A regular expression for device
       paths (for
       example, <code class="literal">/dev/disk/by-path/pci-0:1:2:3-scsi-1</code>) that
       allows selection of devices to be consumed by OSDs. If individual
       devices or <code class="literal">deviceFilter</code> have been specified for a
       node then this filter will be ignored. For example:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">^/dev/sd.</code>: Selects all devices starting with
         <code class="literal">sd</code>
        </p></li><li class="listitem"><p>
         <code class="literal">^/dev/disk/by-path/pci-.*</code>: Selects all devices
         which are connected to PCI bus
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">devices</code>: A list of individual device names belonging
       to this node to include in the storage cluster.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">name</code>: The name of the device (for example,
         <code class="literal">sda</code>), or full udev path (such
         as, <code class="literal">/dev/disk/by-id/ata-ST4000DM004-XXXX</code> —
         this will not change after reboots).
        </p></li><li class="listitem"><p>
         <code class="literal">config</code>: Device-specific configuration settings.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">storageClassDeviceSets</code>: Explained in
       <a class="xref" href="#rook-storage-class-device-sets" title="6.1.3.8. Storage class device sets">Section 6.1.3.8, “Storage class device sets”</a>.
      </p></li></ul></div></section><section class="sect3" id="rook-storage-class-device-sets" data-id-title="Storage class device sets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.8 </span><span class="title-name">Storage class device sets</span> <a title="Permalink" class="permalink" href="#rook-storage-class-device-sets">#</a></h4></div></div></div><p>
     The following are the settings for Storage Class Device Sets which can be
     configured to create OSDs that are backed by block mode PVs.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: A name for the set.
      </p></li><li class="listitem"><p>
       <code class="literal">count</code>: The number of devices in the set.
      </p></li><li class="listitem"><p>
       <code class="literal">resources</code>: The CPU and RAM requests or limits for the
       devices (optional).
      </p></li><li class="listitem"><p>
       <code class="literal">placement</code>: The placement criteria for the devices
       (optional; default is no placement criteria).
      </p><p>
       The syntax is the same as for
       <a class="xref" href="#rook-placement-configuration-settings" title="6.1.3.11. Placement configuration settings">Section 6.1.3.11, “Placement configuration settings”</a>. It supports
       <code class="literal">nodeAffinity</code>, <code class="literal">podAffinity</code>,
       <code class="literal">podAntiAffinity</code> and <code class="literal">tolerations</code>
       keys.
      </p><p>
       We recommend configuring the placement such that the OSDs will be as
       evenly spread across nodes as possible. At a minimum, anti-affinity
       should be added, so at least one OSD will be placed on each available
       node.
      </p><p>
       However, if there are more OSDs than nodes, this anti-affinity will not
       be effective. Another placement scheme to consider is adding labels to
       the nodes in such a way that the OSDs can be grouped on those nodes,
       create multiple <code class="literal">storageClassDeviceSets</code>, and add node
       affinity to each of the device sets that will place the OSDs in those
       sets of nodes.
      </p></li><li class="listitem"><p>
       <code class="literal">preparePlacement</code>: The placement criteria for the
       preparation of the OSD devices. Creating OSDs is a two-step process and
       the prepare job may require different placement than the OSD daemons. If
       the <code class="literal">preparePlacement</code> is not specified, the
       <code class="literal">placement</code> will instead be applied for consistent
       placement for the OSD prepare jobs and OSD deployments. The
       <code class="literal">preparePlacement</code> is only useful for
       <code class="literal">portable</code> OSDs in the device sets. OSDs that are not
       portable will be tied to the host where the OSD prepare job initially
       runs.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         For example, provisioning may require topology spread constraints
         across zones, but the OSD daemons may require constraints across hosts
         within the zones.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">portable</code>: If <code class="literal">true</code>, the OSDs will
       be allowed to move between nodes during failover. This requires a
       storage class that supports portability (for
       example, <code class="literal">aws-ebs</code>, but not the local storage
       provisioner). If <code class="literal">false</code>, the OSDs will be assigned to
       a node permanently. Rook will configure Ceph’s CRUSH map to support
       the portability.
      </p></li><li class="listitem"><p>
       <code class="literal">tuneDeviceClass</code>: If <code class="literal">true</code>, because
       the OSD can be on a slow device class, Rook will adapt to that by
       tuning the OSD process. This will make Ceph perform better under that
       slow device.
      </p></li><li class="listitem"><p>
       <code class="literal">volumeClaimTemplates</code>: A list of PVC templates to use
       for provisioning the underlying storage devices.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">resources.requests.storage</code>: The desired capacity
         for the underlying storage devices.
        </p></li><li class="listitem"><p>
         <code class="literal">storageClassName</code>: The StorageClass to provision
         PVCs from. The default is to use the cluster-default StorageClass.
         This StorageClass should provide a raw block device, multipath device,
         or logical volume. Other types are not supported. If you want to use
         logical volumes, please see the known issue of OSD on LV-backed PVC:
         <a class="link" href="https://github.com/rook/rook/blob/master/Documentation/ceph-common-issues.md#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc" target="_blank">https://github.com/rook/rook/blob/master/Documentation/ceph-common-issues.md#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc</a>
        </p></li><li class="listitem"><p>
         <code class="literal">volumeMode</code>: The volume mode to be set for the PVC.
        </p></li><li class="listitem"><p>
         <code class="literal">accessModes</code>: The access mode for the PVC to be
         bound by OSD.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">schedulerName</code>: Scheduler name for OSD pod placement
       (optional).
      </p></li><li class="listitem"><p>
       <code class="literal">encrypted</code>: whether to encrypt all the OSDs in a given
       storageClassDeviceSet.
      </p></li></ul></div></section><section class="sect3" id="rook-storage-selection-via-ceph-drive-groups" data-id-title="Storage selection via Ceph DriveGroups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.9 </span><span class="title-name">Storage selection via Ceph DriveGroups</span> <a title="Permalink" class="permalink" href="#rook-storage-selection-via-ceph-drive-groups">#</a></h4></div></div></div><p>
     Ceph DriveGroups allow for specifying highly advanced OSD layouts. Refer
     to <span class="intraxref">Book “Administration and Operations Guide”, Chapter 13 “Operational tasks”, Section 13.4.3 “Adding OSDs using DriveGroups specification”</span> for both general information and
     detailed specification of DriveGroups with useful examples.
    </p><div id="id-1.7.4.6.3.7.11.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
      When managing a Rook/Ceph cluster’s OSD layouts with DriveGroups, the
      <code class="literal">storage</code> configuration is mostly ignored.
      <code class="literal">storageClassDeviceSets</code> can still be used to create
      OSDs on PVC, but Rook will no longer use <code class="literal">storage</code>
      configurations for creating OSDs on a node's devices. To avoid confusion,
      we recommend using the <code class="literal">storage</code> configuration
      <span class="emphasis"><em>or</em></span> <code class="literal">DriveGroups</code>, but never both.
      Because <code class="literal">storage</code> and <code class="literal">DriveGroups</code>
      should not be used simultaneously, Rook only supports provisioning OSDs
      with DriveGroups on new Rook-Ceph clusters.
     </p></div><p>
     DriveGroups are defined by a name, a Ceph DriveGroups spec, and a Rook
     placement.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: A name for the DriveGroups.
      </p></li><li class="listitem"><p>
       <code class="literal">spec</code>: The Ceph DriveGroups spec. Some components of
       the spec are treated differently in the context of Rook as noted
       below.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         Rook overrides Ceph’s definition of <code class="literal">placement</code>
         in order to use Rook’s <code class="literal">placement</code> below.
        </p></li><li class="listitem"><p>
         Rook overrides Ceph’s <code class="literal">service_id</code> field to be
         the same as the DriveGroups <code class="literal">name</code> above.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">placement</code>: The placement criteria for nodes to
       provision with the DriveGroups (optional; default is no placement
       criteria, which matches all untainted nodes). The syntax is the same as
       for <a class="xref" href="#rook-placement-configuration-settings" title="6.1.3.11. Placement configuration settings">Section 6.1.3.11, “Placement configuration settings”</a>.
      </p></li></ul></div></section><section class="sect3" id="rook-annotations-and-labels" data-id-title="Annotations and labels"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.10 </span><span class="title-name">Annotations and labels</span> <a title="Permalink" class="permalink" href="#rook-annotations-and-labels">#</a></h4></div></div></div><p>
     Annotations and Labels can be specified so that the Rook components will
     have those annotations or labels added to them.
    </p><p>
     You can set annotations and labels for Rook components for the list of
     key value pairs:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">all</code>: Set annotations / labels for all components
      </p></li><li class="listitem"><p>
       <code class="literal">mgr</code>: Set annotations / labels for MGRs
      </p></li><li class="listitem"><p>
       <code class="literal">mon</code>: Set annotations / labels for MONs
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: Set annotations / labels for OSDs
      </p></li><li class="listitem"><p>
       <code class="literal">prepareosd</code>: Set annotations / labels for OSD Prepare
       Jobs
      </p></li></ul></div><p>
     When other keys are set, <code class="literal">all</code> will be merged together
     with the specific component.
    </p></section><section class="sect3" id="rook-placement-configuration-settings" data-id-title="Placement configuration settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.11 </span><span class="title-name">Placement configuration settings</span> <a title="Permalink" class="permalink" href="#rook-placement-configuration-settings">#</a></h4></div></div></div><p>
     Placement configuration for the cluster services. It includes the
     following keys: <code class="literal">mgr</code>, <code class="literal">mon</code>,
     <code class="literal">osd</code>, <code class="literal">cleanup</code>, and
     <code class="literal">all</code>. Each service will have its placement configuration
     generated by merging the generic configuration under
     <code class="literal">all</code> with the most specific one (which will override any
     attributes).
    </p><div id="id-1.7.4.6.3.7.13.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Placement of OSD pods is controlled using the
      <a class="xref" href="#rook-storage-class-device-sets" title="6.1.3.8. Storage class device sets">Section 6.1.3.8, “Storage class device sets”</a>, not the general
      <code class="literal">placement</code> configuration.
     </p></div><p>
     A placement configuration is specified (according to the Kubernetes PodSpec)
     as:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">nodeAffinity</code>
      </p></li><li class="listitem"><p>
       <code class="literal">podAffinity</code>
      </p></li><li class="listitem"><p>
       <code class="literal">podAntiAffinity</code>
      </p></li><li class="listitem"><p>
       <code class="literal">tolerations</code>
      </p></li><li class="listitem"><p>
       <code class="literal">topologySpreadConstraints</code>
      </p></li></ul></div><p>
     If you use <code class="literal">labelSelector</code> for OSD pods, you must write
     two rules both for <code class="literal">rook-ceph-osd</code> and
     <code class="literal">rook-ceph-osd-prepare</code>.
    </p><p>
     The Rook Ceph operator creates a job called
     <code class="literal">rook-ceph-detect-version</code> to detect the full Ceph
     version used by the given <code class="literal">cephVersion.image</code>. The
     placement from the MON section is used for the job except for the
     <code class="literal">PodAntiAffinity</code> field.
    </p></section><section class="sect3" id="rook-cluster-wide-resources-configuration-settings" data-id-title="Cluster-wide resources configuration settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.12 </span><span class="title-name">Cluster-wide resources configuration settings</span> <a title="Permalink" class="permalink" href="#rook-cluster-wide-resources-configuration-settings">#</a></h4></div></div></div><p>
     Resources should be specified so that the Rook components are handled
     after Kubernetes Pod Quality of Service classes. This allows to keep Rook
     components running when for example a node runs out of memory and the
     Rook components are not killed depending on their Quality of Service
     class.
    </p><p>
     You can set resource requests/limits for Rook components through the
     <a class="xref" href="#rook-resource-requirementslimits" title="6.1.3.13. Resource requirements and limits">Section 6.1.3.13, “Resource requirements and limits”</a> structure in the
     following keys:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">mgr</code>: Set resource requests/limits for MGRs.
      </p></li><li class="listitem"><p>
       <code class="literal">mon</code>: Set resource requests/limits for MONs.
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: Set resource requests/limits for OSDs.
      </p></li><li class="listitem"><p>
       <code class="literal">prepareosd</code>: Set resource requests/limits for OSD
       prepare job.
      </p></li><li class="listitem"><p>
       <code class="literal">crashcollector</code>: Set resource requests and limits for
       crash. This pod runs wherever there is a Ceph pod running. It scrapes
       for Ceph daemon core dumps and sends them to the Ceph manager crash
       module so that core dumps are centralized and can be easily
       listed/accessed.
      </p></li><li class="listitem"><p>
       <code class="literal">cleanup</code>: Set resource requests and limits for cleanup
       job, responsible for wiping cluster’s data after uninstall.
      </p></li></ul></div><p>
     In order to provide the best possible experience running Ceph in
     containers, Rook internally recommends minimum memory limits if resource
     limits are passed. If a user configures a limit or request value that is
     too low, Rook will still run the pod(s) and print a warning to the
     operator log.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">mon</code>: 1024 MB
      </p></li><li class="listitem"><p>
       <code class="literal">mgr</code>: 512 MB
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: 2048 MB
      </p></li><li class="listitem"><p>
       <code class="literal">mds</code>: 4096 MB
      </p></li><li class="listitem"><p>
       <code class="literal">prepareosd</code>: 50 MB
      </p></li><li class="listitem"><p>
       <code class="literal">crashcollector</code>: 60MB
      </p></li></ul></div></section><section class="sect3" id="rook-resource-requirementslimits" data-id-title="Resource requirements and limits"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.13 </span><span class="title-name">Resource requirements and limits</span> <a title="Permalink" class="permalink" href="#rook-resource-requirementslimits">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">requests</code>: Requests for CPU or memory.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">cpu</code>: Request for CPU (example: one CPU core
         <code class="literal">1</code>, 50% of one CPU core <code class="literal">500m</code>).
        </p></li><li class="listitem"><p>
         <code class="literal">memory</code>: Limit for Memory (example: one gigabyte of
         memory <code class="literal">1Gi</code>, half a gigabyte of memory
         <code class="literal">512Mi</code>).
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">limits</code>: Limits for CPU or memory.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">cpu</code>: Limit for CPU (example: one CPU core
         <code class="literal">1</code>, 50% of one CPU core <code class="literal">500m</code>).
        </p></li><li class="listitem"><p>
         <code class="literal">memory</code>: Limit for Memory (example: one gigabyte of
         memory <code class="literal">1Gi</code>, half a gigabyte of memory
         <code class="literal">512Mi</code>).
        </p></li></ul></div></li></ul></div></section><section class="sect3" id="rook-priority-class-names-configuration-settings" data-id-title="Priority class names configuration settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.14 </span><span class="title-name">Priority class names configuration settings</span> <a title="Permalink" class="permalink" href="#rook-priority-class-names-configuration-settings">#</a></h4></div></div></div><p>
     Priority class names can be specified so that the Rook components will
     have those priority class names added to them.
    </p><p>
     You can set priority class names for Rook components for the list of key
     value pairs:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">all</code>: Set priority class names for MGRs, MONs, OSDs.
      </p></li><li class="listitem"><p>
       <code class="literal">mgr</code>: Set priority class names for MGRs.
      </p></li><li class="listitem"><p>
       <code class="literal">mon</code>: Set priority class names for MONs.
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: Set priority class names for OSDs.
      </p></li></ul></div><p>
     The specific component keys will act as overrides to
     <code class="literal">all</code>.
    </p></section><section class="sect3" id="rook-health-settings" data-id-title="Health settings"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.3.15 </span><span class="title-name">Health settings</span> <a title="Permalink" class="permalink" href="#rook-health-settings">#</a></h4></div></div></div><p>
     Rook-Ceph will monitor the state of the CephCluster on various components
     by default. The following CRD settings are available:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">healthCheck</code>: main Ceph cluster health monitoring
       section
      </p></li></ul></div><p>
     Currently three health checks are implemented:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">mon</code>: health check on the Ceph monitors. Basic check
       as to whether monitors are members of the quorum. If after a certain
       timeout a given monitor has not rejoined the quorum, it will be failed
       over and replaced by a new monitor.
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: health check on the Ceph OSDs.
      </p></li><li class="listitem"><p>
       <code class="literal">status</code>: Ceph health status check; periodically
       checks the Ceph health state, and reflects it in the CephCluster CR
       status field.
      </p></li></ul></div><p>
     The liveness probe of each daemon can also be controlled via
     <code class="literal">livenessProbe</code>. The setting is valid for
     <code class="literal">mon</code>, <code class="literal">mgr</code> and <code class="literal">osd</code>.
     Here is a complete example for both <code class="literal">daemonHealth</code> and
     <code class="literal">livenessProbe</code>:
    </p><div class="verbatim-wrap"><pre class="screen">healthCheck:
  daemonHealth:
    mon:
      disabled: false
      interval: 45s
      timeout: 600s
    osd:
      disabled: false
      interval: 60s
    status:
      disabled: false
  livenessProbe:
    mon:
      disabled: false
    mgr:
      disabled: false
    osd:
      disabled: false</pre></div><p>
     You can change the <code class="literal">mgr</code> probe by applying the following:
    </p><div class="verbatim-wrap"><pre class="screen">healthCheck:
  livenessProbe:
    mgr:
      disabled: false
      probe:
        httpGet:
          path: /
          port: 9283
        initialDelaySeconds: 3
        periodSeconds: 3</pre></div><p>
     Changing the liveness probe is an advanced operation and should rarely be
     necessary. If you want to change these settings, start with the probe
     specification that Rook generates by default and then modify the desired
     settings.
    </p></section></section><section class="sect2" id="rook-samples" data-id-title="Samples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.1.4 </span><span class="title-name">Samples</span> <a title="Permalink" class="permalink" href="#rook-samples">#</a></h3></div></div></div><p>
    Here are several samples for configuring Ceph clusters. Each of the
    samples must also include the namespace and corresponding access granted
    for management by the Ceph operator. See the common cluster resources
    below.
   </p><section class="sect3" id="rook-storage-configuration-all-devices" data-id-title="Storage configuration: All devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.1 </span><span class="title-name">Storage configuration: All devices</span> <a title="Permalink" class="permalink" href="#rook-storage-configuration-all-devices">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: true
  dashboard:
    enabled: true
  # cluster level storage configuration and selection
  storage:
    useAllNodes: true
    useAllDevices: true
    deviceFilter:
    config:
      metadataDevice:
      databaseSizeMB: "1024" # this value can be removed for environments with normal sized disks (100 GB or larger)
      journalSizeMB: "1024"  # this value can be removed for environments with normal sized disks (20 GB or larger)
      osdsPerDevice: "1"</pre></div></section><section class="sect3" id="rook-storage-configuration-specific-devices" data-id-title="Storage configuration: Specific devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.2 </span><span class="title-name">Storage configuration: Specific devices</span> <a title="Permalink" class="permalink" href="#rook-storage-configuration-specific-devices">#</a></h4></div></div></div><p>
     Individual nodes and their configurations can be specified so that only
     the named nodes below will be used as storage resources. Each node’s
     <span class="quote">“<span class="quote">name</span>”</span> field should match their
     <span class="quote">“<span class="quote">kubernetes.io/hostname</span>”</span> label.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: true
  dashboard:
    enabled: true
  # cluster level storage configuration and selection
  storage:
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    config:
      metadataDevice:
      databaseSizeMB: "1024" # this value can be removed for environments with normal sized disks (100 GB or larger)
    nodes:
    - name: "172.17.4.201"
      devices:             # specific devices to use for storage can be specified for each node
      - name: "sdb" # Whole storage device
      - name: "sdc1" # One specific partition. Should not have a file system on it.
      - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # both device name and explicit udev links are supported
      config:         # configuration can be specified at the node level which overrides the cluster level config
        storeType: bluestore
    - name: "172.17.4.301"
      deviceFilter: "^sd."</pre></div></section><section class="sect3" id="rook-node-affinity" data-id-title="Node affinity"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.3 </span><span class="title-name">Node affinity</span> <a title="Permalink" class="permalink" href="#rook-node-affinity">#</a></h4></div></div></div><p>
     To control where various services will be scheduled by Kubernetes, use the
     placement configuration sections below. The example under
     <span class="quote">“<span class="quote">all</span>”</span> would have all services scheduled on Kubernetes nodes
     labeled with <span class="quote">“<span class="quote">role=storage-node</span>”</span> and tolerate taints with a
     key of <span class="quote">“<span class="quote">storage-node</span>”</span>.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the Ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      tolerations:
      - key: storage-node
        operator: Exists
    mgr:
      nodeAffinity:
      tolerations:
    mon:
      nodeAffinity:
      tolerations:
    osd:
      nodeAffinity:
      tolerations:</pre></div></section><section class="sect3" id="rook-resource-requestslimits" data-id-title="Resource requests and limits"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.4 </span><span class="title-name">Resource requests and limits</span> <a title="Permalink" class="permalink" href="#rook-resource-requestslimits">#</a></h4></div></div></div><p>
     To control how many resources the Rook components can request/use, you
     can set requests and limits in Kubernetes for them. You can override these
     requests and limits for OSDs per node when using <code class="literal">useAllNodes:
     false</code> in the <code class="literal">node</code> item in the
     <code class="literal">nodes</code> list.
    </p><div id="id-1.7.4.6.3.8.6.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Before setting resource requests/limits, review the Ceph documentation
      for hardware recommendations for each component.
     </p></div><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: true
  # enable the Ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
  # cluster level resource requests/limits configuration
  resources:
  storage:
    useAllNodes: false
    nodes:
    - name: "172.17.4.201"
      resources:
        limits:
          cpu: "2"
          memory: "4096Mi"
        requests:
          cpu: "2"
          memory: "4096Mi"</pre></div></section><section class="sect3" id="rook-osd-topology" data-id-title="OSD topology"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.5 </span><span class="title-name">OSD topology</span> <a title="Permalink" class="permalink" href="#rook-osd-topology">#</a></h4></div></div></div><p>
     The topology of the cluster is important in production environments where
     you want your data spread across failure domains. The topology can be
     controlled by adding labels to the nodes. When the labels are found on a
     node at first OSD deployment, Rook will add them to the desired level in
     the CRUSH map.
    </p><p>
     The complete list of labels in hierarchy order from highest to lowest is:
    </p><div class="verbatim-wrap"><pre class="screen">topology.kubernetes.io/region
topology.kubernetes.io/zone
topology.rook.io/datacenter
topology.rook.io/room
topology.rook.io/pod
topology.rook.io/pdu
topology.rook.io/row
topology.rook.io/rack
topology.rook.io/chassis</pre></div><p>
     For example, if the following labels were added to a node:
    </p><div class="verbatim-wrap"><pre class="screen">kubectl label node mynode topology.kubernetes.io/zone=zone1
kubectl label node mynode topology.rook.io/rack=rack1</pre></div><div id="id-1.7.4.6.3.8.7.7" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      For versions previous to K8s 1.17, use the topology key:
      <code class="literal">failure-domain.beta.kubernetes.io/zone</code> or region.
     </p></div><p>
     These labels would result in the following hierarchy for OSDs on that node
     (this command can be run in the Rook toolbox):
    </p><div class="verbatim-wrap"><pre class="screen">[root@mynode /]# ceph osd tree
ID CLASS WEIGHT  TYPE NAME                 STATUS REWEIGHT PRI-AFF
-1       0.01358 root default
-5       0.01358     zone zone1
-4       0.01358         rack rack1
-3       0.01358             host mynode
 0   hdd 0.00679                 osd.0         up  1.00000 1.00000
 1   hdd 0.00679                 osd.1         up  1.00000 1.00000</pre></div><p>
     Ceph requires unique names at every level in the hierarchy (CRUSH map).
     For example, you cannot have two racks with the same name that are in
     different zones. Racks in different zones must be named uniquely.
    </p><p>
     Note that the <code class="literal">host</code> is added automatically to the
     hierarchy by Rook. The host cannot be specified with a topology label.
     All topology labels are optional.
    </p><div id="id-1.7.4.6.3.8.7.12" class="admonition tip normal"><img class="symbol" alt="Tip" title="Tip" src="static/images/icon-tip.svg"/><h6>Tip</h6><p>
      When setting the node labels prior to <code class="literal">CephCluster</code>
      creation, these settings take immediate effect. However, applying this to
      an already deployed <code class="literal">CephCluster</code> requires removing each
      node from the cluster first and then re-adding it with new configuration
      to take effect. Do this node by node to keep your data safe! Check the
      result with <code class="literal">ceph osd tree</code> from the
      <a class="xref" href="#admin-caasp-cephtoolbox" title="Chapter 8. Toolboxes">Chapter 8, <em>Toolboxes</em></a>. The OSD tree should display
      the hierarchy for the nodes that already have been re-added.
     </p></div><p>
     To utilize the <code class="literal">failureDomain</code> based on the node labels,
     specify the corresponding option in the <code class="literal">CephBlockPool</code>.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: rack  # this matches the topology labels on nodes
  replicated:
    size: 3</pre></div><p>
     This configuration will split the replication of volumes across unique
     racks in the data center setup.
    </p></section><section class="sect3" id="rook-using-pvc-storage-for-monitors" data-id-title="Using PVC storage for monitors"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.6 </span><span class="title-name">Using PVC storage for monitors</span> <a title="Permalink" class="permalink" href="#rook-using-pvc-storage-for-monitors">#</a></h4></div></div></div><p>
     In the CRD specification below three monitors are created each using a
     10Gi PVC created by Rook using the <code class="literal">local-storage</code>
     storage class.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v15.2.4
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  dashboard:
    enabled: true
  storage:
    useAllNodes: true
    useAllDevices: true
    deviceFilter:
    config:
      metadataDevice:
      databaseSizeMB: "1024" # this value can be removed for environments with normal sized disks (100 GB or larger)
      journalSizeMB: "1024"  # this value can be removed for environments with normal sized disks (20 GB or larger)
      osdsPerDevice: "1"</pre></div></section><section class="sect3" id="rook-using-storageclassdevicesets" data-id-title="Using StorageClassDeviceSets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.7 </span><span class="title-name">Using StorageClassDeviceSets</span> <a title="Permalink" class="permalink" href="#rook-using-storageclassdevicesets">#</a></h4></div></div></div><p>
     In the CRD specification below, three OSDs (having specific placement and
     resource values) and three MONs with each using a 10Gi PVC, are created by
     Rook using the <code class="literal">local-storage</code> storage class.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-storage
        resources:
          requests:
            storage: 10Gi
  cephVersion:
    image: ceph/ceph:v15.2.4
    allowUnsupported: false
  dashboard:
    enabled: true
  network:
    hostNetwork: false
  storage:
    storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      resources:
        limits:
          cpu: "500m"
          memory: "4Gi"
        requests:
          cpu: "500m"
          memory: "4Gi"
      placement:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: "rook.io/cluster"
                  operator: In
                  values:
                    - cluster1
                topologyKey: "topology.kubernetes.io/zone"
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          storageClassName: local-storage
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div></section><section class="sect3" id="rook-dedicated-metadata-and-wal-device-for-osd-on-pvc" data-id-title="Dedicated metadata and WAL device for OSD on PVC"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.8 </span><span class="title-name">Dedicated metadata and WAL device for OSD on PVC</span> <a title="Permalink" class="permalink" href="#rook-dedicated-metadata-and-wal-device-for-osd-on-pvc">#</a></h4></div></div></div><p>
     In the simplest case, Ceph OSD BlueStore consumes a single (primary)
     storage device. BlueStore is the engine used by the OSD to store data.
    </p><p>
     The storage device is normally used as a whole, occupying the full device
     that is managed directly by BlueStore. It is also possible to deploy
     BlueStore across additional devices such as a DB device. This device can
     be used for storing BlueStore’s internal metadata. BlueStore (or
     rather, the embedded RocksDB) will put as much metadata as it can on the
     DB device to improve performance. If the DB device fills up, metadata will
     spill back onto the primary device (where it would have been otherwise).
     Again, it is only helpful to provision a DB device if it is faster than
     the primary device.
    </p><p>
     You can have multiple <code class="literal">volumeClaimTemplates</code> where each
     might either represent a device or a metadata device. So just taking the
     <code class="literal">storage</code> section this will give something like:
    </p><div class="verbatim-wrap"><pre class="screen">  storage:
   storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: gp2
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
      - metadata:
          name: metadata
        spec:
          resources:
            requests:
              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
              storage: 5Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)
          storageClassName: io1
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div><div id="id-1.7.4.6.3.8.10.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Rook only supports three naming conventions for a given template:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <span class="emphasis"><em>data</em></span>: represents the main OSD block device, where
        your data is being stored.
       </p></li><li class="listitem"><p>
        <span class="emphasis"><em>metadata:</em></span> represents the metadata (including
        <code class="literal">block.db</code> and <code class="literal">block.wal</code>) device
        used to store the Ceph Bluestore database for an OSD.
       </p></li><li class="listitem"><p>
        <span class="quote">“<span class="quote">wal</span>”</span>: represents the <code class="literal">block.wal</code> device
        used to store the Ceph BlueStore database for an OSD. If this
        device is set, <span class="quote">“<span class="quote">metadata</span>”</span> device will refer specifically
        to the <code class="literal">block.db</code> device. It is recommended to use a
        faster storage class for the metadata or wal device, with a slower
        device for the data. Otherwise, having a separate metadata device will
        not improve the performance.
       </p></li></ul></div></div><p>
     The BlueStore partition has the following reference combinations
     supported by the ceph-volume utility:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       A single <span class="quote">“<span class="quote">data</span>”</span> device.
      </p><div class="verbatim-wrap"><pre class="screen">  storage:
    storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: gp2
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div></li><li class="listitem"><p>
       A <span class="emphasis"><em>data</em></span> device and a <span class="emphasis"><em>metadata</em></span>
       device.
      </p><div class="verbatim-wrap"><pre class="screen">  storage:
    storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: gp2
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
      - metadata:
          name: metadata
        spec:
          resources:
            requests:
              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
              storage: 5Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)
          storageClassName: io1
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div></li><li class="listitem"><p>
       A <span class="emphasis"><em>data</em></span> device and a <span class="emphasis"><em>WAL</em></span>
       device. A WAL device can be used for BlueStore’s internal journal or
       write-ahead log (<code class="literal">block.wal</code>). It is only useful to use
       a WAL device if the device is faster than the primary device (the data
       device). There is no separate <span class="emphasis"><em>metadata</em></span> device in
       this case; the data of main OSD block and <code class="literal">block.db</code>
       are located in <span class="emphasis"><em>data</em></span> device.
      </p><div class="verbatim-wrap"><pre class="screen">  storage:
    storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: gp2
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
      - metadata:
          name: wal
        spec:
          resources:
            requests:
              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
              storage: 5Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)
          storageClassName: io1
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div></li><li class="listitem"><p>
       A <span class="emphasis"><em>data</em></span> device, a <span class="emphasis"><em>metadata</em></span>
       device and a <span class="emphasis"><em>wal</em></span> device.
      </p><div class="verbatim-wrap"><pre class="screen">  storage:
    storageClassDeviceSets:
    - name: set1
      count: 3
      portable: false
      tuneDeviceClass: false
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          resources:
            requests:
              storage: 10Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
          storageClassName: gp2
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
      - metadata:
          name: metadata
        spec:
          resources:
            requests:
              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
              storage: 5Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)
          storageClassName: io1
          volumeMode: Block
          accessModes:
            - ReadWriteOnce
      - metadata:
          name: wal
        spec:
          resources:
            requests:
              # Find the right size https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#sizing
              storage: 5Gi
          # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, io1)
          storageClassName: io1
          volumeMode: Block
          accessModes:
            - ReadWriteOnce</pre></div></li></ul></div><p>
     With the present configuration, each OSD will have its main block
     allocated a 10 GB device as well a 5 GB device to act as a
     BlueStore database.
    </p></section><section class="sect3" id="rook-external-cluster" data-id-title="External cluster"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.9 </span><span class="title-name">External cluster</span> <a title="Permalink" class="permalink" href="#rook-external-cluster">#</a></h4></div></div></div><p>
     The minimum supported Ceph version for the External Cluster is Luminous
     12.2.x.
    </p><p>
     The features available from the external cluster will vary depending on
     the version of Ceph. The following table shows the minimum version of Ceph
     for some of the features:
    </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="text-align: left; "/><col style="text-align: left; "/></colgroup><thead><tr><th style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; ">FEATURE</th><th style="text-align: left; border-bottom: 1px solid ; ">CEPH VERSION</th></tr></thead><tbody><tr><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; ">Dynamic provisioning RBD</td><td style="text-align: left; border-bottom: 1px solid ; ">12.2.X</td></tr><tr><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; ">Configure extra CRDs (object, file, NFS)<a href="#ftn.id-1.7.4.6.3.8.11.4.1.4.2.1.1" class="footnote"><sup class="footnote" id="id-1.7.4.6.3.8.11.4.1.4.2.1.1">[a]</sup></a>
        </td><td style="text-align: left; border-bottom: 1px solid ; ">13.2.3</td></tr><tr><td style="text-align: left; border-right: 1px solid ; ">Dynamic provisioning CephFS</td><td style="text-align: left; ">14.2.3</td></tr></tbody><tbody class="footnotes"><tr><td colspan="2"><div id="ftn.id-1.7.4.6.3.8.11.4.1.4.2.1.1" class="footnote"><p><a href="#id-1.7.4.6.3.8.11.4.1.4.2.1.1" class="para"><sup class="para">[a] </sup></a>
           Configure an object store, shared file system, or NFS resources in
           the local cluster to connect to the external Ceph cluster
          </p></div></td></tr></tbody></table></div><section class="sect4" id="rook-pre-requisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">6.1.4.9.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#rook-pre-requisites">#</a></h5></div></div></div><p>
      In order to configure an external Ceph cluster with Rook, we need to
      inject some information in order to connect to that cluster. You can use
      the
      <code class="filename">cluster/examples/kubernetes/ceph/import-external-cluster.sh</code>
      script to achieve that. The script will look for the following populated
      environment variables:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">NAMESPACE</code>: the namespace where the configmap and
        secrets should be injected
       </p></li><li class="listitem"><p>
        <code class="literal">ROOK_EXTERNAL_FSID</code>: the FSID of the external Ceph
        cluster. This can be retrieved via the <code class="literal">ceph fsid</code>
        command.
       </p></li><li class="listitem"><p>
        <code class="literal">ROOK_EXTERNAL_CEPH_MON_DATA</code>: this is a comma-
        separated list of running monitors' IP addresses along with their
        ports. For example,
        <code class="literal">a=172.17.0.4:6789,b=172.17.0.5:6789,c=172.17.0.6:6789</code>.
        You do not need to specify all the monitors; you can simply pass one,
        and the operator will discover the rest. The name of the monitor is the
        name that appears in the <code class="command">ceph status</code> output.
       </p></li></ul></div><p>
      Now, we need to give Rook a key to connect to the cluster in order to
      perform various operations, such as cluster health checks, CSI keys
      management, etc. We recommend generating keys with minimal access, so the
      admin key does not need to be used by the external cluster. In this case,
      the admin key is only needed to generate the keys that will be used by
      the external cluster. If the admin key is to be used by the external
      cluster, however, set the following variable:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="literal">ROOK_EXTERNAL_ADMIN_SECRET</code>:
        <span class="strong"><strong>OPTIONAL:</strong></span> the external Ceph
        cluster admin secret key. This can be retrieved via the <code class="literal">ceph
        auth get-key client.admin</code> command.
       </p></li></ul></div><div id="id-1.7.4.6.3.8.11.5.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       <span class="strong"><strong>WARNING</strong></span>: If you plan to create CRs
       (pool, rgw, mds, nfs) in the external cluster, you
       <span class="strong"><strong>MUST</strong></span> inject the client.admin keyring
       as well as injecting <code class="literal">cluster-external-management.yaml</code>
      </p></div><p>
      <span class="strong"><strong>Example</strong></span>:
     </p><div class="verbatim-wrap"><pre class="screen">export NAMESPACE=rook-ceph-external
export ROOK_EXTERNAL_FSID=3240b4aa-ddbc-42ee-98ba-4ea7b2a61514
export ROOK_EXTERNAL_CEPH_MON_DATA=a=172.17.0.4:6789
export ROOK_EXTERNAL_ADMIN_SECRET=AQC6Ylxdja+NDBAAB7qy9MEAr4VLLq4dCIvxtg==</pre></div><p>
      If the Ceph admin key is not provided, the following script needs to be
      executed on a machine that can connect to the Ceph cluster using the
      Ceph admin key. On that machine, run
      <code class="filename">cluster/examples/kubernetes/ceph/create-external-cluster-resources.sh</code>.
      The script will automatically create users and keys with the lowest
      possible privileges and populate the necessary environment variables for
      <code class="filename">cluster/examples/kubernetes/ceph/import-external-cluster.sh</code>
      to work correctly.
     </p><p>
      Finally, execute the script like this from a machine that has access to
      your Kubernetes cluster:
     </p><div class="verbatim-wrap"><pre class="screen">bash cluster/examples/kubernetes/ceph/import-external-cluster.sh</pre></div></section><section class="sect4" id="rook-cephcluster-example-consumer" data-id-title="CephCluster example (consumer)"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">6.1.4.9.2 </span><span class="title-name">CephCluster example (consumer)</span> <a title="Permalink" class="permalink" href="#rook-cephcluster-example-consumer">#</a></h5></div></div></div><p>
      Assuming the above section has successfully completed, here is a CR
      example:
     </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph-external
  namespace: rook-ceph-external
spec:
  external:
    enable: true
  crashCollector:
    disable: true
  # optionally, the ceph-mgr IP address can be pass to gather metric from the prometheus exporter
  #monitoring:
    #enabled: true
    #rulesNamespace: rook-ceph
    #externalMgrEndpoints:
      #- ip: 192.168.39.182</pre></div><p>
      Choose the namespace carefully; if you have an existing cluster managed
      by Rook, you have likely already injected
      <code class="filename">common.yaml</code>. Additionally, you need to inject
      <code class="filename">common-external.yaml</code> too.
     </p><p>
      You can now create it like this:
     </p><div class="verbatim-wrap"><pre class="screen">kubectl create -f cluster/examples/kubernetes/ceph/cluster-external.yaml</pre></div><p>
      If the previous section has not been completed, the Rook Operator will
      still acknowledge the CR creation but will wait forever to receive
      connection information.
     </p><div id="id-1.7.4.6.3.8.11.6.8" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
       If no cluster is managed by the current Rook Operator, you need to
       inject <code class="filename">common.yaml</code>, then modify
       <code class="filename">cluster-external.yaml</code> and specify
       <code class="filename">rook-ceph</code> as <code class="filename">namespace</code>.
      </p></div><p>
      If this is successful, you will see the CephCluster status as <code class="literal">
      connected</code>.
     </p><div class="verbatim-wrap"><pre class="screen">kubectl get CephCluster -n rook-ceph-external
NAME                 DATADIRHOSTPATH   MONCOUNT   AGE    STATE       HEALTH
rook-ceph-external   /var/lib/rook                162m   Connected   HEALTH_OK</pre></div><p>
      Before you create a StorageClass with this cluster you will need to
      create a pool in your external Ceph Cluster.
     </p></section><section class="sect4" id="rook-example-storageclass-based-on-external-ceph-pool" data-id-title="Example StorageClass based on external Ceph pool"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">6.1.4.9.3 </span><span class="title-name">Example StorageClass based on external Ceph pool</span> <a title="Permalink" class="permalink" href="#rook-example-storageclass-based-on-external-ceph-pool">#</a></h5></div></div></div><p>
      In the cluster, list the pools available:
     </p><div class="verbatim-wrap"><pre class="screen">rados df
POOL_NAME     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS  RD WR_OPS  WR USED COMPR UNDER COMPR
replicated_2g  0 B       0      0      0                  0       0        0      0 0 B      0 0 B        0 B         0 B</pre></div><p>
      Here is an example StorageClass configuration that uses the
      <code class="literal">replicated_2g</code> pool from the external cluster:
     </p><div class="verbatim-wrap"><pre class="screen">cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block-ext
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph-external
    # Ceph pool into which the RBD image shall be created
    pool: replicated_2g

    # RBD image format. Defaults to "2".
    imageFormat: "2"

    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph-external
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph-external
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph-external

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
allowVolumeExpansion: true
EOF</pre></div><p>
      You can now create a persistent volume based on this StorageClass.
     </p></section><section class="sect4" id="rook-cephcluster-example-management" data-id-title="CephCluster example (management)"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">6.1.4.9.4 </span><span class="title-name">CephCluster example (management)</span> <a title="Permalink" class="permalink" href="#rook-cephcluster-example-management">#</a></h5></div></div></div><p>
      The following CephCluster CR represents a cluster that will perform
      management tasks on the external cluster. It will not only act as a
      consumer, but will also allow the deployment of other CRDs such as
      CephFilesystem or CephObjectStore. As mentioned above, you would need to
      inject the admin keyring for that.
     </p><p>
      The corresponding YAML example:
     </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph-external
  namespace: rook-ceph-external
spec:
  external:
    enable: true
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: ceph/ceph:v15.2.4 # Should match external cluster version</pre></div></section></section><section class="sect3" id="rook-cleanup-policy" data-id-title="Cleanup policy"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.1.4.10 </span><span class="title-name">Cleanup policy</span> <a title="Permalink" class="permalink" href="#rook-cleanup-policy">#</a></h4></div></div></div><p>
     Rook has the ability to cleanup resources and data that were deployed
     when a <code class="command">delete cephcluster</code> command is issued. The policy
     represents the confirmation that cluster data should be forcibly deleted.
     The <code class="literal">cleanupPolicy</code> should only be added to the cluster
     when the cluster is about to be deleted. After the
     <code class="literal">confirmation</code> field of the cleanup policy is set, Rook
     will stop configuring the cluster as if the cluster is about to be
     destroyed in order to prevent these settings from being deployed
     unintentionally. The <code class="literal">cleanupPolicy</code> CR settings has
     different fields:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">confirmation</code>: Only an empty string and
       <code class="literal">yes-really-destroy-data</code> are valid values for this
       field. If an empty string is set, Rook will only remove Ceph’s
       metadata. A re-installation will not be possible unless the hosts are
       cleaned first. If <code class="literal">yes-really-destroy-data</code> the
       operator will automatically delete data on the hostpath of cluster nodes
       and clean devices with OSDs. The cluster can then be re-installed if
       desired with no further steps.
      </p></li><li class="listitem"><p>
       <code class="literal">sanitizeDisks</code>: sanitizeDisks represents advanced
       settings that can be used to sanitize drives. This field only affects if
       <code class="literal">confirmation</code> is set to
       <code class="literal">yes-really-destroy-data</code>. However, the administrator
       might want to sanitize the drives in more depth with the following
       flags:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">method</code>: indicates whether the entire disk should be
         sanitized or Ceph metadata only. Possible choices are
         <span class="quote">“<span class="quote">quick</span>”</span> (default) or <span class="quote">“<span class="quote">complete</span>”</span>.
        </p></li><li class="listitem"><p>
         <code class="literal">dataSource</code>: indicate where to get random bytes from
         to write on the disk. Possible choices are <span class="quote">“<span class="quote">zero</span>”</span>
         (default) or <span class="quote">“<span class="quote">random</span>”</span>. Using random sources will consume
         entropy from the system and will take much more time then the zero
         source.
        </p></li><li class="listitem"><p>
         <code class="literal">iteration</code>: overwrite N times instead of the default
         (1). Takes an integer value.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">allowUninstallWithVolumes</code>: If set to true, then the
       cephCluster deletion does not wait for the PVCs to be deleted. Default
       is <code class="literal">false</code>.
      </p></li></ul></div><p>
     To automate activation of the cleanup, you can use the following command:
    </p><div id="id-1.7.4.6.3.8.12.5" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Data will be permanently deleted.
     </p></div><div class="verbatim-wrap"><pre class="screen">kubectl -n rook-ceph patch cephcluster rook-ceph --type merge \
 -p '{"spec":{"cleanupPolicy":{"confirmation":"yes-really-destroy-data"}}}'</pre></div><p>
     Nothing will happen until the deletion of the CR is requested, so this can
     still be reverted. However, all new configuration by the operator will be
     blocked with this cleanup policy enabled.
    </p><p>
     Rook waits for the deletion of PVs provisioned using the CephCluster
     before proceeding to delete the CephCluster. To force deletion of the
     CephCluster without waiting for the PVs to be deleted, you can set the
     <code class="literal">allowUninstallWithVolumes</code> to <code class="literal">true</code>
     under <code class="filename">spec.CleanupPolicy</code>.
    </p></section></section></section><section class="sect1" id="rook-ceph-block-pool-crd" data-id-title="Ceph block pool CRD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.2 </span><span class="title-name">Ceph block pool CRD</span> <a title="Permalink" class="permalink" href="#rook-ceph-block-pool-crd">#</a></h2></div></div></div><p>
   Rook allows creation and customization of storage pools through the custom
   resource definitions (CRDs). The following settings are available for pools.
  </p><section class="sect2" id="rook-samples-rep" data-id-title="Samples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.1 </span><span class="title-name">Samples</span> <a title="Permalink" class="permalink" href="#rook-samples-rep">#</a></h3></div></div></div><section class="sect3" id="replicated" data-id-title="Replicated"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1.1 </span><span class="title-name">Replicated</span> <a title="Permalink" class="permalink" href="#replicated">#</a></h4></div></div></div><p>
     For optimal performance, while also adding redundancy, this sample will
     configure Ceph to make three full copies of the data on multiple nodes.
    </p><div id="id-1.7.4.6.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least one OSD per node, with each OSD located on
      three different nodes.
     </p></div><p>
     Each OSD must be located on a different node, because the
     <code class="literal">failureDomain</code> is set to <code class="literal">host</code> and the
     <code class="literal">replicated.size</code> is set to three.
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
  deviceClass: hdd</pre></div></section><section class="sect3" id="rook-erasure-coded-sample" data-id-title="Erasure coded"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.1.2 </span><span class="title-name">Erasure coded</span> <a title="Permalink" class="permalink" href="#rook-erasure-coded-sample">#</a></h4></div></div></div><p>
     This sample will lower the overall storage capacity requirement, while
     also adding redundancy by using <a class="xref" href="#rook-erasure-coding" title="6.2.2.4. Erasure coding">Section 6.2.2.4, “Erasure coding”</a>.
    </p><div id="id-1.7.4.6.4.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least three BlueStore OSDs.
     </p></div><p>
     The OSDs can be located on a single Ceph node or spread across multiple
     nodes, because the <code class="literal">failureDomain</code> is set to
     <code class="literal">osd</code> and the <code class="literal">erasureCoded</code> chunk
     settings require at least three different OSDs (two
     <code class="literal">dataChunks</code> + one <code class="literal">codingChunks</code>).
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ecpool
  namespace: rook-ceph
spec:
  failureDomain: osd
  erasureCoded:
    dataChunks: 2
    codingChunks: 1
  deviceClass: hdd</pre></div><p>
     High performance applications typically will not use erasure coding due to
     the performance overhead of creating and distributing the chunks in the
     cluster.
    </p><p>
     When creating an erasure-coded pool, we recommend creating the pool when
     you have BlueStore OSDs in your cluster.
    </p></section></section><section class="sect2" id="rook-pool-settings" data-id-title="Pool settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.2.2 </span><span class="title-name">Pool settings</span> <a title="Permalink" class="permalink" href="#rook-pool-settings">#</a></h3></div></div></div><section class="sect3" id="rook-metadata-pool-settings" data-id-title="Metadata"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.1 </span><span class="title-name">Metadata</span> <a title="Permalink" class="permalink" href="#rook-metadata-pool-settings">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name of the pool to create.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>: The namespace of the Rook cluster where
       the pool is created.
      </p></li></ul></div></section><section class="sect3" id="rook-spec" data-id-title="Specification"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.2 </span><span class="title-name">Specification</span> <a title="Permalink" class="permalink" href="#rook-spec">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">replicated</code>: Settings for a replicated pool. If
       specified, <code class="literal">erasureCoded</code> settings must not be
       specified.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">size</code>: The desired number of copies to make of the
         data in the pool.
        </p></li><li class="listitem"><p>
         <code class="literal">requireSafeReplicaSize</code>: set to false if you want to
         create a pool with size one, setting pool size one could lead to data
         loss without recovery.
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">erasureCoded</code>: Settings for an erasure-coded pool. If
       specified, <code class="literal">replicated</code> settings must not be specified.
       See below for more details on <a class="xref" href="#rook-erasure-coding" title="6.2.2.4. Erasure coding">Section 6.2.2.4, “Erasure coding”</a>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">dataChunks</code>: Number of chunks to divide the original
         object into
        </p></li><li class="listitem"><p>
         <code class="literal">codingChunks</code>: Number of coding chunks to generate
        </p></li></ul></div></li><li class="listitem"><p>
       <code class="literal">failureDomain</code>: The failure domain across which the
       data will be spread. This can be set to a value of either
       <code class="literal">osd</code> or <code class="literal">host</code>, with
       <code class="literal">host</code> being the default setting. A failure domain can
       also be set to a different type (for example, <code class="literal">rack</code>),
       if it is added as a <code class="literal">location</code> Storage Selection
       Settings. If a replicated pool of size three is configured and the
       <code class="literal">failureDomain</code> is set to <code class="literal">host</code>, all
       three copies of the replicated data will be placed on OSDs located on
       three different Ceph hosts. This case is guaranteed to tolerate a
       failure of two hosts without a loss of data. Similarly, a failure domain
       set to <code class="literal">osd</code>, can tolerate a loss of two OSD devices.
      </p><p>
       If erasure coding is used, the data and coding chunks are spread across
       the configured failure domain.
      </p><div id="id-1.7.4.6.4.4.3.2.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
        Neither Rook, nor Ceph, prevent the creation of a cluster where the
        replicated data (or erasure coded chunks) can be written safely. By
        design, Ceph will delay checking for suitable OSDs until a write
        request is made and this write can hang if there are not sufficient
        OSDs to satisfy the request.
       </p></div></li><li class="listitem"><p>
       <code class="literal">deviceClass</code>: Sets up the CRUSH rule for the pool to
       distribute data only on the specified device class. If left empty or
       unspecified, the pool will use the cluster’s default CRUSH root, which
       usually distributes data over all OSDs, regardless of their class.
      </p></li><li class="listitem"><p>
       <code class="literal">crushRoot</code>: The root in the crush map to be used by
       the pool. If left empty or unspecified, the default root will be used.
       Creating a crush hierarchy for the OSDs currently requires the Rook
       toolbox to run the Ceph tools.
      </p></li><li class="listitem"><p>
       <code class="literal">enableRBDStats</code>: Enables collecting RBD per-image IO
       statistics by enabling dynamic OSD performance counters. Defaults to
       <code class="literal">false</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">parameters</code>: Sets any parameters listed to the given
       pool
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">target_size_ratio:</code> gives a hint (%) to Ceph in
         terms of expected consumption of the total cluster capacity of a given
         pool.
        </p></li><li class="listitem"><p>
         <code class="literal">compression_mode</code>: Sets up the pool for inline
         compression when using a BlueStore OSD. If left unspecified does not
         setup any compression mode for the pool. Values supported are the same
         as BlueStore inline compression modes, such as
         <code class="literal">none</code>,
         <code class="literal">passive</code>,<code class="literal">aggressive</code>, and
         <code class="literal">force</code>.
        </p></li></ul></div></li></ul></div></section><section class="sect3" id="rook-add-specific-pool-properties" data-id-title="Add specific pool properties"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.3 </span><span class="title-name">Add specific pool properties</span> <a title="Permalink" class="permalink" href="#rook-add-specific-pool-properties">#</a></h4></div></div></div><p>
     With <code class="literal">poolProperties</code> you can set any pool property:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  parameters:
    &lt;name of the parameter&gt;: &lt;parameter value&gt;</pre></div><p>
     For example:
    </p><div class="verbatim-wrap"><pre class="screen">spec:
  parameters:
    min_size: 1</pre></div></section><section class="sect3" id="rook-erasure-coding" data-id-title="Erasure coding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.2.2.4 </span><span class="title-name">Erasure coding</span> <a title="Permalink" class="permalink" href="#rook-erasure-coding">#</a></h4></div></div></div><p>
     <a class="link" href="http://docs.ceph.com/docs/master/rados/operations/erasure-code/" target="_blank">Erasure
     coding</a> allows you to keep your data safe while reducing the storage
     overhead. Instead of creating multiple replicas of the data, erasure
     coding divides the original data into chunks of equal size, then generates
     extra chunks of that same size for redundancy.
    </p><p>
     For example, if you have an object of size 2 MB, the simplest erasure
     coding with two data chunks would divide the object into two chunks of
     size 1 MB each (data chunks). One more chunk (coding chunk) of size
     1 MB will be generated. In total, 3 MB will be stored in the
     cluster. The object will be able to suffer the loss of any one of the
     chunks and still be able to reconstruct the original object.
    </p><p>
     The number of data and coding chunks you choose will depend on your
     resiliency to loss and how much storage overhead is acceptable in your
     storage cluster. Here are some examples to illustrate how the number of
     chunks affects the storage and loss toleration.
    </p><div class="informaltable"><table style="border-collapse: collapse; border-top: 1px solid ; border-bottom: 1px solid ; border-left: 1px solid ; border-right: 1px solid ; "><colgroup><col style="text-align: left; "/><col style="text-align: left; "/><col style="text-align: left; "/><col style="text-align: left; "/><col style="text-align: left; "/></colgroup><thead><tr><th style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> Data chunks (k) </th><th style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> Coding chunks (m) </th><th style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> Total storage </th><th style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> Losses Tolerated </th><th style="text-align: left; border-bottom: 1px solid ; "> OSDs required </th></tr></thead><tbody><tr><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 1 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 1.5x </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 1 </td><td style="text-align: left; border-bottom: 1px solid ; "> 3 </td></tr><tr><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2x </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-bottom: 1px solid ; "> 4 </td></tr><tr><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 4 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 1.5x </td><td style="text-align: left; border-right: 1px solid ; border-bottom: 1px solid ; "> 2 </td><td style="text-align: left; border-bottom: 1px solid ; "> 6 </td></tr><tr><td style="text-align: left; border-right: 1px solid ; "> 16 </td><td style="text-align: left; border-right: 1px solid ; "> 4 </td><td style="text-align: left; border-right: 1px solid ; "> 1.25x </td><td style="text-align: left; border-right: 1px solid ; "> 4 </td><td style="text-align: left; "> 20 </td></tr></tbody></table></div><p>
     The <code class="literal">failureDomain</code> must be also be taken into account
     when determining the number of chunks. The failure domain determines the
     level in the Ceph CRUSH hierarchy where the chunks must be uniquely
     distributed. This decision will impact whether node losses or disk losses
     are tolerated. There could also be performance differences of placing the
     data across nodes or OSDs.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">host</code>: All chunks will be placed on unique hosts
      </p></li><li class="listitem"><p>
       <code class="literal">osd</code>: All chunks will be placed on unique OSDs
      </p></li></ul></div><p>
     If you do not have a sufficient number of hosts or OSDs for unique
     placement the pool can be created, writing to the pool will hang.
    </p><p>
     Rook currently only configures two levels in the CRUSH map. It is also
     possible to configure other levels such as <code class="literal">rack</code> with by
     adding topology labels to the nodes.
    </p></section></section></section><section class="sect1" id="rook-ceph-shared-filesystem-crd" data-id-title="Ceph shared file system CRD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">6.3 </span><span class="title-name">Ceph shared file system CRD</span> <a title="Permalink" class="permalink" href="#rook-ceph-shared-filesystem-crd">#</a></h2></div></div></div><p>
   Rook allows creation and customization of shared file systems through the
   custom resource definitions (CRDs). The following settings are available for
   Ceph file systems.
  </p><section class="sect2" id="rook-samples-rook-rep" data-id-title="Samples"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.1 </span><span class="title-name">Samples</span> <a title="Permalink" class="permalink" href="#rook-samples-rook-rep">#</a></h3></div></div></div><section class="sect3" id="rook-replicated" data-id-title="Replicated"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.3.1.1 </span><span class="title-name">Replicated</span> <a title="Permalink" class="permalink" href="#rook-replicated">#</a></h4></div></div></div><div id="id-1.7.4.6.5.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least one OSD per node, with each OSD located on
      three different nodes.
     </p></div><p>
     Each OSD must be located on a different node, because both of the defined
     pools set the <code class="literal">failureDomain</code> to <code class="literal">host</code>
     and the <code class="literal">replicated.size</code> to three.
    </p><p>
     The <code class="literal">failureDomain</code> can also be set to another location
     type (for example, <code class="literal">rack</code>), if it has been added as a
     <code class="literal">location</code> in the Storage Selection Settings.
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephFilesystem
  metadata:
    name: myfs
    namespace: rook-ceph
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPools:
      - failureDomain: host
        replicated:
          size: 3
    preservePoolsOnDelete: true
    metadataServer:
      activeCount: 1
      activeStandby: true
      # A key/value list of annotations
      annotations:
      #  key: value
      placement:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: role
      #          operator: In
      #          values:
      #          - mds-node
      #  tolerations:
      #  - key: mds-node
      #    operator: Exists
      #  podAffinity:
      #  podAntiAffinity:
      #  topologySpreadConstraints:
      resources:
      #  limits:
      #    cpu: "500m"
      #    memory: "1024Mi"
      #  requests:
      #    cpu: "500m"
      #    memory: "1024Mi"</pre></div><p>
     These definitions can be found in the <code class="filename">filesystem.yaml</code>
     file.
    </p></section><section class="sect3" id="rook-erasure-coded" data-id-title="Erasure coded"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.3.1.2 </span><span class="title-name">Erasure coded</span> <a title="Permalink" class="permalink" href="#rook-erasure-coded">#</a></h4></div></div></div><p>
     Erasure coded pools require the OSDs to use BlueStore for the configured
     <code class="literal">storeType</code>. Additionally, erasure coded pools can only
     be used with <code class="literal">dataPools</code>. The
     <code class="literal">metadataPool</code> must use a replicated pool.
    </p><div id="id-1.7.4.6.5.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least three BlueStore OSDs, with each OSD
      located on a different node.
     </p></div><p>
     The OSDs must be located on different nodes, because the
     <code class="literal">failureDomain</code> will be set to <code class="literal">host</code> by
     default, and the <code class="literal">erasureCoded</code> chunk settings require at
     least three different OSDs (two <code class="literal">dataChunks</code> + one
     <code class="literal">codingChunks</code>).
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephFilesystem
  metadata:
    name: myfs-ec
    namespace: rook-ceph
  spec:
    metadataPool:
      replicated:
        size: 3
    dataPools:
      - erasureCoded:
          dataChunks: 2
          codingChunks: 1
    metadataServer:
      activeCount: 1
      activeStandby: true</pre></div><p>
     These definitions can also be found in the
     <code class="filename">filesystem-ec.yaml</code> file.
    </p></section></section><section class="sect2" id="rook-filesystem-settings" data-id-title="File system settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.2 </span><span class="title-name">File system settings</span> <a title="Permalink" class="permalink" href="#rook-filesystem-settings">#</a></h3></div></div></div><section class="sect3" id="rook-metadata-filesystem" data-id-title="Metadata"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.3.2.1 </span><span class="title-name">Metadata</span> <a title="Permalink" class="permalink" href="#rook-metadata-filesystem">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name of the file system to create, which
       will be reflected in the pool and other resource names.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>: The namespace of the Rook cluster where
       the file system is created.
      </p></li></ul></div></section><section class="sect3" id="rook-pools" data-id-title="Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">6.3.2.2 </span><span class="title-name">Pools</span> <a title="Permalink" class="permalink" href="#rook-pools">#</a></h4></div></div></div><p>
     The pools allow all of the settings defined in the Pool CRD spec. In the
     example above, there must be at least three hosts (size three) and at
     least eight devices (six data + two coding chunks) in the cluster.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">metadataPool</code>: The settings used to create the
       filesystem metadata pool. Must use replication.
      </p></li><li class="listitem"><p>
       <code class="literal">dataPools</code>: The settings to create the file system
       data pools. If multiple pools are specified, Rook will add the pools
       to the file system. The data pools can use replication or erasure
       coding. If erasure coding pools are specified, the cluster must be
       running with BlueStore enabled on the OSDs.
      </p></li><li class="listitem"><p>
       <code class="literal">preservePoolsOnDelete</code>: If it is set to
       <code class="literal">true</code> the pools used to support the file system will
       remain when the file system will be deleted. This is a security measure
       to avoid accidental loss of data. It is set to <code class="literal">false</code>
       by default. If not specified is also deemed as <code class="literal">false</code>.
      </p></li></ul></div></section></section><section class="sect2" id="rook-metadata-server-settings" data-id-title="Metadata server settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">6.3.3 </span><span class="title-name">Metadata server settings</span> <a title="Permalink" class="permalink" href="#rook-metadata-server-settings">#</a></h3></div></div></div><p>
    The metadata server settings correspond to the MDS daemon settings.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">activeCount</code>: The number of active MDS instances. As
      load increases, CephFS will automatically partition the file system
      across the MDS instances. Rook will create double the number of MDS
      instances as requested by the active count. The extra instances will be
      in standby mode for failover.
     </p></li><li class="listitem"><p>
      <code class="literal">activeStandby</code>: If true, the extra MDS instances will
      be in active standby mode and will keep a warm cache of the file system
      metadata for faster failover. The instances will be assigned by CephFS
      in failover pairs. If false, the extra MDS instances will all be on
      passive standby mode and will not maintain a warm cache of the metadata.
     </p></li><li class="listitem"><p>
      <code class="literal">annotations</code>: Key value pair list of annotations to
      add.
     </p></li><li class="listitem"><p>
      <code class="literal">labels</code>: Key value pair list of labels to add.
     </p></li><li class="listitem"><p>
      <code class="literal">placement</code>: The mds pods can be given standard
      Kubernetes placement restrictions with <code class="literal">nodeAffinity</code>,
      <code class="literal">tolerations</code>, <code class="literal">podAffinity</code>, and
      <code class="literal">podAntiAffinity</code> similar to placement defined for
      daemons configured by the cluster CRD.
     </p></li><li class="listitem"><p>
      <code class="literal">resources</code>: Set resource requests and limits for the
      Filesystem MDS Pod(s).
     </p></li><li class="listitem"><p>
      <code class="literal">priorityClassName</code>: Set priority class name for the
      File system MDS Pod(s)
     </p></li></ul></div></section></section></section><section class="chapter" id="admin-caasp-cephconfig" data-id-title="Configuration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">7 </span><span class="title-name">Configuration</span> <a title="Permalink" class="permalink" href="#admin-caasp-cephconfig">#</a></h1></div></div></div><section class="sect1" id="configuration" data-id-title="Ceph configuration"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">7.1 </span><span class="title-name">Ceph configuration</span> <a title="Permalink" class="permalink" href="#configuration">#</a></h2></div></div></div><p>
   For almost any Ceph cluster, the user will want—and may need—
   to change some Ceph configurations. These changes often may be warranted
   in order to alter performance to meet SLAs, or to update default data
   resiliency settings.
  </p><div id="id-1.7.4.7.3.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
    Modify Ceph settings carefully, and review the Ceph configuration
    documentation before making any changes. Changing the settings could result
    in unhealthy daemons or even data loss if used incorrectly.
   </p></div><section class="sect2" id="required-configurations" data-id-title="Required configurations"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.1 </span><span class="title-name">Required configurations</span> <a title="Permalink" class="permalink" href="#required-configurations">#</a></h3></div></div></div><p>
    Rook and Ceph both strive to make configuration as easy as possible,
    but there are some configuration options which users are well advised to
    consider for any production cluster.
   </p><section class="sect3" id="default-pg-and-pgp-counts" data-id-title="Default PG and PGP counts"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.1.1 </span><span class="title-name">Default PG and PGP counts</span> <a title="Permalink" class="permalink" href="#default-pg-and-pgp-counts">#</a></h4></div></div></div><p>
     The number of PGs and PGPs can be configured on a per-pool basis, but it
     is highly advised to set default values that are appropriate for your
     Ceph cluster. Appropriate values depend on the number of OSDs the user
     expects to have backing each pool.
    </p><p>
     Pools created prior to v1.1 will have a default PG count of 100. Pools
     created after v1.1 will have Ceph's default PG count.
    </p><p>
     An easier option exists for Rook-Ceph clusters running Ceph Nautilus
     (v14.2.x) or newer. Nautilus introduced the PG auto-scaler mgr module
     capable of automatically managing PG and PGP values for pools.
    </p><p>
     In Nautilus, this module is not enabled by default, but can be enabled by
     the following setting in the CephCluster CR:
    </p><div class="verbatim-wrap"><pre class="screen">  spec:
    mgr:
      modules:
      - name: pg_autoscaler
        enabled: true</pre></div><p>
     In Octopus (v15.2.x), this module is enabled by default without the
     aforementioned setting.
    </p><p>
     With that setting, the autoscaler will be enabled for all new pools. If
     you do not desire to have the autoscaler enabled for all new pools, you
     will need to use the Rook toolbox to enable the module and enable the
     autoscaling on individual pools.
    </p><p>
     The autoscaler is not enabled for the existing pools after enabling the
     module. So if you want to enable the autoscaling for these existing pools,
     they must be configured from the toolbox.
    </p></section></section><section class="sect2" id="specifying-configuration-options" data-id-title="Specifying configuration options"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">7.1.2 </span><span class="title-name">Specifying configuration options</span> <a title="Permalink" class="permalink" href="#specifying-configuration-options">#</a></h3></div></div></div><section class="sect3" id="toolbox-ceph-cli" data-id-title="Toolbox and the Ceph CLI"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.2.1 </span><span class="title-name">Toolbox and the Ceph CLI</span> <a title="Permalink" class="permalink" href="#toolbox-ceph-cli">#</a></h4></div></div></div><p>
     The most recommended way of configuring Ceph is to set Ceph's
     configuration directly. The first method for doing so is to use Ceph's
     CLI from the Rook-Ceph toolbox pod. From the toolbox, the user can change
     Ceph configurations, enable manager modules, create users and pools, and
     much more.
    </p></section><section class="sect3" id="rook-ceph-dashboard" data-id-title="Ceph Dashboard"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.2.2 </span><span class="title-name">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#rook-ceph-dashboard">#</a></h4></div></div></div><p>
     The Ceph Dashboard is another way of setting some of Ceph’s configuration
     directly. Configuration by the Ceph Dashboard is recommended with the same
     priority as configuration via the Ceph CLI (above).
    </p></section><section class="sect3" id="advanced-configuration-via-ceph-conf-override-configmap" data-id-title="Advanced configuration via ceph.conf overrides ConfigMap"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">7.1.2.3 </span><span class="title-name">Advanced configuration via <code class="filename">ceph.conf</code> overrides ConfigMap</span> <a title="Permalink" class="permalink" href="#advanced-configuration-via-ceph-conf-override-configmap">#</a></h4></div></div></div><p>
     Setting configuration options via Ceph’s CLI requires that at least
     one MON be available for the configuration options to be set, and setting
     configuration options via dashboard requires at least one mgr to be
     available. Ceph may also have a small number of very advanced settings
     that are not able to be modified easily via CLI or dashboard. The
     <span class="strong"><strong>least</strong></span> recommended method for
     configuring Ceph is intended as a last-resort fallback in situations
     like these.
    </p></section></section></section></section><section class="chapter" id="admin-caasp-cephtoolbox" data-id-title="Toolboxes"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">8 </span><span class="title-name">Toolboxes</span> <a title="Permalink" class="permalink" href="#admin-caasp-cephtoolbox">#</a></h1></div></div></div><section class="sect1" id="rook-toolbox" data-id-title="Rook toolbox"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">8.1 </span><span class="title-name">Rook toolbox</span> <a title="Permalink" class="permalink" href="#rook-toolbox">#</a></h2></div></div></div><p>
   The Rook toolbox is a container with common tools used for rook debugging
   and testing. The toolbox is based on SUSE Linux Enterprise Server, so more
   tools of your choosing can be installed with <code class="command">zypper</code>.
  </p><p>
   The toolbox can be run in two modes:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     <a class="xref" href="#interactive-toolbox" title="8.1.1. Interactive toolbox">Section 8.1.1, “Interactive toolbox”</a>: Start a toolbox pod where you can
     connect and execute Ceph commands from a shell.
    </p></li><li class="listitem"><p>
     <a class="xref" href="#toolbox-job" title="8.1.2. Running the toolbox job">Section 8.1.2, “Running the toolbox job”</a>: Run a script with Ceph commands and
     collect the results from the job log.
    </p></li></ul></div><div id="id-1.7.4.8.3.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
    Prerequisite: Before running the toolbox you should have a running Rook
    cluster deployed.
   </p></div><section class="sect2" id="interactive-toolbox" data-id-title="Interactive toolbox"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.1.1 </span><span class="title-name">Interactive toolbox</span> <a title="Permalink" class="permalink" href="#interactive-toolbox">#</a></h3></div></div></div><p>
    The Rook toolbox can run as a deployment in a Kubernetes cluster where you
    can connect and run arbitrary Ceph commands.
   </p><p>
    Save the tools spec as <code class="filename">toolbox.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: rook-ceph-tools
    namespace: rook-ceph
    labels:
      app: rook-ceph-tools
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-tools
    template:
      metadata:
        labels:
          app: rook-ceph-tools
      spec:
        dnsPolicy: ClusterFirstWithHostNet
        containers:
        - name: rook-ceph-tools
          image: registry.suse.com/ses/7/rook/ceph:<em class="replaceable">LATEST_TAG</em>
          command: ["/tini"]
          args: ["-g", "--", "/usr/bin/toolbox.sh"]
          imagePullPolicy: IfNotPresent
          env:
            - name: ROOK_CEPH_USERNAME
              valueFrom:
                secretKeyRef:
                  name: rook-ceph-mon
                  key: ceph-username
            - name: ROOK_CEPH_SECRET
              valueFrom:
                secretKeyRef:
                  name: rook-ceph-mon
                  key: ceph-secret
          volumeMounts:
            - mountPath: /etc/ceph
              name: ceph-config
            - name: mon-endpoint-volume
              mountPath: /etc/rook
        volumes:
          - name: mon-endpoint-volume
            configMap:
              name: rook-ceph-mon-endpoints
              items:
              - key: data
                path: mon-endpoints
          - name: ceph-config
            emptyDir: {}
        tolerations:
          - key: "node.kubernetes.io/unreachable"
            operator: "Exists"
            effect: "NoExecute"
            tolerationSeconds: 5</pre></div><p>
    Launch the <code class="literal">rook-ceph-tools</code> pod:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f toolbox.yaml</pre></div><p>
    Wait for the toolbox pod to download its container and get to the
    <code class="literal">running</code> state:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l "app=rook-ceph-tools"</pre></div><p>
    Once the rook-ceph-tools pod is running, you can connect to it with:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash</pre></div><p>
    All available tools in the toolbox are ready for your troubleshooting
    needs.
   </p><p>
    <span class="strong"><strong>Example</strong></span>:
   </p><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
      <code class="command">ceph status</code>
     </p></li><li class="listitem"><p>
      <code class="command">ceph osd status</code>
     </p></li><li class="listitem"><p>
      <code class="command">ceph df</code>
     </p></li><li class="listitem"><p>
      <code class="command">rados df</code>
     </p></li></ul></div><p>
    When you are done with the toolbox, you can remove the deployment:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph delete deployment rook-ceph-tools</pre></div></section><section class="sect2" id="toolbox-job" data-id-title="Running the toolbox job"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">8.1.2 </span><span class="title-name">Running the toolbox job</span> <a title="Permalink" class="permalink" href="#toolbox-job">#</a></h3></div></div></div><p>
    If you want to run Ceph commands as a one-time operation and collect the
    results later from the logs, you can run a script as a Kubernetes Job. The
    toolbox job will run a script that is embedded in the job spec. The script
    has the full flexibility of a bash script.
   </p><p>
    In this example, the <code class="command">ceph status</code> command is executed
    when the job is created.
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: batch/v1
  kind: Job
  metadata:
    name: rook-ceph-toolbox-job
    namespace: rook-ceph
    labels:
      app: ceph-toolbox-job
  spec:
    template:
      spec:
        initContainers:
        - name: config-init
          image: registry.suse.com/ses/7/rook/ceph:<em class="replaceable">LATEST_TAG</em>
          command: ["/usr/bin/toolbox.sh"]
          args: ["--skip-watch"]
          imagePullPolicy: IfNotPresent
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: ceph-username
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: ceph-secret
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - name: mon-endpoint-volume
            mountPath: /etc/rook
        containers:
        - name: script
          image: registry.suse.com/ses/7/rook/ceph:<em class="replaceable">LATEST_TAG</em>
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
            readOnly: true
          command:
          - "bash"
          - "-c"
          - |
            # Modify this script to run any ceph, rbd, radosgw-admin, or other commands that could
            # be run in the toolbox pod. The output of the commands can be seen by getting the pod log.
            #
            # example: print the ceph status
            ceph status
        volumes:
        - name: mon-endpoint-volume
          configMap:
            name: rook-ceph-mon-endpoints
            items:
            - key: data
              path: mon-endpoints
        - name: ceph-config
          emptyDir: {}
        restartPolicy: Never</pre></div><p>
    Create the toolbox job:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f toolbox-job.yaml</pre></div><p>
    After the job completes, see the results of the script:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph logs -l job-name=rook-ceph-toolbox-job</pre></div></section></section></section><section class="chapter" id="admin-caasp-cephosd" data-id-title="Ceph OSD management"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">9 </span><span class="title-name">Ceph OSD management</span> <a title="Permalink" class="permalink" href="#admin-caasp-cephosd">#</a></h1></div></div></div><section class="sect1" id="ceph-osd-management" data-id-title="Ceph OSD management"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">9.1 </span><span class="title-name">Ceph OSD management</span> <a title="Permalink" class="permalink" href="#ceph-osd-management">#</a></h2></div></div></div><p>
   Ceph Object Storage Daemons (OSDs) are the heart and soul of the Ceph
   storage platform. Each OSD manages a local device and together they provide
   the distributed storage. Rook will automate creation and management of
   OSDs to hide the complexity based on the desired state in the CephCluster CR
   as much as possible. This guide will walk through some of the scenarios to
   configure OSDs where more configuration may be required.
  </p><section class="sect2" id="osd-health" data-id-title="Analyzing OSD health"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.1 </span><span class="title-name">Analyzing OSD health</span> <a title="Permalink" class="permalink" href="#osd-health">#</a></h3></div></div></div><p>
    The <code class="literal">rook-ceph-tools</code> pod provides a simple environment to
    run Ceph tools. The Ceph commands mentioned in this document should be
    run from the toolbox.
   </p><p>
    Once created, connect to the pod to execute the <code class="command">ceph</code>
    commands to analyze the health of the cluster, in particular the OSDs and
    placement groups (PGs). Some common commands to analyze OSDs include:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
<code class="prompt user">cephuser@adm &gt; </code>ceph osd tree
<code class="prompt user">cephuser@adm &gt; </code>ceph osd status
<code class="prompt user">cephuser@adm &gt; </code>ceph osd df
<code class="prompt user">cephuser@adm &gt; </code>ceph osd utilization</pre></div><div class="verbatim-wrap"><pre class="screen">kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash</pre></div></section><section class="sect2" id="add-an-osd" data-id-title="Adding an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.2 </span><span class="title-name">Adding an OSD</span> <a title="Permalink" class="permalink" href="#add-an-osd">#</a></h3></div></div></div><p>
    To add more OSDs, Rook automatically watches for new nodes and devices
    being added to your cluster. If they match the filters or other settings in
    the <code class="literal">storage</code> section of the cluster CR, the operator will
    create new OSDs.
   </p></section><section class="sect2" id="add-an-osd-on-a-pvc" data-id-title="Adding an OSD on a PVC"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.3 </span><span class="title-name">Adding an OSD on a PVC</span> <a title="Permalink" class="permalink" href="#add-an-osd-on-a-pvc">#</a></h3></div></div></div><p>
    In more dynamic environments where storage can be dynamically provisioned
    with a raw block storage provider, the OSDs can be backed by PVCs.
   </p><p>
    To add more OSDs, you can either increase the <code class="literal">count</code> of
    the OSDs in an existing device set or you can add more device sets to the
    cluster CR. The operator will then automatically create new OSDs according
    to the updated cluster CR.
   </p></section><section class="sect2" id="remove-an-osd" data-id-title="Removing an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.4 </span><span class="title-name">Removing an OSD</span> <a title="Permalink" class="permalink" href="#remove-an-osd">#</a></h3></div></div></div><p>
    Removal of OSDs is intentionally not automated. Rook’s charter is to
    keep your data safe, not to delete it. If you are sure you need to remove
    OSDs, it can be done. We just want you to be in control of this action.
   </p><p>
    To remove an OSD due to a failed disk or other re-configuration, consider
    the following to ensure the health of the data through the removal process:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Confirm you will have enough space on your cluster after removing your
      OSDs to properly handle the deletion.
     </p></li><li class="step"><p>
      Confirm the remaining OSDs and their placement groups (PGs) are healthy
      in order to handle the rebalancing of the data
     </p></li><li class="step"><p>
      Do not remove too many OSDs at once, wait for rebalancing between
      removing multiple OSDs
     </p></li><li class="step"><p>
      On host-based clusters, you may need to stop the Rook Operator while
      performing OSD removal steps in order to prevent Rook from detecting
      the old OSD and trying to re-create it before the disk is wiped or
      removed.
     </p></li></ol></div></div><p>
    If all the PGs are <code class="literal">active+clean</code> and there are no
    warnings about being low on space, this means the data is fully replicated
    and it is safe to proceed. If an OSD is failing, the PGs will not be
    perfectly clean, and you will need to proceed anyway.
   </p><section class="sect3" id="from-the-toolbox" data-id-title="From the toolbox"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.1 </span><span class="title-name">From the toolbox</span> <a title="Permalink" class="permalink" href="#from-the-toolbox">#</a></h4></div></div></div><div class="orderedlist"><ol class="orderedlist compact" type="1"><li class="listitem"><p>
       Determine the OSD ID for the OSD to be removed. The OSD pod may be in an
       error state, such as <code class="literal">CrashLoopBackoff</code>, or the
       <code class="command">ceph</code> commands in the toolbox may show which OSD is
       <code class="literal">down</code>.
      </p></li><li class="listitem"><p>
       Mark the OSD as <code class="literal">out</code> if not already marked as such by
       Ceph. This signals Ceph to start moving (backfilling) the data that
       was on that OSD to another OSD.
      </p><div class="verbatim-wrap"><pre class="screen">ceph osd out osd.<em class="replaceable">ID</em></pre></div><p>
       For example:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="systemitem">cephuser</code>ceph osd out osd.23</pre></div></li><li class="listitem"><p>
       Wait for the data to finish backfilling to other OSDs.
      </p><p>
       <code class="command">ceph status</code> will indicate the backfilling is done
       when all of the PGs are <code class="literal">active+clean</code>. It’s safe to
       remove the disk after that.
      </p></li><li class="listitem"><p>
       Update your CephCluster CR such that the operator will not create an OSD
       on the device anymore. Depending on your CR settings, you may need to
       remove the device from the list or update the device filter. If you are
       using <code class="option">useAllDevices: true</code>, no change to the CR is
       necessary.
      </p></li><li class="listitem"><p>
       Remove the OSD from the Ceph cluster
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd purge <em class="replaceable">ID</em> --yes-i-really-mean-it</pre></div></li><li class="listitem"><p>
       Verify the OSD is removed from the node in the CRUSH map:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tree</pre></div></li></ol></div></section><section class="sect3" id="remove-the-osd-deployment" data-id-title="Removing the OSD deployment"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">9.1.4.2 </span><span class="title-name">Removing the OSD deployment</span> <a title="Permalink" class="permalink" href="#remove-the-osd-deployment">#</a></h4></div></div></div><p>
     The operator can automatically remove OSD deployments that are considered
     <span class="quote">“<span class="quote">safe-to-destroy</span>”</span> by Ceph. After the steps above, the OSD
     will be considered safe to remove since the data has all been moved to
     other OSDs. But this will only be done automatically by the operator if
     you have this setting in the cluster CR:
    </p><div class="verbatim-wrap"><pre class="screen">removeOSDsIfOutAndSafeToRemove: true</pre></div><p>
     Otherwise, you will need to delete the deployment directly:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl delete deployment -n rook-ceph rook-ceph-osd-<em class="replaceable">ID</em></pre></div></section></section><section class="sect2" id="replace-an-osd" data-id-title="Replacing an OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.5 </span><span class="title-name">Replacing an OSD</span> <a title="Permalink" class="permalink" href="#replace-an-osd">#</a></h3></div></div></div><p>
    To replace a disk that has failed:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Run the steps in the previous section to <a class="xref" href="#remove-an-osd" title="9.1.4. Removing an OSD">Section 9.1.4, “Removing an OSD”</a>.
     </p></li><li class="step"><p>
      Replace the physical device and verify the new device is attached.
     </p></li><li class="step"><p>
      Check if your cluster CR will find the new device. If you are using
      <code class="option">useAllDevices: true</code> you can skip this step. If your
      cluster CR lists individual devices or uses a device filter you may need
      to update the CR.
     </p></li><li class="step"><p>
      The operator ideally will automatically create the new OSD within a few
      minutes of adding the new device or updating the CR. If you do not see a
      new OSD automatically created, restart the operator (by deleting the
      operator pod) to trigger the OSD creation.
     </p></li><li class="step"><p>
      Verify if the OSD is created on the node by running <code class="command">ceph osd
      tree</code> from the toolbox.
     </p></li></ol></div></div><div id="id-1.7.4.9.3.7.4" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     The OSD might have a different ID than the previous OSD that was replaced.
    </p></div></section><section class="sect2" id="remove-an-osd-from-a-pvc" data-id-title="Removing an OSD from a PVC"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">9.1.6 </span><span class="title-name">Removing an OSD from a PVC</span> <a title="Permalink" class="permalink" href="#remove-an-osd-from-a-pvc">#</a></h3></div></div></div><p>
    If you have installed your OSDs on top of PVCs and you desire to reduce the
    size of your cluster by removing OSDs:
   </p><div class="procedure"><div class="procedure-contents"><ol class="procedure" type="1"><li class="step"><p>
      Shrink the number of OSDs in the <code class="literal">storageClassDeviceSet</code>
      in the CephCluster CR.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph edit cephcluster rook-ceph</pre></div><p>
      Reduce the <code class="literal">count</code> of the OSDs to the desired number.
      Rook will not take any action to automatically remove the extra OSD(s),
      but will effectively stop managing the orphaned OSD.
     </p></li><li class="step"><p>
      Identify the orphaned PVC that belongs to the orphaned OSD.
     </p><div id="id-1.7.4.9.3.8.3.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The orphaned PVC will have the highest index among the PVCs for the
       device set.
      </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pvc -l ceph.rook.io/DeviceSet=<em class="replaceable">deviceSet</em></pre></div><p>
      For example if the device set is named <code class="literal">set1</code> and the
      <code class="literal">count</code> was reduced from <code class="literal">3</code> to
      <code class="literal">2</code>, the orphaned PVC would have the index
      <code class="literal">2</code> and might be named
      <code class="literal">set1-2-data-vbwcf</code>
     </p></li><li class="step"><p>
      Identify the orphaned OSD.
     </p><div id="id-1.7.4.9.3.8.3.3.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
       The OSD assigned to the PVC can be found in the labels on the PVC.
      </p></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l ceph.rook.io/pvc=<em class="replaceable">ORPHANED_PVC</em> -o yaml | grep ceph-osd-id</pre></div><p>
      For example, this might return:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph-osd-id: "0"</pre></div></li><li class="step"><p>
      Now proceed with the steps in the section above to
      <a class="xref" href="#remove-an-osd" title="9.1.4. Removing an OSD">Section 9.1.4, “Removing an OSD”</a> for the orphaned OSD ID.
     </p></li><li class="step"><p>
      If desired, delete the orphaned PVC after the OSD is removed.
     </p></li></ol></div></div></section></section></section><section class="chapter" id="admin-caasp-ceph-examples" data-id-title="Ceph examples"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">10 </span><span class="title-name">Ceph examples</span> <a title="Permalink" class="permalink" href="#admin-caasp-ceph-examples">#</a></h1></div></div></div><section class="sect1" id="ceph-examples" data-id-title="Ceph examples"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">10.1 </span><span class="title-name">Ceph examples</span> <a title="Permalink" class="permalink" href="#ceph-examples">#</a></h2></div></div></div><p>
   Configuration for Rook and Ceph can be configured in multiple ways to
   provide block devices, shared file system volumes, or object storage in a
   Kubernetes namespace. We have provided several examples to simplify storage
   setup, but remember there are many tunables and you will need to decide what
   settings work for your use case and environment.
  </p><section class="sect2" id="common-resources" data-id-title="Creating common resources"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.1 </span><span class="title-name">Creating common resources</span> <a title="Permalink" class="permalink" href="#common-resources">#</a></h3></div></div></div><p>
    The first step to deploy Rook is to create the common resources. The
    configuration for these resources will be the same for most deployments.
    The <code class="filename">common.yaml</code> sets these resources up.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f common.yaml</pre></div><p>
    The examples all assume the operator and all Ceph daemons will be started
    in the same namespace. If you want to deploy the operator in a separate
    namespace, see the comments throughout <code class="filename">common.yaml</code>.
   </p></section><section class="sect2" id="operator" data-id-title="Creating the operator"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.2 </span><span class="title-name">Creating the operator</span> <a title="Permalink" class="permalink" href="#operator">#</a></h3></div></div></div><p>
    After the common resources are created, the next step is to create the
    Operator deployment.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="filename">operator.yaml</code>: The most common settings for
      production deployments
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f operator.yaml</pre></div></li><li class="listitem"><p>
      <code class="filename">operator-openshift.yaml</code>: Includes all of the
      operator settings for running a basic Rook cluster in an OpenShift
      environment.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>oc create -f operator-openshift.yaml</pre></div></li></ul></div><p>
    Settings for the operator are configured through environment variables on
    the operator deployment. The individual settings are documented in the
    <code class="filename">common.yaml</code>.
   </p></section><section class="sect2" id="cluster-crd" data-id-title="Creating the cluster CRD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.3 </span><span class="title-name">Creating the cluster CRD</span> <a title="Permalink" class="permalink" href="#cluster-crd">#</a></h3></div></div></div><p>
    Now that your operator is running, create your Ceph storage cluster. This
    CR contains the most critical settings that will influence how the operator
    configures the storage. It is important to understand the various ways to
    configure the cluster. These examples represent a very small set of the
    different ways to configure the storage.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="filename">cluster.yaml</code>: This file contains common settings for
      a production storage cluster. Requires at least three nodes.
     </p></li><li class="listitem"><p>
      <code class="filename">cluster-test.yaml</code>: Settings for a test cluster where
      redundancy is not configured. Requires only a single node.
     </p></li><li class="listitem"><p>
      <code class="filename">cluster-on-pvc.yaml</code>: This file contains common
      settings for backing the Ceph MONs and OSDs by PVs. Useful when running
      in cloud environments or where local PVs have been created for Ceph to
      consume.
     </p></li><li class="listitem"><p>
      <code class="filename">cluster-with-drive-groups.yaml</code>: This file contains
      example configurations for creating advanced OSD layouts on nodes using
      Ceph Drive Groups.
     </p></li><li class="listitem"><p>
      <code class="literal">cluster-external</code>: Connect to an external Ceph
      cluster with minimal access to monitor the health of the cluster and
      connect to the storage.
     </p></li><li class="listitem"><p>
      <code class="literal">cluster-external-management</code>: Connect to an external
      Ceph cluster with the admin key of the external cluster to enable
      remote creation of pools and configure services such as an Object Storage or
      Shared file system.
     </p></li></ul></div></section><section class="sect2" id="setting-up-consumable-storage" data-id-title="Setting up consumable storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">10.1.4 </span><span class="title-name">Setting up consumable storage</span> <a title="Permalink" class="permalink" href="#setting-up-consumable-storage">#</a></h3></div></div></div><p>
    Now we are ready to setup block, shared file system or object storage in
    the Rook Ceph cluster. These kinds of storage are respectively referred
    to as <code class="literal">CephBlockPool</code>, <code class="literal">Cephfilesystem</code>
    and <code class="literal">CephObjectStore</code> in the spec files.
   </p><section class="sect3" id="block-devices" data-id-title="Provisioning block devices"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.1 </span><span class="title-name">Provisioning block devices</span> <a title="Permalink" class="permalink" href="#block-devices">#</a></h4></div></div></div><p>
     Ceph can provide raw block device volumes to pods. Each example below
     sets up a storage class which can then be used to provision a block device
     in Kubernetes pods.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">storageclass.yaml</code>: This example illustrates
       replication of three for production scenarios and requires at least
       three nodes. Your data is replicated on three different Kubernetes worker
       nodes and intermittent or long-lasting single node failures will not
       result in data unavailability or loss.
      </p></li><li class="listitem"><p>
       <code class="filename">storageclass-ec.yaml</code>: Configures erasure coding for
       data durability rather than replication.
      </p></li><li class="listitem"><p>
       <code class="filename">storageclass-test.yaml</code>: Replication of one for test
       scenarios and it requires only a single node. Do not use this for
       applications that store valuable data or have high-availability storage
       requirements, since a single node failure can result in data loss.
      </p></li></ul></div><p>
     The storage classes are found in different sub-directories depending on
     the driver:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">csi/rbd</code>: The CSI driver for block devices.
      </p></li></ul></div></section><section class="sect3" id="shared-filesystem" data-id-title="Shared file system"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.2 </span><span class="title-name">Shared file system</span> <a title="Permalink" class="permalink" href="#shared-filesystem">#</a></h4></div></div></div><p>
     CephFS (CephFS) allows the user to mount a shared POSIX-compliant folder
     into one or more hosts (pods in the container world). This storage is
     similar to NFS shared storage or CIFS shared folders.
    </p><p>
     File storage contains multiple pools that can be configured for different
     scenarios:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">filesystem.yaml</code>: Replication of three for
       production scenarios. Requires at least three nodes.
      </p></li><li class="listitem"><p>
       <code class="filename">filesystem-ec.yaml</code>: Erasure coding for production
       scenarios. Requires at least three nodes.
      </p></li><li class="listitem"><p>
       <code class="filename">filesystem-test.yaml</code>: Replication of one for test
       scenarios. Requires only a single node.
      </p></li></ul></div><p>
     Dynamic provisioning is possible with the CSI driver. The storage class
     for shared file systems is found in the <code class="filename">csi/cephfs</code>
     directory.
    </p></section><section class="sect3" id="ceph-examples-object-storage" data-id-title="Object Storage"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.3 </span><span class="title-name">Object Storage</span> <a title="Permalink" class="permalink" href="#ceph-examples-object-storage">#</a></h4></div></div></div><p>
     Ceph supports storing blobs of data called objects that support
     HTTP[S]-type get/put/post and delete semantics.
    </p><p>
     Object Storage contains multiple pools that can be configured for different
     scenarios:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">object.yaml</code>: Replication of three for production
       scenarios. Requires at least three nodes.
      </p></li><li class="listitem"><p>
       <code class="filename">object-openshift.yaml</code>: Replication of three with
       Object Gateway in a port range valid for OpenShift. Requires at least three
       nodes.
      </p></li><li class="listitem"><p>
       <code class="filename">object-ec.yaml</code>: Erasure coding rather than
       replication for production scenarios. Requires at least three nodes.
      </p></li><li class="listitem"><p>
       <code class="filename">object-test.yaml</code>: Replication of one for test
       scenarios. Requires only a single node.
      </p></li></ul></div></section><section class="sect3" id="object-storage-user" data-id-title="Object Storage user"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.4 </span><span class="title-name">Object Storage user</span> <a title="Permalink" class="permalink" href="#object-storage-user">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">object-user.yaml</code>: Creates a simple object storage
       user and generates credentials for the S3 API.
      </p></li></ul></div></section><section class="sect3" id="object-storage-buckets" data-id-title="Object Storage buckets"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">10.1.4.5 </span><span class="title-name">Object Storage buckets</span> <a title="Permalink" class="permalink" href="#object-storage-buckets">#</a></h4></div></div></div><p>
     The Ceph operator also runs an object store bucket provisioner which can
     grant access to existing buckets or dynamically provision new buckets.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="filename">object-bucket-claim-retain.yaml</code>: Creates a request
       for a new bucket by referencing a StorageClass which saves the bucket
       when the initiating OBC is deleted.
      </p></li><li class="listitem"><p>
       <code class="filename">object-bucket-claim-delete.yaml</code>: Creates a request
       for a new bucket by referencing a StorageClass which deletes the bucket
       when the initiating OBC is deleted.
      </p></li><li class="listitem"><p>
       <code class="filename">storageclass-bucket-retain.yaml</code>: Creates a new
       StorageClass which defines the Ceph Object Store, a region, and
       retains the bucket after the initiating OBC is deleted.
      </p></li><li class="listitem"><p>
       <code class="filename">storageclass-bucket-delete.yaml</code> Creates a new
       StorageClass which defines the Ceph Object Store, a region, and
       deletes the bucket after the initiating OBC is deleted.
      </p></li></ul></div></section></section></section></section><section class="chapter" id="admin-caasp-advanced-config" data-id-title="Advanced configuration"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">11 </span><span class="title-name">Advanced configuration</span> <a title="Permalink" class="permalink" href="#admin-caasp-advanced-config">#</a></h1></div></div></div><section class="sect1" id="advanced-configuration" data-id-title="Performing advanced configuration tasks"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">11.1 </span><span class="title-name">Performing advanced configuration tasks</span> <a title="Permalink" class="permalink" href="#advanced-configuration">#</a></h2></div></div></div><p>
   These examples show how to perform advanced configuration tasks on your Rook
   storage cluster.
  </p><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
     <a class="xref" href="#advanced-config-prerequisites" title="11.1.1. Prerequisites">Section 11.1.1, “Prerequisites”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#use-custom-ceph-user-and-secret-for-mounting" title="11.1.2. Using custom Ceph user and secret for mounting">Section 11.1.2, “Using custom Ceph user and secret for mounting”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#log-collection" title="11.1.3. Collecting logs">Section 11.1.3, “Collecting logs”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#osd-information" title="11.1.4. OSD information">Section 11.1.4, “OSD information”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#separate-storage-groups" title="11.1.5. Separate storage groups">Section 11.1.5, “Separate storage groups”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#configuring-pools" title="11.1.6. Configure pools">Section 11.1.6, “Configure pools”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#custom-cephconf-settings" title="11.1.7. Creating custom ceph.conf settings">Section 11.1.7, “Creating custom <code class="filename">ceph.conf</code> settings”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#osd-crush-settings" title="11.1.8. OSD CRUSH settings">Section 11.1.8, “OSD CRUSH settings”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#phantom-osd-removal" title="11.1.9. Removing phantom OSD">Section 11.1.9, “Removing phantom OSD”</a>
    </p></li><li class="listitem"><p>
     <a class="xref" href="#change-failure-domain" title="11.1.10. Changing the failure domain">Section 11.1.10, “Changing the failure domain”</a>
    </p></li></ul></div><section class="sect2" id="advanced-config-prerequisites" data-id-title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.1 </span><span class="title-name">Prerequisites</span> <a title="Permalink" class="permalink" href="#advanced-config-prerequisites">#</a></h3></div></div></div><p>
    Most of the examples make use of the <code class="command">ceph</code> client
    command. A quick way to use the Ceph client suite is from a
    <a class="link" href="https://github.com/rook/rook/blob/master/Documentation/ceph-toolbox.md" target="_blank">Rook
    Toolbox container</a>.
   </p><p>
    The Kubernetes based examples assume Rook OSD pods are in the
    <code class="literal">rook-ceph</code> namespace. If you run them in a different
    namespace, modify <code class="command">kubectl -n rook-ceph [...]</code> to fit your
    situation.
   </p></section><section class="sect2" id="use-custom-ceph-user-and-secret-for-mounting" data-id-title="Using custom Ceph user and secret for mounting"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.2 </span><span class="title-name">Using custom Ceph user and secret for mounting</span> <a title="Permalink" class="permalink" href="#use-custom-ceph-user-and-secret-for-mounting">#</a></h3></div></div></div><div id="id-1.7.4.11.3.5.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     For extensive info about creating Ceph users, refer to
     <span class="intraxref">Book “Administration and Operations Guide”, Chapter 30 “Authentication with <code class="systemitem">cephx</code>”, Section 30.2.2 “Managing users”</span>
    </p></div><p>
    Using a custom Ceph user and secret key can be done for both file system
    and block storage.
   </p><p>
    Create a custom user in Ceph with read-write access in the
    <code class="filename">/bar</code> directory on CephFS (For Ceph Mimic or newer,
    use <code class="literal">data=POOL_NAME</code> instead of
    <code class="literal">pool=POOL_NAME</code>):
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph auth get-or-create-key client.user1 mon \
 'allow r' osd 'allow rw tag cephfs <em class="replaceable">pool=YOUR_FS_DATA_POOL</em>' \
 mds 'allow r, allow rw path=/bar'</pre></div><p>
    The command will return a Ceph secret key. This key should be added as a
    secret in Kubernetes like this:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create secret generic ceph-user1-secret --from-literal=key=YOUR_CEPH_KEY</pre></div><div id="id-1.7.4.11.3.5.8" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     This secret key must be created with the same name in each namespace where
     the StorageClass will be used.
    </p></div><p>
    In addition to this secret key, you must create a RoleBinding to allow the
    Rook Ceph agent to get the secret from each namespace. The RoleBinding is
    optional if you are using a ClusterRoleBinding for the Rook Ceph agent
    secret-key access. A ClusterRole which contains the permissions which are
    needed and used for the Bindings is shown as an example after the next
    step.
   </p><p>
    On a StorageClass <code class="literal">parameters</code> set the following options:
   </p><div class="verbatim-wrap"><pre class="screen">mountUser: user1
mountSecret: ceph-user1-secret</pre></div><p>
    If you want the Rook Ceph agent to require a <code class="literal">mountUser</code>
    and <code class="literal">mountSecret</code> to be set in StorageClasses using Rook,
    you need to set the environment variable
    <code class="varname">AGENT_MOUNT_SECURITY_MODE</code> to
    <code class="literal">Restricted</code> on the Rook Ceph operator deployment.
   </p><p>
    For more information on using the Ceph feature to limit access to
    CephFS paths, see
    <a class="link" href="http://docs.ceph.com/docs/mimic/cephfs/client-auth/#path-restriction" target="_blank">http://docs.ceph.com/docs/mimic/cephfs/client-auth/#path-restriction</a>.
   </p><section class="sect3" id="clusterrole" data-id-title="Creating the ClusterRole"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.2.1 </span><span class="title-name">Creating the <code class="literal">ClusterRole</code></span> <a title="Permalink" class="permalink" href="#clusterrole">#</a></h4></div></div></div><div id="id-1.7.4.11.3.5.14.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      When you are using the Helm chart to install the Rook Ceph operator,
      and have set <code class="literal">mountSecurityMode</code> to—for
      example— <code class="literal">Restricted</code>, then the below
      <code class="literal">ClusterRole</code> has already been created for you.
     </p></div><p>
     <span class="strong"><strong>This <code class="literal">ClusterRole</code> is needed no
     matter whether you want to use one <code class="literal">RoleBinding</code> per
     namespace or a <code class="literal">ClusterRoleBinding</code>.</strong></span>
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: rook-ceph-agent-mount
  labels:
    operator: rook
    storage-backend: ceph
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get</pre></div></section><section class="sect3" id="rolebinding" data-id-title="Creating the RoleBinding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.2.2 </span><span class="title-name">Creating the <code class="literal">RoleBinding</code></span> <a title="Permalink" class="permalink" href="#rolebinding">#</a></h4></div></div></div><div id="id-1.7.4.11.3.5.15.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      You either need a <code class="literal">RoleBinding</code> in each namespace in
      which a mount secret resides in, or create a
      <code class="literal">ClusterRoleBinding</code> with which the Rook Ceph agent
      has access to Kubernetes secrets in all namespaces.
     </p></div><p>
     Create the <code class="literal">RoleBinding</code> shown here in each namespace for
     which the Rook Ceph agent should read secrets for mounting. The
     <code class="literal">RoleBinding</code> subjects' <code class="literal">namespace</code> must
     be the one the Rook Ceph agent runs in (default
     <code class="literal">rook-ceph</code> for version 1.0 and newer; for previous
     versions, the default namespace was <code class="literal">rook-ceph-system</code>).
    </p><p>
     Replace <code class="literal">namespace:
     <em class="replaceable">name-of-namespace-with-mountsecret</em></code>
     according to the name of all namespaces a <code class="literal">mountSecret</code>
     can be in.
    </p><div class="verbatim-wrap"><pre class="screen">kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rook-ceph-agent-mount
  namespace: <em class="replaceable">name-of-namespace-with-mountsecret</em>
  labels:
    operator: rook
    storage-backend: ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-agent-mount
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph</pre></div></section><section class="sect3" id="clusterrolebinding" data-id-title="Creating the ClusterRoleBinding"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.2.3 </span><span class="title-name">Creating the <code class="literal">ClusterRoleBinding</code></span> <a title="Permalink" class="permalink" href="#clusterrolebinding">#</a></h4></div></div></div><p>
     This <code class="literal">ClusterRoleBinding</code> only needs to be created once,
     as it covers the whole cluster.
    </p><div class="verbatim-wrap"><pre class="screen">kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rook-ceph-agent-mount
  labels:
    operator: rook
    storage-backend: ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-agent-mount
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph</pre></div></section></section><section class="sect2" id="log-collection" data-id-title="Collecting logs"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.3 </span><span class="title-name">Collecting logs</span> <a title="Permalink" class="permalink" href="#log-collection">#</a></h3></div></div></div><p>
    All Rook logs can be collected in a Kubernetes environment with the following
    command:
   </p><div class="verbatim-wrap"><pre class="screen">for p in $(kubectl -n rook-ceph get pods -o jsonpath='{.items[*].metadata.name}')
do
  for c in $(kubectl -n rook-ceph get pod ${p} -o jsonpath='{.spec.containers[*].name}')
  do
    echo "BEGIN logs from pod: ${p} ${c}"
    kubectl -n rook-ceph logs -c ${c} ${p}
    echo "END logs from pod: ${p} ${c}"
  done
done</pre></div><p>
    This gets the logs for every container in every Rook pod, and then
    compresses them into a <code class="literal">.gz</code> archive for easy sharing.
    Note that instead of <code class="literal">gzip</code>, you could instead pipe to
    <code class="command">less</code> or to a single text file.
   </p></section><section class="sect2" id="osd-information" data-id-title="OSD information"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.4 </span><span class="title-name">OSD information</span> <a title="Permalink" class="permalink" href="#osd-information">#</a></h3></div></div></div><p>
    Keeping track of OSDs and their underlying storage devices can be
    difficult. The following scripts will clear things up quickly.
   </p><section class="sect3" id="kubernetes" data-id-title="Kubernetes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.4.1 </span><span class="title-name">Kubernetes</span> <a title="Permalink" class="permalink" href="#kubernetes">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen"># Get OSD Pods
# This uses the example/default cluster name "rook"
OSD_PODS=$(kubectl get pods --all-namespaces -l \
app=rook-ceph-osd,rook_cluster=rook-ceph -o jsonpath='{.items[*].metadata.name}')

# Find node and drive associations from OSD pods
for pod in $(echo ${OSD_PODS})
do
  echo "Pod:  ${pod}"
  echo "Node: $(kubectl -n rook-ceph get pod ${pod} -o jsonpath='{.spec.nodeName}')"
  kubectl -n rook-ceph exec ${pod} -- sh -c '\
  for i in /var/lib/ceph/osd/ceph-*; do
    [ -f ${i}/ready ] || continue
    echo -ne "-$(basename ${i}) "
    echo $(lsblk -n -o NAME,SIZE ${i}/block 2&gt; /dev/null || \
    findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type)
  done | sort -V
  echo'
done</pre></div><p>
     The output should look as follows:
    </p><div class="verbatim-wrap"><pre class="screen">Pod:  osd-m2fz2
Node: node1.zbrbdl
-osd0  sda3  557.3G  bluestore
-osd1  sdf3  110.2G  bluestore
-osd2  sdd3  277.8G  bluestore
-osd3  sdb3  557.3G  bluestore
-osd4  sde3  464.2G  bluestore
-osd5  sdc3  557.3G  bluestore

Pod:  osd-nxxnq
Node: node3.zbrbdl
-osd6   sda3  110.7G  bluestore
-osd17  sdd3  1.8T    bluestore
-osd18  sdb3  231.8G  bluestore
-osd19  sdc3  231.8G  bluestore

Pod:  osd-tww1h
Node: node2.zbrbdl
-osd7   sdc3  464.2G  bluestore
-osd8   sdj3  557.3G  bluestore
-osd9   sdf3  66.7G   bluestore
-osd10  sdd3  464.2G  bluestore
-osd11  sdb3  147.4G  bluestore
-osd12  sdi3  557.3G  bluestore
-osd13  sdk3  557.3G  bluestore
-osd14  sde3  66.7G   bluestore
-osd15  sda3  110.2G  bluestore
-osd16  sdh3  135.1G  bluestore</pre></div></section></section><section class="sect2" id="separate-storage-groups" data-id-title="Separate storage groups"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.5 </span><span class="title-name">Separate storage groups</span> <a title="Permalink" class="permalink" href="#separate-storage-groups">#</a></h3></div></div></div><div id="id-1.7.4.11.3.8.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     Instead of manually needing to set this, the
     <code class="literal">deviceClass</code> property can be used on Pool structures in
     <code class="literal">CephBlockPool</code>, <code class="literal">CephFilesystem</code> and
     <code class="literal">CephObjectStore</code> CRD objects.
    </p></div><p>
    By default Rook-Ceph puts all storage under one replication rule in the
    CRUSH Map which provides the maximum amount of storage capacity for a
    cluster. If you would like to use different storage endpoints for different
    purposes, you need to create separate storage groups.
   </p><p>
    In the following example we will separate SSD drives from spindle-based
    drives, a common practice for those looking to target certain workloads
    onto faster (database) or slower (file archive) storage.
   </p></section><section class="sect2" id="configuring-pools" data-id-title="Configure pools"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.6 </span><span class="title-name">Configure pools</span> <a title="Permalink" class="permalink" href="#configuring-pools">#</a></h3></div></div></div><section class="sect3" id="placement-group-sizing" data-id-title="Sizing placement groups"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.6.1 </span><span class="title-name">Sizing placement groups</span> <a title="Permalink" class="permalink" href="#placement-group-sizing">#</a></h4></div></div></div><div id="id-1.7.4.11.3.9.2.2" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      Since Ceph Nautilus (v14.x), you can use the Ceph Manager
      <code class="literal">pg_autoscaler</code> module to auto-scale the PGs as needed.
      If you want to enable this feature, please refer to
      <a class="xref" href="#default-pg-and-pgp-counts" title="7.1.1.1. Default PG and PGP counts">Section 7.1.1.1, “Default PG and PGP counts”</a>.
     </p></div><p>
     The general rules for deciding how many PGs your pool(s) should contain
     is:
    </p><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
       Less than five OSDs: set <code class="option">pg_num</code> to 128.
      </p></li><li class="listitem"><p>
       Between 5 and 10 OSDs: set <code class="option">pg_num</code> to 512.
      </p></li><li class="listitem"><p>
       Between 10 and 50 OSDs: set <code class="option">pg_num</code> to 1024.
      </p></li></ul></div><p>
     If you have more than 50 OSDs, you need to know how to calculate the
     <code class="option">pg_num</code> value by yourself. For calculating
     <code class="option">pg_num</code> yourself, please make use of the <span class="emphasis"><em>pgcalc
     </em></span> tool at <a class="link" href="http://ceph.com/pgcalc/" target="_blank">http://ceph.com/pgcalc/</a>.
    </p><p>
     If you are already using a pool, it is generally safe to set
     <code class="option">pg_count</code> on the fly (see
     <a class="xref" href="#setting-pg-count" title="11.1.6.2. Setting PG count">Section 11.1.6.2, “Setting PG count”</a>). Decreasing the PG count is not
     recommended on a pool that is in use. The safest way to decrease the PG
     count is to back up the data, delete the pool, and recreate it.
    </p></section><section class="sect3" id="setting-pg-count" data-id-title="Setting PG count"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.6.2 </span><span class="title-name">Setting PG count</span> <a title="Permalink" class="permalink" href="#setting-pg-count">#</a></h4></div></div></div><p>
     Be sure to read the <a class="xref" href="#placement-group-sizing" title="11.1.6.1. Sizing placement groups">Section 11.1.6.1, “Sizing placement groups”</a> section
     before changing the number of PGs.
    </p><div class="verbatim-wrap"><pre class="screen"># Set the number of PGs in the rbd pool to 512
<code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set rbd pg_num 512</pre></div></section></section><section class="sect2" id="custom-cephconf-settings" data-id-title="Creating custom ceph.conf settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.7 </span><span class="title-name">Creating custom <code class="filename">ceph.conf</code> settings</span> <a title="Permalink" class="permalink" href="#custom-cephconf-settings">#</a></h3></div></div></div><div id="id-1.7.4.11.3.10.2" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     The advised method for controlling Ceph configuration is to manually use
     the Ceph CLI or the Ceph Dashboard, because this offers the most
     flexibility. It is highly recommended that this is used only when
     absolutely necessary, and that the <code class="literal">config</code> is reset to
     an empty string if or when the configurations are no longer necessary.
     Configurations in the config file will make the Ceph cluster less
     configurable from the CLI and Ceph Dashboard and may make future tuning or
     debugging difficult.
    </p></div><p>
    Setting configs via Ceph's CLI requires that at least one MON is
    available for the configs to be set, and setting configs via Ceph Dashboard
    requires at least one MGR to be available. Ceph may also have a small
    number of very advanced settings that are not able to be modified easily
    via CLI or Ceph Dashboard. In order to set configurations before MONs are
    available or to set problematic configuration settings, the
    <code class="literal">rook-config-override</code> ConfigMap exists, and the
    <code class="literal">config</code> field can be set with the contents of a
    <code class="filename">ceph.conf</code> file. The contents will be propagated to all
    MON, MGR, OSD, MDS, and RGW daemons as an
    <code class="filename">/etc/ceph/ceph.conf</code> file.
   </p><div id="id-1.7.4.11.3.10.4" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
     Rook performs no validation on the config, so the validity of the
     settings is the user's responsibility.
    </p></div><p>
    If the <code class="literal">rook-config-override</code> ConfigMap is created before
    the cluster is started, the Ceph daemons will automatically pick up the
    settings. If you add the settings to the ConfigMap after the cluster has
    been initialized, each daemon will need to be restarted where you want the
    settings applied:
   </p><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
      MONs: ensure all three MONs are online and healthy before restarting each
      mon pod, one at a time.
     </p></li><li class="listitem"><p>
      MGRs: the pods are stateless and can be restarted as needed, but note
      that this will disrupt the Ceph dashboard during restart.
     </p></li><li class="listitem"><p>
      OSDs: restart your the pods by deleting them, one at a time, and running
      <code class="command">ceph -s</code> between each restart to ensure the cluster
      goes back to <span class="quote">“<span class="quote">active/clean</span>”</span> state.
     </p></li><li class="listitem"><p>
      RGW: the pods are stateless and can be restarted as needed.
     </p></li><li class="listitem"><p>
      MDS: the pods are stateless and can be restarted as needed.
     </p></li></ul></div><p>
    After the pod restart, the new settings should be in effect. Note that if
    the ConfigMap in the Ceph cluster's namespace is created before the
    cluster is created, the daemons will pick up the settings at first launch.
   </p><section class="sect3" id="example" data-id-title="Custom ceph.conf example"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.7.1 </span><span class="title-name">Custom <code class="filename">ceph.conf</code> example</span> <a title="Permalink" class="permalink" href="#example">#</a></h4></div></div></div><p>
     In this example we will set the default pool <code class="literal">size</code> to
     two, and tell OSD daemons not to change the weight of OSDs on startup.
    </p><div id="id-1.7.4.11.3.10.8.3" class="admonition warning normal"><img class="symbol" alt="Warning" title="Warning" src="static/images/icon-warning.svg"/><h6>Warning</h6><p>
      Modify Ceph settings carefully. You are leaving the sandbox tested by
      Rook. Changing the settings could result in unhealthy daemons or even
      data loss if used incorrectly.
     </p></div><p>
     When the Rook Operator creates a cluster, a placeholder ConfigMap is
     created that will allow you to override Ceph configuration settings.
     When the daemon pods are started, the settings specified in this ConfigMap
     will be merged with the default settings generated by Rook.
    </p><p>
     The default override settings are blank. Cutting out the extraneous
     properties, we would see the following defaults after creating a cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get ConfigMap rook-config-override -o yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: rook-config-override
  namespace: rook-ceph
data:
  config: ""</pre></div><p>
     To apply your desired configuration, you will need to update this
     ConfigMap. The next time the daemon pod(s) start, they will use the
     updated configs.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph edit configmap rook-config-override</pre></div><p>
     Modify the settings and save. Each line you add should be indented from
     the <code class="literal">config</code> property as such:
    </p><div class="verbatim-wrap"><pre class="screen">apiVersion: v1
kind: ConfigMap
metadata:
  name: rook-config-override
  namespace: rook-ceph
data:
  config: |
    [global]
    osd crush update on start = false
    osd pool default size = 2</pre></div></section></section><section class="sect2" id="osd-crush-settings" data-id-title="OSD CRUSH settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.8 </span><span class="title-name">OSD CRUSH settings</span> <a title="Permalink" class="permalink" href="#osd-crush-settings">#</a></h3></div></div></div><p>
    A useful view of the CRUSH Map (see <span class="intraxref">Book “Administration and Operations Guide”, Chapter 17 “Stored data management”</span>
    for more details) is generated with the following command:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tree</pre></div><p>
    In this section we will be tweaking some of the values seen in the output.
   </p><section class="sect3" id="osd-weight" data-id-title="OSD weight"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.8.1 </span><span class="title-name">OSD weight</span> <a title="Permalink" class="permalink" href="#osd-weight">#</a></h4></div></div></div><p>
     The CRUSH weight controls the ratio of data that should be distributed to
     each OSD. This also means a higher or lower amount of disk I/O operations
     for an OSD with higher or lower weight, respectively.
    </p><p>
     By default, OSDs get a weight relative to their storage capacity, which
     maximizes overall cluster capacity by filling all drives at the same rate,
     even if drive sizes vary. This should work for most use-cases, but the
     following situations could warrant weight changes:
    </p><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
       Your cluster has some relatively slow OSDs or nodes. Lowering their
       weight can reduce the impact of this bottleneck.
      </p></li><li class="listitem"><p>
       You are using BlueStore drives provisioned with Rook v0.3.1 or older.
       In this case you may notice OSD weights did not get set relative to
       their storage capacity. Changing the weight can fix this and maximize
       cluster capacity.
      </p></li></ul></div><p>
     This example sets the weight of osd.0 which is 600 GiB.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd crush reweight osd.0 .600</pre></div></section><section class="sect3" id="osd-primary-affinity" data-id-title="OSD primary affinity"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">11.1.8.2 </span><span class="title-name">OSD primary affinity</span> <a title="Permalink" class="permalink" href="#osd-primary-affinity">#</a></h4></div></div></div><p>
     When pools are set with a size setting greater than one, data is
     replicated between nodes and OSDs. For every chunk of data a Primary OSD
     is selected to be used for reading that data to be sent to clients. You
     can control how likely it is for an OSD to become a Primary using the
     Primary Affinity setting. This is similar to the OSD weight setting,
     except it only affects reads on the storage device, not capacity or
     writes.
    </p><p>
     In this example, we will make sure <code class="literal">osd.0</code> is only
     selected as Primary if all other OSDs holding replica data are
     unavailable:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code> osd primary-affinity osd.0 0</pre></div></section></section><section class="sect2" id="phantom-osd-removal" data-id-title="Removing phantom OSD"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.9 </span><span class="title-name">Removing phantom OSD</span> <a title="Permalink" class="permalink" href="#phantom-osd-removal">#</a></h3></div></div></div><p>
    If you have OSDs in which are not showing any disks, you can remove those
    <span class="quote">“<span class="quote">Phantom OSDs</span>”</span> by following the instructions below. To check
    for <span class="quote">“<span class="quote">Phantom OSDs</span>”</span>, you can run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd tree</pre></div><p>
    An example output looks like this:
   </p><div class="verbatim-wrap"><pre class="screen">ID  CLASS WEIGHT   TYPE NAME                STATUS REWEIGHT PRI-AFF
-1        57.38062 root default
-13        7.17258 host node1.example.com
2   hdd    3.61859      osd.2               up     1.00000  1.00000
-7              0  host node2.example.com   down   0        1.00000</pre></div><p>
    The host <code class="literal">node2.example.com</code> in the output has no disks,
    so it is most likely a <span class="quote">“<span class="quote">Phantom OSD</span>”</span>.
   </p><p>
    Now to remove it, use the ID in the first column of the output and replace
    <code class="literal">&lt;ID&gt;</code> with it. In the example output above the ID
    would be <code class="literal">-7</code>. The commands are:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd out <em class="replaceable">ID</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd crush remove osd.<em class="replaceable">ID</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph auth del osd.<em class="replaceable">ID</em>
<code class="prompt user">cephuser@adm &gt; </code>ceph osd rm <em class="replaceable">ID</em></pre></div><p>
    To recheck that the Phantom OSD was removed, re-run the following command
    and check if the OSD with the ID does not show up anymore:
   </p><div class="verbatim-wrap"><pre class="screen">ceph osd tree</pre></div></section><section class="sect2" id="change-failure-domain" data-id-title="Changing the failure domain"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">11.1.10 </span><span class="title-name">Changing the failure domain</span> <a title="Permalink" class="permalink" href="#change-failure-domain">#</a></h3></div></div></div><p>
    In Rook, it is now possible to indicate how the default CRUSH failure
    domain rule must be configured in order to ensure that replicas or erasure
    code shards are separated across hosts, and a single host failure does not
    affect availability. For instance, this is an example manifest of a block
    pool named <code class="literal">replicapool</code> configured with a
    <code class="literal">failureDomain</code> set to <code class="literal">osd</code>:
   </p><div class="verbatim-wrap"><pre class="screen">apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook
spec:
  # The failure domain will spread the replicas of the data across different failure zones
  failureDomain: osd
[...]</pre></div><p>
    However, due to several reasons, we may need to change such failure domain
    to its other value: <code class="literal">host</code>. Unfortunately, changing it
    directly in the YAML manifest is not currently handled by Rook, so we need
    to perform the change directly using Ceph commands using the Rook tools
    pod, for instance:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool get replicapool crush_rule
crush_rule: replicapool
<code class="prompt user">cephuser@adm &gt; </code>ceph osd crush rule create-replicated replicapool_host_rule default host</pre></div><p>
    Notice that the suffix <code class="literal">host_rule</code> in the name of the rule
    is just for clearness about the type of rule we are creating here, and can
    be anything else as long as it is different from the existing one. Once the
    new rule has been created, we simply apply it to our block pool:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool set replicapool crush_rule replicapool_host_rule</pre></div><p>
    And validate that it has been actually applied properly:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph osd pool get replicapool crush_rule
crush_rule: replicapool_host_rule</pre></div><p>
    If the cluster's health was <code class="literal">HEALTH_OK</code> when we performed
    this change, immediately, the new rule is applied to the cluster
    transparently without service disruption.
   </p><p>
    Exactly the same approach can be used to change from
    <code class="literal">host</code> back to <code class="literal">osd</code>.
   </p></section></section></section><section class="chapter" id="admin-caasp-object-storage" data-id-title="Object Storage"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">12 </span><span class="title-name">Object Storage</span> <a title="Permalink" class="permalink" href="#admin-caasp-object-storage">#</a></h1></div></div></div><section class="sect1" id="rook-object-storage" data-id-title="Object Storage"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.1 </span><span class="title-name">Object Storage</span> <a title="Permalink" class="permalink" href="#rook-object-storage">#</a></h2></div></div></div><p>
   Object Storage exposes an S3 API to the storage cluster for applications to
   <code class="literal">put</code> and <code class="literal">get</code> data.
  </p><section class="sect2" id="configure-an-object-store" data-id-title="Configuring the Object Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.1 </span><span class="title-name">Configuring the Object Storage</span> <a title="Permalink" class="permalink" href="#configure-an-object-store">#</a></h3></div></div></div><p>
    Rook has the ability to either deploy an Object Storage in Kubernetes or to connect
    to an external Object Gateway service. Most commonly, the Object Storage will be
    configured locally by Rook.
   </p><section class="sect3" id="create-a-local-object-store" data-id-title="Creating a local Object Storage"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.1.1 </span><span class="title-name">Creating a local Object Storage</span> <a title="Permalink" class="permalink" href="#create-a-local-object-store">#</a></h4></div></div></div><p>
     The below sample will create a <code class="literal">CephObjectStore</code> that
     starts the Object Gateway service in the cluster with an S3 API.
    </p><div id="id-1.7.4.12.3.3.3.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least three BlueStore OSDs, with each OSD
      located on a different node.
     </p></div><p>
     The OSDs must be located on different nodes, because the
     <code class="literal">failureDomain</code> is set to <code class="literal">host</code> and the
     <code class="literal">erasureCoded</code> chunk settings require at least three
     different OSDs (two <code class="literal">dataChunks</code> + one
     <code class="literal">codingChunks</code>).
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: my-store
    namespace: rook-ceph
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPool:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
    preservePoolsOnDelete: true
    gateway:
      type: s3
      sslCertificateRef:
      port: 80
      securePort:
      instances: 1
    healthCheck:
      bucket:
        disabled: false
        interval: 60s</pre></div><p>
     After the <code class="literal">CephObjectStore</code> is created, the Rook operator
     will then create all the pools and other resources necessary to start the
     service. This may take a minute to complete.
    </p><p>
     Create the object store:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f object.yaml</pre></div><p>
     To confirm the object store is configured, wait for the rgw pod to start:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-rgw</pre></div></section><section class="sect3" id="connect-external-object-store" data-id-title="Connecting to an external Object Storage"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.1.2 </span><span class="title-name">Connecting to an external Object Storage</span> <a title="Permalink" class="permalink" href="#connect-external-object-store">#</a></h4></div></div></div><p>
     Rook can connect to existing Object Gateway gateways to work in conjunction with
     the external mode of the CephCluster CRD. If you have an external
     CephCluster CR, you can instruct Rook to consume external gateways with
     the following:
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: external-store
    namespace: rook-ceph
  spec:
    gateway:
      port: 8080
      externalRgwEndpoints:
        - ip: 192.168.39.182
    healthCheck:
      bucket:
        enabled: true
        interval: 60s</pre></div><p>
     You can use the existing <code class="filename">object-external.yaml</code> file.
     When ready the <code class="literal">ceph-object-controller</code> will output a
     message in the Operator log similar to this one:
    </p><div class="verbatim-wrap"><pre class="screen">ceph-object-controller: ceph object store gateway service running at 10.100.28.138:8080</pre></div><p>
     You can now get and access the store via:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get svc -l app=rook-ceph-rgw
NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
rook-ceph-rgw-my-store   ClusterIP   10.100.28.138     none        8080/TCP   6h59m</pre></div><p>
     Any pod from your cluster can now access this endpoint:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>curl 10.100.28.138:8080</pre></div><p>
     It is also possible to use the internally registered DNS name:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>curl rook-ceph-rgw-my-store.rook-ceph:8080</pre></div><p>
     The DNS name is created with the following schema:
     <code class="literal">rook-ceph-rgw-$STORE_NAME.$NAMESPACE</code>.
    </p></section></section><section class="sect2" id="create-a-bucket" data-id-title="Creating a bucket"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.2 </span><span class="title-name">Creating a bucket</span> <a title="Permalink" class="permalink" href="#create-a-bucket">#</a></h3></div></div></div><p>
    Now that the object store is configured, next we need to create a bucket
    where a client can read and write objects. A bucket can be created by
    defining a storage class, similar to the pattern used by block and file
    storage. First, define the storage class that will allow object clients to
    create a bucket. The storage class defines the object storage system, the
    bucket retention policy, and other properties required by the
    administrator. Save the following as
    <code class="literal">storageclass-bucket-delete.yaml</code> (the example is named as
    such due to the <code class="literal">Delete</code> reclaim policy).
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
     name: rook-ceph-bucket
  provisioner: rook-ceph.ceph.rook.io/bucket
  reclaimPolicy: Delete
  parameters:
    objectStoreName: my-store
    objectStoreNamespace: rook-ceph
    region: us-east-1</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f storageclass-bucket-delete.yaml</pre></div><p>
    Based on this storage class, an object client can now request a bucket by
    creating an Object Bucket Claim (OBC). When the OBC is created, the
    Rook-Ceph bucket provisioner will create a new bucket. Notice that the OBC
    references the storage class that was created above. Save the following as
    <code class="literal">object-bucket-claim-delete.yaml</code> (the example is named as
    such due to the <code class="literal">Delete</code> reclaim policy):
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    name: ceph-bucket
  spec:
    generateBucketName: ceph-bkt
    storageClassName: rook-ceph-bucket</pre></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f object-bucket-claim-delete.yaml</pre></div><p>
    Now that the claim is created, the operator will create the bucket as well
    as generate other artifacts to enable access to the bucket. A secret and
    ConfigMap are created with the same name as the OBC and in the same
    namespace. The secret contains credentials used by the application pod to
    access the bucket. The ConfigMap contains bucket endpoint information and
    is also consumed by the pod.
   </p><section class="sect3" id="client-connections" data-id-title="Client connections"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.2.1 </span><span class="title-name">Client connections</span> <a title="Permalink" class="permalink" href="#client-connections">#</a></h4></div></div></div><p>
     The following commands extract key pieces of information from the secret
     and configmap:
    </p><div class="verbatim-wrap"><pre class="screen">#config-map, secret, OBC will part of default if no specific name space mentioned
export AWS_HOST=$(kubectl -n default get cm ceph-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}')
export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode)
export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode)</pre></div></section></section><section class="sect2" id="consume-the-object-storage" data-id-title="Consuming the Object Storage"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.3 </span><span class="title-name">Consuming the Object Storage</span> <a title="Permalink" class="permalink" href="#consume-the-object-storage">#</a></h3></div></div></div><p>
    Now that you have the Object Storage configured and a bucket created, you can
    consume the object storage from an S3 client.
   </p><p>
    This section will guide you through testing the connection to the
    <code class="literal">CephObjectStore</code> and uploading and downloading from it.
    Run the following commands after you have connected to the Rook toolbox.
   </p><section class="sect3" id="connection-environment-variables" data-id-title="Setting environment variables"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.3.1 </span><span class="title-name">Setting environment variables</span> <a title="Permalink" class="permalink" href="#connection-environment-variables">#</a></h4></div></div></div><p>
     To simplify the S3 client commands, you will want to set the four
     environment variables for use by your client (for example, inside the
     toolbox). See above for retrieving the variables for a bucket created by
     an <code class="literal">ObjectBucketClaim</code>.
    </p><div class="verbatim-wrap"><pre class="screen">export AWS_HOST=<em class="replaceable">HOST</em>
export AWS_ENDPOINT=<em class="replaceable">ENDPOINT</em>
export AWS_ACCESS_KEY_ID=<em class="replaceable">ACCESS_KEY</em>
export AWS_SECRET_ACCESS_KEY=<em class="replaceable">SECRET_KEY</em></pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">Host</code>: The DNS host name where the Object Gateway service is
       found in the cluster. Assuming you are using the default
       <code class="literal">rook-ceph</code> cluster, it will be
       <code class="literal">rook-ceph-rgw-my-store.rook-ceph</code>.
      </p></li><li class="listitem"><p>
       <code class="literal">Endpoint</code>: The endpoint where the Object Gateway service is
       listening. Run the following command and then combine the clusterIP and
       the port.
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get svc rook-ceph-rgw-my-store</pre></div></li><li class="listitem"><p>
       <code class="literal">Access key</code>: The user’s <code class="literal">access_key</code>
       as printed above
      </p></li><li class="listitem"><p>
       <code class="literal">Secret key</code>: The user’s <code class="literal">secret_key</code>
       as printed above
      </p></li></ul></div><p>
     The variables for the user generated in this example might be:
    </p><div class="verbatim-wrap"><pre class="screen">export AWS_HOST=rook-ceph-rgw-my-store.rook-ceph
export AWS_ENDPOINT=10.104.35.31:80
export AWS_ACCESS_KEY_ID=XEZDB3UJ6X7HVBE7X7MA
export AWS_SECRET_ACCESS_KEY=7yGIZON7EhFORz0I40BFniML36D2rl8CQQ5kXU6l</pre></div><p>
     The access key and secret key can be retrieved as described in the section
     above on <a class="xref" href="#client-connections" title="12.1.2.1. Client connections">Section 12.1.2.1, “Client connections”</a> or below in the section
     <a class="xref" href="#create-a-user" title="12.1.5. Creating a user">Section 12.1.5, “Creating a user”</a> if you are not creating the buckets with
     an <code class="literal">ObjectBucketClaim</code>.
    </p></section><section class="sect3" id="install-s3cmd" data-id-title="Installing the s3cmd package"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.3.2 </span><span class="title-name">Installing the <span class="package">s3cmd</span> package</span> <a title="Permalink" class="permalink" href="#install-s3cmd">#</a></h4></div></div></div><p>
     To test the <code class="literal">CephObjectStore</code> we will install the
     <code class="literal">s3cmd</code> tool into the toolbox pod.
    </p><div class="verbatim-wrap"><pre class="screen">zypper --assumeyes install s3cmd</pre></div></section><section class="sect3" id="put-or-get-an-object" data-id-title="PUT or GET an object"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.1.3.3 </span><span class="title-name">PUT or GET an object</span> <a title="Permalink" class="permalink" href="#put-or-get-an-object">#</a></h4></div></div></div><p>
     Upload a file to the newly created bucket:
    </p><div class="verbatim-wrap"><pre class="screen">echo "Hello Rook" &gt; /tmp/rookObj
s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://rookbucket</pre></div><p>
     Download and verify the file from the bucket:
    </p><div class="verbatim-wrap"><pre class="screen">s3cmd get s3://rookbucket/rookObj /tmp/rookObj-download --no-ssl --host=${AWS_HOST} --host-bucket=
cat /tmp/rookObj-download</pre></div></section></section><section class="sect2" id="access-external-cluster" data-id-title="Setting up external access to the cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.4 </span><span class="title-name">Setting up external access to the cluster</span> <a title="Permalink" class="permalink" href="#access-external-cluster">#</a></h3></div></div></div><p>
    Rook sets up the object storage so pods will have access internal to the
    cluster. If your applications are running outside the cluster, you will
    need to setup an external service through a <code class="literal">NodePort</code>.
   </p><p>
    First, note the service that exposes RGW internal to the cluster. We will
    leave this service intact and create a new service for external access.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get service rook-ceph-rgw-my-store
NAME                     CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE
rook-ceph-rgw-my-store   10.3.0.177   none        80/TCP      2m</pre></div><p>
    Save the external service as <code class="filename">rgw-external.yaml</code>:
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: v1
  kind: Service
  metadata:
    name: rook-ceph-rgw-my-store-external
    namespace: rook-ceph
    labels:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: my-store
  spec:
    ports:
    - name: rgw
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: my-store
    sessionAffinity: None
    type: NodePort</pre></div><p>
    Now create the external service:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f rgw-external.yaml</pre></div><p>
    See both Object Gateway services running and notice what port the external service
    is running on:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external
NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
rook-ceph-rgw-my-store            ClusterIP   10.104.82.228    none          80/TCP         4m
rook-ceph-rgw-my-store-external   NodePort    10.111.113.237   none          80:31536/TCP   39s</pre></div><p>
    Internally the Object Gateway service is running on port 80. The external port in
    this case is 31536.
   </p></section><section class="sect2" id="create-a-user" data-id-title="Creating a user"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.1.5 </span><span class="title-name">Creating a user</span> <a title="Permalink" class="permalink" href="#create-a-user">#</a></h3></div></div></div><p>
    If you need to create an independent set of user credentials to access the
    S3 endpoint, create a <code class="literal">CephObjectStoreUser</code>. The user will
    be used to connect to the Object Gateway service in the cluster using the S3 API.
    The user will be independent of any object bucket claims that you might
    have created in the earlier instructions in this document.
   </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephObjectStoreUser
  metadata:
    name: my-user
    namespace: rook-ceph
  spec:
    store: my-store
    displayName: "my display name"</pre></div><p>
    When the <code class="literal">CephObjectStoreUser</code> is created, the Rook
    operator will then create the RGW user on the specified
    <code class="literal">CephObjectStore</code> and store the Access Key and Secret Key
    in a kubernetes secret in the same namespace as the
    <code class="literal">CephObjectStoreUser</code>.
   </p><p>
    Create the object store user:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f object-user.yaml</pre></div><p>
    To confirm the object store user is configured, describe the secret:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph describe secret rook-ceph-object-user-my-store-my-user
  Name:		rook-ceph-object-user-my-store-my-user
  Namespace:	rook-ceph
  Labels:			app=rook-ceph-rgw
  			      rook_cluster=rook-ceph
  			      rook_object_store=my-store
  Annotations:	none

  Type:	kubernetes.io/rook

  Data
  ====
  AccessKey:	20 bytes
  SecretKey:	40 bytes</pre></div><p>
    The <code class="literal">AccessKey</code> and <code class="literal">SecretKey</code> data
    fields can be mounted in a pod as an environment variable.
   </p><p>
    To directly retrieve the secrets:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml \
 | grep AccessKey | awk '{print $2}' | base64 --decode
<code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml \
 | grep SecretKey | awk '{print $2}' | base64 --decode</pre></div></section></section><section class="sect1" id="ceph-object-store-crd" data-id-title="Ceph Object Storage CRD"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.2 </span><span class="title-name">Ceph Object Storage CRD</span> <a title="Permalink" class="permalink" href="#ceph-object-store-crd">#</a></h2></div></div></div><p>
   Rook allows creation and customization of object stores through the custom
   resource definitions (CRDs). The following settings are available for Ceph
   Object Storage.
  </p><section class="sect2" id="rook-objectstore-crd-sample" data-id-title="Sample"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.1 </span><span class="title-name">Sample</span> <a title="Permalink" class="permalink" href="#rook-objectstore-crd-sample">#</a></h3></div></div></div><section class="sect3" id="rook-objectstore-crd-erasure-coded" data-id-title="Erasure coded"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.2.1.1 </span><span class="title-name">Erasure coded</span> <a title="Permalink" class="permalink" href="#rook-objectstore-crd-erasure-coded">#</a></h4></div></div></div><p>
     Erasure-coded pools require the OSDs to use <code class="literal">bluestore</code>
     for the configured <code class="literal">storeType</code>. Additionally, erasure-
     coded pools can only be used with <code class="literal">dataPools</code>. The
     <code class="literal">metadataPool</code> must use a replicated pool.
    </p><div id="id-1.7.4.12.4.3.2.3" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
      This sample requires at least three BlueStore OSDs, with each OSD
      located on a different node.
     </p></div><p>
     The OSDs must be located on different nodes, because the
     <code class="literal">failureDomain</code> is set to <code class="literal">host</code> and the
     <code class="literal">erasureCoded</code> chunk settings require at least three
     different OSDs (two <code class="literal">dataChunks</code> + one
     <code class="literal">codingChunks</code>).
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: my-store
    namespace: rook-ceph
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPool:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
    preservePoolsOnDelete: true
    gateway:
      type: s3
      sslCertificateRef:
      port: 80
      securePort:
      instances: 1
      # A key/value list of annotations
      annotations:
      #  key: value
      placement:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: role
      #          operator: In
      #          values:
      #          - rgw-node
      #  tolerations:
      #  - key: rgw-node
      #    operator: Exists
      #  podAffinity:
      #  podAntiAffinity:
      #  topologySpreadConstraints:
      resources:
      #  limits:
      #    cpu: "500m"
      #    memory: "1024Mi"
      #  requests:
      #    cpu: "500m"
      #    memory: "1024Mi"
    #zone:
      #name: zone-a</pre></div></section></section><section class="sect2" id="object-store-settings" data-id-title="Object store settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.2 </span><span class="title-name">Object store settings</span> <a title="Permalink" class="permalink" href="#object-store-settings">#</a></h3></div></div></div><section class="sect3" id="objectstore-settings-metadata" data-id-title="Metadata"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.2.2.1 </span><span class="title-name">Metadata</span> <a title="Permalink" class="permalink" href="#objectstore-settings-metadata">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name of the object store to create, which
       will be reflected in the pool and other resource names.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>: The namespace of the Rook cluster where
       the object store is created.
      </p></li></ul></div></section><section class="sect3" id="pools" data-id-title="Pools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.2.2.2 </span><span class="title-name">Pools</span> <a title="Permalink" class="permalink" href="#pools">#</a></h4></div></div></div><p>
     The pools allow all of the settings defined in the Pool CRD spec. In the
     example above, there must be at least three hosts (size 3) and at least
     three devices (two data + one coding chunks) in the cluster.
    </p><p>
     When the <code class="literal">zone</code> section is set, pools with the object
     store's name will not be created, since the object-store will the using
     the pools created by the ceph-object-zone.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">metadataPool</code>: The settings used to create all of the
       object store metadata pools. Must use replication.
      </p></li><li class="listitem"><p>
       <code class="literal">dataPool</code>: The settings to create the object store
       data pool. Can use replication or erasure coding.
      </p></li><li class="listitem"><p>
       <code class="literal">preservePoolsOnDelete</code>: If it is set to
       <span class="quote">“<span class="quote">true</span>”</span>, the pools used to support the object store will
       remain when the object store will be deleted. This is a security measure
       to avoid accidental loss of data. It is set to <span class="quote">“<span class="quote">false</span>”</span> by
       default. If it is not specified, this is also deemed as
       <span class="quote">“<span class="quote">false</span>”</span>.
      </p></li></ul></div></section></section><section class="sect2" id="gateway-settings" data-id-title="Creating gateway settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.3 </span><span class="title-name">Creating gateway settings</span> <a title="Permalink" class="permalink" href="#gateway-settings">#</a></h3></div></div></div><p>
    The gateway settings correspond to the Object Gateway daemon settings.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">type</code>: <code class="literal">S3</code> is supported
     </p></li><li class="listitem"><p>
      <code class="literal">sslCertificateRef</code>: If the certificate is not
      specified, SSL will not be configured. If specified, this is the name of
      the Kubernetes secret that contains the SSL certificate to be used for secure
      connections to the object store. Rook will look in the secret provided
      at the <code class="literal">cert</code> key name. The value of the
      <code class="literal">cert</code> key must be in the format expected by the Object Gateway
      service: <span class="quote">“<span class="quote">The server key, server certificate, and any other CA or
      intermediate certificates be supplied in one file. Each of these items
      must be in pem form.</span>”</span>
     </p></li><li class="listitem"><p>
      <code class="literal">port</code>: The port on which the Object service will be
      reachable. If host networking is enabled, the Object Gateway daemons will also
      listen on that port. If running on SDN, the Object Gateway daemon listening port
      will be 8080 internally.
     </p></li><li class="listitem"><p>
      <code class="literal">securePort</code>: The secure port on which Object Gateway pods will
      be listening. An SSL certificate must be specified.
     </p></li><li class="listitem"><p>
      <code class="literal">instances</code>: The number of pods that will be started to
      load-balance this object store.
     </p></li><li class="listitem"><p>
      <code class="literal">externalRgwEndpoints</code>: A list of IP addresses to
      connect to external existing Object Gateways (works with external mode). This
      setting will be ignored if the <code class="literal">CephCluster</code> does not
      have <code class="literal">external</code> spec enabled.
     </p></li><li class="listitem"><p>
      <code class="literal">annotations</code>: Key-value pair list of annotations to
      add.
     </p></li><li class="listitem"><p>
      <code class="literal">labels</code>: Key-value pair list of labels to add.
     </p></li><li class="listitem"><p>
      <code class="literal">placement</code>: The Kubernetes placement settings to determine
      where the Object Gateway pods should be started in the cluster.
     </p></li><li class="listitem"><p>
      <code class="literal">resources</code>: Set resource requests/limits for the
      Gateway Pod(s).
     </p></li><li class="listitem"><p>
      <code class="literal">priorityClassName</code>: Set priority class name for the
      Gateway Pod(s).
     </p></li></ul></div><p>
    Example of external Object Gateway endpoints to connect to:
   </p><div class="verbatim-wrap"><pre class="screen">  gateway:
    port: 80
    externalRgwEndpoints:
      - ip: 192.168.39.182</pre></div><p>
    This will create a service with the endpoint
    <code class="literal">192.168.39.182</code> on port <code class="literal">80</code>, pointing
    to the Ceph object external gateway. All the other settings from the
    gateway section will be ignored, except for <code class="literal">securePort</code>.
   </p></section><section class="sect2" id="zone-settings" data-id-title="Zone settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.4 </span><span class="title-name">Zone settings</span> <a title="Permalink" class="permalink" href="#zone-settings">#</a></h3></div></div></div><p>
    The
    <a class="link" href="https://github.com/rook/rook/blob/master/Documentation/ceph-object-multisite.md" target="_blank">zone</a>
    settings allow the object store to join custom created
    <a class="link" href="https://github.com/rook/rook/blob/master/Documentation/ceph-object-multisite-crd.md" target="_blank">ceph-object-zone</a>.
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">name</code>: the name of the
      <code class="literal">ceph-object-zone</code> the object store will be in.
     </p></li></ul></div></section><section class="sect2" id="runtime-settings" data-id-title="Runtime settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.5 </span><span class="title-name">Runtime settings</span> <a title="Permalink" class="permalink" href="#runtime-settings">#</a></h3></div></div></div><section class="sect3" id="mime-types" data-id-title="MIME types"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.2.5.1 </span><span class="title-name">MIME types</span> <a title="Permalink" class="permalink" href="#mime-types">#</a></h4></div></div></div><p>
     Rook provides a default <code class="filename">mime.types</code> file for each
     Ceph Object Storage. This file is stored in a Kubernetes ConfigMap with the
     name <code class="literal">rook-ceph-rgw-&lt;STORE-NAME&gt;-mime-types</code>. For
     most users, the default file should suffice, however, the option is
     available to users to edit the <code class="filename">mime.types</code> file in the
     ConfigMap as they desire. Users may have their own special file types, and
     particularly security conscious users may wish to pare down the file to
     reduce the possibility of a file type execution attack.
    </p><p>
     Rook will not overwrite an existing <code class="filename">mime.types</code>
     ConfigMap so that user modifications will not be destroyed. If the object
     store is destroyed and recreated, the ConfigMap will also be destroyed and
     created anew.
    </p></section></section><section class="sect2" id="health-settings" data-id-title="Health settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.2.6 </span><span class="title-name">Health settings</span> <a title="Permalink" class="permalink" href="#health-settings">#</a></h3></div></div></div><p>
    Rook-Ceph will be default monitor the state of the object store endpoints.
    The following CRD settings are available:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      <code class="literal">healthCheck</code>: main object store health monitoring
      section
     </p></li></ul></div><p>
    For example:
   </p><div class="verbatim-wrap"><pre class="screen">  healthCheck:
    bucket:
      disabled: false
      interval: 60s</pre></div><p>
    The endpoint health check procedure is the following:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Create an S3 user.
     </p></li><li class="listitem"><p>
      Create a bucket with that user.
     </p></li><li class="listitem"><p>
      PUT the file in the object store.
     </p></li><li class="listitem"><p>
      GET the file from the object store.
     </p></li><li class="listitem"><p>
      Verify object consistency.
     </p></li><li class="listitem"><p>
      Update CR health status check.
     </p></li></ol></div><p>
    Rook-Ceph always keeps the bucket and the user for the health check; it
    just does a PUT and GET of an S3 object, since creating a bucket is an
    expensive operation.
   </p></section></section><section class="sect1" id="ceph-object-bucket-claim" data-id-title="Ceph object bucket claim"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.3 </span><span class="title-name">Ceph object bucket claim</span> <a title="Permalink" class="permalink" href="#ceph-object-bucket-claim">#</a></h2></div></div></div><p>
   Rook supports the creation of new buckets and access to existing buckets via
   two custom resources:
  </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
     An <code class="literal">Object Bucket Claim (OBC)</code> is custom resource which
     requests a bucket (new or existing) and is described by a Custom Resource
     Definition (CRD) shown below.
    </p></li><li class="listitem"><p>
     An <code class="literal">Object Bucket (OB)</code> is a custom resource
     automatically generated when a bucket is provisioned. It is a global
     resource, typically not visible to non-admin users, and contains
     information specific to the bucket. It is described by an OB CRD, also
     shown below.
    </p></li></ul></div><p>
   An OBC references a storage class which is created by an administrator. The
   storage class defines whether the bucket requested is a new bucket or an
   existing bucket. It also defines the bucket retention policy. Users request
   a new or existing bucket by creating an OBC which is shown below. The ceph
   provisioner detects the OBC and creates a new bucket or grants access to an
   existing bucket, depending the the storage class referenced in the OBC. It
   also generates a Secret which provides credentials to access the bucket, and
   a ConfigMap which contains the bucket’s endpoint. Application pods consume
   the information in the Secret and ConfigMap to access the bucket. Please
   note that to make provisioner watch the cluster namespace only you need to
   set <code class="literal">ROOK_OBC_WATCH_OPERATOR_NAMESPACE</code> to
   <code class="literal">true</code> in the operator manifest, otherwise it watches all
   namespaces.
  </p><section class="sect2" id="rook-objectstore-bucket-sample" data-id-title="Sample"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.3.1 </span><span class="title-name">Sample</span> <a title="Permalink" class="permalink" href="#rook-objectstore-bucket-sample">#</a></h3></div></div></div><section class="sect3" id="obc-custom-resource" data-id-title="OBC custom resource"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.3.1.1 </span><span class="title-name">OBC custom resource</span> <a title="Permalink" class="permalink" href="#obc-custom-resource">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    name: ceph-bucket [1]
    namespace: rook-ceph [2]
  spec:
    bucketName: [3]
    generateBucketName: photo-booth [4]
    storageClassName: rook-ceph-bucket [4]
    additionalConfig: [5]
      maxObjects: "1000"
      maxSize: "2G"</pre></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       <code class="literal">name</code> of the <code class="literal">ObjectBucketClaim</code>.
       This name becomes the name of the Secret and ConfigMap.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>(optional) of the
       <code class="literal">ObjectBucketClaim</code>, which is also the namespace of the
       ConfigMap and Secret.
      </p></li><li class="listitem"><p>
       <code class="literal">bucketName</code> name of the <code class="literal">bucket</code>.
       <span class="strong"><strong>Not</strong></span> recommended for new buckets,
       since names must be unique within an entire object store.
      </p></li><li class="listitem"><p>
       <code class="literal">generateBucketName</code> value becomes the prefix for a
       randomly-generated name; if supplied, then <code class="literal">bucketName</code>
       must be empty. If both <code class="literal">bucketName</code> and
       <code class="literal">generateBucketName</code> are supplied, then
       <code class="literal">BucketName</code> has precedence and
       <code class="literal">GenerateBucketName</code> is ignored. If both
       <code class="literal">bucketName</code> and <code class="literal">generateBucketName</code>
       are blank or omitted, then the storage class is expected to contain the
       name of an <span class="emphasis"><em>existing</em></span> bucket. It is an error if all
       three bucket-related names are blank or omitted.
      </p></li><li class="listitem"><p>
       <code class="literal">storageClassName</code> which defines the StorageClass which
       contains the names of the bucket provisioner, the object-store, and
       specifies the bucket-retention policy.
      </p></li><li class="listitem"><p>
       <code class="literal">additionalConfig</code> is an optional list of key-value
       pairs used to define attributes specific to the bucket being provisioned
       by this OBC. This information is typically tuned to a particular bucket
       provisioner, and may limit application portability. Options supported:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <code class="literal">maxObjects</code>: The maximum number of objects in the
         bucket
        </p></li><li class="listitem"><p>
         <code class="literal">maxSize</code>: The maximum size of the bucket, please
         note minimum recommended value is 4K.
        </p></li></ul></div></li></ol></div></section><section class="sect3" id="obc-custom-resource-after-bucket-provisioning" data-id-title="OBC custom resource after bucket provisioning"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.3.1.2 </span><span class="title-name">OBC custom resource after bucket provisioning</span> <a title="Permalink" class="permalink" href="#obc-custom-resource-after-bucket-provisioning">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    creationTimestamp: "2019-10-18T09:54:01Z"
    generation: 2
    name: ceph-bucket
    namespace: default [1]
    resourceVersion: "559491"
  spec:
    ObjectBucketName: obc-default-ceph-bucket [2]
    additionalConfig: null
    bucketName: photo-booth-c1178d61-1517-431f-8408-ec4c9fa50bee [3]
    cannedBucketAcl: ""
    ssl: false
    storageClassName: rook-ceph-bucket [4]
    versioned: false
  status:
    Phase: bound [5]</pre></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       <code class="literal">namespace</code> where OBC got created.
      </p></li><li class="listitem"><p>
       <code class="literal">ObjectBucketName</code> generated OB name created using name
       space and OBC name.
      </p></li><li class="listitem"><p>
       The generated (in this case), unique <code class="literal">bucket name</code> for
       the new bucket.
      </p></li><li class="listitem"><p>
       Name of the storage class from OBC got created.
      </p></li><li class="listitem"><p>
       Phases of bucket creation:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         <span class="emphasis"><em>Pending</em></span>: the operator is processing the request.
        </p></li><li class="listitem"><p>
         <span class="emphasis"><em>Bound</em></span>: the operator finished processing the
         request and linked the OBC and OB
        </p></li><li class="listitem"><p>
         <span class="emphasis"><em>Released</em></span>: the OB has been deleted, leaving the
         OBC unclaimed but unavailable.
        </p></li><li class="listitem"><p>
         <span class="emphasis"><em>Failed</em></span>: not currently set.
        </p></li></ul></div></li></ol></div></section><section class="sect3" id="app-pod" data-id-title="App pod"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.3.1.3 </span><span class="title-name">App pod</span> <a title="Permalink" class="permalink" href="#app-pod">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: v1
  kind: Pod
  metadata:
    name: app-pod
    namespace: dev-user
  spec:
    containers:
    - name: mycontainer
      image: redis
      envFrom: [1]
      - configMapRef:
          name: ceph-bucket [2]
      - secretRef:
          name: ceph-bucket [3]</pre></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       Use <code class="literal">env:</code> if mapping of the defined key names to the
       environment-variable names used by the app is needed.
      </p></li><li class="listitem"><p>
       Makes available to the pod as environment variables:
       <code class="literal">BUCKET_HOST</code>, <code class="literal">BUCKET_PORT</code>,
       <code class="literal">BUCKET_NAME</code>
      </p></li><li class="listitem"><p>
       makes available to the pod as environment variables:
       <code class="literal">AWS_ACCESS_KEY_ID</code>,
       <code class="literal">AWS_SECRET_ACCESS_KEY</code>
      </p></li></ol></div></section><section class="sect3" id="storageclass" data-id-title="StorageClass"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.3.1.4 </span><span class="title-name"><code class="literal">StorageClass</code></span> <a title="Permalink" class="permalink" href="#storageclass">#</a></h4></div></div></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: rook-ceph-bucket
    labels:
      aws-s3/object [1]
  provisioner: rook-ceph.ceph.rook.io/bucket [2]
  parameters: [3]
    objectStoreName: my-store
    objectStoreNamespace: rook-ceph
    region: us-west-1
    bucketName: ceph-bucket [4]
  reclaimPolicy: Delete [5]</pre></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       <code class="literal">label</code>(optional) here associates this
       <code class="literal">StorageClass</code> to a specific provisioner.
      </p></li><li class="listitem"><p>
       <code class="literal">provisioner</code> responsible for handling
       <code class="literal">OBCs</code> referencing this
       <code class="literal">StorageClass</code>.
      </p></li><li class="listitem"><p>
       <span class="strong"><strong>all</strong></span> <code class="literal">parameter</code>
       required.
      </p></li><li class="listitem"><p>
       <code class="literal">bucketName</code> is required for access to existing buckets
       but is omitted when provisioning new buckets. Unlike greenfield
       provisioning, the brownfield bucket name appears in the
       <code class="literal">StorageClass</code>, not the <code class="literal">OBC</code>.
      </p></li><li class="listitem"><p>
       rook-ceph provisioner decides how to treat the
       <code class="literal">reclaimPolicy</code> when an <code class="literal">OBC</code> is
       deleted for the bucket.
      </p></li></ol></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <span class="emphasis"><em>Delete</em></span> = physically delete the bucket.
      </p></li><li class="listitem"><p>
       <span class="emphasis"><em>Retain</em></span> = do not physically delete the bucket.
      </p></li></ul></div></section></section></section><section class="sect1" id="ceph-object-store-user-crd" data-id-title="Ceph Object Storage user custom resource definitions (CRD)"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">12.4 </span><span class="title-name">Ceph Object Storage user custom resource definitions (CRD)</span> <a title="Permalink" class="permalink" href="#ceph-object-store-user-crd">#</a></h2></div></div></div><p>
   Rook allows creation and customization of object store users through the
   custom resource definitions (CRDs). The following settings are available for
   Ceph object store users.
  </p><section class="sect2" id="rook-objectstore-user-crd-sample" data-id-title="Sample"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.4.1 </span><span class="title-name">Sample</span> <a title="Permalink" class="permalink" href="#rook-objectstore-user-crd-sample">#</a></h3></div></div></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: ceph.rook.io/v1
  kind: CephObjectStoreUser
  metadata:
    name: my-user
    namespace: rook-ceph
  spec:
    store: my-store
    displayName: my-display-name</pre></div></section><section class="sect2" id="object-store-user-settings" data-id-title="Object Storage user settings"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">12.4.2 </span><span class="title-name">Object Storage user settings</span> <a title="Permalink" class="permalink" href="#object-store-user-settings">#</a></h3></div></div></div><section class="sect3" id="objectstore-user-settings-metadata" data-id-title="Metadata"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.4.2.1 </span><span class="title-name">Metadata</span> <a title="Permalink" class="permalink" href="#objectstore-user-settings-metadata">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">name</code>: The name of the object store user to create,
       which will be reflected in the secret and other resource names.
      </p></li><li class="listitem"><p>
       <code class="literal">namespace</code>: The namespace of the Rook cluster where
       the object store user is created.
      </p></li></ul></div></section><section class="sect3" id="spec" data-id-title="Specification"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">12.4.2.2 </span><span class="title-name">Specification</span> <a title="Permalink" class="permalink" href="#spec">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">store</code>: The object store in which the user will be
       created. This matches the name of the Object Storage CRD.
      </p></li><li class="listitem"><p>
       <code class="literal">displayName</code>: The display name which will be passed to
       the <code class="literal">radosgw-admin user create</code> command.
      </p></li></ul></div></section></section></section></section><section class="chapter" id="admin-caasp-dashboard" data-id-title="Ceph Dashboard"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">13 </span><span class="title-name">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#admin-caasp-dashboard">#</a></h1></div></div></div><section class="sect1" id="caasp-ceph-dashboard" data-id-title="Ceph Dashboard"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">13.1 </span><span class="title-name">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#caasp-ceph-dashboard">#</a></h2></div></div></div><p>
   The Ceph Dashboard is a helpful tool to give you an overview of the status of
   your Ceph cluster, including overall health, status of the MOPN quorum,
   status of the MGR, OSD, and other Ceph daemons, view pools and PG status,
   show logs for the daemons, and more. Rook makes it simple to enable the
   dashboard.
  </p><div class="figure" id="id-1.7.4.13.3.3"><div class="figure-contents"><div class="mediaobject"><a href="images/dashboard_homepage.png"><img src="images/dashboard_homepage.png" alt="The Ceph Dashboard" title="The Ceph Dashboard"/></a></div></div><div class="figure-title-wrap"><h6 class="figure-title"><span class="title-number">Figure 13.1: </span><span class="title-name">The Ceph Dashboard </span><a title="Permalink" class="permalink" href="#id-1.7.4.13.3.3">#</a></h6></div></div><section class="sect2" id="enable-the-ceph-dashboard" data-id-title="Enabling the Ceph Dashboard"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.1 </span><span class="title-name">Enabling the Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#enable-the-ceph-dashboard">#</a></h3></div></div></div><p>
    The
    <a class="link" href="http://docs.ceph.com/docs/mimic/mgr/dashboard/" target="_blank">dashboard</a>
    can be enabled with settings in the CephCluster CRD. The CephCluster CRD
    must have the dashboard <code class="literal">enabled</code> setting set to
    <code class="literal">true</code>. This is the default setting in the example
    manifests.
   </p><div class="verbatim-wrap"><pre class="screen">  spec:
    dashboard:
      enabled: true</pre></div><p>
    The Rook operator will enable the <code class="literal">ceph-mgr</code> dashboard
    module. A service object will be created to expose that port inside the
    Kubernetes cluster. Rook will enable port 8443 for HTTPS access.
   </p><p>
    This example shows that port 8443 was configured.
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get service
NAME                         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr                ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         3h
rook-ceph-mgr-dashboard      ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         3h</pre></div><p>
    The first service is for reporting the Prometheus metrics, while the
    latter service is for the dashboard. If you are on a node in the cluster,
    you will be able to connect to the dashboard by using either the DNS name
    of the service at
    <code class="literal">https://rook-ceph-mgr-dashboard-https:8443</code> or by
    connecting to the cluster IP, in this example at
    <code class="literal">https://10.110.113.240:8443</code>.
   </p><div id="id-1.7.4.13.3.4.8" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
     The dashboard will only be enabled for the first Ceph object store
     created by Rook.
    </p></div><section class="sect3" id="login-credentials" data-id-title="Creating login credentials"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.1.1.1 </span><span class="title-name">Creating login credentials</span> <a title="Permalink" class="permalink" href="#login-credentials">#</a></h4></div></div></div><p>
     After you connect to the dashboard, you will need to login for secure
     access. Rook creates a default user named <code class="literal">admin</code> and
     generates a secret called
     <code class="literal">rook-ceph-dashboard-admin-password</code> in the namespace
     where the Rook Ceph cluster is running. To retrieve the generated
     password, you can run the following:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get secret rook-ceph-dashboard-password \
 -o jsonpath="{['data']['password']}" | base64 --decode &amp;&amp; echo</pre></div></section></section><section class="sect2" id="configure-the-dashboard" data-id-title="Configuring the Ceph Dashboard"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.2 </span><span class="title-name">Configuring the Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#configure-the-dashboard">#</a></h3></div></div></div><p>
    The following dashboard configuration settings are supported:
   </p><div class="verbatim-wrap"><pre class="screen">  spec:
    dashboard:
      urlPrefix: /ceph-dashboard
      port: 8443
      ssl: true</pre></div><div class="itemizedlist"><ul class="itemizedlist compact"><li class="listitem"><p>
      <code class="literal">urlPrefix</code> If you are accessing the dashboard via a
      reverse proxy, you may wish to serve it under a URL prefix. To get the
      dashboard to use hyperlinks that include your prefix, you can set the
      <code class="literal">urlPrefix</code> setting.
     </p></li><li class="listitem"><p>
      <code class="literal">port</code> The port that the dashboard is served on may be
      changed from the default using the <code class="literal">port</code> setting. The
      corresponding K8s service exposing the port will automatically be
      updated.
     </p></li><li class="listitem"><p>
      <code class="literal">ssl</code> The dashboard may be served without SSL (useful
      for when you deploy the dashboard behind a proxy already served using
      SSL) by setting the <code class="literal">ssl</code> option to be false.
     </p></li></ul></div></section><section class="sect2" id="viewing-the-dashboard-external-to-the-cluster" data-id-title="Viewing the Ceph Dashboard external to the cluster"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">13.1.3 </span><span class="title-name">Viewing the Ceph Dashboard external to the cluster</span> <a title="Permalink" class="permalink" href="#viewing-the-dashboard-external-to-the-cluster">#</a></h3></div></div></div><p>
    Commonly, you will want to view the dashboard from outside the cluster. For
    example, on a development machine with the cluster running inside minikube,
    you will want to access the dashboard from the host.
   </p><p>
    There are several ways to expose a service, which will depend on the
    environment you are running in. You can use an Ingress Controller or other
    methods for exposing services such as NodePort, LoadBalancer, or
    ExternalIPs.
   </p><section class="sect3" id="node-port" data-id-title="Node port"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.1.3.1 </span><span class="title-name">Node port</span> <a title="Permalink" class="permalink" href="#node-port">#</a></h4></div></div></div><p>
     The simplest way to expose the service in minikube or similar environments
     is using the NodePort to open a port on the VM that can be accessed by the
     host. To create a service with the NodePort, save this YAML file as
     <code class="filename">dashboard-external-https.yaml</code>.
    </p><div class="verbatim-wrap"><pre class="screen">  apiVersion: v1
  kind: Service
  metadata:
    name: rook-ceph-mgr-dashboard-external-https
    namespace: rook-ceph
    labels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
  spec:
    ports:
    - name: dashboard
      port: 8443
      protocol: TCP
      targetPort: 8443
    selector:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: NodePort</pre></div><p>
     Now create the service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f dashboard-external-https.yaml</pre></div><p>
     You will see the new service
     <code class="literal">rook-ceph-mgr-dashboard-external-https</code> created:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get service
NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
rook-ceph-mgr                           ClusterIP   10.108.111.192   &lt;none&gt;        9283/TCP         4h
rook-ceph-mgr-dashboard                 ClusterIP   10.110.113.240   &lt;none&gt;        8443/TCP         4h
rook-ceph-mgr-dashboard-external-https  NodePort    10.101.209.6     &lt;none&gt;        8443:31176/TCP   4h</pre></div><p>
     In this example, port <code class="literal">31176</code> will be opened to expose
     port <code class="literal">8443</code> from the ceph-mgr pod. Find the IP address of
     the VM. If using minikube, you can run <code class="literal">minikube ip</code> to
     find the IP address. Now you can enter the URL in your browser such as
     <code class="literal">https://192.168.99.110:31176</code> and the dashboard will
     appear.
    </p></section><section class="sect3" id="load-balancer" data-id-title="Creating the load balancer service"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.1.3.2 </span><span class="title-name">Creating the load balancer service</span> <a title="Permalink" class="permalink" href="#load-balancer">#</a></h4></div></div></div><p>
     If you have a cluster on a cloud provider that supports load balancers,
     you can create a service that is provisioned with a public hostname. The
     yaml is the same as <code class="literal">dashboard-external-https.yaml</code>
     except for the following property:
    </p><div class="verbatim-wrap"><pre class="screen">  spec:
  [...]
    type: LoadBalancer</pre></div><p>
     Now create the service:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f dashboard-loadbalancer.yaml</pre></div><p>
     You will see the new service
     <code class="literal">rook-ceph-mgr-dashboard-loadbalancer</code> created:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get service
NAME                                     TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)             AGE
rook-ceph-mgr                            ClusterIP      172.30.11.40     &lt;none&gt;                                                                    9283/TCP      4h
rook-ceph-mgr-dashboard                  ClusterIP      172.30.203.185   &lt;none&gt;                                                                    8443/TCP      4h
rook-ceph-mgr-dashboard-loadbalancer     LoadBalancer   172.30.27.242    a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com   8443:32747/TCP      4h</pre></div><p>
     Now you can enter the URL in your browser such as
     <code class="literal">https://a7f23e8e2839511e9b7a5122b08f2038-1251669398.us-east-1.elb.amazonaws.com:8443</code>
     and the dashboard will appear.
    </p></section><section class="sect3" id="ingress-controller" data-id-title="Ingress controller"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">13.1.3.3 </span><span class="title-name">Ingress controller</span> <a title="Permalink" class="permalink" href="#ingress-controller">#</a></h4></div></div></div><p>
     If you have a cluster with an Nginx Ingress Controller and a Certificate
     Manager, then you can create an Ingress like the one below. This example
     achieves four things:
    </p><div class="orderedlist"><ol class="orderedlist compact" type="1"><li class="listitem"><p>
       Exposes the dashboard on the Internet (using an reverse proxy).
      </p></li><li class="listitem"><p>
       Issues an valid TLS Certificate for the specified domain name.
      </p></li><li class="listitem"><p>
       Tells the reverse proxy that the dashboard itself uses HTTPS.
      </p></li><li class="listitem"><p>
       Tells the reverse proxy that the dashboard itself does not have a valid
       certificate (it is self-signed).
      </p></li></ol></div><div class="verbatim-wrap"><pre class="screen">  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: rook-ceph-mgr-dashboard
    namespace: rook-ceph
    annotations:
      kubernetes.io/ingress.class: "nginx"
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/server-snippet: |
        proxy_ssl_verify off;
  spec:
    tls:
     - hosts:
       - rook-ceph.example.com
       secretName: rook-ceph.example.com
    rules:
    - host: rook-ceph.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: rook-ceph-mgr-dashboard
            servicePort: https-dashboard</pre></div><p>
     Customise the Ingress resource to match your cluster. Replace the example
     domain name <code class="literal">rook-ceph.example.com</code> with a domain name
     that will resolve to your Ingress Controller (creating the DNS entry if
     required).
    </p><p>
     Now create the Ingress:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl create -f dashboard-ingress-https.yaml</pre></div><p>
     You will see the new Ingress <code class="literal">rook-ceph-mgr-dashboard</code>
     created:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get ingress
NAME                      HOSTS                      ADDRESS   PORTS     AGE
rook-ceph-mgr-dashboard   rook-ceph.example.com      80, 443   5m</pre></div><p>
     And the new Secret for the TLS certificate:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get secret rook-ceph.example.com
NAME                       TYPE                DATA      AGE
rook-ceph.example.com      kubernetes.io/tls   2         4m</pre></div><p>
     You can now browse to <code class="literal">https://rook-ceph.example.com/</code> to
     log into the dashboard.
    </p></section></section></section></section></div><div class="part" id="rook-ses-troubleshooting" data-id-title="Troubleshooting Ceph on SUSE CaaS Platform"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">Part III </span><span class="title-name">Troubleshooting Ceph on SUSE CaaS Platform </span><a title="Permalink" class="permalink" href="#rook-ses-troubleshooting">#</a></h1></div></div></div><div class="toc"><ul><li><span class="chapter"><a href="#atroubleshooting-caasp-debugging-rook"><span class="title-number">14 </span><span class="title-name">Troubleshooting</span></a></span></li><dd class="toc-abstract"><p>
   There are a number of basic actions a user might need to take during
   debugging. These actions are defined here for reference when they are
   mentioned in more documentation below.
  </p></dd><li><span class="chapter"><a href="#admin-caasp-ceph-common-issues"><span class="title-number">15 </span><span class="title-name">Common issues</span></a></span></li><dd class="toc-abstract"><p>Many of these problem cases are hard to summarize down to a short phrase that adequately describes the problem. Each problem will start with a bulleted list of symptoms. Keep in mind that all symptoms may not apply, depending on the configuration of Rook. If the majority of the symptoms are seen, th…</p></dd></ul></div><section class="chapter" id="atroubleshooting-caasp-debugging-rook" data-id-title="Troubleshooting"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">14 </span><span class="title-name">Troubleshooting</span> <a title="Permalink" class="permalink" href="#atroubleshooting-caasp-debugging-rook">#</a></h1></div></div></div><section class="sect1" id="debugging-caasp-rook-methods" data-id-title="Debugging Rook"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">14.1 </span><span class="title-name">Debugging Rook</span> <a title="Permalink" class="permalink" href="#debugging-caasp-rook-methods">#</a></h2></div></div></div><p>
   There are a number of basic actions a user might need to take during
   debugging. These actions are defined here for reference when they are
   mentioned in more documentation below.
  </p><div id="id-1.7.5.2.3.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
    This document is not devoted to an in-depth explanation of what Kubernetes is,
    what its features are, how it is used, how to navigate it, or how to debug
    applications that run on it. This document will use Kubernetes terms, and users
    are expected to know how to look up Kubernetes information they do not already
    have. This document will give an outline of how to use Kubernetes tools to get
    any information needed in the Rook-Ceph context and, when relevant, will
    briefly explain how Rook uses Kubernetes features.
   </p></div><section class="sect2" id="set-ops-log-debug" data-id-title="Setting the operator log level to debug"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.1 </span><span class="title-name">Setting the operator log level to debug</span> <a title="Permalink" class="permalink" href="#set-ops-log-debug">#</a></h3></div></div></div><p>
    In general, the first place to look when encountering a failure is to get
    logs for the <code class="systemitem">rook-ceph-operator</code>
    pod. To get the most informative logs possible, set the operator log level
    to <code class="literal">DEBUG</code>.
   </p><p>
    To do this, modify Helm's <code class="filename">values.yaml</code> or modify the
    <code class="filename">operator.yaml</code> manifest. Regardless of the method
    chosen, the log level can always be set by editing the deployment directly
    with <code class="command">kubectl</code>. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl --namespace rook-ceph set env deployment/rook-ceph-operator ROOK_LOG_LEVEL=DEBUG</pre></div><p>
    After editing the deployment, the operator pod will restart automatically
    and will start outputting logs with the new log level.
   </p><div id="id-1.7.5.2.3.4.6" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     If you are experiencing a particular failure, it may take some time for
     the Rook operator to reach the failure location again to report debug
     logs.
    </p></div></section><section class="sect2" id="use-toolbox-pod" data-id-title="Using the toolbox pod"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.2 </span><span class="title-name">Using the toolbox pod</span> <a title="Permalink" class="permalink" href="#use-toolbox-pod">#</a></h3></div></div></div><p>
    Use the Rook toolbox pod to interface directly with the Ceph cluster
    via the CLI. For example:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl --namespace rook-ceph exec -it deploy/rook-ceph-tools -- bash</pre></div><p>
    If the <code class="systemitem">rook-ceph-tools</code>
    deployment does not exist, it should be created using the
    <code class="filename">toolbox.yaml</code> manifest.
   </p><div id="id-1.7.5.2.3.5.5" class="admonition note normal"><img class="symbol" alt="Note" title="Note" src="static/images/icon-note.svg"/><h6>Note</h6><p>
     To set log levels for Ceph daemons, it is advised to use the Ceph CLI
     from the <code class="systemitem">toolbox</code> pod.
    </p></div></section><section class="sect2" id="ses-supportutils-plugin-rook" data-id-title="Using the SES supportutils plugin"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">14.1.3 </span><span class="title-name">Using the SES supportutils plugin</span> <a title="Permalink" class="permalink" href="#ses-supportutils-plugin-rook">#</a></h3></div></div></div><p>
    The <code class="literal">supportutils</code> plugin for SUSE Enterprise Storage works with
    Rook clusters. It is installed by the
    <code class="filename">supportutils-plugin-ses</code> package. The plugin collects
    container logs and more information about a Rook-Ceph cluster, making
    collection of logs easy. Once the logs are collected, you can browse the
    collected information and logs without needing to progressively collect
    more detailed information at each step.
   </p><p>
    The <code class="literal">supportutils</code>plugin does not alter the Rook log
    level to <code class="literal">DEBUG</code>, and it is advised to set this to
    <code class="literal">DEBUG</code> before running the plugin. The plugin also does
    not change any Ceph log levels; also consider changing those if the
    failure merits it before running the plugin.
   </p></section></section></section><section class="chapter" id="admin-caasp-ceph-common-issues" data-id-title="Common issues"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">15 </span><span class="title-name">Common issues</span> <a title="Permalink" class="permalink" href="#admin-caasp-ceph-common-issues">#</a></h1></div></div></div><section class="sect1" id="ceph-common-issues" data-id-title="Ceph common issues"><div class="titlepage"><div><div><h2 class="title"><span class="title-number">15.1 </span><span class="title-name">Ceph common issues</span> <a title="Permalink" class="permalink" href="#ceph-common-issues">#</a></h2></div></div></div><p>
   Many of these problem cases are hard to summarize down to a short phrase
   that adequately describes the problem. Each problem will start with a
   bulleted list of symptoms. Keep in mind that all symptoms may not apply,
   depending on the configuration of Rook. If the majority of the symptoms
   are seen, then there is a fair chance that you are experiencing that
   problem.
  </p><section class="sect2" id="troubleshooting-techniques" data-id-title="Troubleshooting techniques"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.1 </span><span class="title-name">Troubleshooting techniques</span> <a title="Permalink" class="permalink" href="#troubleshooting-techniques">#</a></h3></div></div></div><p>
    There are two main categories of information you will need to investigate
    issues in the cluster:
   </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
      Kubernetes status and logs.
     </p></li><li class="listitem"><p>
      Ceph cluster status.
     </p></li></ol></div><section class="sect3" id="ceph-tools" data-id-title="Running Ceph tools"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.1.1 </span><span class="title-name">Running Ceph tools</span> <a title="Permalink" class="permalink" href="#ceph-tools">#</a></h4></div></div></div><p>
     After you verify the basic health of the running pods, next you will want
     to run Ceph tools for status of the storage components. There are two
     ways to run the Ceph tools, either in the Rook toolbox or inside other
     Rook pods that are already running.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Logs on a specific node to find why a PVC is failing to mount: Rook
       agent errors around the attach and detach:
      </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl logs -n rook-ceph <em class="replaceable">rook-ceph-agent-pod</em></pre></div></li><li class="listitem"><p>
       See the <a class="xref" href="#log-collection" title="11.1.3. Collecting logs">Section 11.1.3, “Collecting logs”</a> for a script that will help you
       gather the logs.
      </p></li><li class="listitem"><p>
       Other artifacts:
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         The monitors that are expected to be in quorum:
        </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n &lt;cluster-namespace&gt; get configmap rook-ceph-mon-endpoints -o yaml | grep data</pre></div></li></ul></div></li></ul></div><section class="sect4" id="tools-in-the-rook-toolbox" data-id-title="Using tools in the Rook toolbox"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.1.1.1 </span><span class="title-name">Using tools in the Rook toolbox</span> <a title="Permalink" class="permalink" href="#tools-in-the-rook-toolbox">#</a></h5></div></div></div><p>
      The <code class="literal">rook-ceph-tools pod</code> provides a simple environment
      to run Ceph tools. Once the pod is up and running, connect to the pod
      to execute Ceph commands to evaluate that current state of the cluster.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash</pre></div></section><section class="sect4" id="ceph-commands" data-id-title="Ceph commands"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.1.1.2 </span><span class="title-name">Ceph commands</span> <a title="Permalink" class="permalink" href="#ceph-commands">#</a></h5></div></div></div><p>
      Here are some common commands to troubleshoot a Ceph cluster:
     </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
        <code class="command">ceph status</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph osd status</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph osd df</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph osd utilization</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph osd pool stats</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph osd tree</code>
       </p></li><li class="listitem"><p>
        <code class="command">ceph pg stat</code>
       </p></li></ul></div><p>
      The first two status commands provide the overall cluster health. The
      normal state for cluster operations is <code class="literal">HEALTH_OK</code>, but
      will still function when the state is in a <code class="literal">HEALTH_WARN</code>
      state. If you are in a <code class="literal">WARN</code> state, then the cluster is
      in a condition that it may enter the <code class="literal">HEALTH_ERROR</code>
      state at which point <span class="emphasis"><em>all</em></span> disk I/O operations are
      halted. If a <code class="literal">HEALTH_WARN</code> state is observed, then one
      should take action to prevent the cluster from halting when it enters the
      <code class="literal">HEALTH_ERROR</code> state.
     </p></section></section></section><section class="sect2" id="cluster-failing-to-service-requests" data-id-title="Cluster failing to service requests"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.2 </span><span class="title-name">Cluster failing to service requests</span> <a title="Permalink" class="permalink" href="#cluster-failing-to-service-requests">#</a></h3></div></div></div><section class="sect3" id="symptoms-1" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.2.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-1">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Execution of the Ceph command hangs.
      </p></li><li class="listitem"><p>
       <code class="literal">PersistentVolumes</code> are not being created.
      </p></li><li class="listitem"><p>
       Large amount of slow requests are blocking.
      </p></li><li class="listitem"><p>
       Large amount of stuck requests are blocking.
      </p></li><li class="listitem"><p>
       One or more MONs are restarting periodically.
      </p></li></ul></div></section><section class="sect3" id="investigation" data-id-title="Investigating the current state of Ceph"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.2.2 </span><span class="title-name">Investigating the current state of Ceph</span> <a title="Permalink" class="permalink" href="#investigation">#</a></h4></div></div></div><p>
     Create a <code class="literal">rook-ceph-tools pod</code> to investigate the current
     state of Ceph. Here is an example of what one might see. In this case,
     the <code class="command">ceph status</code> command would just hang, so a CTRL-C
     needed to be sent.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
<code class="prompt user">cephuser@adm &gt; </code>ceph status
^CCluster connection interrupted or timed out</pre></div><p>
     Another indication is when one or more of the MON pods restart frequently.
     Note the <span class="quote">“<span class="quote">mon107</span>”</span> that has only been up for 16 minutes in the
     following output.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get all -o wide --show-all
  NAME                                 READY     STATUS    RESTARTS   AGE       IP               NODE
  po/rook-ceph-mgr0-2487684371-gzlbq   1/1       Running   0          17h       192.168.224.46   k8-host-0402
  po/rook-ceph-mon107-p74rj            1/1       Running   0          16m       192.168.224.28   k8-host-0402
  rook-ceph-mon1-56fgm                 1/1       Running   0          2d        192.168.91.135   k8-host-0404
  rook-ceph-mon2-rlxcd                 1/1       Running   0          2d        192.168.123.33   k8-host-0403
  rook-ceph-osd-bg2vj                  1/1       Running   0          2d        192.168.91.177   k8-host-0404
  rook-ceph-osd-mwxdm                  1/1       Running   0          2d        192.168.123.31   k8-host-0403</pre></div></section><section class="sect3" id="solution" data-id-title="Identifying the solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.2.3 </span><span class="title-name">Identifying the solution</span> <a title="Permalink" class="permalink" href="#solution">#</a></h4></div></div></div><p>
     What is happening here is that the MON pods are restarting and one or more
     of the Ceph daemons are not getting configured with the proper cluster
     information. This is commonly the result of not specifying a value for
     <code class="literal">dataDirHostPath</code> in your Cluster CRD.
    </p><p>
     The <code class="literal">dataDirHostPath</code> setting specifies a path on the
     local host for the Ceph daemons to store configuration and data. Setting
     this to a path like <code class="filename">/var/lib/rook</code>, reapplying your
     cluster CRD and restarting all the Ceph daemons (MON, MGR, OSD, RGW)
     should solve this problem. After the Object Gateway daemons have been restarted, it
     is advisable to restart the <code class="literal">rook-tools</code> pod.
    </p></section></section><section class="sect2" id="monitors-are-the-only-pods-running" data-id-title="Monitors are the only PODs running"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.3 </span><span class="title-name">Monitors are the only PODs running</span> <a title="Permalink" class="permalink" href="#monitors-are-the-only-pods-running">#</a></h3></div></div></div><section class="sect3" id="symptoms-2" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.3.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-2">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Rook operator is running.
      </p></li><li class="listitem"><p>
       Either a single mon starts or the MONs skip letters, specifically named
       <code class="literal">a</code>, <code class="literal">d</code>, and <code class="literal">f</code>.
      </p></li><li class="listitem"><p>
       No MGR, OSD, or other daemons are created.
      </p></li></ul></div></section><section class="sect3" id="investigation-1" data-id-title="Investigating MON health"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.3.2 </span><span class="title-name">Investigating MON health</span> <a title="Permalink" class="permalink" href="#investigation-1">#</a></h4></div></div></div><p>
     When the operator is starting a cluster, the operator will start one MON
     at a time and check that they are healthy before continuing to bring up
     all three MONs. If the first MON is not detected healthy, the operator
     will continue to check until it is healthy. If the first MON fails to
     start, a second and then a third MON may attempt to start. However, they
     will never form a quorum, and orchestration will be blocked from
     proceeding.
    </p><p>
     The likely causes for the MON health not being detected:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The operator pod does not have network connectivity to the MON pod.
      </p></li><li class="listitem"><p>
       The MON pod is failing to start.
      </p></li><li class="listitem"><p>
       One or more MON pods are in running state, but are not able to form a
       quorum.
      </p></li></ul></div><section class="sect4" id="operator-fails-to-connect-to-the-mon" data-id-title="Failing to connect to the MON"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.3.2.1 </span><span class="title-name">Failing to connect to the MON</span> <a title="Permalink" class="permalink" href="#operator-fails-to-connect-to-the-mon">#</a></h5></div></div></div><p>
      First look at the logs of the operator to confirm if it is able to
      connect to the MONs.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph logs -l app=rook-ceph-operator</pre></div><p>
      Likely you will see an error similar to the following that the operator
      is timing out when connecting to the MON. The last command is
      <code class="command">ceph mon_status</code>, followed by a timeout message five
      minutes later.
     </p><div class="verbatim-wrap"><pre class="screen">  2018-01-21 21:47:32.375833 I | exec: Running command: ceph mon_status --cluster=rook --conf=/var/lib/rook/rook-ceph/rook.config --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json --out-file /tmp/442263890
  2018-01-21 21:52:35.370533 I | exec: 2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
  2018-01-21 21:52:35.071462 7f96a3b82700  0 monclient(hunting): authenticate timed out after 300
  2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
  2018-01-21 21:52:35.071524 7f96a3b82700  0 librados: client.admin authentication error (110) Connection timed out
  [errno 110] error connecting to the cluster</pre></div><p>
      The error would appear to be an authentication error, but it is
      misleading. The real issue is a timeout.
     </p></section><section class="sect4" id="solution-1" data-id-title="Identifying the solution"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.3.2.2 </span><span class="title-name">Identifying the solution</span> <a title="Permalink" class="permalink" href="#solution-1">#</a></h5></div></div></div><p>
      If you see the timeout in the operator log, verify if the MON pod is
      running (see the next section). If the MON pod is running, check the
      network connectivity between the operator pod and the MON pod. A common
      issue is that the CNI is not configured correctly.
     </p></section><section class="sect4" id="failing-mon-pod" data-id-title="Failing MON pod"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.3.2.3 </span><span class="title-name">Failing MON pod</span> <a title="Permalink" class="permalink" href="#failing-mon-pod">#</a></h5></div></div></div><p>
      We need to verify if the MON pod started successfully.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-mon
NAME                                READY     STATUS               RESTARTS   AGE
rook-ceph-mon-a-69fb9c78cd-58szd    1/1       CrashLoopBackOff     2          47s</pre></div><p>
      If the MON pod is failing as in this example, you will need to look at
      the <code class="command">mon pod status</code> or logs to determine the cause. If
      the pod is in a crash loop backoff state, you should see the reason by
      describing the pod.
     </p><p>
      The pod shows a termination status that the keyring does not match the
      existing keyring.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph describe pod -l mon=rook-ceph-mon0
[...]
Last State:    Terminated
Reason:    Error
Message:    The keyring does not match the existing keyring in /var/lib/rook/rook-ceph-mon0/data/keyring.
You may need to delete the contents of dataDirHostPath on the host from a previous deployment.
[...]</pre></div><p>
      See the solution in the next section regarding cleaning up the
      <code class="literal">dataDirHostPath</code> on the nodes.
     </p><p>
      If you see the three mons running with the names <code class="literal">a</code>,
      <code class="literal">d</code>, and <code class="literal">f</code>, they likely did not form
      quorum even though they are running.
     </p><div class="verbatim-wrap"><pre class="screen">NAME                               READY   STATUS    RESTARTS   AGE
rook-ceph-mon-a-7d9fd97d9b-cdq7g   1/1     Running   0          10m
rook-ceph-mon-d-77df8454bd-r5jwr   1/1     Running   0          9m2s
rook-ceph-mon-f-58b4f8d9c7-89lgs   1/1     Running   0          7m38s</pre></div></section><section class="sect4" id="solution-2" data-id-title="Identifying the solution"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.3.2.4 </span><span class="title-name">Identifying the solution</span> <a title="Permalink" class="permalink" href="#solution-2">#</a></h5></div></div></div><p>
      This is a common problem reinitializing the Rook cluster when the local
      directory used for persistence has <span class="strong"><strong>not</strong></span>
      been purged. This directory is the <code class="literal">dataDirHostPath</code>
      setting in the cluster CRD, and is typically set to
      <code class="filename">/var/lib/rook</code>. To fix the issue, you will need to
      delete all components of Rook and then delete the contents of
      <code class="filename">/var/lib/rook</code> (or the directory specified by
      <code class="literal">dataDirHostPath</code>) on each of the hosts in the cluster.
      Then, when the cluster CRD is applied to start a new cluster, the
      rook-operator should start all the pods as expected.
     </p><div id="id-1.7.5.3.3.5.3.8.3" class="admonition important normal"><img class="symbol" alt="Important" title="Important" src="static/images/icon-important.svg"/><h6>Important</h6><p>
       Deleting the <code class="literal">dataDirHostPath</code> folder is destructive to
       the storage. Only delete the folder if you are trying to permanently
       purge the Rook cluster.
      </p></div></section></section></section><section class="sect2" id="pvcs-stay-in-pending-state" data-id-title="PVCs stay in pending state"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.4 </span><span class="title-name">PVCs stay in pending state</span> <a title="Permalink" class="permalink" href="#pvcs-stay-in-pending-state">#</a></h3></div></div></div><section class="sect3" id="symptoms-3" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.4.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-3">#</a></h4></div></div></div><p>
     When you create a PVC based on a Rook storage class, it stays pending
     indefinitely.
    </p><p>
     For the Wordpress example, you might see two PVCs in the pending state.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl get pvc
NAME             STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE
mysql-pv-claim   Pending                                      rook-ceph-block   8s
wp-pv-claim      Pending                                      rook-ceph-block   16s</pre></div></section><section class="sect3" id="investigation-2" data-id-title="Investigating common causes"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.4.2 </span><span class="title-name">Investigating common causes</span> <a title="Permalink" class="permalink" href="#investigation-2">#</a></h4></div></div></div><p>
     There are two common causes for the PVCs staying in the pending state:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
       There are no OSDs in the cluster.
      </p></li><li class="listitem"><p>
       The CSI provisioner pod is not running or is not responding to the
       request to provision the storage.
      </p></li></ol></div><section class="sect4" id="confirm-if-there-are-osds" data-id-title="Confirming if there are OSDs"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.4.2.1 </span><span class="title-name">Confirming if there are OSDs</span> <a title="Permalink" class="permalink" href="#confirm-if-there-are-osds">#</a></h5></div></div></div><p>
      To confirm if you have OSDs in your cluster, connect to the Rook
      Toolbox and run the <code class="command">ceph status</code> command. You should
      see that you have at least one OSD <code class="literal">up</code> and
      <code class="literal">in</code>. The minimum number of OSDs required depends on the
      <code class="literal">replicated.size</code> setting in the pool created for the
      storage class. In a <span class="quote">“<span class="quote">test</span>”</span> cluster, only one OSD is required
      (see <code class="filename">storageclass-test.yaml</code>). In the production
      storage class example (<code class="filename">storageclass.yaml</code>), three
      OSDs would be required.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
  cluster:
  id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c
  health: HEALTH_OK

  services:
  mon: 3 daemons, quorum a,b,c (age 11m)
  mgr: a(active, since 10m)
  osd: 1 osds: 1 up (since 46s), 1 in (since 109m)</pre></div></section><section class="sect4" id="osd-prepare-logs" data-id-title="Preparing OSD logs"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.4.2.2 </span><span class="title-name">Preparing OSD logs</span> <a title="Permalink" class="permalink" href="#osd-prepare-logs">#</a></h5></div></div></div><p>
      If you do not see the expected number of OSDs, investigate why they were
      not created. On each node where Rook looks for OSDs to configure, you
      will see an <span class="quote">“<span class="quote">osd prepare</span>”</span> pod.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
NAME                                 ...  READY   STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-minikube-9twvk   0/2     Completed   0          30m</pre></div><p>
      See the section on
      <a class="xref" href="#osd-pods-are-not-created-on-my-devices" title="15.1.6. OSD pods are not created on my devices">Section 15.1.6, “OSD pods are not created on my devices”</a> to investigate
      the logs.
     </p></section><section class="sect4" id="csi-driver" data-id-title="Checking CSI driver"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.4.2.3 </span><span class="title-name">Checking CSI driver</span> <a title="Permalink" class="permalink" href="#csi-driver">#</a></h5></div></div></div><p>
      The CSI driver may not be responding to the requests. Look in the logs of
      the CSI provisioner pod to see if there are any errors during the
      provisioning.
     </p><p>
      There are two provisioner pods:
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=csi-rbdplugin-provisioner</pre></div><p>
      Get the logs of each of the pods. One of them should be the leader and be
      responding to requests.
     </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph logs csi-cephfsplugin-provisioner-d77bb49c6-q9hwq csi-provisioner</pre></div></section><section class="sect4" id="operator-unresponsiveness" data-id-title="Restarting the operator"><div class="titlepage"><div><div><h5 class="title"><span class="title-number">15.1.4.2.4 </span><span class="title-name">Restarting the operator</span> <a title="Permalink" class="permalink" href="#operator-unresponsiveness">#</a></h5></div></div></div><p>
      Lastly, if you have OSDs <code class="literal">up</code> and <code class="literal">in</code>,
      the next step is to confirm the operator is responding to the requests.
      Look in the operator pod logs around the time when the PVC was created to
      confirm if the request is being raised. If the operator does not show
      requests to provision the block image, the operator may be stuck on some
      other operation. In this case, restart the operator pod to get things
      going again.
     </p></section></section><section class="sect3" id="solution-3" data-id-title="Identifying the solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.4.3 </span><span class="title-name">Identifying the solution</span> <a title="Permalink" class="permalink" href="#solution-3">#</a></h4></div></div></div><p>
     If the OSD prepare logs did not give you enough clues about why the OSDs
     were not being created, review your <code class="filename">cluster.yaml</code>
     configuration. The common mistakes include:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       If <code class="literal">useAllDevices: true</code>, Rook expects to find local
       devices attached to the nodes. If no devices are found, no OSDs will be
       created.
      </p></li><li class="listitem"><p>
       If <code class="literal">useAllDevices: false</code>, OSDs will only be created if
       <code class="literal">deviceFilter</code> is specified.
      </p></li><li class="listitem"><p>
       Only local devices attached to the nodes will be configurable by Rook.
       In other words, the devices must show up under
       <code class="filename">/dev</code>.
      </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
         The devices must not have any partitions or file systems on them.
         Rook will only configure raw devices. Partitions are not yet
         supported.
        </p></li></ul></div></li></ul></div></section></section><section class="sect2" id="osd-pods-are-failing-to-start" data-id-title="OSD pods are failing to start"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.5 </span><span class="title-name">OSD pods are failing to start</span> <a title="Permalink" class="permalink" href="#osd-pods-are-failing-to-start">#</a></h3></div></div></div><section class="sect3" id="symptoms-4" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.5.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-4">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       OSD pods are failing to start.
      </p></li><li class="listitem"><p>
       You have started a cluster after tearing down another cluster.
      </p></li></ul></div></section><section class="sect3" id="investigation-3" data-id-title="Investigating configuration errors"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.5.2 </span><span class="title-name">Investigating configuration errors</span> <a title="Permalink" class="permalink" href="#investigation-3">#</a></h4></div></div></div><p>
     When an OSD starts, the device or directory will be configured for
     consumption. If there is an error with the configuration, the pod will
     crash and you will see the <code class="literal">CrashLoopBackoff</code> status for
     the pod. Look in the OSD pod logs for an indication of the failure.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph logs rook-ceph-osd-fl8fs</pre></div><p>
     One common case for failure is that you have re-deployed a test cluster
     and some state may remain from a previous deployment. If your cluster is
     larger than a few nodes, you may get lucky enough that the monitors were
     able to start and form a quorum. However, now the OSDs pods may fail to
     start due to the old state. Looking at the OSD pod logs, you will see an
     error about the file already existing.
    </p><div class="verbatim-wrap"><pre class="screen">kubectl -n rook-ceph logs rook-ceph-osd-fl8fs
[...]
2017-10-31 20:13:11.187106 I | mkfs-osd0: 2017-10-31 20:13:11.186992 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _read_fsid unparsable uuid
2017-10-31 20:13:11.187208 I | mkfs-osd0: 2017-10-31 20:13:11.187026 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) _setup_block_symlink_or_file failed to create block symlink to /dev/disk/by-partuuid/651153ba-2dfc-4231-ba06-94759e5ba273: (17) File exists
2017-10-31 20:13:11.187233 I | mkfs-osd0: 2017-10-31 20:13:11.187038 7f0059d62e00 -1 bluestore(/var/lib/rook/osd0) mkfs failed, (17) File exists
2017-10-31 20:13:11.187254 I | mkfs-osd0: 2017-10-31 20:13:11.187042 7f0059d62e00 -1 OSD::mkfs: ObjectStore::mkfs failed with error (17) File exists
2017-10-31 20:13:11.187275 I | mkfs-osd0: 2017-10-31 20:13:11.187121 7f0059d62e00 -1  ** ERROR: error creating empty object store in /var/lib/rook/osd0: (17) File exists</pre></div></section><section class="sect3" id="solution-4" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.5.3 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-4">#</a></h4></div></div></div><p>
     If the error is from the file that already exists, this is a common
     problem reinitializing the Rook cluster when the local directory used
     for persistence has <span class="strong"><strong>not</strong></span> been purged.
     This directory is the <code class="literal">dataDirHostPath</code> setting in the
     cluster CRD and is typically set to <code class="filename">/var/lib/rook</code>. To
     fix the issue you will need to delete all components of Rook and then
     delete the contents of <code class="filename">/var/lib/rook</code> (or the
     directory specified by <code class="literal">dataDirHostPath</code>) on each of the
     hosts in the cluster. Then when the cluster CRD is applied to start a new
     cluster, the rook-operator should start all the pods as expected.
    </p></section></section><section class="sect2" id="osd-pods-are-not-created-on-my-devices" data-id-title="OSD pods are not created on my devices"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.6 </span><span class="title-name">OSD pods are not created on my devices</span> <a title="Permalink" class="permalink" href="#osd-pods-are-not-created-on-my-devices">#</a></h3></div></div></div><section class="sect3" id="symptoms-5" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.6.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-5">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       No OSD pods are started in the cluster.
      </p></li><li class="listitem"><p>
       Devices are not configured with OSDs even though specified in the
       cluster CRD.
      </p></li><li class="listitem"><p>
       One OSD pod is started on each node instead of multiple pods for each
       device.
      </p></li></ul></div></section><section class="sect3" id="investigation-4" data-id-title="Investigating"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.6.2 </span><span class="title-name">Investigating</span> <a title="Permalink" class="permalink" href="#investigation-4">#</a></h4></div></div></div><p>
     First, ensure that you have specified the devices correctly in the CRD.
     The cluster CRD has several ways to specify the devices that are to be
     consumed by the Rook storage:
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">useAllDevices: true</code>: Rook will consume all devices
       it determines to be available.
      </p></li><li class="listitem"><p>
       <code class="literal">deviceFilter</code>: Consume all devices that match this
       regular expression.
      </p></li><li class="listitem"><p>
       <code class="literal">devices</code>: Explicit list of device names on each node
       to consume.
      </p></li></ul></div><p>
     Second, if Rook determines that a device is not available (has existing
     partitions or a formatted file system), Rook will skip consuming the
     devices. If Rook is not starting OSDs on the devices you expect, Rook
     may have skipped it for this reason. To see if a device was skipped, view
     the OSD preparation log on the node where the device was skipped. Note
     that it is completely normal and expected for OSD prepare pod to be in the
     <code class="literal">completed</code> state. After the job is complete, Rook
     leaves the pod around in case the logs need to be investigated.
    </p><p>
     Get the prepare pods in the cluster:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare
NAME                                   READY     STATUS      RESTARTS   AGE
rook-ceph-osd-prepare-node1-fvmrp      0/1       Completed   0          18m
rook-ceph-osd-prepare-node2-w9xv9      0/1       Completed   0          22m
rook-ceph-osd-prepare-node3-7rgnv      0/1       Completed   0          22m</pre></div><p>
     View the logs for the node of interest in the "provision"
     container:
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph logs rook-ceph-osd-prepare-node1-fvmrp provision</pre></div><p>
     Here are some key lines to look for in the log. A device will be skipped
     if Rook sees it has partitions or a file system:
    </p><div class="verbatim-wrap"><pre class="screen">2019-05-30 19:02:57.353171 W | cephosd: skipping device sda that is in use
2019-05-30 19:02:57.452168 W | skipping device "sdb5": ["Used by ceph-disk"]</pre></div><p>
     Other messages about a disk being unusable by Ceph include:
    </p><div class="verbatim-wrap"><pre class="screen">Insufficient space (&lt;5GB) on vgs
Insufficient space (&lt;5GB)
LVM detected
Has BlueStore device label
locked
read-only</pre></div><p>
     A device is going to be configured:
    </p><div class="verbatim-wrap"><pre class="screen">2019-05-30 19:02:57.535598 I | cephosd: device sdc to be configured by ceph-volume</pre></div><p>
     For each device configured, you will see a report printed to the log:
    </p><div class="verbatim-wrap"><pre class="screen">2019-05-30 19:02:59.844642 I |   Type            Path                                                    LV Size         % of device
2019-05-30 19:02:59.844651 I | ----------------------------------------------------------------------------------------------------
2019-05-30 19:02:59.844677 I |   [data]          /dev/sdc                                                7.00 GB         100%</pre></div></section><section class="sect3" id="solution-5" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.6.3 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-5">#</a></h4></div></div></div><p>
     Either update the CR with the correct settings, or clean the partitions or
     file system from your devices.
    </p><p>
     After the settings are updated or the devices are cleaned, trigger the
     operator to analyze the devices again by restarting the operator. Each
     time the operator starts, it will ensure all the desired devices are
     configured. The operator does automatically deploy OSDs in most scenarios,
     but an operator restart will cover any scenarios that the operator does
     not detect automatically.
    </p><p>
     Restart the operator to ensure devices are configured. A new pod will
     automatically be started when the current operator pod is deleted.
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">kubectl@adm &gt; </code>kubectl -n rook-ceph delete pod -l app=rook-ceph-operator</pre></div></section></section><section class="sect2" id="rook-agent-modprobe-exec-format-error" data-id-title="Rook agent modprobe exec format error"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.7 </span><span class="title-name">Rook agent modprobe exec format error</span> <a title="Permalink" class="permalink" href="#rook-agent-modprobe-exec-format-error">#</a></h3></div></div></div><section class="sect3" id="symptoms-7" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.7.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-7">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">PersistentVolumes</code> from Ceph fail or timeout to
       mount.
      </p></li><li class="listitem"><p>
       Rook Agent logs contain <code class="literal">modinfo: ERROR: could not get modinfo
       from 'rbd': Exec format error</code> lines.
      </p></li></ul></div></section><section class="sect3" id="solution-7" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.7.2 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-7">#</a></h4></div></div></div><p>
     If it is feasible to upgrade your kernel, you should upgrade to 4.x, even
     better is 4.7 or above, due to a feature for CephFS added to the kernel.
    </p><p>
     If you are unable to upgrade the kernel, you need to go to each host that
     will consume storage and run:
    </p><div class="verbatim-wrap"><pre class="screen">modprobe rbd</pre></div><p>
     This command inserts the <code class="literal">rbd</code> module into the kernel.
    </p><p>
     To persist this fix, you need to add the <code class="literal">rbd</code> kernel
     module to either <code class="filename">/etc/modprobe.d/</code> or
     <code class="filename">/etc/modules-load.d/</code>. For both paths create a file
     called <code class="filename">rbd.conf</code> with the following content:
    </p><div class="verbatim-wrap"><pre class="screen">rbd</pre></div><p>
     Now when a host is restarted, the module should be loaded automatically.
    </p></section></section><section class="sect2" id="using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47" data-id-title="Using multiple shared file systems (CephFS) is attempted on a kernel version older than 4.7"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.8 </span><span class="title-name">Using multiple shared file systems (CephFS) is attempted on a kernel version older than 4.7</span> <a title="Permalink" class="permalink" href="#using-multiple-shared-filesystem-cephfs-is-attempted-on-a-kernel-version-older-than-47">#</a></h3></div></div></div><section class="sect3" id="symptoms-9" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.8.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-9">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       More than one shared file system (CephFS) has been created in the
       cluster.
      </p></li><li class="listitem"><p>
       A pod attempts to mount any other shared file system besides the
       <span class="strong"><strong>first</strong></span> one that was created.
      </p></li><li class="listitem"><p>
       The pod incorrectly gets the first file system mounted instead of the
       intended file system.
      </p></li></ul></div></section><section class="sect3" id="solution-9" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.8.2 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-9">#</a></h4></div></div></div><p>
     The only solution to this problem is to upgrade your kernel to 4.7 or
     higher. This is due to a <code class="command">mount</code> flag added in kernel
     version 4.7, which allows choosing the file system by name.
    </p></section></section><section class="sect2" id="activate-log-to-file-for-a-particular-ceph-daemon" data-id-title="Activating log to file for a particular Ceph daemon"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.9 </span><span class="title-name">Activating log to file for a particular Ceph daemon</span> <a title="Permalink" class="permalink" href="#activate-log-to-file-for-a-particular-ceph-daemon">#</a></h3></div></div></div><p>
    They are cases where looking at Kubernetes logs is not enough for various
    reasons, but just to name a few:
   </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
      Not everyone is familiar for Kubernetes logging and expects to find logs in
      traditional directories.
     </p></li><li class="listitem"><p>
      Logs get eaten (buffer limit from the log engine) and thus not
      requestable from Kubernetes.
     </p></li></ul></div><p>
    So for each daemon, <code class="literal">dataDirHostPath</code> is used to store
    logs, if logging is activated. Rook will bind-mount
    <code class="literal">dataDirHostPath</code> for every pod. As of Ceph Nautilus
    14.2.1, it is possible to enable logging for a particular daemon on the
    fly. Let us say you want to enable logging for <code class="literal">mon.a</code>,
    but only for this daemon. Using the toolbox or from inside the operator
    run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config daemon mon.a log_to_file true</pre></div><p>
    This will activate logging on the file system, you will be able to find
    logs in <code class="filename">dataDirHostPath/$NAMESPACE/log</code>, so typically
    this would mean <code class="filename">/var/lib/rook/rook-ceph/log</code>. You do
    not need to restart the pod, the effect will be immediate.
   </p><p>
    To disable the logging on file, simply set <code class="literal">log_to_file</code>
    to <code class="literal">false</code>.
   </p><p>
    For Ceph Luminous and Mimic releases,
    <code class="literal">mon_cluster_log_file</code> and
    <code class="literal">cluster_log_file</code> can be set to
    <code class="filename">/var/log/ceph/XXXX</code> in the config override ConfigMap to
    enable logging.
   </p></section><section class="sect2" id="a-worker-node-using-rbd-devices-hangs-up" data-id-title="A worker node using RBD devices hangs up"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.10 </span><span class="title-name">A worker node using RBD devices hangs up</span> <a title="Permalink" class="permalink" href="#a-worker-node-using-rbd-devices-hangs-up">#</a></h3></div></div></div><section class="sect3" id="symptoms-10" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.10.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-10">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       There is no progress on I/O from/to one of RBD devices
       (<code class="filename">/dev/rbd*</code> or <code class="filename">/dev/nbd*</code>).
      </p></li><li class="listitem"><p>
       After that, the whole worker node hangs up.
      </p></li></ul></div></section><section class="sect3" id="investigation-6" data-id-title="Investigating"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.10.2 </span><span class="title-name">Investigating</span> <a title="Permalink" class="permalink" href="#investigation-6">#</a></h4></div></div></div><p>
     This happens when the following conditions are satisfied.
    </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       The problematic RBD device and the corresponding OSDs are co-located.
      </p></li><li class="listitem"><p>
       There is an XFS file system on top of this device.
      </p></li></ul></div><p>
     In addition, when this problem happens, you can see the following messages
     in <code class="literal">dmesg</code>.
    </p><div class="verbatim-wrap"><pre class="screen">dmesg
...
[51717.039319] INFO: task kworker/2:1:5938 blocked for more than 120 seconds.
[51717.039361]       Not tainted 4.15.0-72-generic #81-Ubuntu
[51717.039388] "echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs" disables this message.
...</pre></div><p>
     This is the so-called <code class="literal">hung_task</code> problem and means that
     there is a deadlock in the kernel.
    </p></section><section class="sect3" id="solution-10" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.10.3 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-10">#</a></h4></div></div></div><p>
     You can bypass this problem by using ext4 or any other file systems rather
     than XFS. The file system type can be specified with
     <code class="literal">csi.storage.k8s.io/fstype</code> in StorageClass resource.
    </p></section></section><section class="sect2" id="too-few-pgs-per-osd-warning-is-shown" data-id-title="Too few PGs per OSD warning is shown"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.11 </span><span class="title-name">Too few PGs per OSD warning is shown</span> <a title="Permalink" class="permalink" href="#too-few-pgs-per-osd-warning-is-shown">#</a></h3></div></div></div><section class="sect3" id="symptoms-11" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.11.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-11">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       <code class="literal">ceph status</code> shows <span class="quote">“<span class="quote">too few PGs per OSD</span>”</span>
       warning as follows.
      </p></li></ul></div><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph status
cluster:
id:     fd06d7c3-5c5c-45ca-bdea-1cf26b783065
health: HEALTH_WARN
too few PGs per OSD (16 &lt; min 30)</pre></div></section><section class="sect3" id="solution-11" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.11.2 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-11">#</a></h4></div></div></div><p>
     See <span class="intraxref">Book “Troubleshooting Guide”, Chapter 5 “Troubleshooting placement groups (PGs)”</span> for more information.
    </p></section></section><section class="sect2" id="lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc" data-id-title="LVM metadata can be corrupted with OSD on LV-backed PVC"><div class="titlepage"><div><div><h3 class="title"><span class="title-number">15.1.12 </span><span class="title-name">LVM metadata can be corrupted with OSD on LV-backed PVC</span> <a title="Permalink" class="permalink" href="#lvm-metadata-can-be-corrupted-with-osd-on-lv-backed-pvc">#</a></h3></div></div></div><section class="sect3" id="symptoms-12" data-id-title="Identifying symptoms"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.12.1 </span><span class="title-name">Identifying symptoms</span> <a title="Permalink" class="permalink" href="#symptoms-12">#</a></h4></div></div></div><p>
     There is a critical flaw in OSD on LV-backed PVC. LVM metadata can be
     corrupted if both the host and OSD container modify it simultaneously. For
     example, the administrator might modify it on the host, while the OSD
     initialization process in a container could modify it too. In addition, if
     <code class="literal">lvmetad</code> is running, the possibility of occurrence gets
     higher. In this case, the change of LVM metadata in OSD container is not
     reflected to LVM metadata cache in host for a while.
    </p><p>
     If you still decide to configure an OSD on LVM, keep the following in mind
     to reduce the probability of this issue.
    </p></section><section class="sect3" id="solution-12" data-id-title="Solution"><div class="titlepage"><div><div><h4 class="title"><span class="title-number">15.1.12.2 </span><span class="title-name">Solution</span> <a title="Permalink" class="permalink" href="#solution-12">#</a></h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
       Disable <code class="literal">lvmetad</code>.
      </p></li><li class="listitem"><p>
       Avoid configuration of LVs from the host. In addition, do not touch the
       VGs and physical volumes that back these LVs.
      </p></li><li class="listitem"><p>
       Avoid incrementing the <code class="literal">count</code> field of
       <code class="literal">storageClassDeviceSets</code> and create a new LV that backs
       a OSD simultaneously.
      </p></li></ul></div><p>
     You can know whether the above-mentioned tag exists tag by running
    </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt root"># </code> lvs -o lv_name,lv_tags</pre></div><p>
     If the <code class="literal">lv_tag</code> field is empty in an LV corresponding to
     the OSD lv_tags, this OSD encountered the problem. In this case, retire
     this OSD or replace with other new OSD before restarting.
    </p></section></section></section></section></div><section class="appendix" id="id-1.7.6" data-id-title="Ceph maintenance updates based on upstream Octopus point releases"><div class="titlepage"><div><div><h1 class="title"><span class="title-number">A </span><span class="title-name">Ceph maintenance updates based on upstream 'Octopus' point releases</span> <a title="Permalink" class="permalink" href="#id-1.7.6">#</a></h1></div></div></div><p>
  Several key packages in SUSE Enterprise Storage 7 are based on the
  Octopus release series of Ceph. When the Ceph project
  (<a class="link" href="https://github.com/ceph/ceph" target="_blank">https://github.com/ceph/ceph</a>) publishes new point
  releases in the Octopus series, SUSE Enterprise Storage 7 is updated
  to ensure that the product benefits from the latest upstream bug fixes and
  feature backports.
 </p><p>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been—or is planned to be—included in the
  product.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.5"><span class="name">Octopus 15.2.11 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.5">#</a></h2></div><p>
  This release includes a security fix that ensures the
  <code class="option">global_id</code> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </p><p>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
<code class="prompt user">cephuser@adm &gt; </code>ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h</pre></div><p>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph config set mon auth_allow_insecure_global_id_reclaim false</pre></div><p>
  For more details, refer ro
  <a class="link" href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/" target="_blank">https://docs.ceph.com/en/latest/security/CVE-2021-20288/</a>.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.12"><span class="name">Octopus 15.2.10 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.12">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The containers include an updated <code class="literal">tcmalloc</code> that avoids
    crashes seen on 15.2.9.
   </p></li><li class="listitem"><p>
    RADOS: BlueStore handling of huge (&gt;4GB) writes from RocksDB to BlueFS
    has been fixed.
   </p></li><li class="listitem"><p>
    When upgrading from a previous cephadm release,
    <code class="command">systemctl</code> may hang when trying to start or restart the
    monitoring containers. This is caused by a change in the <code class="systemitem">systemd</code> unit to
    use <code class="option">type=forking</code>.) After the upgrade, please run:
   </p><div class="verbatim-wrap"><pre class="screen"><code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy nfs
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy iscsi
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy node-exporter
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy prometheus
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy grafana
<code class="prompt user">cephuser@adm &gt; </code>ceph orch redeploy alertmanager</pre></div></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.15"><span class="name">Octopus 15.2.9 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.15">#</a></h2></div><p>
  This backport release includes the following fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    MGR: progress module can now be turned on/off, using the commands:
    <code class="command">ceph progress on</code> and <code class="command">ceph progress
    off</code>.
   </p></li><li class="listitem"><p>
    OSD: PG removal has been optimized in this release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.18"><span class="name">Octopus 15.2.8 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.18">#</a></h2></div><p>
  This release fixes a security flaw in CephFS and includes a number of bug
  fixes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    OpenStack Manila use of <code class="filename">ceph_volume_client.py</code> library
    allowed tenant access to any Ceph credential’s secret.
   </p></li><li class="listitem"><p>
    <code class="command">ceph-volume</code>: The <code class="command">lvm batch</code> subcommand
    received a major rewrite. This closed a number of bugs and improves
    usability in terms of size specification and calculation, as well as
    idempotency behaviour and disk replacement process. Please refer to
    <a class="link" href="https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/" target="_blank">https://docs.ceph.com/en/latest/ceph-volume/lvm/batch/</a>
    for more detailed information.
   </p></li><li class="listitem"><p>
    MON: The cluster log now logs health detail every
    <code class="option">mon_health_to_clog_interval</code>, which has been changed from
    1hr to 10min. Logging of health detail will be skipped if there is no
    change in health summary since last known.
   </p></li><li class="listitem"><p>
    The <code class="command">ceph df</code> command now lists the number of PGs in each
    pool.
   </p></li><li class="listitem"><p>
    The <code class="option">bluefs_preextend_wal_files</code> option has been removed.
   </p></li><li class="listitem"><p>
    It is now possible to specify the initial monitor to contact for Ceph
    tools and daemons using the <code class="option">mon_host_override</code> config
    option or <code class="option">--mon-host-override</code> command line switch. This
    generally should only be used for debugging and only affects initial
    communication with Ceph's monitor cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.21"><span class="name">Octopus 15.2.7 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.21">#</a></h2></div><p>
  This release fixes a serious bug in RGW that has been shown to cause data
  loss when a read of a large RGW object (for example, one with at least one
  tail segment) takes longer than one half the time specified in the
  configuration option <code class="option">rgw_gc_obj_min_wait</code>. The bug causes the
  tail segments of that read object to be added to the RGW garbage collection
  queue, which will in turn cause them to be deleted after a period of time.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.23"><span class="name">Octopus 15.2.6 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.23">#</a></h2></div><p>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus.
 </p><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.25"><span class="name">Octopus 15.2.5 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.25">#</a></h2></div><p>
  The Octopus point release 15.2.5 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CephFS: Automatic static sub-tree partitioning policies may now be
    configured using the new distributed and random ephemeral pinning extended
    attributes on directories. See the following documentation for more
    information:
    <a class="link" href="https://docs.ceph.com/docs/master/cephfs/multimds/" target="_blank">https://docs.ceph.com/docs/master/cephfs/multimds/</a>
   </p></li><li class="listitem"><p>
    Monitors now have a configuration option
    <code class="option">mon_osd_warn_num_repaired</code>, which is set to 10 by default.
    If any OSD has repaired more than this many I/O errors in stored data a
    <code class="literal">OSD_TOO_MANY_REPAIRS</code> health warning is generated.
   </p></li><li class="listitem"><p>
    Now, when <code class="literal">no scrub</code> and/or <code class="literal">no
    deep-scrub</code> flags are set globally or per pool, scheduled scrubs
    of the type disabled will be aborted. All user initiated scrubs are NOT
    interrupted.
   </p></li><li class="listitem"><p>
    Fixed an issue with osdmaps not being trimmed in a healthy cluster.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.28"><span class="name">Octopus 15.2.4 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.28">#</a></h2></div><p>
  The Octopus point release 15.2.4 brought the following fixes and other
  changes:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </p></li><li class="listitem"><p>
    Object Gateway: The <code class="command">radosgw-admin</code> sub-commands dealing with
    orphans—<code class="command">radosgw-admin orphans find</code>,
    <code class="command">radosgw-admin orphans finish</code>, and <code class="command">radosgw-admin
    orphans list-jobs</code>—have been deprecated. They had not been
    actively maintained, and since they store intermediate results on the
    cluster, they could potentially fill a nearly-full cluster. They have been
    replaced by a tool, <code class="command">rgw-orphan-list</code>, which is currently
    considered experimental.
   </p></li><li class="listitem"><p>
    RBD: The name of the RBD pool object that is used to store RBD trash purge
    schedule is changed from <code class="literal">rbd_trash_trash_purge_schedule</code>
    to <code class="literal">rbd_trash_purge_schedule</code>. Users that have already
    started using RBD trash purge schedule functionality and have per pool or
    name space schedules configured should copy the
    <code class="literal">rbd_trash_trash_purge_schedule</code> object to
    <code class="literal">rbd_trash_purge_schedule</code> before the upgrade and remove
    <code class="literal">rbd_trash_purge_schedule</code> using the following commands in
    every RBD pool and name space where a trash purge schedule was previously
    configured:
   </p><div class="verbatim-wrap"><pre class="screen">rados -p <em class="replaceable">pool-name</em> [-N namespace] cp rbd_trash_trash_purge_schedule rbd_trash_purge_schedule
rados -p <em class="replaceable">pool-name</em> [-N namespace] rm rbd_trash_trash_purge_schedule</pre></div><p>
    Alternatively, use any other convenient way to restore the schedule after
    the upgrade.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.31"><span class="name">Octopus 15.2.3 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.31">#</a></h2></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    The Octopus point release 15.2.3 was a hot-fix release to address an
    issue where WAL corruption was seen when
    <code class="option">bluefs_preextend_wal_files</code> and
    <code class="option">bluefs_buffered_io</code> were enabled at the same time. The fix
    in 15.2.3 is only a temporary measure (changing the default value of
    <code class="option">bluefs_preextend_wal_files</code> to <code class="literal">false</code>).
    The permanent fix will be to remove the
    <code class="option">bluefs_preextend_wal_files</code> option completely: this fix
    will most likely arrive in the 15.2.6 point release.
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.33"><span class="name">Octopus 15.2.2 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.33">#</a></h2></div><p>
  The Octopus point release 15.2.2 patched one security vulnerability:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-10736: Fixed an authorization bypass in MONs and MGRs
   </p></li></ul></div><div class="sect1 bridgehead"><h2 class="title" id="id-1.7.6.36"><span class="name">Octopus 15.2.1 Point Release</span><a title="Permalink" class="permalink" href="#id-1.7.6.36">#</a></h2></div><p>
  The Octopus point release 15.2.1 fixed an issue where upgrading quickly
  from Luminous (SES5.5) to Nautilus (SES6) to Octopus (SES7) caused OSDs to
  crash. In addition, it patched two security vulnerabilities that were present
  in the initial Octopus (15.2.0) release:
 </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </p></li><li class="listitem"><p>
    CVE-2020-1760: Fixed XSS because of RGW GetObject header-splitting
   </p></li></ul></div></section><section class="glossary"><div class="titlepage"><div><div><h1 class="title"><span class="title-number"> </span><span class="title-name">Glossary</span> <a title="Permalink" class="permalink" href="#id-1.7.7">#</a></h1></div></div></div><div class="line"/><div class="glossdiv" id="id-1.7.7.3" data-id-title="General"><h3 class="title">General</h3><dl><dt id="id-1.7.7.3.2"><span><span class="glossterm">Admin node</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.2">#</a></span></dt><dd class="glossdef"><p>
     The host from which you run the Ceph-related commands to administer
     cluster hosts.
    </p></dd><dt id="id-1.7.7.3.3"><span><span class="glossterm">Alertmanager</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.3">#</a></span></dt><dd class="glossdef"><p>
     A single binary which handles alerts sent by the Prometheus server and
     notifies the end user.
    </p></dd><dt id="id-1.7.7.3.4"><span><span class="glossterm">archive sync module</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.4">#</a></span></dt><dd class="glossdef"><p>
     Module that enables creating an Object Gateway zone for keeping the history of S3
     object versions.
    </p></dd><dt id="id-1.7.7.3.5"><span><span class="glossterm">Bucket</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.5">#</a></span></dt><dd class="glossdef"><p>
     A point that aggregates other nodes into a hierarchy of physical
     locations.
    </p></dd><dt id="id-1.7.7.3.9"><span><span class="glossterm">Ceph Client</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.9">#</a></span></dt><dd class="glossdef"><p>
     The collection of Ceph components which can access a Ceph Storage
     Cluster. These include the Object Gateway, the Ceph Block Device, the CephFS,
     and their corresponding libraries, kernel modules, and FUSE clients.
    </p></dd><dt id="id-1.7.7.3.14"><span><span class="glossterm">Ceph Dashboard</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.14">#</a></span></dt><dd class="glossdef"><p>
     A built-in Web-based Ceph management and monitoring application to
     administer various aspects and objects of the cluster. The dashboard is
     implemented as a Ceph Manager module.
    </p></dd><dt id="id-1.7.7.3.19"><span><span class="glossterm">Ceph Manager</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.19">#</a></span></dt><dd class="glossdef"><p>
     Ceph Manager or MGR is the Ceph manager software, which collects all the state
     from the whole cluster in one place.
    </p></dd><dt id="id-1.7.7.3.18"><span><span class="glossterm">Ceph Monitor</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.18">#</a></span></dt><dd class="glossdef"><p>
     Ceph Monitor or MON is the Ceph monitor software.
    </p></dd><dt id="id-1.7.7.3.24"><span><span class="glossterm">Ceph Object Storage</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.24">#</a></span></dt><dd class="glossdef"><p>
     The object storage "product", service or capabilities, which consists of a
     Ceph Storage Cluster and a Ceph Object Gateway.
    </p></dd><dt id="id-1.7.7.3.22"><span><span class="glossterm">Ceph OSD Daemon</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.22">#</a></span></dt><dd class="glossdef"><p>
     The <code class="command">ceph-osd</code> daemon is the component of Ceph that is
     responsible for storing objects on a local file system and providing
     access to them over the network.
    </p></dd><dt id="id-1.7.7.3.11"><span><span class="glossterm">Ceph Storage Cluster</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.11">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data. Such a set
     consists of Ceph monitors and OSDs.
    </p></dd><dt id="id-1.7.7.3.10"><span><span class="glossterm"><code class="systemitem">ceph-salt</code></span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.10">#</a></span></dt><dd class="glossdef"><p>
     Provides tooling for deploying Ceph clusters managed by cephadm using
     Salt.
    </p></dd><dt id="id-1.7.7.3.6"><span><span class="glossterm">cephadm</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.6">#</a></span></dt><dd class="glossdef"><p>
     cephadm deploys and manages a Ceph cluster by connecting to hosts from
     the manager daemon via SSH to add, remove, or update Ceph daemon
     containers.
    </p></dd><dt id="id-1.7.7.3.7"><span><span class="glossterm">CephFS</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.7">#</a></span></dt><dd class="glossdef"><p>
     The Ceph file system.
    </p></dd><dt id="id-1.7.7.3.8"><span><span class="glossterm">CephX</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.8">#</a></span></dt><dd class="glossdef"><p>
     The Ceph authentication protocol. Cephx operates like Kerberos, but it
     has no single point of failure.
    </p></dd><dt id="id-1.7.7.3.13"><span><span class="glossterm">CRUSH rule</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.13">#</a></span></dt><dd class="glossdef"><p>
     The CRUSH data placement rule that applies to a particular pool or pools.
    </p></dd><dt id="id-1.7.7.3.12"><span><span class="glossterm">CRUSH, CRUSH Map</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.12">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Controlled Replication Under Scalable Hashing</em></span>: An
     algorithm that determines how to store and retrieve data by computing data
     storage locations. CRUSH requires a map of the cluster to pseudo-randomly
     store and retrieve data in OSDs with a uniform distribution of data across
     the cluster.
    </p></dd><dt id="id-1.7.7.3.15"><span><span class="glossterm">DriveGroups</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.15">#</a></span></dt><dd class="glossdef"><p>
     DriveGroups are a declaration of one or more OSD layouts that can be mapped
     to physical drives. An OSD layout defines how Ceph physically allocates
     OSD storage on the media matching the specified criteria.
    </p></dd><dt id="id-1.7.7.3.16"><span><span class="glossterm">Grafana</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.16">#</a></span></dt><dd class="glossdef"><p>
     Database analytics and monitoring solution.
    </p></dd><dt id="id-1.7.7.3.17"><span><span class="glossterm">Metadata Server</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.17">#</a></span></dt><dd class="glossdef"><p>
     Metadata Server or MDS is the Ceph metadata software.
    </p></dd><dt id="id-1.7.7.3.36"><span><span class="glossterm">Multi-zone</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.36">#</a></span></dt><dd class="glossdef"/><dt id="id-1.7.7.3.20"><span><span class="glossterm">Node</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.20">#</a></span></dt><dd class="glossdef"><p>
     Any single machine or server in a Ceph cluster.
    </p></dd><dt id="id-1.7.7.3.25"><span><span class="glossterm">Object Gateway</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.25">#</a></span></dt><dd class="glossdef"><p>
     The S3/Swift gateway component for Ceph Object Store. Also known as the
     RADOS Gateway (RGW).
    </p></dd><dt id="id-1.7.7.3.21"><span><span class="glossterm">OSD</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.21">#</a></span></dt><dd class="glossdef"><p>
     <span class="emphasis"><em>Object Storage Device</em></span>: A physical or logical storage
     unit.
    </p></dd><dt id="id-1.7.7.3.23"><span><span class="glossterm">OSD node</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.23">#</a></span></dt><dd class="glossdef"><p>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     Ceph monitors by checking other Ceph OSD daemons.
    </p></dd><dt id="id-1.7.7.3.26"><span><span class="glossterm">PG</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.26">#</a></span></dt><dd class="glossdef"><p>
     Placement Group: a sub-division of a <span class="emphasis"><em>pool</em></span>, used for
     performance tuning.
    </p></dd><dt id="id-1.7.7.3.27"><span><span class="glossterm">Point Release</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.27">#</a></span></dt><dd class="glossdef"><p>
     Any ad-hoc release that includes only bug or security fixes.
    </p></dd><dt id="id-1.7.7.3.28"><span><span class="glossterm">Pool</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.28">#</a></span></dt><dd class="glossdef"><p>
     Logical partitions for storing objects such as disk images.
    </p></dd><dt id="id-1.7.7.3.29"><span><span class="glossterm">Prometheus</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.29">#</a></span></dt><dd class="glossdef"><p>
     Systems monitoring and alerting toolkit.
    </p></dd><dt id="id-1.7.7.3.31"><span><span class="glossterm">RADOS Block Device (RBD)</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.31">#</a></span></dt><dd class="glossdef"><p>
     The block storage component of Ceph. Also known as the Ceph block
     device.
    </p></dd><dt id="id-1.7.7.3.30"><span><span class="glossterm">Reliable Autonomic Distributed Object Store (RADOS)</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.30">#</a></span></dt><dd class="glossdef"><p>
     The core set of storage software which stores the user's data (MON+OSD).
    </p></dd><dt id="id-1.7.7.3.33"><span><span class="glossterm">Routing tree</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.33">#</a></span></dt><dd class="glossdef"><p>
     A term given to any diagram that shows the various routes a receiver can
     run.
    </p></dd><dt id="id-1.7.7.3.32"><span><span class="glossterm">Rule Set</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.32">#</a></span></dt><dd class="glossdef"><p>
     Rules to determine data placement for a pool.
    </p></dd><dt id="id-1.7.7.3.34"><span><span class="glossterm">Samba</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.34">#</a></span></dt><dd class="glossdef"><p>
     Windows integration software.
    </p></dd><dt id="id-1.7.7.3.35"><span><span class="glossterm">Samba Gateway</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.35">#</a></span></dt><dd class="glossdef"><p>
     The Samba Gateway joins the Active Directory in the Windows domain to authenticate
     and authorize users.
    </p></dd><dt id="id-1.7.7.3.37"><span><span class="glossterm">zonegroup</span> <a title="Permalink" class="permalink" href="#id-1.7.7.3.37">#</a></span></dt><dd class="glossdef"/></dl></div></section></section></article><aside id="_side-toc-page" class="side-toc"><div class="side-title">Give feedback</div><ul class="feedback" id="_give-feedback"><li><a id="_feedback-editurl" href="https://github.com/SUSE/doc-ses/edit/main/xml/book_storage_rook.xml" rel="nofollow" target="_blank">Edit source document</a></li></ul><div class="side-title">Share this page</div><ul class="share"><li><a id="_share-fb" href="#" title="Facebook"> </a></li><li><a id="_share-in" href="#" title="LinkedIn"> </a></li><li><a id="_share-tw" href="#" title="Twitter"> </a></li><li><a id="_share-mail" href="#" title="E-Mail"> </a></li><li><a id="_print-button" href="#" title="Print this page"> </a></li></ul> </aside></main><footer id="_footer"><div class="growth-inhibitor"><div class="copy"><span class="copy__rights">© SUSE
                 2022</span></div></div></footer></body></html>